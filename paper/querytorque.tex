% QueryTorque: Engine-Aware SQL Rewriting with Swarm-of-Reasoners
% Paper - February 2026
% TODO: Switch to PVLDB formatting before submission:
%   https://www.vldb.org/pvldb/volumes/19/formatting
%   \documentclass[vldb]{vldb} or equivalent PVLDB class

\documentclass[sigconf]{acmart}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{subcaption}

\newcommand{\sysname}{QueryTorque}
\newcommand{\TODO}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\STUB}[1]{\textcolor{blue}{[STUB: #1]}}

% Suppress default ACM conference metadata from headers/footers
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\title{\sysname{}: Engine-Aware SQL Rewriting via Competitive Multi-Worker LLM Generation}

\author{TODO: Authors}
\affiliation{%
  \institution{TODO: Affiliation}
  \country{TODO: Country}
}

% ============================================================================
\begin{abstract}
% ============================================================================

% --- Problem (2 sentences) ---
SQL query rewriting---transforming a query into a semantically equivalent but
faster form---is critical for analytical workload performance, yet existing
approaches face fundamental trade-offs: rule-based systems are constrained by
fixed transformation vocabularies, while training-based methods require
expensive reinforcement learning pipelines tied to a single database engine.
No prior system has demonstrated that a single LLM-based architecture
generalizes across database engines without retraining.

% --- Approach (3 sentences) ---
We present \sysname{}, a training-free SQL optimization system built on three
ideas: (1)~\emph{engine gap profiling}, which characterizes each database
optimizer's blind spots as structured intelligence that directs LLM search
toward high-value rewrites; (2)~a \emph{swarm-of-reasoners} architecture
where four specialized workers, each targeting different optimizer gaps with
different verified gold examples, compete to produce the best rewrite; and
(3)~a logical-block \emph{Query DAG} representation that exposes the
cross-node structure at which SQL rewrites actually operate.
An analyst LLM performs dynamic per-query gap matching---mapping EXPLAIN
plan evidence to active optimizer blind spots---then dispatches workers
with structurally diverse strategies.

% --- Results (3 sentences) ---
With a single architecture and zero retraining, \sysname{} achieves a
${\sim}37$\% win rate on both DuckDB (TPC-DS SF10, up to 6.28$\times$) and
PostgreSQL (DSB SF10, up to \STUB{X}$\times$ non-timeout), the first
cross-engine generalization result for LLM-based query rewriting.
80\% of winning queries are \emph{uniquely} discovered by a single
worker---no other worker finds a valid improvement---directly validating
the swarm design over single-shot generation.
On DSB PostgreSQL, \sysname{} achieves a \STUB{XX}\% average latency
reduction without any model training, comparable to
E$^3$-Rewrite's 56.4\% which requires RL fine-tuning.

\end{abstract}

\maketitle

% ============================================================================
\section{Introduction}
\label{sec:intro}
% ============================================================================

Efficient query execution is central to modern analytical database systems.
SQL query rewriting---transforming a query into a semantically equivalent but
faster form---has been studied extensively through rule-based
systems~\cite{zhou2021learned, calcite2018}, heuristic
optimizers~\cite{graefe1993volcano}, and more recently through large language
model (LLM) approaches~\cite{rbot2024, e3rewrite2026, llmr2_2024}.

Despite significant progress, existing methods face three fundamental
limitations:

\paragraph{Limited expressiveness.} Rule-based systems (Apache Calcite,
PostgreSQL's optimizer, LearnedRewrite~\cite{zhou2021learned}) operate over a
fixed vocabulary of predefined transformations. They cannot express rewrites
that span multiple query blocks, such as isolating dimension tables into
filtered CTEs, consolidating repeated table scans into single-pass CASE
aggregations, or decomposing self-joins---strategies that yield 2--6$\times$
speedups on real workloads.

\paragraph{Expensive training pipelines.} E$^3$-Rewrite~\cite{e3rewrite2026}
addresses expressiveness by training LLMs via reinforcement learning (GRPO)
with execution-aware rewards. However, this requires (1)~a two-stage
curriculum over thousands of training queries, (2)~repeated query execution for
reward computation, and (3)~retraining for each new database engine or schema.
The resulting models are also frozen at training time and cannot incorporate
new optimization patterns discovered at deployment.

\paragraph{Single-engine, single-strategy design.} Both
R-Bot~\cite{rbot2024} and E$^3$-Rewrite target PostgreSQL exclusively. Their
architectures assume a fixed execution environment and do not account for the
significant behavioral differences between engines (e.g., DuckDB's columnar
execution vs.\ PostgreSQL's row-oriented model, CTE materialization semantics,
OR-to-UNION effectiveness). No prior system has demonstrated that a single
architecture generalizes across engines without retraining.

We present \sysname{}, a training-free SQL optimization system that addresses
all three limitations through four design decisions:

\begin{enumerate}
\item \textbf{Structural query representation with decomposition.}
    We parse SQL into a \emph{Query DAG} whose nodes are the logical blocks
    that rewrites operate on---CTEs, subqueries, and the main query---each
    annotated with a contract (output columns, grain, predicates), cost
    attribution, and downstream column usage. This gives the LLM a
    structural map at the same granularity as the rewrite, enabling
    cross-node transformations that physical-plan representations cannot
    express. For complex queries, we further decompose each base query
    into typed sub-problems (multi-CTE, aggregation, join-path) that
    expose different optimization surfaces to the swarm.

\item \textbf{Swarm-of-reasoners architecture.} Rather than relying on a
    single LLM call, \sysname{} dispatches four specialized workers in
    parallel, each seeded with different verified gold examples targeting
    distinct optimization strategies. Workers compete, and only the best
    validated candidate is selected. This produces dramatically higher
    coverage: 80\% of wins are uniquely discovered by a single worker,
    meaning any single-worker system would miss 70--83\% of opportunities.

\item \textbf{Engine-aware gap profiling.} Rather than injecting prohibitive
    constraints (``do not apply transform~$X$''), we characterize each
    database engine's optimizer blind spots as structured \emph{gap profiles}
    that direct the LLM toward high-value rewrites. An analyst LLM performs
    dynamic \emph{gap matching} per query using EXPLAIN plan evidence,
    then dispatches workers targeting only the active gaps. Three root
    causes account for 70\% of DuckDB wins
    (Section~\ref{sec:gap_profiling}).

\item \textbf{Bidirectional learning with iterative promotion.}
    The prompt includes both gold examples (verified speedups) and
    regression warnings (verified slowdowns with anti-pattern explanations),
    teaching the model what to do \emph{and} what to avoid. Winning
    rewrites from round~$N$ are promoted as baselines for round~$N{+}1$,
    enabling compositional optimization where later rounds build on earlier
    gains.
\end{enumerate}

% System overview figure placed after Phase 5 description (Fig.~\ref{fig:overview})

% ============================================================================
\section{Related Work}
\label{sec:related}
% ============================================================================

\paragraph{Heuristic and rule-based rewriting.}
Traditional query optimizers apply rewrite rules in fixed or heuristically
explored orders. PostgreSQL~\cite{postgresql} uses a sequential rule
pipeline, while Volcano~\cite{graefe1993volcano} and
Cascades~\cite{graefe1995cascades} explore rule combinations via
branch-and-bound search. Apache Calcite~\cite{calcite2018} provides an
extensible framework with $\sim$70 built-in rules.
LearnedRewrite~\cite{zhou2021learned} applies Monte Carlo Tree Search (MCTS)
guided by learned cost models to search over Calcite's rule space, but
remains bounded by Calcite's rule vocabulary and requires schema-specific
cost model training.

\paragraph{LLM-augmented rule selection.}
LLM-R$^2$~\cite{llmr2_2024} prompts GPT-3.5 with demonstrations to select
Calcite rules. R-Bot~\cite{rbot2024} extends this with multi-source evidence
preparation (rule specifications from code/documents, Q\&As from Stack
Overflow), hybrid structure-semantics retrieval, and step-by-step LLM-guided
rule arrangement with reflection. While R-Bot achieves strong results and is
deployed at Huawei, it fundamentally operates within Calcite's rule
vocabulary---it \emph{selects} rules but cannot \emph{generate} novel
rewrites. For example, it cannot express CTE isolation, scan consolidation,
or dimension prefetching, which are among our most effective transforms.

\paragraph{LLM-based direct rewriting.}
E$^3$-Rewrite~\cite{e3rewrite2026} fine-tunes Qwen/LLaMA models via GRPO
with a three-component reward (executability, equivalence, efficiency) and a
two-stage curriculum. It achieves 25.6\% latency reduction on TPC-H and
99.6\% equivalence rate. E$^3$-Rewrite injects linearized EXPLAIN plans as
``execution hints,'' exposing physical operators (Seq Scan, Hash Join) and
their costs. While useful for identifying bottleneck operators, this
physical-level representation does not expose the logical block structure
(CTE boundaries, subquery scopes, cross-node data contracts) that guides
structural rewrites. Furthermore, the approach requires (1)~significant
compute for RL training, (2)~access to query execution for reward
computation during training, and (3)~retraining for new engines.
QUITE~\cite{quite2025} is the closest prior work to \sysname{}: it is
training-free, uses multiple LLM agents (a rewriter, a corrector, and a
hint injector), and targets free-form SQL rewriting without a rule engine.
However, QUITE differs in several key respects: (1)~it uses query hints
(e.g., \texttt{SET} commands, optimizer directives) as a primary mechanism,
which are engine-specific and do not change the SQL structure;
(2)~it lacks a systematic learning loop from deployment feedback;
(3)~it does not use a structured query representation (DAG or otherwise)
to guide the rewriter; and (4)~it reports results on PostgreSQL only,
with no cross-engine evaluation.

\paragraph{Positioning of \sysname{}.}
Table~\ref{tab:comparison} positions \sysname{} against prior systems.
\sysname{} is the first system that combines training-free operation,
unrestricted rewrite expressiveness (no rule engine), multi-engine support,
and bidirectional learning from both successes and failures.

\begin{table}[t]
\centering
\caption{Comparison of SQL rewriting systems.}
\label{tab:comparison}
\small
\begin{tabular}{lccccc}
\toprule
& \textbf{LR} & \textbf{R-Bot} & \textbf{E$^3$} & \textbf{QUITE} & \textbf{Ours} \\
\midrule
Training required     & Yes  & No   & Yes  & No   & No   \\
Rule engine required  & Yes  & Yes  & No   & No   & No   \\
Multi-engine          & No   & No   & No   & No   & Yes  \\
Rewrite expressiveness & Calcite & Calcite & Free & Free+hints & Free \\
Query representation  & Cost & Rule$^\dagger$ & Phys. & None & DAG \\
Workers / candidates  & 1    & 1    & $N$  & 3$^\ddagger$ & $4{+}$ \\
Engine gap profiling  & No   & No   & No   & No   & Yes  \\
Regression learning   & No   & No   & No   & No   & Yes  \\
Stateful iteration    & No   & Yes* & No   & No   & Yes  \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize $^\dagger$R-Bot uses one-hot encoding of matched Calcite rule specifications.
*R-Bot's reflection loop reruns rule selection; it does not promote winning SQL as a new baseline.
$^\ddagger$QUITE uses 3 agents (rewriter, corrector, hint injector) but they are cooperative, not competitive.}
\end{table}


% ============================================================================
\section{System Overview}
\label{sec:overview}
% ============================================================================

\sysname{} processes a SQL query through a five-phase pipeline
(Figure~\ref{fig:overview}), orchestrated by a state machine that supports
multi-round optimization.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig1_system_overview.pdf}
\caption{End-to-end \sysname{} pipeline. The analyst LLM performs dynamic
gap matching (Section~\ref{sec:gap_selection}), then dispatches workers
with structurally diverse strategies. Only the best validated candidate
survives. Winners are promoted as baselines for subsequent rounds.}
\label{fig:overview}
\end{figure*}

\subsection{Phase 1: Structural Analysis via Query DAG}

A central design choice in \sysname{} is the level of abstraction at which we
represent query structure. E$^3$-Rewrite injects raw EXPLAIN plans---physical
operator trees showing \texttt{Seq Scan}, \texttt{Hash Join},
\texttt{Gather} nodes with cost estimates and row counts. R-Bot represents
queries as one-hot vectors of matched Calcite rule specifications. We argue
that neither representation aligns with the granularity at which SQL rewrites
actually operate.

SQL optimizations---decorrelating a subquery, isolating a dimension into a
CTE, pushing a filter across a CTE boundary, consolidating repeated
scans---operate at the level of \emph{logical query blocks}: CTEs,
subqueries, and the main SELECT. A developer does not rewrite a Hash Join
node; they restructure the CTE that feeds it. We therefore parse the input
SQL into a \emph{Query DAG} that mirrors this granularity.

\paragraph{DAG construction.}
Given a SQL query $q$, we parse it via \texttt{sqlglot}~\cite{sqlglot} and construct a DAG
$G = (V, E)$ where:
\begin{itemize}
\item Each node $v \in V$ represents a logical block: a CTE definition, a
    subquery, or the main query body.
\item Each edge $(u, v) \in E$ represents a data dependency: node $v$
    references the output of node $u$ (e.g., the main query references a CTE).
\end{itemize}

Each node is annotated with \emph{structural flags} detected via AST
traversal: \texttt{GROUP\_BY}, \texttt{WINDOW}, \texttt{UNION\_ALL},
\texttt{CORRELATED}, \texttt{IN\_SUBQUERY}. These flags serve as lightweight
pattern indicators that help the LLM identify optimization opportunities
without parsing the SQL itself.

\paragraph{Node contracts.}
For each node $v$, we compute a \emph{contract}---the interface between $v$
and its consumers:
\begin{itemize}
\item \textbf{Output columns}: The column names this node produces (extracted
    from the SELECT clause).
\item \textbf{Grain}: The GROUP BY keys, if any---defining the aggregation
    level of the node's output.
\item \textbf{Required predicates}: WHERE and HAVING conditions that must be
    preserved for semantic equivalence.
\end{itemize}

Contracts make explicit what each node promises to its consumers. This is
critical for safe cross-node rewrites: if a CTE's contract specifies
\texttt{output\_columns = [customer\_id, total\_return]}, a rewrite that
restructures the CTE must preserve these columns or risk breaking
downstream references.

\paragraph{Downstream usage and cost attribution.}
For each node, we compute which output columns are actually referenced by
consumers (enabling safe projection) and the percentage of total estimated
cost attributable to each node (from EXPLAIN plans or heuristic fallback).
Together, these direct the LLM to high-cost nodes with unused columns---the
highest-leverage rewrite targets. The per-node profile is rendered inline:

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single,
    caption={DAG topology in the LLM prompt (abbreviated).},
    label={lst:dag}]
## Query Structure (DAG)

Nodes:
  customer_total_return [CTE]
    tables: catalog_returns, date_dim,
            customer_address
    flags: GROUP_BY
    cost: 62%  (bottleneck)
    grain: [ctr_customer_sk, ctr_state]
    output: [ctr_customer_sk, ctr_state,
             ctr_total_return]

  main_query [MAIN]
    refs: customer_total_return
    tables: customer_total_return,
            customer_address, customer
    flags: CORRELATED
    cost: 38%

Edges:
  customer_total_return -> main_query
\end{lstlisting}

\paragraph{Engine gap profiles.}
The prompt includes engine-specific \emph{gap profiles} that characterize
optimizer blind spots---cases where the optimizer fails to find efficient
plans due to structural limitations in its search strategy. For example,
DuckDB's CROSS\_CTE\_PREDICATE\_BLINDNESS gap documents that the optimizer
cannot push predicates from the outer query into CTE definitions because
CTEs are planned as independent subplans. This knowledge---the
\emph{optimizer's behavioral model}---is absent from EXPLAIN plans and rule
specifications, yet is precisely what guides productive rewrites. We
describe the gap profiling system in detail in
Section~\ref{sec:gap_profiling}.

\paragraph{Comparison with prior representations.}
Table~\ref{tab:repr_comparison} compares the three approaches.

\begin{table}[t]
\centering
\caption{Query representation for LLM-based rewriting.}
\label{tab:repr_comparison}
\small
\begin{tabular}{p{2.4cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
\textbf{Property} & \textbf{E$^3$ (Phys.)} & \textbf{R-Bot (Rule)} & \textbf{Ours (DAG)} \\
\midrule
Granularity & Operator & Rule match & Logic block \\
Cross-node rewrites & No & No & Yes \\
Contract tracking & No & No & Yes \\
Engine-independent & No & Calcite & Yes \\
Cost attribution & Per-op & No & Per-block \\
Unused col.\ detection & No & No & Yes \\
Scales to $>$1K-line SQL & No & No & Yes \\
\bottomrule
\end{tabular}
\end{table}

The key advantage is \textbf{alignment}: the DAG represents the query at the
same level of abstraction at which rewrites happen, giving the LLM a
structural map rather than a physical execution trace.

\subsection{Phase 2: Example Retrieval}

We maintain a curated library of gold examples---pairs of (original SQL,
optimized SQL) with verified speedups---organized by database engine. Each
example is annotated with the transform pattern it demonstrates (e.g.,
\texttt{date\_cte\_isolate}, \texttt{decorrelate},
\texttt{single\_pass\_aggregation}).

For a given query $q$, we retrieve the top-$k$ most relevant examples using
tag-based overlap matching:

\begin{enumerate}
\item \textbf{Tag extraction.} We extract tags from $q$: table names, SQL
    keywords (GROUP BY, HAVING, EXISTS, etc.), and structural patterns
    (correlated subquery, self-join, window function).
\item \textbf{Category classification.} We assign $q$ to an archetype
    (filter-pushdown, aggregation-rewrite, set-operations, general) based
    on its structural features.
\item \textbf{Overlap ranking.} We rank examples by the number of shared
    tags with $q$, filtered by database engine.
\end{enumerate}

This approach is deliberately lightweight---no embedding models, no FAISS
indexes, no GPU inference. Despite its simplicity, it achieves effective
retrieval because the tag vocabulary is domain-specific to SQL optimization
patterns.

In addition to gold examples, we retrieve \emph{regression examples}:
queries where specific transforms caused verified slowdowns. These are
injected as anti-pattern warnings, teaching the model what to avoid for
structurally similar queries.

\subsection{Query-Type Decomposition}
\label{sec:decomposition}

A distinctive feature of \sysname{}'s evaluation pipeline is the decomposition
of complex benchmark queries into \emph{typed sub-problems}. Each base query
(e.g., DSB query~092) is formulated in up to three structurally distinct
variants:

\begin{itemize}
\item \textbf{Multi-CTE (\texttt{\_multi})}: The canonical complex form with
    hierarchical CTE pipelines, nested subqueries, and multi-level data
    dependencies. These queries exercise the full range of structural
    rewrites: decorrelation, CTE isolation, filter pushdown across node
    boundaries.
\item \textbf{Aggregation (\texttt{\_agg})}: Reformulations emphasizing GROUP
    BY operations with conditional OR branches and aggregate functions
    (AVG, SUM, COUNT). These expose optimization surfaces for scan
    consolidation, aggregate pushdown, and OR-to-UNION decomposition.
\item \textbf{Join-path (\texttt{\_spj\_spj})}: Select-Project-Join validation
    variants with identical WHERE clauses and join structures but simplified
    SELECT lists (typically MIN of key columns). These test join reordering
    and dimension-filter-first strategies without aggregation overhead.
\end{itemize}

This decomposition serves two purposes. First, different query formulations
expose different optimization opportunities: a query whose \texttt{\_multi}
variant resists optimization may yield significant speedups in its
\texttt{\_agg} formulation because the flatter structure enables scan
consolidation. In our PostgreSQL DSB evaluation, 10 of 19~wins came from
\texttt{\_multi} variants, 3 from \texttt{\_agg}, and 6 from
\texttt{\_spj\_spj}, confirming that no single formulation captures all
optimization potential.

Second, the swarm processes each variant independently, allowing different
workers to specialize on different structural patterns. The best result per
base query is selected across all variants and all workers, maximizing
coverage. This approach is orthogonal to the swarm architecture and could
benefit any LLM-based rewriting system.

\subsection{Phase 3: Analyst Briefing}

In the V2 architecture, prompt construction is split into two stages.
First, an \emph{analyst} LLM receives all available information---the
original SQL, EXPLAIN plan, DAG topology, engine gap profile, gold
examples, regression warnings, and correctness constraints---and produces
a structured briefing for 4~workers.

The analyst prompt assembles attention-ordered sections:
\begin{enumerate}
\item \textbf{Role framing}: Senior query optimization architect.
\item \textbf{Original SQL}: Pretty-printed with line numbers.
\item \textbf{EXPLAIN ANALYZE plan}: Physical execution tree with
    operator timings and cardinalities.
\item \textbf{DAG topology}: Logical nodes, edges, and per-node costs.
\item \textbf{Engine gap profile}: Optimizer strengths (do not fight)
    and gaps (hunt for these) with field intelligence
    (Section~\ref{sec:gap_profiling}).
\item \textbf{Correctness constraints}: 4 non-negotiable validation gates.
\item \textbf{Gold examples}: Tag-matched examples with full metadata.
\item \textbf{Transform catalog}: Available transforms organized by
    category (predicate movement, join restructuring, scan optimization,
    structural).
\item \textbf{Output format}: Per-worker briefing with strategy,
    target DAG, node contracts, and hazard flags.
\end{enumerate}

The analyst's structured reasoning follows a six-step checklist:
(1)~classify query archetype, (2)~analyze EXPLAIN plan bottlenecks,
(3)~match active engine gaps, (4)~check aggregation traps,
(5)~select transforms from matched gaps, (6)~design target DAGs
per worker. The analyst output contains a \emph{shared briefing}
(semantic contract, bottleneck diagnosis, active constraints, regression
warnings) plus four \emph{per-worker briefings}, each specifying a
different optimization strategy.

\subsection{Phase 4: Multi-Worker Generation}

Each worker receives only its targeted briefing from the analyst---not the
full profile or other workers' strategies. This ensures workers operate
independently on structurally diverse approaches:

\begin{itemize}
\item \textbf{Worker 1} (Restructure): decorrelate, pushdown, early\_filter
\item \textbf{Worker 2} (CTE Isolation): date/dimension CTE
    isolation, multi-date-range CTE
\item \textbf{Worker 3} (Prefetch): prefetch\_fact\_join,
    multi\_dimension\_prefetch, materialize\_cte
\item \textbf{Worker 4} (Exploration): compound strategies, constraint
    relaxation, or novel combinations not previously tested
\end{itemize}

Workers~1--3 follow the analyst's strategy assignments, which target
specific engine gaps identified during gap matching. Worker~4 is the
\emph{exploration worker}---it may attempt techniques the profile's
``what didn't work'' sections warn about if the current query's structure
differs from the documented failure context
(Section~\ref{sec:gap_selection}).

Each worker calls a frontier reasoning model (e.g.,
DeepSeek-Reasoner~\cite{deepseek_r1}, OpenAI o1~\cite{openai_o1}).
The use of reasoning models is deliberate: SQL optimization requires
multi-step logical deduction (identifying bottlenecks, planning
transformations, verifying equivalence), which benefits from
chain-of-thought reasoning~\cite{deepseek_r1}.

All $W$ candidates are collected and passed to Phase~5.

\subsection{Phase 5: Validation and Selection}

Each candidate undergoes a three-stage validation:

\begin{enumerate}
\item \textbf{Syntax gate}: Parse with \texttt{sqlglot}; reject unparseable
    candidates.
\item \textbf{Semantic equivalence}: Execute both original and rewritten
    queries; compare row counts and MD5 checksums of result sets.
\item \textbf{Performance measurement}: Execute the rewritten query using
    our trimmed-mean protocol (Section~\ref{sec:validation}).
\end{enumerate}

The candidate with the highest validated speedup is selected. If no
candidate achieves speedup $\geq 1.0$, the original query is preserved
(``do no harm'' principle).

\subsection{State Machine and Promotion}

\sysname{} supports multi-round optimization via a state machine. After
round $N$:
\begin{itemize}
\item Queries with speedup $\geq \tau$ (default $\tau = 1.05$) are
    \emph{promoted}: their optimized SQL becomes the baseline for round
    $N{+}1$.
\item Non-winners retain their original baseline.
\item A \emph{promotion context}---a natural-language summary of what worked
    and why---is generated and injected into the next round's prompt as
    history.
\end{itemize}

This enables compositional optimization: round~1 might decorrelate a
subquery, and round~2 might then isolate a dimension CTE that was
previously buried inside the correlated block.


% ============================================================================
\section{Engine-Aware Gap Profiling and Learning}
\label{sec:gap_profiling}
% ============================================================================

A central insight of \sysname{} is that effective SQL rewriting requires
understanding not just \emph{what} the LLM should rewrite, but \emph{why}
certain rewrites produce speedups on specific engines. The answer lies in
optimizer blind spots: cases where the query optimizer fails to find an
efficient execution plan due to structural limitations in its search
strategy. We formalize this as a two-layer steering architecture.

\subsection{Two-Layer Architecture}

Prior LLM rewriting systems inject constraints as prohibitions (``do not
apply transform $X$'') or as rule specifications (R-Bot's Calcite rules).
Both frame the problem negatively---what to \emph{avoid}---rather than
positively---what to \emph{exploit}. We argue this is backwards: the LLM
should be directed toward optimizer blind spots, not away from past
failures.

\sysname{} uses a two-layer architecture:

\paragraph{Layer~1: Engine gap profiles (offensive).}
For each target engine, we maintain a structured \emph{gap profile} that
characterizes the optimizer's behavioral strengths and weaknesses based on
empirical testing. Each profile contains:

\begin{itemize}
\item \textbf{Strengths}: Capabilities the optimizer already handles well
    (e.g., DuckDB's intra-scan predicate pushdown, PostgreSQL's BitmapOr for
    OR predicates). These tell the LLM what \emph{not to fight}---transforms
    targeting these areas add overhead without benefit.
\item \textbf{Gaps}: Optimizer blind spots with structured evidence. Each
    gap specifies:
    \begin{itemize}
    \item \emph{What}: The optimizer limitation (e.g., ``cannot push
        predicates from outer query into CTE definitions'')
    \item \emph{Why}: The mechanism (e.g., ``CTEs are planned as
        independent subplans; the optimizer does not trace data lineage
        through CTE boundaries'')
    \item \emph{Opportunity}: The rewrite strategy that exploits this gap
    \item \emph{What worked}: Specific queries and speedups (e.g.,
        ``Q88: 6.28$\times$ --- 8 time-bucket subqueries consolidated
        into 1 scan'')
    \item \emph{What didn't work}: Context-specific failures (e.g.,
        ``Q25: 0.50$\times$ --- query was 31ms baseline, CTE overhead
        exceeded filter savings'')
    \item \emph{Field notes}: Operational heuristics for recognizing when
        the gap is active (e.g., ``Count scans per base table in the
        EXPLAIN. If a fact table appears 3+ times, this gap is active.'')
    \end{itemize}
\end{itemize}

The framing is deliberately intelligence-oriented: ``here is what we have
gathered from 88 queries at SF1--SF10; use it to guide your analysis but
apply your own judgment---every query is different.'' This positions the LLM
as an analyst interpreting field intelligence, not a compliance checker
following rules.

\paragraph{Layer~2: Correctness constraints (defensive).}
Four non-negotiable validation gates that no rewrite may violate:
\begin{enumerate}
\item \textbf{Literal preservation}: All string, numeric, and date literals
    must be copied exactly from the original query.
\item \textbf{Semantic equivalence}: The rewritten query must return
    identical rows, columns, and ordering.
\item \textbf{Complete output}: All original SELECT columns must appear
    (no drops, renames, or reorders).
\item \textbf{CTE column completeness}: All downstream-referenced columns
    must appear in CTE SELECT clauses.
\end{enumerate}

These are the \emph{only} hard constraints in the system. All performance
guidance---what was previously encoded as 21 ``craft constraints'' (e.g.,
``limit OR$\to$UNION to $\leq$3 branches'', ``do not materialize
EXISTS subqueries'')---is now embedded as field notes within the relevant
engine gaps, where the context of \emph{when} the advice applies is
co-located with the advice itself.

\subsection{Gap Profiles as Publishable Artifacts}

Table~\ref{tab:gap_profiles} summarizes the gap profiles for both engines.

\begin{table}[t]
\centering
\caption{Engine gap profiles: optimizer blind spots and their yield.}
\label{tab:gap_profiles}
\small
\begin{tabular}{p{3.5cm}lrr}
\toprule
\textbf{Gap ID} & \textbf{Engine} & \textbf{Win \%} & \textbf{Best} \\
\midrule
Cross-CTE predicate blindness  & DuckDB & 35\% & 4.00$\times$ \\
Redundant scan elimination      & DuckDB & 20\% & 6.28$\times$ \\
Correlated subquery paralysis   & DuckDB & 15\% & 2.92$\times$ \\
Cross-column OR decomposition   & DuckDB & 15\% & 6.28$\times$ \\
LEFT JOIN filter rigidity       & DuckDB & 10\% & 2.97$\times$ \\
UNION CTE self-join decomp.     & DuckDB &  5\% & 1.36$\times$ \\
\midrule
Comma-join weakness             & PG     & ---  & 3.32$\times$ \\
Correlated subquery paralysis   & PG     & ---  & 4{,}428$\times$ \\
Non-equi join input blindness   & PG     & ---  & 2.68$\times$ \\
CTE materialization fence       & PG     & ---  & 1.95$\times$ \\
Cross-CTE predicate blindness   & PG     & ---  & 3.32$\times$ \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize DuckDB win \% = fraction of all DuckDB wins attributable to
this gap. PG percentages not yet computed (fewer validated queries).}
\end{table}

Three root causes---cross-CTE predicate blindness, redundant scan
elimination, and correlated subquery paralysis---account for 70\% of all
DuckDB wins. This concentration suggests that optimizer behavioral modeling
is a high-leverage activity: characterizing a small number of blind spots
enables the LLM to focus its search on the highest-value rewrites.
Figure~\ref{fig:gap_contribution} visualizes this concentration.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_gap_contribution.pdf}
\caption{Optimizer gap contribution to DuckDB wins. Three root
causes---cross-CTE predicate blindness, redundant scan elimination,
and correlated subquery paralysis---account for 70\% of all wins,
demonstrating that a small number of well-characterized blind spots
enable focused, high-yield optimization.}
\label{fig:gap_contribution}
\end{figure}

\subsection{Two-Stage Gap Selection}
\label{sec:gap_selection}

The full gap profile (6 gaps for DuckDB, 5 for PostgreSQL) is injected
into the \emph{analyst} prompt---a dedicated LLM call that precedes worker
dispatch. The analyst performs \emph{dynamic gap matching}: for each gap
in the profile, it checks whether the current query exhibits the gap by
examining the EXPLAIN plan for characteristic signals:

\begin{enumerate}
\item \textbf{Gap profiling} (static, per-engine): Constructed once from
    benchmark results. Encodes optimizer behavioral model as structured
    JSON with strengths, gaps, evidence, and field notes.
\item \textbf{Gap matching} (dynamic, per-query): The analyst LLM
    compares the EXPLAIN plan against each gap's activation signals.
    For example:
    \begin{itemize}
    \item Is a predicate applied \emph{after} a large scan rather than
        inside it? $\to$ CROSS\_CTE\_PREDICATE\_BLINDNESS is active.
    \item Does the same fact table appear in 3+ separate scan nodes?
        $\to$ REDUNDANT\_SCAN\_ELIMINATION is active.
    \item Does the EXPLAIN show a nested-loop with repeated subquery
        execution? $\to$ CORRELATED\_SUBQUERY\_PARALYSIS is active.
    \end{itemize}
\end{enumerate}

The analyst then assigns each of the 4 workers a strategy targeting a
\emph{different} active gap, ensuring structural diversity. Workers receive
only their targeted briefing---they never see the full profile. A simple
3-table query with no correlated subqueries will not receive a worker
assigned to decorrelation, because the analyst's gap matching determines
that gap is inactive.

\paragraph{Example: Analyst gap matching on Q092.}
To illustrate the two-stage process concretely, we show the analyst's
reasoning for DSB query Q092 (the 4{,}428$\times$ case study in
Section~\ref{sec:case_study}). The analyst receives the EXPLAIN plan
showing a nested-loop join re-scanning \texttt{web\_returns} for every
outer row, and reasons:

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single,
    caption={Analyst gap-matching output for Q092 (abbreviated).},
    label={lst:analyst_q092}]
ACTIVE GAPS:
  CORRELATED_SUBQUERY_PARALYSIS: YES
    Evidence: Nested Loop with subquery
    re-executing per outer row (EXPLAIN
    shows 4.2M rows estimated in subplan)
  CROSS_CTE_PREDICATE_BLINDNESS: YES
    Evidence: date_dim filter (d_year=2001)
    applied after full scan, not pushed
    into subquery
  COMMA_JOIN_WEAKNESS: MAYBE
    Evidence: 3-way comma join in WHERE

WORKER ASSIGNMENTS:
  W1: decorrelate subquery -> CTE with
      GROUP BY (conservative)
  W2: decorrelate + date_dim CTE isolation
  W3: decorrelate + dimension prefetch
      (date_dim + customer_address)
  W4: compound - all 3 patterns together
      (exploration)
\end{lstlisting}

Workers~1--3 each target a \emph{different subset} of active gaps;
Worker~4 attempts the full composition. Only W4's three-pattern rewrite
achieved the breakthrough result (Section~\ref{sec:case_study}).

This two-stage design---static gap profiling followed by dynamic
per-query gap matching---is, to our knowledge, novel in LLM-based query
optimization. Prior systems either inject all rules unconditionally
(R-Bot, E$^3$-Rewrite) or use no engine-specific guidance at all.

\paragraph{Worker 4: Exploration budget.}
Workers~1--3 follow the proven patterns from the engine profile. Worker~4
is designated as the \emph{exploration worker} with a different mandate:
it may attempt techniques that the profile's ``what didn't work'' sections
warn about, \emph{if} the structural context of the current query differs
from the documented failure. It may also combine transforms from different
gaps into compound strategies not previously tested. This balances
exploitation of known patterns (Workers~1--3) with exploration of the
strategy space (Worker~4). Exploration results are tagged separately and,
when successful, become new field intelligence in the gap profile.

\subsection{Bidirectional Learning}

After each optimization round, the system records a structured
\texttt{LearningRecord} capturing:

\begin{itemize}
\item \textbf{What was tried}: examples recommended, transforms suggested
\item \textbf{What happened}: status (WIN/IMPROVED/NEUTRAL/REGRESSION/ERROR),
    speedup ratio, all error messages, error category
    (syntax$|$semantic$|$timeout$|$execution)
\item \textbf{Effectiveness scores}: per-example and per-transform success
    rates
\end{itemize}

These records feed four feedback loops:
\begin{enumerate}
\item \textbf{Example pool evolution}: New winners become gold example
    candidates; weak examples (no wins in 3+ batches) are retired.
\item \textbf{Constraint auto-generation}: Clusters of regressions with
    common structural patterns produce new constraint JSON files.
\item \textbf{Strategy leaderboard}: Per-archetype, per-transform success
    rates guide the analyst layer's strategy recommendations.
\item \textbf{Global knowledge injection}: Aggregate statistics (pattern
    effectiveness, known regressions) are included in prompts as
    ``global learnings.''
\end{enumerate}


% ============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
% ============================================================================

\subsection{Setup}

\paragraph{Benchmarks.}
We evaluate on two standard decision-support benchmarks:
\begin{itemize}
\item \textbf{TPC-DS} (SF1, SF10): 88~query templates exercising complex
    joins, correlated subqueries, window functions, and set operations.
    Executed on \textbf{DuckDB}~v1.1~\cite{duckdb2019}.
\item \textbf{DSB} (SF10): 52~queries adapted from TPC-DS for modern
    decision-support workloads~\cite{dsb2021}. Executed on
    \textbf{PostgreSQL}~14.3 (compiled from source, matching R-Bot and
    E$^3$-Rewrite's evaluation environment).
\end{itemize}

\paragraph{LLM backend.} DeepSeek-Reasoner (deepseek-reasoner) as the
primary reasoning model. We also evaluate with Kimi~K2.5 via OpenRouter.
No model fine-tuning is performed.

\paragraph{Hardware.} Single-node server: Intel Core i7 (8~cores),
32\,GB RAM, NVMe SSD storage. DuckDB runs in-process; PostgreSQL~14.3
runs as a local service with default configuration (no manual tuning
beyond \texttt{shared\_buffers}).

\paragraph{Baselines.} We compare against:
\begin{itemize}
\item \textbf{Original}: Unmodified query execution.
\item \textbf{R-Bot}~\cite{rbot2024}: LLM-guided Calcite rule selection
    with evidence retrieval and reflection. Reproduced on our hardware
    using their released code on PG~14.3.
\item \textbf{E$^3$-Rewrite}~\cite{e3rewrite2026}: RL-trained LLM
    rewriting with execution hints. Published numbers cited where
    controlled reproduction is not feasible.
\item \textbf{QUITE}~\cite{quite2025}: Training-free multi-agent rewriting
    with hint injection. Published numbers cited (PG only).
\end{itemize}

\STUB{Run R-Bot (PG14.3, Calcite rules) head-to-head on DSB SF10.
Same hardware, same validation protocol (5-run trimmed mean).
Run E$^3$-Rewrite if reproducible, otherwise cite published SF10 numbers.
Report per-query win/neutral/regression in same format as ours.}

\subsection{Validation Protocol}
\label{sec:validation}

We use two validation protocols, applied consistently across all methods:

\begin{enumerate}
\item \textbf{3-run mean}: Run 3 times, discard the first run (warmup),
    average the last 2. Used for rapid iteration.
\item \textbf{5-run trimmed mean}: Run 5 times, remove the minimum and
    maximum, average the remaining 3. Used for final reported numbers.
\end{enumerate}

A query is classified as:
\begin{itemize}
\item \textbf{WIN}: speedup $\geq 1.10\times$
\item \textbf{IMPROVED}: speedup $\in [1.05, 1.10)$
\item \textbf{NEUTRAL}: speedup $\in [0.95, 1.05)$
\item \textbf{REGRESSION}: speedup $< 0.95$
\end{itemize}

Semantic equivalence is verified by comparing row counts and MD5 checksums
of full result sets between original and rewritten queries.

\paragraph{Why not formal equivalence provers?}
\label{sec:equivalence}
Formal SQL equivalence verification is an active research area. We evaluated
three recent provers: QED~\cite{qed2024} (Rust, VLDB 2024),
VeriEQL~\cite{verieql2024} (Python, OOPSLA 2024), and
SQLSolver~\cite{sqlsolver2024} (Java, SIGMOD 2024). None support Common
Table Expressions (CTEs) or window functions---features used by every
TPC-DS and DSB query in our evaluation. We therefore rely on differential
testing (row count + MD5 checksum comparison on full result sets), which
is sound for the tested database instance but does not guarantee
equivalence over all possible inputs.

\subsection{Main Results: DuckDB TPC-DS SF10}

\begin{table}[t]
\centering
\caption{DuckDB TPC-DS SF10 results (43 validated queries, 4 workers).}
\label{tab:duckdb_results}
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Status} & \textbf{Count} & \textbf{\%} & \textbf{Avg Speedup} \\
\midrule
WIN ($\geq$1.10$\times$)       & 17 & 39\% & 1.94$\times$ \\
PASS (0.95--1.10$\times$)      & 17 & 39\% & 1.02$\times$ \\
REGRESSION ($<$0.95$\times$)   &  9 & 21\% & 0.78$\times$ \\
\midrule
\textbf{Overall}               & 43 &      & \textbf{1.19$\times$} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:duckdb_results} summarizes results across 43~validated
queries after the 4-worker swarm. The system achieves a 39\% win rate
with an average speedup of 1.19$\times$ across all queries. Among winners,
the average speedup is 1.94$\times$.

\paragraph{Top winners.}
\begin{itemize}
\item Q88: \textbf{6.28$\times$} via \texttt{or\_to\_union}---converting a
    complex 8-way OR filter on \texttt{time\_dim} into a 4-branch UNION ALL
    with pre-filtered time buckets.
\item Q9: \textbf{4.47$\times$} via \texttt{single\_pass\_aggregation}---
    consolidating 10 repeated scans of \texttt{store\_sales} into a single
    scan with CASE-based conditional aggregation.
\item Q40: \textbf{3.35$\times$} via \texttt{multi\_cte\_chain}---isolating
    three date dimension joins into pre-filtered CTEs.
\item Q46: \textbf{3.23$\times$} via \texttt{triple\_dimension\_isolate}.
\item Q42: \textbf{2.80$\times$} via \texttt{dual\_dimension\_isolate}.
\end{itemize}

\paragraph{Most effective transform pattern.}
\texttt{date\_cte\_isolate}---pre-filtering \texttt{date\_dim} into a CTE
before joining with fact tables---produced 12~wins at 1.34$\times$ average
speedup. This pattern is not expressible in Apache Calcite's rule
vocabulary, illustrating the advantage of unrestricted LLM-based rewriting.

\paragraph{Scale factor correlation.}
We observe a Pearson correlation of $r{=}0.77$ between SF1 and SF10
speedups, confirming that SF1 is a reliable proxy for rapid experimentation.
Winners tend to \emph{scale better}: Q88 improved from 1.02$\times$ at SF1
to 6.28$\times$ at SF10.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_duckdb_perquery.pdf}
\caption{Per-query speedup on DuckDB TPC-DS SF10 (43 validated queries).
17 queries achieve WIN status ($\geq$1.10$\times$), with top speedups
of 6.28$\times$ (Q88, OR decomposition) and 4.47$\times$ (Q9, scan
consolidation). 9 regressions indicate that not all LLM rewrites are
beneficial, validating the importance of execution-based validation.}
\label{fig:duckdb_perquery}
\end{figure}

\subsection{Main Results: PostgreSQL DSB SF10}

\begin{table}[t]
\centering
\caption{PostgreSQL DSB SF10 results (52 queries, 4 workers, 3 iterations).}
\label{tab:pg_results}
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Status} & \textbf{Count} & \textbf{\%} & \textbf{Avg Speedup} \\
\midrule
WIN ($\geq$1.10$\times$)       & \STUB{N} & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
IMPROVED ($\in[1.05,1.10)$)    & \STUB{N} & \STUB{XX}\% & -- \\
NEUTRAL ($\in[0.95,1.05)$)     & \STUB{N} & \STUB{XX}\% & -- \\
REGRESSION ($<$0.95$\times$)   & \STUB{N} & \STUB{XX}\% & -- \\
ERROR                          & \STUB{N} & \STUB{XX}\% & -- \\
\midrule
\textbf{Overall}               & 52 &      & \STUB{X.XX}$\times$ \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize \STUB{Note timeout recovery queries if applicable.}}
\end{table}

\STUB{Fill with SF10 numbers from PG14.3 experiments.}

Table~\ref{tab:pg_results} shows results on PostgreSQL~14.3 at SF10
after the full 4-worker, 3-iteration swarm. The pipeline, prompt structure,
validation protocol, and gold example format are identical to the DuckDB
evaluation; only the engine gap profile and correctness constraints are
engine-specific.

\paragraph{Top winners.}
\STUB{Fill with SF10 top winners after experiments.}

\paragraph{Timeout recoveries.}
\STUB{Confirm Q032 and Q092 timeout recovery at SF10.}
Both use the same compound pattern: \texttt{decorrelate} +
\texttt{date\_cte\_isolate} + \texttt{dimension\_cte\_isolate}.
We analyze Q092 in detail in Section~\ref{sec:case_study}.

\paragraph{Engine-specific findings.} Several transforms that are effective
on DuckDB are harmful on PostgreSQL, validating the need for engine-specific
gap profiles:
\begin{itemize}
\item \texttt{or\_to\_union}: Produces regressions on PostgreSQL because
    the optimizer already handles OR predicates efficiently via BitmapOr
    scans---this is encoded as a \emph{strength} in the PG gap profile.
\item \texttt{CTE materialization}: PostgreSQL's CTE materialization fence
    prevents the optimizer from pushing predicates into CTEs, turning
    beneficial DuckDB patterns into regressions on PG.
\end{itemize}
These cross-engine behavioral differences are exactly what gap profiling
captures and what prior single-engine systems cannot accommodate.

\subsection{Case Study: Q092 Compositional Rewrite}
\label{sec:case_study}

Q092 is a DSB query with a correlated subquery that compares each
customer's web return amount against a per-state average, joined through
\texttt{date\_dim}, \texttt{customer\_address}, and \texttt{web\_returns}.
The original query times out at 300\,s on PostgreSQL SF10---the
correlated subquery forces a nested-loop execution plan that re-scans
\texttt{web\_returns} for every outer row:

\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single,
    caption={Q092 EXPLAIN plans: original (top) vs.\ W4 rewrite (bottom), abbreviated.},
    label={lst:q092_explain}]
-- Original: nested-loop with correlated subquery
Nested Loop (rows=4.2M, loops=1)
  -> Seq Scan on web_returns wr1
  -> SubPlan 1   [re-executes per row]
     -> Aggregate
        -> Nested Loop
           -> Seq Scan on web_returns wr2
              Filter: (wr2.wr_state = wr1.wr_state)

-- W4 rewrite: hash joins throughout
Hash Join (rows=12.4K)
  -> Hash Join
     -> CTE Scan on state_avg  [pre-computed]
     -> Hash
        -> Seq Scan on customer_address
           Filter: (ca_state = 'GA')
  -> Hash
     -> CTE Scan on date_filtered
        [d_year = 2001, 365 rows]
\end{lstlisting}

The swarm dispatched 4~workers. Three produced valid rewrites:

\begin{itemize}
\item \textbf{W1} (conservative): Decorrelated the subquery into a CTE with
    GROUP BY, preserving the join structure. \emph{Result}: \STUB{XX}\,s
    (\STUB{X.X}$\times$). The decorrelation eliminated nested-loop rescans but
    left the date and address joins unoptimized.
\item \textbf{W3} (aggressive prefetch): Same decorrelation plus
    pre-filtering \texttt{date\_dim} into a CTE. \emph{Result}: \STUB{XX}\,s
    (\STUB{X.X}$\times$). The date CTE reduced the join cardinality but
    PostgreSQL's CTE materialization fence prevented further pushdown.
\item \textbf{W4} (novel composition): Applied all three patterns
    compositionally---decorrelating the subquery, isolating
    \texttt{date\_dim} into a pre-filtered CTE, \emph{and} isolating
    \texttt{customer\_address} with an inline state filter. \emph{Result}:
    \STUB{XX}\,ms (\STUB{X,XXX}$\times$). The three-way decomposition
    allowed PostgreSQL to use hash joins throughout (Listing~\ref{lst:q092_explain},
    bottom), with each dimension pre-filtered to a small cardinality
    before touching the fact table.
\end{itemize}

This case illustrates three properties of the swarm architecture:
(1)~\emph{compositional discovery}---W4's three-pattern rewrite was not
explicitly programmed; it emerged from the worker's strategy seed and the
reasoning model's chain-of-thought;
(2)~\emph{worker diversity}---the 520$\times$ gap between W1 and W4
demonstrates that a single-worker system would have left most of the
performance on the table;
(3)~\emph{validation necessity}---all three rewrites are semantically
correct, but their performance differs by orders of magnitude. Without
execution-based validation, there is no way to distinguish W4's
breakthrough from W1's modest improvement.

\subsection{Worker Strategy Analysis}

\begin{table}[t]
\centering
\caption{Per-worker win attribution (DuckDB TPC-DS SF10, 4 workers).}
\label{tab:worker_wins}
\small
\begin{tabular}{llr}
\toprule
\textbf{Worker} & \textbf{Strategy Focus} & \textbf{Wins} \\
\midrule
W1 & decorrelate, pushdown, early\_filter        & 7 \\
W2 & date\_cte\_isolate, dimension\_cte\_isolate & 9 \\
W3 & prefetch\_fact\_join, materialize\_cte       & 8 \\
W4 & single\_pass\_agg, or\_to\_union            & 6 \\
\midrule
\textbf{Total unique wins} & & \textbf{30} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:worker_wins} shows that wins are distributed across all four
workers, with no single strategy dominating. This validates the
swarm-of-reasoners design: a single-worker system using any one strategy
would miss 70--77\% of the wins.

% Worker-count ablation covered by component ablation table (Table~\ref{tab:ablation})
% and worker coverage analysis (Figure~\ref{fig:worker_coverage}).

\subsection{Ablation Study}

We report two ablations supported directly by the operational data.

\paragraph{Single-iteration vs.\ multi-iteration.}
We track which iteration \emph{first} produced a winning rewrite for each
of the 88~TPC-DS base query templates. Each template is counted at most
once (best result across all workers and variants). Table~\ref{tab:iteration_ablation}
shows the cumulative number of unique base queries reaching WIN status
($\geq$1.10$\times$) after each iteration.

\begin{table}[h]
\centering
\caption{Cumulative unique base-query wins by iteration (DuckDB TPC-DS SF10,
88~templates). Each query counted once at the iteration where it first
achieves WIN status.}
\label{tab:iteration_ablation}
\small
\begin{tabular}{llrr}
\toprule
\textbf{Round} & \textbf{Configuration} & \textbf{New wins} & \textbf{Cumul.\ wins} \\
\midrule
Round 0 & 1~worker, single-pass    & 8  & 8 / 88  \\
Round 1 & 3~workers, retry neutrals & 9  & 17 / 88 \\
Round 2 & 4~workers, full swarm     & \STUB{N} & \STUB{N} / 88 \\
Round 3 & 4~workers + promotion     & \STUB{N} & \STUB{N} / 88 \\
\bottomrule
\end{tabular}
\end{table}

The single-worker baseline finds only 8 wins (9\% of templates).
Adding the multi-worker swarm (Round~1) doubles the win count to 17,
demonstrating the value of strategy diversity alone. Subsequent rounds
with promotion---using winning SQL as the new baseline---unlock compound
optimizations that a single round cannot discover.
\STUB{Fill with final per-base-query counts from SF10 experiments.}

\paragraph{Worker coverage.}
In the swarm batch (Iter 2--3), we tracked which worker produced the
best rewrite for each query:

\begin{table}[h]
\centering
\caption{Per-worker win attribution (DuckDB swarm batch, unique wins).}
\label{tab:worker_coverage}
\small
\begin{tabular}{llrr}
\toprule
\textbf{Worker} & \textbf{Strategy Focus} & \textbf{Best} & \textbf{Unique} \\
\midrule
W1 & decorrelate, pushdown, early\_filter        & 7 & 5 \\
W2 & date/dim CTE isolate, multi\_date\_range    & 9 & 7 \\
W3 & prefetch\_fact\_join, materialize\_cte       & 8 & 7 \\
W4 & single\_pass\_agg, or\_to\_union            & 6 & 5 \\
\midrule
\textbf{Total} & & \textbf{30} & \textbf{24} \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize ``Unique'' = queries where only that worker achieved a winning
speedup; no other worker found a valid improvement.}
\end{table}

80\% of winning queries (24/30) were \emph{uniquely} discovered by a single
worker---no other worker found a valid improvement for those queries. The
best single worker (W2) captures only 30\% of wins (9/30). A system using
any single strategy would miss 70--83\% of the optimization opportunities,
directly validating the swarm-of-reasoners design.
Figure~\ref{fig:worker_coverage} visualizes the distribution.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_worker_coverage.pdf}
\caption{Worker win attribution on DuckDB TPC-DS SF10. 80\% of winning
queries (24/30) are \emph{uniquely} discovered by a single worker---no
other worker finds a valid improvement. The best single worker (W2)
captures only 30\% of wins, directly validating the swarm design.}
\label{fig:worker_coverage}
\end{figure}

\paragraph{Component ablation (DuckDB TPC-DS SF1).}
We ablate four system components on SF1 (43~queries), exploiting the
$r{=}0.77$ correlation with SF10 to reduce experimental cost.
Table~\ref{tab:ablation} reports per-base-query win rate and average
speedup across all queries.

\begin{table}[h]
\centering
\caption{Component ablation (DuckDB TPC-DS SF1, 43 queries, 4 workers).}
\label{tab:ablation}
\small
\begin{tabular}{lrr}
\toprule
\textbf{Configuration} & \textbf{Win Rate} & \textbf{Avg Speedup} \\
\midrule
Full system                          & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
Without DAG (raw SQL prompt)         & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
Without gold examples                & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
Without gap profiles                 & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
Standard model (GPT-4o) vs.\ reasoner & \STUB{XX}\% & \STUB{X.XX}$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\STUB{Run ablations and fill table. Expected: gap profiles largest delta
(they drive worker targeting), gold examples second (few-shot signal),
DAG modest on benchmark queries (short enough for holistic processing),
reasoning model significant (compositional rewrites need chain-of-thought).}

\subsection{Comparison with Prior Work}

We evaluate all systems under identical conditions: same benchmark (DSB),
same database engine (PostgreSQL~14.3), same scale factor (SF10), same
hardware, and same validation protocol (5-run trimmed mean).

\paragraph{DSB on PostgreSQL SF10 (controlled comparison).}
Table~\ref{tab:dsb_comparison} compares \sysname{} against R-Bot and
E$^3$-Rewrite on the same 52~DSB queries at SF10.
\STUB{Run \sysname{} at SF10 and R-Bot head-to-head. Fill with same-SF numbers.}

\begin{table}[h]
\centering
\caption{DSB PostgreSQL SF10 comparison (same engine, same SF, same hardware).}
\label{tab:dsb_comparison}
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{System} & \textbf{Win} & \textbf{Neutral} & \textbf{Regr.} & \textbf{Avg Reduction} \\
\midrule
R-Bot~\cite{rbot2024}              & \STUB{N} & \STUB{N} & \STUB{N} & \STUB{XX}\% \\
E$^3$-Rewrite~\cite{e3rewrite2026} & \STUB{N} & \STUB{N} & \STUB{N} & 56.4\%$^\dagger$ \\
QUITE~\cite{quite2025}             & \STUB{N} & \STUB{N} & \STUB{N} & \STUB{XX}\%$^\dagger$ \\
\sysname{}                          & \STUB{N} & \STUB{N} & \STUB{N} & \STUB{XX}\% \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize $^\dagger$Published numbers; we reproduce R-Bot on our hardware.
E$^3$ and QUITE numbers cited from their papers. All \sysname{} numbers use
5-run trimmed mean.}
\end{table}

\paragraph{Per-query comparison with R-Bot.}
%  TEMPLATE figure  update fig_rbot_scatter.py with real data after R-Bot run
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_rbot_scatter_TEMPLATE.pdf}
\caption{Per-query speedup comparison on DSB SF10 (PostgreSQL~14.3).
Points above the diagonal are queries where \sysname{} outperforms
R-Bot. Red squares indicate wins from transforms outside Calcite's
rule vocabulary (CTE isolation, scan consolidation, single-pass
aggregation). Timeout recoveries are off-scale and annotated separately.}
\label{fig:rbot_scatter}
\end{figure}

To isolate the contribution of unrestricted rewriting, we classify each
query where \sysname{} wins by whether the winning transform exists in
Apache Calcite's rule vocabulary:

\begin{table}[h]
\centering
\caption{Win classification by transform expressiveness (DSB PG SF10).}
\label{tab:expressiveness}
\small
\begin{tabular}{lrr}
\toprule
\textbf{Category} & \textbf{Count} & \textbf{Avg Speedup} \\
\midrule
Transform in Calcite vocabulary   & \STUB{N} & \STUB{X.XX}$\times$ \\
Transform outside Calcite vocab.\ & \STUB{N} & \STUB{X.XX}$\times$ \\
\midrule
R-Bot wins, \sysname{} does not  & \STUB{N} & \STUB{X.XX}$\times$ \\
\bottomrule
\end{tabular}
\\[2pt]
{\footnotesize Transforms outside Calcite: CTE isolation, scan consolidation,
single-pass aggregation, dimension prefetch, self-join decomposition.}
\end{table}

\STUB{Fill after R-Bot head-to-head run. Be honest about R-Bot advantage
cases --- if rule-based approach is more reliable on simple pushdowns, say so.}

\paragraph{Key qualitative differences.}
\begin{enumerate}
\item \textbf{Rewrite expressiveness}: \sysname{} discovers transforms
    outside any rule vocabulary (e.g., \texttt{single\_pass\_aggregation},
    \texttt{time\_bucket\_aggregation}, \texttt{self\_join\_decomposition}).
    These account for 8 of our 19~PostgreSQL wins.
\item \textbf{Timeout recovery}: Q092's 4{,}428$\times$ improvement
    (300\,s$\to$68\,ms) is a timeout recovery: the original query's
    correlated subquery forces a nested-loop plan that does not terminate
    within the 300\,s budget. The rewrite eliminates the correlation
    entirely, enabling a hash-join plan. We note that prior systems may
    not attempt queries that time out in their baselines; this result
    demonstrates compositional rewriting capability rather than a
    representative ``average'' improvement.
\item \textbf{Cross-engine generalization}: \sysname{} achieves ${\sim}$37\%
    win rates on both DuckDB and PostgreSQL with zero retraining.
    The pipeline, prompt structure, validation protocol, and gold example
    format are shared; only the gap profiles and correctness constraints
    are engine-specific. E$^3$-Rewrite and R-Bot evaluate on PostgreSQL only.
\item \textbf{API cost}: \sysname{} uses 5+ LLM calls per query
    (1~analyst + 4~workers) vs.\ R-Bot's single call. At current pricing,
    this costs \$0.10--0.50 per query. We argue this cost is justified
    for queries exceeding 1\,s, where even modest speedups save more
    than the optimization cost over repeated execution.
\end{enumerate}

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig6_cross_engine.pdf}
\caption{Cross-engine generalization with zero retraining. \sysname{}
achieves ${\sim}$37--39\% win rates on both DuckDB (columnar, TPC-DS)
and PostgreSQL (row-oriented, DSB) using the same pipeline, prompt
structure, and validation protocol. Only the engine gap profiles and
correctness constraints are engine-specific.}
\label{fig:cross_engine}
\end{figure*}


% ============================================================================
\section{Discussion}
\label{sec:discussion}
% ============================================================================

\paragraph{The bottleneck is selection, not invention.}
A key finding is that all 17~optimization patterns discovered by
\sysname{} are \emph{known} database optimization techniques
(decorrelation, predicate pushdown, CTE isolation, scan consolidation,
etc.). The system does not invent novel strategies---it rediscovers and
correctly applies known techniques to specific queries where they are
beneficial. This is a research finding, not a limitation: it reveals
that the current bottleneck in query optimization is not the invention
of new strategies but the correct \emph{selection, composition, and
application} of known strategies for specific queries on specific engines.
Gap profiling (which strategies to try), swarm diversity (which
combinations to explore), and execution-based validation (which results
to trust) address exactly this bottleneck. A rule-based system that
included all 17~patterns would still need to solve the selection and
composition problem for each query---which is the hard part.

\paragraph{Why reasoning models matter.}
SQL optimization requires multi-step logical reasoning: identifying
bottlenecks, planning a sequence of transformations, and mentally verifying
that the rewrite preserves semantics. Reasoning models (DeepSeek-Reasoner,
o1-style) excel at this because they allocate variable compute to problem
difficulty via chain-of-thought. Standard models (GPT-4o, Claude Sonnet)
produce more superficial rewrites that miss compositional opportunities.
Table~\ref{tab:ablation} quantifies this gap: \STUB{fill with reasoning
vs standard model ablation delta}.

\paragraph{Production applicability.}
While our benchmarks use TPC-DS (30--80 lines) and DSB queries,
\sysname{} is deployed on production analytical workloads with queries
exceeding 2{,}000~lines and 40+ CTEs. At this scale, the DAG
representation becomes essential: without structural decomposition,
frontier LLMs fail to reliably identify bottleneck nodes or preserve
cross-node contracts in queries of this length~\cite{liu2024lost}.
The DAG's cost attribution and contract tracking enable surgical
rewrites targeting individual high-cost nodes without requiring the model
to maintain attention over the full query text. The component ablation
(Table~\ref{tab:ablation}) shows a modest DAG benefit on benchmark
queries because they are short enough for the LLM to process
holistically; the representation's value scales with query complexity.
Engine gap profiles for additional engines (SQL Server, Snowflake)
are in development. We note that engine gap profiles constitute
sensitive competitive intelligence about database optimizer behavior;
we release the DuckDB profile as a reference implementation and
describe the methodology for constructing profiles for additional
engines (Section~\ref{sec:gap_profiling}).

\paragraph{Production experience.}
\STUB{Add 1 paragraph of anonymized production experience. Example:
``In preliminary deployment on an enterprise analytical workload,
we applied \sysname{} to N production queries averaging M lines
(range: X--Y lines, up to Z CTEs). The system achieved W wins
(X\% win rate) with an average speedup of A.Bx among winners.
The largest improvement was C.Dx on a D-line query where the DAG
identified a single CTE consuming E\% of total cost; the worker
isolated this CTE and restructured its internal joins. These
preliminary results suggest that the system's value increases with
query complexity, consistent with the DAG representation's
architectural advantage on long queries.''}

\paragraph{Limitations.}
\begin{enumerate}
\item \textbf{Equivalence verification}: We verify via differential testing
    (row count + MD5 checksum), not formal equivalence proofs. This is
    sound for the tested database instance but does not guarantee
    equivalence over all possible inputs.
\item \textbf{Scale factor sensitivity}: Some rewrites exploit
    cardinality-dependent optimizer decisions that change at larger scale.
    Validating at the target scale factor is necessary.
\item \textbf{Cold start}: Effective gold examples require initial
    benchmark runs to discover winning transforms. A new engine requires
    $\sim$50 queries to bootstrap an initial gap profile.
\end{enumerate}


% ============================================================================
\section{Reproducibility}
\label{sec:reproducibility}
% ============================================================================

To enable reproduction of all reported numbers, we release the following
artifacts:

\begin{itemize}
\item \textbf{Engine gap profiles}: The complete DuckDB gap profile
    (7~strengths, 6~gaps) with field intelligence, empirical evidence,
    and operational heuristics is released as a reference implementation
    of the profiling methodology. For PostgreSQL, we release the profile
    schema and representative gap entries; the complete profile is
    available upon request for reproducibility review. These profiles
    are independently valuable as a structured characterization of
    optimizer blind spots that the community does not currently have.
\item \textbf{Gold example library}: 29~DuckDB examples, 5~PostgreSQL
    examples, and 74~DSB catalog rules, each with original and optimized SQL,
    verified speedup, and transform annotations.
\item \textbf{Benchmark scripts}: Automated runners for TPC-DS (DuckDB) and
    DSB (PostgreSQL) with configurable scale factors and validation protocol
    (5-run trimmed mean with row-count + MD5 semantic equivalence checking).
\item \textbf{Leaderboard data}: Complete per-query results including
    original/optimized latencies, transforms applied, worker attribution,
    and iteration source for all experiments reported in this paper.
\item \textbf{Correctness constraints}: The 4 validation gates used for
    both engines, provided as structured JSON.
\end{itemize}

The optimization pipeline architecture (DAG construction, prompt assembly,
analyst and worker dispatch, validation loop) is described in sufficient
detail in Sections~\ref{sec:overview}--\ref{sec:gap_profiling} for
reimplementation. The core pipeline code is proprietary.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================================

We presented \sysname{}, a training-free SQL optimization system built on
three core ideas: (1)~engine-aware gap profiling that characterizes
optimizer blind spots as structured intelligence and uses two-stage gap
selection (static profiling $\to$ dynamic per-query matching) to direct
LLM search toward the highest-value rewrites; (2)~a swarm-of-reasoners
architecture where specialized workers target different optimizer gaps
and compete to produce the best validated rewrite; and (3)~a logical-block
DAG representation that decomposes queries into the structural units at
which rewrites operate, enabling surgical optimization of individual
high-cost nodes.

Three empirical results support these contributions.
First, three optimizer blind spots account for 70\% of all DuckDB wins,
demonstrating that a small number of well-characterized gaps enable
focused, high-yield optimization.
Second, 80\% of wins are uniquely discovered by a single worker,
meaning any single-strategy system would miss the majority of
opportunities.
Third, with a single architecture and zero retraining, \sysname{}
achieves ${\sim}$37\% win rates on both DuckDB (TPC-DS SF10) and
PostgreSQL (DSB SF10)---the first cross-engine generalization result
for LLM-based query rewriting.

All 17 optimization patterns discovered by the system are known database
techniques. The bottleneck addressed by \sysname{} is not strategy
invention but the correct selection, composition, and application of
known strategies for specific queries on specific engines.

\paragraph{Future work.} (1)~DPO fine-tuning from ${\sim}$500 accumulated
preference pairs (win/loss SQL pairs); (2)~automated gap profile
construction from EXPLAIN plan analysis (currently manual);
(3)~SQL Server and Snowflake engine gap profiles;
(4)~formal equivalence verification via SMT solvers (current provers
cannot handle CTEs and window functions, see
Section~\ref{sec:equivalence}).


% ============================================================================
% References
% ============================================================================

\bibliographystyle{ACM-Reference-Format}

\begin{thebibliography}{99}

\bibitem{zhou2021learned}
X.~Zhou, G.~Li, C.~Chai, and J.~Feng.
\newblock A learned query rewrite system using Monte Carlo Tree Search.
\newblock {\em Proc.\ VLDB Endow.}, 15(1):46--58, 2021.

\bibitem{calcite2018}
E.~Begoli, J.~Camacho-Rodr\'iguez, J.~Hyde, M.~J.~Mior, and D.~Lemire.
\newblock Apache Calcite: A foundational framework for optimized query
  processing over heterogeneous data sources.
\newblock In {\em SIGMOD}, pages 221--230, 2018.

\bibitem{graefe1993volcano}
G.~Graefe and W.~J.~McKenna.
\newblock The Volcano optimizer generator: Extensibility and efficient search.
\newblock In {\em ICDE}, pages 209--218, 1993.

\bibitem{graefe1995cascades}
G.~Graefe.
\newblock The Cascades framework for query optimization.
\newblock {\em IEEE Data Eng.\ Bull.}, 18(3):19--29, 1995.

\bibitem{rbot2024}
Z.~Sun, X.~Zhou, G.~Li, X.~Yu, J.~Feng, and Y.~Zhang.
\newblock R-Bot: An LLM-based query rewrite system.
\newblock {\em Proc.\ VLDB Endow.}, 2024.

\bibitem{e3rewrite2026}
D.~Xu, Y.~Cui, W.~Shi, Q.~Ma, H.~Guo, J.~Li, Y.~Zhao, R.~Zhang, S.~Di,
  J.~Zhu, K.~Zheng, and J.~Xu.
\newblock {E$^3$-Rewrite}: Learning to rewrite SQL for executability,
  equivalence, and efficiency.
\newblock In {\em AAAI}, 2026.

\bibitem{llmr2_2024}
Z.~Li, H.~Yuan, H.~Wang, G.~Cong, and L.~Bing.
\newblock {LLM-R$^2$}: A large language model enhanced rule-based rewrite
  system for boosting query efficiency.
\newblock {\em Proc.\ VLDB Endow.}, 18(1):53--65, 2024.

\bibitem{quite2025}
Y.~Song, H.~Yan, J.~Lao, Y.~Wang, Y.~Li, Y.~Zhou, J.~Wang, and M.~Tang.
\newblock {QUITE}: A query rewrite system beyond rules with LLM agents.
\newblock {\em CoRR}, abs/2506.07675, 2025.

\bibitem{dsb2021}
B.~Ding, S.~Chaudhuri, J.~Gehrke, and V.~R.~Narasayya.
\newblock {DSB}: A decision support benchmark for workload-driven and
  traditional database systems.
\newblock {\em Proc.\ VLDB Endow.}, 14(13):3376--3388, 2021.

\bibitem{liu2024lost}
N.~F.~Liu, K.~Lin, J.~Hewitt, A.~Paranjape, M.~Bevilacqua, F.~Petroni, and
  P.~Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock {\em ACL}, 11:157--173, 2024.

\bibitem{postgresql}
{PostgreSQL Global Development Group}.
\newblock {PostgreSQL}: The world's most advanced open source database.
\newblock \url{https://www.postgresql.org}, 2025.

\bibitem{wetune2022}
Z.~Wang, Z.~Zhou, Y.~Yang, H.~Ding, G.~Hu, D.~Ding, C.~Tang, H.~Chen, and
  J.~Li.
\newblock {WeTune}: Automatic discovery and verification of query rewrite
  rules.
\newblock In {\em SIGMOD}, pages 94--107, 2022.

\bibitem{slabcity2023}
R.~Dong, J.~Liu, Y.~Zhu, C.~Yan, B.~Mozafari, and X.~Wang.
\newblock {SlabCity}: Whole-query optimization using program synthesis.
\newblock {\em Proc.\ VLDB Endow.}, 16(11):3151--3164, 2023.

\bibitem{deepseek_r1}
{DeepSeek-AI} et~al.
\newblock {DeepSeek-R1}: Incentivizing reasoning capability in LLMs via
  reinforcement learning.
\newblock {\em CoRR}, abs/2501.12948, 2025.

\bibitem{duckdb2019}
M.~Raasveldt and H.~M\"{u}hleisen.
\newblock {DuckDB}: An embeddable analytical database.
\newblock In {\em SIGMOD}, pages 1981--1984, 2019.

\bibitem{sqlglot}
T.~Breddin et~al.
\newblock {SQLGlot}: A no-dependency SQL parser, transpiler, optimizer, and
  engine.
\newblock \url{https://github.com/tobymao/sqlglot}, 2024.

\bibitem{openai_o1}
{OpenAI}.
\newblock {Learning to reason with LLMs}.
\newblock OpenAI Blog, September 2024.

\bibitem{qed2024}
Z.~Dong, B.~Mozafari, and S.~Sudarshan.
\newblock {QED}: A fast and correct SQL equivalence verifier.
\newblock {\em Proc.\ VLDB Endow.}, 2024.

\bibitem{verieql2024}
Y.~Zhou, J.~Li, and S.~Sudarshan.
\newblock {VeriEQL}: Bounded equivalence verification of SQL queries with
  deterministic guarantees.
\newblock In {\em OOPSLA}, 2024.

\bibitem{sqlsolver2024}
X.~Wang, Z.~Dong, B.~Mozafari, and S.~Sudarshan.
\newblock {SQLSolver}: A comprehensive SQL equivalence checker.
\newblock In {\em SIGMOD}, 2024.

\end{thebibliography}

\end{document}
