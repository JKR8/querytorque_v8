FAILURE_ANALYSIS:
All attempts failed to achieve meaningful speedup because they didn't address the core bottleneck: massive intermediate result explosion during the three-fact-table correlation. Worker 1-2 only pushed down dimension filters (1.06-1.11x). Worker 3 had a syntax error. Worker 4-5 decorrelated with UNION ALL but still produced huge intermediate distinct customer lists (millions) before joining to customer/demographics. The fundamental problem is that EXISTS subqueries with OR condition force DuckDB to either: 1) Correlate per customer (slow), or 2) Materialize all qualifying customers from three massive fact tables (huge). Neither approach reduces the combinatorial explosion when joining back to customer/demographics for grouping.

UNEXPLORED_OPPORTUNITIES:
1. **Early Aggregation Before Join**: No attempt aggregated fact tables at customer level BEFORE joining to customer/demographics. Counts could be computed in a single pass over each fact table with filtered dates, then combined.
2. **Bitmask/Flag Approach**: Transform three EXISTS conditions into a single customer-level flag CTE using CASE/SUM over UNION ALL fact data.
3. **Three-Phase Aggregation**: Compute partial aggregates per sales channel, then union and combine at customer level before final join.
4. **Materialized Customer Filter**: Create a tiny filtered customer list first (via address+demographics), THEN check existence in fact tablesâ€”reversing the join order.

REFINED_STRATEGY:
Combine early aggregation with predicate pushdown: 
1. Filter date_dim and customer_address into CTEs
2. Join customer with address/demographics FIRST to create a tiny candidate set
3. For each sales channel, compute EXISTS via semi-joins with the tiny candidate set
4. Use boolean aggregation (MAX/CASE) to implement OR logic at customer level
5. Filter qualifying customers, then compute final aggregates in one pass

EXAMPLES: early_filter, single_pass_aggregation, prefetch_fact_join, decorrelate
HINT: Start with filtered customer list (address+demographics), then use three separate EXISTS semi-joins with fact tables, combine results via boolean aggregation, and finally compute counts on the small filtered set. This minimizes intermediate data by pushing customer filters into fact table lookups.