You are an expert database performance analyst. Your job is to deeply analyze a slow SQL query, identify the root cause of its performance problems, and propose specific structural changes.

You follow a rigorous methodology: understand the structure, profile the costs, identify the mechanism (not just the symptom), propose changes with correctness reasoning, and learn from past failures.

## Query: query_23
## Dialect: duckdb

```sql
/* start query 23 in stream 0 using template query23.tpl */
WITH frequent_ss_items AS (
  SELECT
    SUBSTRING(i_item_desc, 1, 30) AS itemdesc,
    i_item_sk AS item_sk,
    d_date AS solddate,
    COUNT(*) AS cnt
  FROM store_sales, date_dim, item
  WHERE
    ss_sold_date_sk = d_date_sk
    AND ss_item_sk = i_item_sk
    AND d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
  GROUP BY
    SUBSTRING(i_item_desc, 1, 30),
    i_item_sk,
    d_date
  HAVING
    COUNT(*) > 4
), max_store_sales AS (
  SELECT
    MAX(csales) AS tpcds_cmax
  FROM (
    SELECT
      c_customer_sk,
      SUM(ss_quantity * ss_sales_price) AS csales
    FROM store_sales, customer, date_dim
    WHERE
      ss_customer_sk = c_customer_sk
      AND ss_sold_date_sk = d_date_sk
      AND d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
    GROUP BY
      c_customer_sk
  )
), best_ss_customer AS (
  SELECT
    c_customer_sk,
    SUM(ss_quantity * ss_sales_price) AS ssales
  FROM store_sales, customer
  WHERE
    ss_customer_sk = c_customer_sk
  GROUP BY
    c_customer_sk
  HAVING
    SUM(ss_quantity * ss_sales_price) > (
      95 / 100.0
    ) * (
      SELECT
        *
      FROM max_store_sales
    )
)
SELECT
  SUM(sales)
FROM (
  SELECT
    cs_quantity * cs_list_price AS sales
  FROM catalog_sales, date_dim
  WHERE
    d_year = 2000
    AND d_moy = 5
    AND cs_sold_date_sk = d_date_sk
    AND cs_item_sk IN (
      SELECT
        item_sk
      FROM frequent_ss_items
    )
    AND cs_bill_customer_sk IN (
      SELECT
        c_customer_sk
      FROM best_ss_customer
    )
  UNION ALL
  SELECT
    ws_quantity * ws_list_price AS sales
  FROM web_sales, date_dim
  WHERE
    d_year = 2000
    AND d_moy = 5
    AND ws_sold_date_sk = d_date_sk
    AND ws_item_sk IN (
      SELECT
        item_sk
      FROM frequent_ss_items
    )
    AND ws_bill_customer_sk IN (
      SELECT
        c_customer_sk
      FROM best_ss_customer
    )
)
LIMIT 100
```

## Query Structure (DAG)

- **frequent_ss_items** (cte, depth 0, **25%** cost) [GROUP_BY]
   tables: store_sales, date_dim, item
  operators: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[date_dim], SEQ_SCAN[item]
  sql: `SELECT SUBSTRING(i_item_desc, 1, 30) AS itemdesc, i_item_sk AS item_sk, d_date AS solddate, COUNT(*) AS cnt FROM store_sales, date_dim, item WHERE ss_sold_date_sk = d_date_sk AND ss_item_sk = i_item_s...`
- **max_store_sales** (cte, depth 0, **25%** cost) [GROUP_BY]
   tables: store_sales, customer, date_dim
  operators: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[customer], SEQ_SCAN[date_dim]
  sql: `SELECT MAX(csales) AS tpcds_cmax FROM (SELECT c_customer_sk, SUM(ss_quantity * ss_sales_price) AS csales FROM store_sales, customer, date_dim WHERE ss_customer_sk = c_customer_sk AND ss_sold_date_sk =...`
- **best_ss_customer** (cte, depth 1, **25%** cost) [GROUP_BY] ← reads [max_store_sales]
   tables: store_sales, customer, max_store_sales
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[store_sales], SEQ_SCAN[customer], SEQ_SCAN[max_store_sales]
  sql: `SELECT c_customer_sk, SUM(ss_quantity * ss_sales_price) AS ssales FROM store_sales, customer WHERE ss_customer_sk = c_customer_sk GROUP BY c_customer_sk HAVING SUM(ss_quantity * ss_sales_price) > (95 ...`
- **main_query** (main, depth 2, **25%** cost) [GROUP_BY] ← reads [best_ss_customer, best_ss_customer, frequent_ss_items, frequent_ss_items]
   tables: catalog_sales, date_dim, web_sales, date_dim, best_ss_customer, best_ss_customer, frequent_ss_items, frequent_ss_items
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[catalog_sales], SEQ_SCAN[date_dim], SEQ_SCAN[web_sales]
  sql: `SELECT SUM(sales) FROM (SELECT cs_quantity * cs_list_price AS sales FROM catalog_sales, date_dim WHERE d_year = 2000 AND d_moy = 5 AND cs_sold_date_sk = d_date_sk AND cs_item_sk IN (SELECT item_sk FRO...`

## Previous Optimization Attempts

- Attempt 1: **date_cte_isolate** → NEUTRAL (1.02x)

## Known Effective Patterns (from benchmark history)

- **date_cte_isolate**: 12 wins, 1.34x avg. Most broadly effective pattern
- **or_to_union**: 3 wins, 3.04x avg. Highest individual wins (Q88 6.28x) but risky >3 branches
- **single_pass_aggregation**: 2 wins, 3.29x avg. Q9 4.47x - consolidate repeated scans
- **early_filter**: 4 wins, 1.45x avg. Push dimension filters before fact joins
- **decorrelate**: 3 wins, 1.52x avg. Convert correlated subquery to JOIN+CTE
- **prefetch_fact_join**: 3 wins, 2.30x avg. Pre-join filtered dates with fact table
- **dimension_cte_isolate**: 2 wins, 1.67x avg. Pre-filter all dimensions
- **multi_dimension_prefetch**: 2 wins, 1.95x avg. Pre-filter date + store dims
- **pushdown**: 2 wins, 1.41x avg. Push predicates into CTEs
- **intersect_to_exists**: 1 wins, 1.83x avg. Q14 only
- **materialize_cte**: 1 wins, 1.37x avg. Q95 only
- **union_cte_split**: 1 wins, 1.36x avg. Q74 only
- **multi_date_range_cte**: 1 wins, 2.35x avg. Q29 only

## Known Regressions (DO NOT repeat these)

- **or_to_union_over_split**: Splitting OR into >3 UNION branches causes severe regression (0.23x-0.41x). Q13, Q48 had 9 branches = disaster.
- **window_function_rewrite**: Window function rewrites (AVG OVER PARTITION) show no improvement on most engines.
- **multi_scan_rewrites**: Rewrites that cause multiple scans of the same fact table are BAD (2x regression).

## Your Task

Analyze this query following these steps IN ORDER:

### 1. STRUCTURAL BREAKDOWN
For each CTE/subquery/block, explain in 1-2 sentences:
- What it computes (in plain language)
- What tables it reads and approximately how many rows
- What it outputs (cardinality estimate)

### 2. BOTTLENECK IDENTIFICATION
Using the DAG costs above, identify the dominant cost center.
Don't just name it — explain the MECHANISM:
- Is it a full table scan that could be filtered?
- Is it a sort for a window function that could be deferred?
- Is it a hash join on a large build side that could be pre-filtered?
- Is it scanning the same table multiple times when once would suffice?

### 3. PROPOSED OPTIMIZATION
Propose 1-3 specific structural changes. For EACH one:
- **What**: Exactly what to change (e.g., 'merge CTEs X and Y into one scan')
- **Why**: The performance mechanism (e.g., 'eliminates a 28M-row rescan of store_sales')
- **Risk**: What semantic constraint could break (e.g., 'the HAVING filter must be preserved')
- **Estimated impact**: minor / moderate / significant

### 4. FAILURE ANALYSIS
For each previous failed/regressed attempt, explain:
- WHY it failed (the specific mechanism)
- What constraint that teaches us for the next attempt

### 5. RECOMMENDED STRATEGY
Synthesize everything into a single recommended optimization approach.
Be specific enough that another engineer could implement it from your description.
