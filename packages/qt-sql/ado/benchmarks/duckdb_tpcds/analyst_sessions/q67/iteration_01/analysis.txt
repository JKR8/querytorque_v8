## 1. STRUCTURAL BREAKDOWN

**Inner-most subquery (dw1)**: 
- Joins `store_sales` (~5.4M rows) with filtered `date_dim` (12 months), `item`, and `store`
- Computes sales sums (`ss_sales_price * ss_quantity`) with `ROLLUP` on 8 dimensions (category through store_id)
- Output: Aggregated rows including rollup subtotals - cardinality likely in tens of thousands

**Middle subquery (dw2)**:
- Takes aggregated results and computes `RANK()` within each `i_category` partition ordered by `sumsales DESC`
- Output: Same cardinality as dw1 plus rank column

**Outer query**:
- Filters to top 100 ranks per category, orders by all dimensions, limits to 100
- Output: Exactly 100 rows

## 2. BOTTLENECK IDENTIFICATION

**Primary bottleneck**: The `GROUP BY ROLLUP` on 8 dimensions after joining the entire fact table.

**Mechanism**: 
1. The query scans `store_sales` (~5.4M rows) and joins with all dimension tables BEFORE aggregation
2. The 8-dimension `ROLLUP` generates a massive number of aggregation combinations (2^8 = 256 possible grouping sets per base row)
3. The window function `RANK()` must sort the entire aggregated result set by `i_category, sumsales DESC`

**Key insight**: The final output only needs top 100 per category, but we compute ALL aggregations first, then rank, then filter. This is wasteful.

**Secondary issue**: The `ROLLUP` creates subtotal rows that get ranked along with detailed rows, but the outer filter `rk <= 100` might exclude many subtotals early.

## 3. PROPOSED OPTIMIZATION

### Change 1: Push rank filter inside aggregation (significant impact)
**What**: Compute rank on detailed rows (without rollup), filter to top 100 per category, THEN compute rollup
**Why**: Reduces the input to `ROLLUP` from all combinations to only top-performing items per category
**Risk**: Rollup subtotals will only include top 100 items per category, not all items
**Impact**: Significant (10-100x reduction in rows to aggregate)

### Change 2: Pre-filter dates and pre-join with store_sales (moderate impact)
**What**: Create CTE with filtered dates, join to store_sales first to reduce fact table rows early
**Why**: Reduces the fact table scan by applying date filter early via efficient hash join
**Risk**: Must preserve all date columns needed for grouping (d_year, d_qoy, d_moy)
**Impact**: Moderate (reduces fact table rows by ~12x)

### Change 3: Separate detailed vs. rollup ranking (minor impact)
**What**: Compute ranks separately for detailed rows and handle rollup rows differently
**Why**: Avoids ranking rollup subtotals against detailed rows (semantic issue)
**Risk**: Complex to implement correctly
**Impact**: Minor (cleaner semantics)

## 4. FAILURE ANALYSIS

**Attempt 2 (decorrelate, materialize_cte)** failed because:
- **Root cause**: Misplaced `ROLLUP` - moved it to AFTER ranking instead of BEFORE
- **Semantic error**: The rewrite aggregated ranked results instead of ranking aggregated results
- **Lesson**: `ROLLUP` must happen before `RANK()` since ranking depends on aggregated sums

**Attempt 1 (date_cte_isolate)** showed regression (0.85x) because:
- **Root cause**: DuckDB may not push the CTE filter effectively into the join
- **Lesson**: Simple CTE isolation without aggressive pushdown may not help in DuckDB

## 5. RECOMMENDED STRATEGY

**Primary strategy**: **deferred_window_aggregation** pattern
1. Pre-filter dates into CTE, join with store_sales early to reduce fact scan
2. Compute detailed aggregations (without ROLLUP)
3. Rank detailed rows and filter to top 100 per category
4. Apply ROLLUP only to the filtered top performers
5. Final ranking on rolled-up results (if needed)

**Implementation sketch**:
```sql
WITH filtered_dates AS (
  SELECT d_date_sk, d_year, d_qoy, d_moy
  FROM date_dim
  WHERE d_month_seq BETWEEN 1206 AND 1206 + 11
),
fact_with_dates AS (
  SELECT ss.*, d.d_year, d.d_qoy, d.d_moy
  FROM store_sales ss
  JOIN filtered_dates d ON ss.ss_sold_date_sk = d.d_date_sk
),
detailed_sales AS (
  SELECT
    i.i_category,
    i.i_class,
    i.i_brand,
    i.i_product_name,
    f.d_year,
    f.d_qoy,
    f.d_moy,
    s.s_store_id,
    SUM(f.ss_sales_price * f.ss_quantity) AS sumsales
  FROM fact_with_dates f
  JOIN item i ON f.ss_item_sk = i.i_item_sk
  JOIN store s ON f.ss_store_sk = s.s_store_sk
  GROUP BY i.i_category, i.i_class, i.i_brand, i.i_product_name,
           f.d_year, f.d_qoy, f.d_moy, s.s_store_id
),
ranked_details AS (
  SELECT *,
    RANK() OVER (PARTITION BY i_category ORDER BY sumsales DESC) AS detail_rk
  FROM detailed_sales
),
top_details AS (
  SELECT *
  FROM ranked_details
  WHERE detail_rk <= 100
)
-- Then apply ROLLUP to top_details and final ranking
```

## 6. EXAMPLE SELECTION

**FAISS picks**: `deferred_window_aggregation`, `prefetch_fact_join`, `decorrelate`

**Assessment**: Good picks, but missing `single_pass_aggregation` which is crucial here since we're scanning store_sales multiple times in the original.

**Better examples**: 
- `deferred_window_aggregation` - Core pattern (rank before expensive operations)
- `prefetch_fact_join` - For early date filtering
- `single_pass_aggregation` - Critical to avoid rescanning store_sales

```
EXAMPLES: deferred_window_aggregation, prefetch_fact_join, single_pass_aggregation
```