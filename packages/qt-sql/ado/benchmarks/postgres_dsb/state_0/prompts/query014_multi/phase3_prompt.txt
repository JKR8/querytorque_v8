You are a SQL query rewrite engine.

Your goal: rewrite the complete SQL query to maximize execution speed
while preserving exact semantic equivalence (same rows, same columns,
same ordering).

You will receive the full query, its DAG structure showing how CTEs and
subqueries connect, cost analysis per node, and suggested rewrite patterns.
You may restructure the query freely: create new CTEs, merge existing ones,
push filters across node boundaries, or decompose subqueries.

## Query: query014_multi

```sql
WITH cross_items AS (
  SELECT
    i_item_sk AS ss_item_sk
  FROM item, (
    SELECT
      iss.i_brand_id AS brand_id,
      iss.i_class_id AS class_id,
      iss.i_category_id AS category_id
    FROM store_sales, item AS iss, date_dim AS d1
    WHERE
      ss_item_sk = iss.i_item_sk
      AND ss_sold_date_sk = d1.d_date_sk
      AND d1.d_year BETWEEN 2000 AND 2000 + 2
      AND i_category IN ('Electronics', 'Home', 'Men')
      AND i_manager_id BETWEEN 25 AND 34
      AND ss_wholesale_cost BETWEEN 34 AND 54
    INTERSECT
    SELECT
      ics.i_brand_id,
      ics.i_class_id,
      ics.i_category_id
    FROM catalog_sales, item AS ics, date_dim AS d2
    WHERE
      cs_item_sk = ics.i_item_sk
      AND cs_sold_date_sk = d2.d_date_sk
      AND d2.d_year BETWEEN 2000 AND 2000 + 2
      AND i_category IN ('Electronics', 'Home', 'Men')
      AND i_manager_id BETWEEN 25 AND 34
      AND cs_wholesale_cost BETWEEN 34 AND 54
    INTERSECT
    SELECT
      iws.i_brand_id,
      iws.i_class_id,
      iws.i_category_id
    FROM web_sales, item AS iws, date_dim AS d3
    WHERE
      ws_item_sk = iws.i_item_sk
      AND ws_sold_date_sk = d3.d_date_sk
      AND ws_wholesale_cost BETWEEN 34 AND 54
      AND d3.d_year BETWEEN 2000 AND 2000 + 2
  ) AS x
  WHERE
    i_brand_id = brand_id
    AND i_class_id = class_id
    AND i_category_id = category_id
    AND i_category IN ('Electronics', 'Home', 'Men')
    AND i_manager_id BETWEEN 25 AND 34
), avg_sales AS (
  SELECT
    AVG(quantity * list_price) AS average_sales
  FROM (
    SELECT
      ss_quantity AS quantity,
      ss_list_price AS list_price
    FROM store_sales, date_dim
    WHERE
      ss_sold_date_sk = d_date_sk
      AND d_year BETWEEN 2000 AND 2000 + 2
      AND ss_wholesale_cost BETWEEN 34 AND 54
    UNION ALL
    SELECT
      cs_quantity AS quantity,
      cs_list_price AS list_price
    FROM catalog_sales, date_dim
    WHERE
      cs_sold_date_sk = d_date_sk
      AND d_year BETWEEN 2000 AND 2000 + 2
      AND cs_wholesale_cost BETWEEN 34 AND 54
    UNION ALL
    SELECT
      ws_quantity AS quantity,
      ws_list_price AS list_price
    FROM web_sales, date_dim
    WHERE
      ws_sold_date_sk = d_date_sk
      AND ws_wholesale_cost BETWEEN 34 AND 54
      AND d_year BETWEEN 2000 AND 2000 + 2
  ) AS x
)
SELECT
  this_year.channel AS ty_channel,
  this_year.i_brand_id AS ty_brand,
  this_year.i_class_id AS ty_class,
  this_year.i_category_id AS ty_category,
  this_year.sales AS ty_sales,
  this_year.number_sales AS ty_number_sales,
  last_year.channel AS ly_channel,
  last_year.i_brand_id AS ly_brand,
  last_year.i_class_id AS ly_class,
  last_year.i_category_id AS ly_category,
  last_year.sales AS ly_sales,
  last_year.number_sales AS ly_number_sales
FROM (
  SELECT
    'store' AS channel,
    i_brand_id,
    i_class_id,
    i_category_id,
    SUM(ss_quantity * ss_list_price) AS sales,
    COUNT(*) AS number_sales
  FROM store_sales, item, date_dim
  WHERE
    ss_item_sk IN (
      SELECT
        ss_item_sk
      FROM cross_items
    )
    AND ss_item_sk = i_item_sk
    AND ss_sold_date_sk = d_date_sk
    AND d_week_seq = (
      SELECT
        d_week_seq
      FROM date_dim
      WHERE
        d_year = 2000 + 1 AND d_moy = 12 AND d_dom = 17
    )
    AND i_category IN ('Electronics', 'Home', 'Men')
    AND i_manager_id BETWEEN 25 AND 34
    AND ss_wholesale_cost BETWEEN 34 AND 54
  GROUP BY
    i_brand_id,
    i_class_id,
    i_category_id
  HAVING
    SUM(ss_quantity * ss_list_price) > (
      SELECT
        average_sales
      FROM avg_sales
    )
) AS this_year, (
  SELECT
    'store' AS channel,
    i_brand_id,
    i_class_id,
    i_category_id,
    SUM(ss_quantity * ss_list_price) AS sales,
    COUNT(*) AS number_sales
  FROM store_sales, item, date_dim
  WHERE
    ss_item_sk IN (
      SELECT
        ss_item_sk
      FROM cross_items
    )
    AND ss_item_sk = i_item_sk
    AND ss_sold_date_sk = d_date_sk
    AND d_week_seq = (
      SELECT
        d_week_seq
      FROM date_dim
      WHERE
        d_year = 2000 AND d_moy = 12 AND d_dom = 17
    )
    AND i_category IN ('Electronics', 'Home', 'Men')
    AND ss_wholesale_cost BETWEEN 34 AND 54
    AND i_manager_id BETWEEN 25 AND 34
  GROUP BY
    i_brand_id,
    i_class_id,
    i_category_id
  HAVING
    SUM(ss_quantity * ss_list_price) > (
      SELECT
        average_sales
      FROM avg_sales
    )
) AS last_year
WHERE
  this_year.i_brand_id = last_year.i_brand_id
  AND this_year.i_class_id = last_year.i_class_id
  AND this_year.i_category_id = last_year.i_category_id
ORDER BY
  this_year.channel,
  this_year.i_brand_id,
  this_year.i_class_id,
  this_year.i_category_id
LIMIT 100
```

## DAG Topology

```sql
-- DAG TOPOLOGY
-- Depth 0:
--   cross_items (cte, 33% cost)
--     outputs: [ss_item_sk]
--   avg_sales (cte, 33% cost) [UNION_ALL]
--     outputs: [average_sales]
-- Depth 1:
--   main_query (main, 33% cost) [GROUP_BY] ← reads [avg_sales, avg_sales, cross_items, cross_items]
--     outputs: [ty_channel, ty_brand, ty_class, ty_category, ty_sales, ty_number_sales, ly_channel, ly_brand, ...]
-- Edges:
--   avg_sales → main_query
--   avg_sales → main_query
--   cross_items → main_query
--   cross_items → main_query
```

## Performance Profile

**cross_items**: 33% of total cost, ~1,000 rows
  operators: SEQ_SCAN[item], SEQ_SCAN[web_sales], SEQ_SCAN[item]
**avg_sales**: 33% of total cost, ~1,000 rows
  operators: SEQ_SCAN[web_sales], SEQ_SCAN[date_dim], SEQ_SCAN[store_sales]
**main_query**: 33% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[store_sales], SEQ_SCAN[item], SEQ_SCAN[date_dim]

## Suggested Rewrite Strategy

Phase 2 analysis identified these optimization opportunities:

- **cross_items** → apply **pushdown**
  Heuristic: 33.3% cost, flags=[]
- **avg_sales** → apply **union_cte_split**
  Heuristic: 33.3% cost, flags=['UNION_ALL']
- **main_query** → apply **date_cte_isolate**
  Heuristic: 33.3% cost, flags=['GROUP_BY']

## Reference Example: date_cte_isolate

### BEFORE (slow)
```sql
[main_query]:
SELECT a.ca_state state, count(*) cnt
FROM customer_address a, customer c, store_sales s, date_dim d, item i
WHERE a.ca_address_sk = c.c_current_addr_sk
  AND c.c_customer_sk = s.ss_customer_sk
  AND s.ss_sold_date_sk = d.d_date_sk
  AND s.ss_item_sk = i.i_item_sk
  AND d.d_month_seq = (SELECT DISTINCT d_month_seq FROM date_dim WHERE d_year = 2000 AND d_moy = 1)
  AND i.i_current_price > 1.2 * (SELECT avg(j.i_current_price) FROM item j WHERE j.i_category = i.i_category)
GROUP BY a.ca_state HAVING count(*) >= 10
ORDER BY cnt, a.ca_state LIMIT 100
```

**Key insight:** Extract date month_seq subquery into CTE. Extract category average into separate CTE with GROUP BY. Join instead of correlated subquery. This allows each CTE to be scanned once.

### AFTER (fast)
[target_month]:
```sql
SELECT DISTINCT d_month_seq FROM date_dim WHERE d_year = 2000 AND d_moy = 1
```
[category_avg_price]:
```sql
SELECT i_category, AVG(i_current_price) * 1.2 AS avg_threshold FROM item GROUP BY i_category
```
[filtered_dates]:
```sql
SELECT d_date_sk FROM date_dim JOIN target_month ON d_month_seq = target_month.d_month_seq
```
[filtered_sales]:
```sql
SELECT ss_customer_sk, ss_item_sk FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk
```
[main_query]:
```sql
SELECT a.ca_state AS state, COUNT(*) AS cnt FROM customer_address a JOIN customer c ON a.ca_address_sk = c.c_current_addr_sk JOIN filtered_sales s ON c.c_customer_sk = s.ss_customer_sk JOIN item i ON s.ss_item_sk = i.i_item_sk JOIN category_avg_price cap ON i.i_category = cap.i_category WHERE i.i_current_price > cap.avg_threshold GROUP BY a.ca_state HAVING COUNT(*) >= 10 ORDER BY cnt, a.ca_state LIMIT 100
```

### When NOT to use this pattern
Do not use when the optimizer already pushes date predicates effectively (e.g., simple equality filters on date columns in self-joins). Do not decompose an already-efficient existing CTE into sub-CTEs — this adds materialization overhead without reducing scans. Caused 0.49x regression on Q31 (DuckDB already optimized the date pushdown) and 0.71x on Q1 (decomposed a well-structured CTE into slower pieces).

## Constraints

### CRITICAL — Correctness Guards (top of sandwich)

**SEMANTIC_EQUIVALENCE**
The rewritten query MUST return exactly the same rows, columns, and
ordering as the original. This is the prime directive.

**LITERAL_PRESERVATION**
Keep all literal values (dates, strings, numbers) exactly as they appear in
the original SQL. Do not round, truncate, or reformat them.

### HIGH — Performance and Style Rules (middle of sandwich)

**NO_UNFILTERED_DIM_CTE**
When creating a new CTE that scans a dimension table, include at least one
filter predicate. Never materialize an entire dimension without a WHERE clause.

**OR_TO_UNION_LIMIT**
When converting OR predicates to UNION ALL, limit to 4 branches maximum.
Beyond 4, the UNION overhead exceeds the OR scan cost for most planners.

**EXPLICIT_JOINS**
Convert comma-separated implicit joins to explicit JOIN ... ON syntax.
This gives the optimizer better join-order freedom.

### CRITICAL — Correctness Guards (bottom of sandwich)

**KEEP_EXISTS_AS_EXISTS**
Preserve EXISTS/NOT EXISTS subqueries as-is. Do not convert them to
IN/NOT IN or to JOINs — this risks NULL-handling semantic changes.

**COMPLETE_OUTPUT**
The rewritten query must output ALL columns from the original SELECT.
Never drop, rename, or reorder output columns.

## Output

Return the complete rewritten SQL query. The query must be syntactically
valid and ready to execute.

```sql
-- Your rewritten query here
```

After the SQL, briefly explain what you changed:

```
Changes: <1-2 sentence summary of the rewrite>
Expected speedup: <estimate>
```

Now output your rewritten SQL: