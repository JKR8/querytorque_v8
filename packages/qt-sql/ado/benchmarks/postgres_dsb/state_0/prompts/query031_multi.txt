You are a SQL query rewrite engine.

Your goal: rewrite the complete SQL query to maximize execution speed
while preserving exact semantic equivalence (same rows, same columns,
same ordering).

You will receive the full query, its DAG structure showing how CTEs and
subqueries connect, cost analysis per node, and suggested rewrite patterns.
You may restructure the query freely: create new CTEs, merge existing ones,
push filters across node boundaries, or decompose subqueries.

## Query: query031_multi

```sql
WITH ss AS (
  SELECT
    ca_county,
    d_qoy,
    d_year,
    SUM(ss_ext_sales_price) AS store_sales
  FROM store_sales, date_dim, customer_address, item
  WHERE
    ss_sold_date_sk = d_date_sk
    AND ss_addr_sk = ca_address_sk
    AND ss_item_sk = i_item_sk
    AND i_color IN ('blanched', 'rosy')
    AND i_manager_id BETWEEN 16 AND 35
    AND ss_list_price BETWEEN 286 AND 300
    AND ca_state IN ('TX', 'VA')
  GROUP BY
    ca_county,
    d_qoy,
    d_year
), ws AS (
  SELECT
    ca_county,
    d_qoy,
    d_year,
    SUM(ws_ext_sales_price) AS web_sales
  FROM web_sales, date_dim, customer_address, item
  WHERE
    ws_sold_date_sk = d_date_sk
    AND ws_bill_addr_sk = ca_address_sk
    AND ws_item_sk = i_item_sk
    AND i_color IN ('blanched', 'rosy')
    AND i_manager_id BETWEEN 16 AND 35
    AND ws_list_price BETWEEN 286 AND 300
    AND ca_state IN ('TX', 'VA')
  GROUP BY
    ca_county,
    d_qoy,
    d_year
)
SELECT
  ss1.ca_county,
  ss1.d_year,
  ws2.web_sales / ws1.web_sales AS web_q1_q2_increase,
  ss2.store_sales / ss1.store_sales AS store_q1_q2_increase,
  ws3.web_sales / ws2.web_sales AS web_q2_q3_increase,
  ss3.store_sales / ss2.store_sales AS store_q2_q3_increase
FROM ss AS ss1, ss AS ss2, ss AS ss3, ws AS ws1, ws AS ws2, ws AS ws3
WHERE
  ss1.d_qoy = 1
  AND ss1.d_year = 1998
  AND ss1.ca_county = ss2.ca_county
  AND ss2.d_qoy = 2
  AND ss2.d_year = 1998
  AND ss2.ca_county = ss3.ca_county
  AND ss3.d_qoy = 3
  AND ss3.d_year = 1998
  AND ss1.ca_county = ws1.ca_county
  AND ws1.d_qoy = 1
  AND ws1.d_year = 1998
  AND ws1.ca_county = ws2.ca_county
  AND ws2.d_qoy = 2
  AND ws2.d_year = 1998
  AND ws1.ca_county = ws3.ca_county
  AND ws3.d_qoy = 3
  AND ws3.d_year = 1998
  AND CASE WHEN ws1.web_sales > 0 THEN ws2.web_sales / ws1.web_sales ELSE NULL END > CASE WHEN ss1.store_sales > 0 THEN ss2.store_sales / ss1.store_sales ELSE NULL END
  AND CASE WHEN ws2.web_sales > 0 THEN ws3.web_sales / ws2.web_sales ELSE NULL END > CASE WHEN ss2.store_sales > 0 THEN ss3.store_sales / ss2.store_sales ELSE NULL END
ORDER BY
  web_q1_q2_increase
```

## DAG Topology

```sql
-- DAG TOPOLOGY
-- Depth 0:
--   ss (cte, 33% cost) [GROUP_BY]
--     outputs: [ca_county, d_qoy, d_year, store_sales]
--   ws (cte, 33% cost) [GROUP_BY]
--     outputs: [ca_county, d_qoy, d_year, web_sales]
-- Depth 1:
--   main_query (main, 33% cost) [GROUP_BY] ← reads [ss, ss, ss, ws, ws, ws]
--     outputs: [ca_county, d_year, web_q1_q2_increase, store_q1_q2_increase, web_q2_q3_increase, store_q2_q3_increase]
-- Edges:
--   ss → main_query
--   ss → main_query
--   ss → main_query
--   ws → main_query
--   ws → main_query
--   ws → main_query
```

## Performance Profile

**ss**: 33% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[date_dim], SEQ_SCAN[customer_address]
**ws**: 33% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, SEQ_SCAN[web_sales], SEQ_SCAN[date_dim], SEQ_SCAN[customer_address]
**main_query**: 33% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[ss], SEQ_SCAN[ss], SEQ_SCAN[ss]

## Suggested Rewrite Strategy

Phase 2 analysis identified these optimization opportunities:

- **ss** → apply **pushdown**
  Heuristic: 33.3% cost, flags=['GROUP_BY']
- **ws** → apply **pushdown**
  Heuristic: 33.3% cost, flags=['GROUP_BY']

Nodes not flagged (low cost or no opportunity):
- main_query: No matching pattern for flags=['GROUP_BY']

## Reference Examples

### 1. pg_materialized_dimension_fact_prefilter (2.68x)

**BEFORE (slow):**
```sql
select i_item_desc, w_warehouse_name, d1.d_week_seq,
  sum(case when p_promo_sk is null then 1 else 0 end) no_promo,
  sum(case when p_promo_sk is not null then 1 else 0 end) promo,
  count(*) total_cnt
from catalog_sales
join inventory on (cs_item_sk = inv_item_sk)
join warehouse on (w_warehouse_sk=inv_warehouse_sk)
join item on (i_item_sk = cs_item_sk)
join customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)
join household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)
join date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)
join date_dim d2 on (inv_date_sk = d2.d_date_sk)
join date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)
left outer join promotion on (cs_promo_sk=p_promo_sk)
left outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)
where d1.d_week_seq = d2.d_week_seq
  and inv_quantity_on_hand < cs_quantity
  and d3.d_date > d1.d_date + interval '3 day'
  and hd_buy_potential = '501-1000'
  and d1.d_year = 1998
  and cd_marital_status = 'M'
  and cd_dep_count between 9 and 11
  and i_category IN ('Home', 'Men', 'Music')
  and cs_wholesale_cost BETWEEN 34 AND 54
group by i_item_desc,w_warehouse_name,d1.d_week_seq
order by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq
limit 100
```

**Key insight:** When a query has expensive non-equi joins (inv_quantity < cs_quantity, d_week_seq correlation across 3 date_dim instances), reducing BOTH dimension AND fact table sizes before the join dramatically shrinks the search space. MATERIALIZED CTEs on PG12+ force early execution. The fact table CTE (cs_wholesale_cost BETWEEN x AND y) removes ~70% of catalog_sales rows before hitting the inventory join. Combined with tiny dimension CTEs (date: ~365 rows from 73K, item: 3 categories, cd/hd: single-value filters), the optimizer gets much smaller inputs for the expensive joins.

**AFTER (fast):**
[filtered_date]:
```sql
SELECT d_date_sk, d_date, d_week_seq FROM date_dim WHERE d_year = 1998
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc FROM item WHERE i_category IN ('Home', 'Men', 'Music')
```
[filtered_cd]:
```sql
SELECT cd_demo_sk FROM customer_demographics WHERE cd_marital_status = 'M' AND cd_dep_count BETWEEN 9 AND 11
```
[filtered_hd]:
```sql
SELECT hd_demo_sk FROM household_demographics WHERE hd_buy_potential = '501-1000'
```
[cs_filtered]:
```sql
SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk, cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost, cs_order_number FROM catalog_sales WHERE cs_wholesale_cost BETWEEN 34 AND 54
```
[main_query]:
```sql
SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq, SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo, SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo, COUNT(*) AS total_cnt FROM cs_filtered cs JOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk JOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk JOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk JOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk JOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk JOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk LEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk LEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk AND cr.cr_order_number = cs.cs_order_number WHERE d1.d_week_seq = d2.d_week_seq AND inv.inv_quantity_on_hand < cs.cs_quantity AND d3.d_date > d1.d_date + INTERVAL '3 day' GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq LIMIT 100
```

### 2. pg_self_join_decomposition (3.93x)

**BEFORE (slow):**
```sql
select s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand
from store, item,
  (select ss_store_sk, avg(revenue) as ave
   from (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
         from store_sales, date_dim
         where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224
           and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48
         group by ss_store_sk, ss_item_sk) sa
   group by ss_store_sk) sb,
  (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
   from store_sales, date_dim
   where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224
     and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48
   group by ss_store_sk, ss_item_sk) sc
where sb.ss_store_sk = sc.ss_store_sk
  and sc.revenue <= 0.1 * sb.ave
  and s_store_sk = sc.ss_store_sk
  and i_item_sk = sc.ss_item_sk
  and i_manager_id BETWEEN 32 AND 36
  and s_state in ('TN','TX','VA')
order by s_store_name, i_item_desc
limit 100
```

**Key insight:** The original scans store_sales+date_dim TWICE with identical predicates (sa/sc subqueries). On PostgreSQL, CTE materialization computes the fact table aggregation ONCE and reuses it for both per-item revenue and per-store averages. Combined with dimension pre-filtering (store, item), this eliminates the dominant I/O cost.

**AFTER (fast):**
[date_filter]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224
```
[store_sales_revenue]:
```sql
SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk
```
[store_avg_revenue]:
```sql
SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk
```
[filtered_store]:
```sql
SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36
```
[main_query]:
```sql
SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100
```

## Constraints

### CRITICAL — Correctness Guards (top of sandwich)

**SEMANTIC_EQUIVALENCE**
The rewritten query MUST return exactly the same rows, columns, and
ordering as the original. This is the prime directive.

**LITERAL_PRESERVATION**
Keep all literal values (dates, strings, numbers) exactly as they appear in
the original SQL. Do not round, truncate, or reformat them.

### HIGH — Performance and Style Rules (middle of sandwich)

**NO_UNFILTERED_DIM_CTE**
When creating a new CTE that scans a dimension table, include at least one
filter predicate. Never materialize an entire dimension without a WHERE clause.

**OR_TO_UNION_LIMIT**
When converting OR predicates to UNION ALL, limit to 4 branches maximum.
Beyond 4, the UNION overhead exceeds the OR scan cost for most planners.

**EXPLICIT_JOINS**
Convert comma-separated implicit joins to explicit JOIN ... ON syntax.
This gives the optimizer better join-order freedom.

### CRITICAL — Correctness Guards (bottom of sandwich)

**KEEP_EXISTS_AS_EXISTS**
Preserve EXISTS/NOT EXISTS subqueries as-is. Do not convert them to
IN/NOT IN or to JOINs — this risks NULL-handling semantic changes.

**COMPLETE_OUTPUT**
The rewritten query must output ALL columns from the original SELECT.
Never drop, rename, or reorder output columns.

## Output

Return the complete rewritten SQL query. The query must be syntactically
valid and ready to execute.

```sql
-- Your rewritten query here
```

After the SQL, briefly explain what you changed:

```
Changes: <1-2 sentence summary of the rewrite>
Expected speedup: <estimate>
```

Now output your rewritten SQL: