You are a SQL query rewrite engine.

Your goal: rewrite the complete SQL query to maximize execution speed
while preserving exact semantic equivalence (same rows, same columns,
same ordering).

You will receive the full query, its DAG structure showing how CTEs and
subqueries connect, cost analysis per node, and suggested rewrite patterns.
You may restructure the query freely: create new CTEs, merge existing ones,
push filters across node boundaries, or decompose subqueries.

## Query: query054_multi

```sql
WITH my_customers AS (
  SELECT DISTINCT
    c_customer_sk,
    c_current_addr_sk
  FROM (
    SELECT
      cs_sold_date_sk AS sold_date_sk,
      cs_bill_customer_sk AS customer_sk,
      cs_item_sk AS item_sk,
      cs_wholesale_cost AS wholesale_cost
    FROM catalog_sales
    UNION ALL
    SELECT
      ws_sold_date_sk AS sold_date_sk,
      ws_bill_customer_sk AS customer_sk,
      ws_item_sk AS item_sk,
      ws_wholesale_cost AS wholesale_cost
    FROM web_sales
  ) AS cs_or_ws_sales, item, date_dim, customer
  WHERE
    sold_date_sk = d_date_sk
    AND item_sk = i_item_sk
    AND i_category = 'Books'
    AND i_class = 'fiction'
    AND c_customer_sk = cs_or_ws_sales.customer_sk
    AND d_moy = 11
    AND d_year = 1998
    AND wholesale_cost BETWEEN 70 AND 100
    AND c_birth_year BETWEEN 1993 AND 2006
), my_revenue AS (
  SELECT
    c_customer_sk,
    SUM(ss_ext_sales_price) AS revenue
  FROM my_customers, store_sales, customer_address, store, date_dim
  WHERE
    c_current_addr_sk = ca_address_sk
    AND ca_county = s_county
    AND ca_state = s_state
    AND ss_sold_date_sk = d_date_sk
    AND c_customer_sk = ss_customer_sk
    AND ss_wholesale_cost BETWEEN 70 AND 100
    AND s_state IN ('GA', 'IA', 'LA', 'MO', 'OH', 'PA', 'SD', 'TN', 'TX', 'VA')
    AND d_month_seq BETWEEN (
      SELECT DISTINCT
        d_month_seq + 1
      FROM date_dim
      WHERE
        d_year = 1998 AND d_moy = 11
    ) AND (
      SELECT DISTINCT
        d_month_seq + 3
      FROM date_dim
      WHERE
        d_year = 1998 AND d_moy = 11
    )
  GROUP BY
    c_customer_sk
), segments AS (
  SELECT
    CAST((
      revenue / 50
    ) AS INT) AS segment
  FROM my_revenue
)
SELECT
  segment,
  COUNT(*) AS num_customers,
  segment * 50 AS segment_base
FROM segments
GROUP BY
  segment
ORDER BY
  segment,
  num_customers
LIMIT 100
```

## DAG Topology

```sql
-- DAG TOPOLOGY
-- Depth 0:
--   my_customers (cte, 25% cost) [UNION_ALL]
--     outputs: [c_customer_sk, c_current_addr_sk]
-- Depth 1:
--   my_revenue (cte, 25% cost) [GROUP_BY] ← reads [my_customers]
--     outputs: [c_customer_sk, revenue]
-- Depth 2:
--   segments (cte, 25% cost) ← reads [my_revenue]
--     outputs: [segment]
-- Depth 3:
--   main_query (main, 25% cost) [GROUP_BY] ← reads [segments]
--     outputs: [segment, num_customers, segment_base]
-- Edges:
--   my_customers → my_revenue
--   my_revenue → segments
--   segments → main_query
```

## Performance Profile

**my_customers**: 25% of total cost, ~1,000 rows
  operators: SEQ_SCAN[item], SEQ_SCAN[date_dim], SEQ_SCAN[customer]
**my_revenue**: 25% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[my_customers], SEQ_SCAN[store_sales], SEQ_SCAN[customer_address]
**segments**: 25% of total cost, ~1,000 rows
  operators: HASH_JOIN, SEQ_SCAN[my_revenue]
**main_query**: 25% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[segments]

## Suggested Rewrite Strategy

Phase 2 analysis identified these optimization opportunities:

- **my_customers** → apply **union_cte_split**
  Heuristic: 25.0% cost, flags=['UNION_ALL']
- **my_revenue** → apply **date_cte_isolate**
  Heuristic: 25.0% cost, flags=['GROUP_BY']
- **segments** → apply **pushdown**
  Heuristic: 25.0% cost, flags=[]

Nodes not flagged (low cost or no opportunity):
- main_query: No matching pattern for flags=['GROUP_BY']

## Reference Example: date_cte_isolate

### BEFORE (slow)
```sql
[main_query]:
SELECT a.ca_state state, count(*) cnt
FROM customer_address a, customer c, store_sales s, date_dim d, item i
WHERE a.ca_address_sk = c.c_current_addr_sk
  AND c.c_customer_sk = s.ss_customer_sk
  AND s.ss_sold_date_sk = d.d_date_sk
  AND s.ss_item_sk = i.i_item_sk
  AND d.d_month_seq = (SELECT DISTINCT d_month_seq FROM date_dim WHERE d_year = 2000 AND d_moy = 1)
  AND i.i_current_price > 1.2 * (SELECT avg(j.i_current_price) FROM item j WHERE j.i_category = i.i_category)
GROUP BY a.ca_state HAVING count(*) >= 10
ORDER BY cnt, a.ca_state LIMIT 100
```

**Key insight:** Extract date month_seq subquery into CTE. Extract category average into separate CTE with GROUP BY. Join instead of correlated subquery. This allows each CTE to be scanned once.

### AFTER (fast)
[target_month]:
```sql
SELECT DISTINCT d_month_seq FROM date_dim WHERE d_year = 2000 AND d_moy = 1
```
[category_avg_price]:
```sql
SELECT i_category, AVG(i_current_price) * 1.2 AS avg_threshold FROM item GROUP BY i_category
```
[filtered_dates]:
```sql
SELECT d_date_sk FROM date_dim JOIN target_month ON d_month_seq = target_month.d_month_seq
```
[filtered_sales]:
```sql
SELECT ss_customer_sk, ss_item_sk FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk
```
[main_query]:
```sql
SELECT a.ca_state AS state, COUNT(*) AS cnt FROM customer_address a JOIN customer c ON a.ca_address_sk = c.c_current_addr_sk JOIN filtered_sales s ON c.c_customer_sk = s.ss_customer_sk JOIN item i ON s.ss_item_sk = i.i_item_sk JOIN category_avg_price cap ON i.i_category = cap.i_category WHERE i.i_current_price > cap.avg_threshold GROUP BY a.ca_state HAVING COUNT(*) >= 10 ORDER BY cnt, a.ca_state LIMIT 100
```

### When NOT to use this pattern
Do not use when the optimizer already pushes date predicates effectively (e.g., simple equality filters on date columns in self-joins). Do not decompose an already-efficient existing CTE into sub-CTEs — this adds materialization overhead without reducing scans. Caused 0.49x regression on Q31 (DuckDB already optimized the date pushdown) and 0.71x on Q1 (decomposed a well-structured CTE into slower pieces).

## Constraints

### CRITICAL — Correctness Guards (top of sandwich)

**SEMANTIC_EQUIVALENCE**
The rewritten query MUST return exactly the same rows, columns, and
ordering as the original. This is the prime directive.

**LITERAL_PRESERVATION**
Keep all literal values (dates, strings, numbers) exactly as they appear in
the original SQL. Do not round, truncate, or reformat them.

### HIGH — Performance and Style Rules (middle of sandwich)

**NO_UNFILTERED_DIM_CTE**
When creating a new CTE that scans a dimension table, include at least one
filter predicate. Never materialize an entire dimension without a WHERE clause.

**OR_TO_UNION_LIMIT**
When converting OR predicates to UNION ALL, limit to 4 branches maximum.
Beyond 4, the UNION overhead exceeds the OR scan cost for most planners.

**EXPLICIT_JOINS**
Convert comma-separated implicit joins to explicit JOIN ... ON syntax.
This gives the optimizer better join-order freedom.

### CRITICAL — Correctness Guards (bottom of sandwich)

**KEEP_EXISTS_AS_EXISTS**
Preserve EXISTS/NOT EXISTS subqueries as-is. Do not convert them to
IN/NOT IN or to JOINs — this risks NULL-handling semantic changes.

**COMPLETE_OUTPUT**
The rewritten query must output ALL columns from the original SELECT.
Never drop, rename, or reorder output columns.

## Output

Return the complete rewritten SQL query. The query must be syntactically
valid and ready to execute.

```sql
-- Your rewritten query here
```

After the SQL, briefly explain what you changed:

```
Changes: <1-2 sentence summary of the rewrite>
Expected speedup: <estimate>
```

Now output your rewritten SQL: