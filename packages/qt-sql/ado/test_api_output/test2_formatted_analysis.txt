## Expert Analysis

### Query Structure
**customer_total_return CTE**: Computes total return amounts per customer per store for the year 2000. It joins store_returns (large fact table, ~10M+ rows) with date_dim (~100K rows, filtered to 1 year) and aggregates. Outputs ~1-5M rows (distinct customer-store pairs for 2000).

**Main query**: Finds customers in Tennessee stores whose total return exceeds 120% of their store's average return. It reads the CTE twice (once as ctr1, once in correlated subquery), joins with store (~1K rows, filtered to TN) and customer (~100K rows). Outputs ≤100 rows after filtering and limit.

### Performance Bottleneck
The dominant cost (50% each for CTE and main query) stems from **re-scanning the same aggregated data multiple times with expensive correlated execution**:

1. **CTE materialization cost**: The CTE performs a large join-aggregation that must complete before main query execution.
2. **Correlated subquery inefficiency**: For each store-customer pair in TN (`ctr1`), the subquery re-scans the **entire CTE** (`ctr2`) to compute per-store averages. This is O(n²) relative to store count.
3. **Missed early filtering**: The CTE aggregates returns for **all stores/years**, but the main query only needs stores in Tennessee (TN). The store filter happens after aggregation.

The **mechanism** is: materializing a large CTE + correlated subquery that rescans it per outer row + late predicate application.

### Recommended Approach
**Combine A + B + C**:
1. Create a `dates_2000` CTE to isolate year filter.
2. Create a `store_TN` CTE or directly filter stores in main CTE.
3. Compute `store_avg_return` CTE (store-level averages) using the same date filter.
4. Compute `customer_total_return` CTE with early store+date filters.
5. In main query, join `customer_total_return` with `store_avg_return` on store key, and compare `ctr_total_return > store_avg`.
6. Remove correlated subquery entirely.

**Implementation sketch**:
```sql
WITH dates_2000 AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000),
store_avg_return AS (
  SELECT sr_store_sk, AVG(sr_return_amt)*1.2 AS store_avg
  FROM store_returns JOIN dates_2000 ON sr_returned_date_sk = d_date_sk
  GROUP BY sr_store_sk
),
customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         SUM(sr_return_amt) AS ctr_total_return
  FROM store_returns
  JOIN dates_2000 ON sr_returned_date_sk = d_date_sk
  JOIN store ON sr_store_sk = s_store_sk AND s_state = 'TN'
  GROUP BY sr_customer_sk, sr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1
JOIN store_avg_return sar ON ctr1.ctr_store_sk = sar.sr_store_sk
JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk
WHERE ctr_total_return > store_avg
ORDER BY c_customer_id LIMIT 100;
```

## 6. EXAMPLE SELECTION

FAISS selected `decorrelate`, `materialize_cte`, `union_cte_split`. Only `decorrelate` is highly relevant. Better choices are:

**EXAMPLES: decorrelate, early_filter, pushdown**

**Rationale**:
- `decorrelate`: Directly addresses the costly correlated subquery.
- `early_filter`: Pushing `s_state = 'TN'` into CTE reduces data volume early.
- `pushdown`: Pushing date filter into CTE improves join selectivity.

`date_cte_isolate` is less relevant because date filtering is already in WHERE clause; the bigger issue is store filtering and correlation.

Apply the recommended strategy above. The analysis has already identified the bottleneck and the specific structural change needed. Focus on implementing it correctly while preserving semantic equivalence.