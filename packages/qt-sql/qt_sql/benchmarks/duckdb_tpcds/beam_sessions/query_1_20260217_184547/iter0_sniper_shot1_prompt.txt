## Role

You are the **Beam Sniper** for SQL optimization on the target runtime dialect.

You receive the full Battle Damage Assessment (BDA) from 4-16 single-transform probes.
You are an evidence-informed analyst: you now have both wide knowledge and query-specific empirical results.

Your task: produce **exactly TWO optimization attempts** as compound PatchPlan candidates.

You may:
- combine winning worker ideas into one SQL patch when compatible
- introduce a new transform not tried by workers when evidence shows workers missed the real bottleneck

You must:
- ground decisions in BDA plus explain deltas
- preserve semantics
- avoid known regressions

---

## Prompt Map (cache friendly)

### Phase A - Cached Context (static)
A1. Dialect reminders plus regression registry
A2. Combination hazards (duplication, multiplicity, CTE fences)
A3. Evidence-first decision procedure (mechanical)
A4. Sniper output contract (strict JSON array)

### Phase B - Query-Specific Input (dynamic; after cache boundary)
B1. Importance star rating (1-3)
B2. Original SQL plus original plan
B3. IR structure plus anchor hashes
B4. BDA table (ALL probes: status, failure_category, speedup, explain delta, failure reasons)
B5. Worker SQL patch outcomes (full rewritten SQL per probe plus top EXPLAIN nodes plus model description)
B6. Schema excerpt (tables, columns, keys, indexes)
B7. Engine-specific knowledge profile (strengths, gaps, contraindications)

---

## Dialect reminders

Use runtime-injected **Engine-Specific Knowledge** as authoritative.
If static defaults conflict with runtime profile, follow runtime profile.

---

## Regression Registry (hard bans)

Do not produce a sniper plan that:
- forces materialization of a simple EXISTS already planned as a semi-join
- duplicates base scans (orphaned original scans after replacement)
- introduces unfiltered massive CTEs
- builds over-deep fact chains that lock join order
- applies same-column OR to UNION ALL by default on PostgreSQL

OR to UNION exception for PostgreSQL:
- only consider it when EXPLAIN evidence shows OR blocks index usage and UNION branches become index scans

---

## Combination hazards (what to watch)

- **Duplicate sources**: merging two plans that each add a filtered fact CTE can scan the same fact twice.
- **Join multiplicity**: turning EXISTS into JOIN can multiply rows unless keys are unique or aggregated.
- **CTE fences**: materialized CTEs can block pushdown and join reorder.
- **Overlapping edits**: if two probes edit the same anchor or predicate, unify them in one rewrite.

---

## Evidence-first decision procedure (mechanical)

1) Read the BDA table:
   - identify best verified winners: PASS/WIN with real speedup and stable equivalence
   - identify near misses: strong plan shape but `failure_category` indicates syntax or fixable equivalence issue
   - identify what still dominates: use explain deltas and original plan to find remaining hotspot

2) Choose a foundation:
   - prefer the best verified winner as the base
   - if none pass, base on the original query and propose the most justified fix

3) Decide the next move:
   - **combine** one compatible improvement from another passing probe if it targets a different hotspot and avoids hazards
   - **invent** one new transform not attempted if workers missed the hotspot, justified by plan evidence
   - for portability-style moves, proceed only when beam evidence and EXPLAIN deltas support transferability and runtime engine knowledge does not contradict it

3.5) Merge-conflict check before output:
   - list each selected source probe as pairs of `(op, target)`
   - if two probes touch the same `(op, target)`, unify into one final edit step
   - never emit sequential overlapping edits that can overwrite each other

4) Produce exactly two PatchPlans:
   - prefer 1-3 steps per plan; if more than 3, justify in `risk_notes`
   - use operationally targeted edits (prefer insert_cte/replace_from/replace_where_predicate)
   - payload SQL must be complete and executable
   - if only one pathway is defensible, make plan 2 an explicit pass-through with `steps: []` and explain why

5) Provide expected EXPLAIN deltas and risks:
   - what should change if it works (operators, loops, rows)
   - biggest semantic risks
   - optional fallback probe if compound plan fails

---

## Sniper Output Contract (MUST follow)

Tier-0 output contract:
- response must be valid JSON
- first character must be `[` (no leading whitespace or newlines)
- top-level value must be an array of exactly two objects
- no markdown fences, no prose, no commentary

Schema rules:
- each object must include: `plan_id`, `dialect`, `hypothesis`, `target_ir`, `steps`
- optional `based_on` must be a string, never an array
- do not emit key `sql`; use `sql_fragment` where SQL fragment payload is required
- steps must target `{"by_node_id":"S0"}` unless an anchor hash is explicitly required

Allowed ops:
- insert_cte
- replace_from
- replace_where_predicate
- replace_body
- replace_expr_subtree
- delete_expr_subtree
- replace_join_condition
- replace_select
- replace_block_with_cte_pair
- wrap_query_with_cte

SQL payload rules:
- `replace_body`, `replace_select`, and `replace_block_with_cte_pair` must place SQL in `payload.sql_fragment`
- payload SQL must be complete and executable

Output JSON shape:
[
  {
    "plan_id": "snipe_p1",
    "dialect": "<target_dialect>",
    "confidence": 0.81,
    "based_on": "p03,p11",
    "strategy": "Foundation plus one compatible add-on",
    "hypothesis": "Plan evidence and expected win mechanism",
    "target_ir": "Short structural description of final query shape",
    "steps": [
      {
        "step_id": "s1",
        "op": "replace_body",
        "target": {"by_node_id": "S0"},
        "payload": {"sql_fragment": "SELECT c_customer_sk FROM customer"}
      }
    ]
  },
  {
    "plan_id": "snipe_p2",
    "dialect": "<target_dialect>",
    "confidence": 0.73,
    "based_on": "p07",
    "strategy": "Alternative independent pathway",
    "hypothesis": "Plan evidence for second pathway",
    "target_ir": "Alternative structural description",
    "steps": [
      {
        "step_id": "s1",
        "op": "insert_cte",
        "target": {"by_node_id": "S0"},
        "payload": {
          "cte_name": "filtered_sales",
          "cte_query_sql": "SELECT ss_customer_sk FROM store_sales WHERE ss_quantity > 0"
        }
      }
    ]
  }
]

Worked example (fully valid output):
[
  {
    "plan_id": "snipe_p1",
    "dialect": "postgres",
    "confidence": 0.84,
    "based_on": "p03,p09",
    "strategy": "Take winning keyset rewrite and fix multiplicity with DISTINCT guard",
    "hypothesis": "p03 reduced nested loop rescans but duplicated rows after join expansion; adding a distinct keyset preserves EXISTS semantics while keeping the same spine reduction.",
    "expected_explain_delta": "Nested Loop on correlated branch is replaced by Hash Join on distinct keyset and total loops on inner fact subtree drop substantially.",
    "target_ir": "Build filtered_keys CTE and join it to customer; preserve original WHERE predicates not part of EXISTS rewrite.",
    "steps": [
      {
        "step_id": "s1",
        "op": "insert_cte",
        "target": {"by_node_id": "S0"},
        "payload": {
          "cte_name": "filtered_keys",
          "cte_query_sql": "SELECT DISTINCT ss_customer_sk AS customer_sk FROM store_sales WHERE ss_list_price BETWEEN 80 AND 169"
        }
      },
      {
        "step_id": "s2",
        "op": "replace_from",
        "target": {"by_node_id": "S0"},
        "payload": {
          "from_sql": "customer c JOIN filtered_keys fk ON fk.customer_sk = c.c_customer_sk"
        }
      },
      {
        "step_id": "s3",
        "op": "replace_where_predicate",
        "target": {"by_node_id": "S0"},
        "payload": {
          "expr_sql": "c.c_current_addr_sk IS NOT NULL"
        }
      }
    ]
  },
  {
    "plan_id": "snipe_p2",
    "dialect": "postgres",
    "confidence": 0.58,
    "based_on": "p03",
    "strategy": "Pass-through second candidate",
    "hypothesis": "No second independent rewrite has sufficient evidence beyond plan 1 without violating regression gates.",
    "expected_explain_delta": "No change expected; this preserves a safe fallback while satisfying two-plan contract.",
    "target_ir": "No structural change.",
    "steps": []
  }
]

---

## Cache Boundary
Everything below is query-specific input.

## Query ID
query_1

## Runtime Dialect Contract
- target_dialect: duckdb
- runtime_dialect_is_source_of_truth: true
- if static examples conflict, follow runtime dialect behavior

## Importance
- importance_stars: 1
- importance_label: *

## Original SQL
```sql
-- start query 1 in stream 0 using template query1.tpl
with customer_total_return as
(select sr_customer_sk as ctr_customer_sk
,sr_store_sk as ctr_store_sk
,sum(SR_FEE) as ctr_total_return
from store_returns
,date_dim
where sr_returned_date_sk = d_date_sk
and d_year =2000
group by sr_customer_sk
,sr_store_sk)
 select c_customer_id
from customer_total_return ctr1
,store
,customer
where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2
from customer_total_return ctr2
where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
and s_store_sk = ctr1.ctr_store_sk
and s_state = 'SD'
and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
 LIMIT 100;

-- end query 1 in stream 0 using template query1.tpl

```

## Original Plan
```
Total execution time: 253ms

CTE [0 rows, 5.2ms, 2%]
  HASH_GROUP_BY [539K rows, 124.6ms, 49%]
    HASH_JOIN INNER on sr_returned_date_sk = d_date_sk [558K rows, 5.8ms, 2%]
      SEQ_SCAN  store_returns [558K of 69.1M rows, 23.7ms, 9%]
      FILTER [366 rows]
        SEQ_SCAN  date_dim [366 of 73K rows, 0.1ms]  Filters: d_year=2000
  TOP_N [100 rows, 5.0ms, 2%]
    FILTER [62K rows, 5.1ms, 2%]
      LEFT_DELIM_JOIN LEFT on ctr_store_sk IS NOT DISTINCT FROM ctr_store_sk [0 rows, 6.9ms, 3%]
        HASH_JOIN INNER on c_customer_sk = ctr_customer_sk [158K rows, 24.6ms, 10%]
          SEQ_SCAN  customer [500K of 2.5M rows, 6.2ms, 2%]
          HASH_JOIN INNER on ctr_store_sk = s_store_sk [158K rows, 2.9ms, 1%]
            CTE_SCAN [539K rows, 1.8ms]
            SEQ_SCAN  store [35 of 102 rows]  Filters: s_state='SD'
        HASH_JOIN LEFT on ctr_store_sk IS NOT DISTINCT FROM ctr_store_sk [158K rows, 14.8ms, 6%]
          HASH_GROUP_BY [15 rows, 15.2ms, 6%]
            HASH_JOIN INNER on ctr_store_sk = ctr_store_sk [158K rows, 3.7ms, 1%]
              CTE_SCAN [539K rows, 1.4ms]
              DELIM_SCAN [0 rows]
        HASH_GROUP_BY [15 rows, 1.0ms]
```

## IR Structure + Anchor Hashes
```
S0 [SELECT]
  CTE: customer_total_return  (via CTE_Q_S0_customer_total_return)
    FROM: store_returns, date_dim
    WHERE [eb0f6bc97f7168d4]: sr_returned_date_sk = d_date_sk AND d_year = 2000
    GROUP BY: sr_customer_sk, sr_store_sk
  MAIN QUERY (via Q_S0)
    FROM: customer_total_return ctr1, store, customer
    WHERE [e5b7485395ff5a80]: ctr1.ctr_total_return > (SELECT AVG(ctr_total_return) * 1.2 FROM customer_total_return AS ctr2 WH...
    ORDER BY: c_customer_id
S1 [OTHER_DDL]

Patch operations (core+advanced): insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree, replace_body, replace_join_condition, replace_select, replace_block_with_cte_pair, wrap_query_with_cte
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

## Schema / Index / Stats Context
- source: duckdb
- referenced_tables: 4

| Table | Rows(est) | PK | Indexes |
|-------|-----------|----|---------|
| customer | 500000 | - | - |
| date_dim | 73049 | - | - |
| store | 102 | - | - |
| store_returns | 2877532 | - | - |

### Column Signatures
| Table | Column | Type | Nullable | Key Hint |
|-------|--------|------|----------|----------|
| customer | c_customer_sk | INTEGER | YES | - |
| customer | c_customer_id | VARCHAR | YES | - |
| customer | c_current_cdemo_sk | INTEGER | YES | - |
| customer | c_current_hdemo_sk | INTEGER | YES | - |
| customer | c_current_addr_sk | INTEGER | YES | - |
| customer | c_first_shipto_date_sk | INTEGER | YES | - |
| customer | c_first_sales_date_sk | INTEGER | YES | - |
| customer | c_salutation | VARCHAR | YES | - |
| customer | c_first_name | VARCHAR | YES | - |
| customer | c_last_name | VARCHAR | YES | - |
| customer | c_preferred_cust_flag | VARCHAR | YES | - |
| customer | c_birth_day | INTEGER | YES | - |
| customer | c_birth_month | INTEGER | YES | - |
| customer | c_birth_year | INTEGER | YES | - |
| customer | c_birth_country | VARCHAR | YES | - |
| customer | c_login | VARCHAR | YES | - |
| customer | c_email_address | VARCHAR | YES | - |
| customer | c_last_review_date_sk | INTEGER | YES | - |
| date_dim | d_date_sk | INTEGER | YES | - |
| date_dim | d_date_id | VARCHAR | YES | - |
| date_dim | d_date | DATE | YES | - |
| date_dim | d_month_seq | INTEGER | YES | - |
| date_dim | d_week_seq | INTEGER | YES | - |
| date_dim | d_quarter_seq | INTEGER | YES | - |
| date_dim | d_year | INTEGER | YES | - |
| date_dim | d_dow | INTEGER | YES | - |
| date_dim | d_moy | INTEGER | YES | - |
| date_dim | d_dom | INTEGER | YES | - |
| date_dim | d_qoy | INTEGER | YES | - |
| date_dim | d_fy_year | INTEGER | YES | - |
| date_dim | d_fy_quarter_seq | INTEGER | YES | - |
| date_dim | d_fy_week_seq | INTEGER | YES | - |
| date_dim | d_day_name | VARCHAR | YES | - |
| date_dim | d_quarter_name | VARCHAR | YES | - |
| date_dim | d_holiday | VARCHAR | YES | - |
| date_dim | d_weekend | VARCHAR | YES | - |
| date_dim | d_following_holiday | VARCHAR | YES | - |
| date_dim | d_first_dom | INTEGER | YES | - |
| date_dim | d_last_dom | INTEGER | YES | - |
| date_dim | d_same_day_ly | INTEGER | YES | - |
| date_dim | d_same_day_lq | INTEGER | YES | - |
| date_dim | d_current_day | VARCHAR | YES | - |
| store | s_store_sk | INTEGER | YES | - |
| store | s_store_id | VARCHAR | YES | - |
| store | s_rec_start_date | DATE | YES | - |
| store | s_rec_end_date | DATE | YES | - |
| store | s_closed_date_sk | INTEGER | YES | - |
| store | s_store_name | VARCHAR | YES | - |
| store | s_number_employees | INTEGER | YES | - |
| store | s_floor_space | INTEGER | YES | - |
| store | s_hours | VARCHAR | YES | - |
| store | s_manager | VARCHAR | YES | - |
| store | s_market_id | INTEGER | YES | - |
| store | s_geography_class | VARCHAR | YES | - |
| store | s_market_desc | VARCHAR | YES | - |
| store | s_market_manager | VARCHAR | YES | - |
| store | s_division_id | INTEGER | YES | - |
| store | s_division_name | VARCHAR | YES | - |
| store | s_company_id | INTEGER | YES | - |
| store | s_company_name | VARCHAR | YES | - |
| store | s_street_number | VARCHAR | YES | - |
| store | s_street_name | VARCHAR | YES | - |
| store | s_street_type | VARCHAR | YES | - |
| store | s_suite_number | VARCHAR | YES | - |
| store | s_city | VARCHAR | YES | - |
| store | s_county | VARCHAR | YES | - |
| store_returns | sr_returned_date_sk | INTEGER | YES | - |
| store_returns | sr_return_time_sk | INTEGER | YES | - |
| store_returns | sr_item_sk | INTEGER | YES | - |
| store_returns | sr_customer_sk | INTEGER | YES | - |
| store_returns | sr_cdemo_sk | INTEGER | YES | - |
| store_returns | sr_hdemo_sk | INTEGER | YES | - |
| store_returns | sr_addr_sk | INTEGER | YES | - |
| store_returns | sr_store_sk | INTEGER | YES | - |
| store_returns | sr_reason_sk | INTEGER | YES | - |
| store_returns | sr_ticket_number | INTEGER | YES | - |
| store_returns | sr_return_quantity | INTEGER | YES | - |
| store_returns | sr_return_amt | DECIMAL(7,2) | YES | - |
| store_returns | sr_return_tax | DECIMAL(7,2) | YES | - |
| store_returns | sr_return_amt_inc_tax | DECIMAL(7,2) | YES | - |
| store_returns | sr_fee | DECIMAL(7,2) | YES | - |
| store_returns | sr_return_ship_cost | DECIMAL(7,2) | YES | - |
| store_returns | sr_refunded_cash | DECIMAL(7,2) | YES | - |
| store_returns | sr_reversed_charge | DECIMAL(7,2) | YES | - |
| store_returns | sr_store_credit | DECIMAL(7,2) | YES | - |
| store_returns | sr_net_loss | DECIMAL(7,2) | YES | - |

## Engine-Specific Knowledge
## Dialect Profile (DUCKDB)

**Combined Intelligence Baseline**: Field intelligence from 88 TPC-DS queries at SF1-SF10. Use it to guide analysis but apply your own judgment — every query is different.

### Optimizer Strengths (don't fight these)
- `INTRA_SCAN_PREDICATE_PUSHDOWN`: If EXPLAIN shows the filter inside the scan node, do not create a CTE to push it.
- `SAME_COLUMN_OR`: Never split same-column ORs into UNION ALL. 0.59x and 0.23x observed.
- `HASH_JOIN_SELECTION`: Focus on reducing join inputs, not reordering joins.
- `CTE_INLINING`: Single-ref CTEs are free — use for clarity. CTE-based strategies are low-cost on DuckDB.

### Known Gaps (exploit these)
- `CROSS_CTE_PREDICATE_BLINDNESS` [HIGH] detect: Row counts flat through CTE chain, sharp drop at late filter. 2+ stage CTE chain + late predicate with columns available earlier. | action: Move selective predicates INTO the CTE definition. Pre-filter dimensions/facts before materialization.
- `REDUNDANT_SCAN_ELIMINATION` [HIGH] detect: N separate SEQ_SCAN nodes on same table, identical joins, different bucket filters. | action: Consolidate N subqueries into 1 scan with CASE WHEN / FILTER() inside aggregates.
- `LEFT_JOIN_FILTER_ORDER_RIGIDITY` [HIGH] detect: LEFT JOIN + WHERE on right-table column (proves right non-null). | action: Convert LEFT→INNER when WHERE proves right non-null, or pre-filter dimension into CTE.
- `AGGREGATE_BELOW_JOIN_BLINDNESS` [HIGH] detect: GROUP BY input rows >> distinct keys, aggregate node sits after join. | action: Pre-aggregate fact table by join key BEFORE dimension join.
- `CROSS_COLUMN_OR_DECOMPOSITION` [MEDIUM] detect: Single scan, OR across DIFFERENT columns, 70%+ rows discarded. CRITICAL: same column in all OR arms → STOP. | action: Split cross-column ORs into UNION ALL branches with targeted single-column filters.

## Dispatcher Hypothesis
CTE materialization (539K rows) followed by two CTE_SCANs and a correlated subquery implemented via LEFT_DELIM_JOIN / HASH_JOIN LEFT is efficient for DuckDB (253ms total). However, the HASH_GROUP_BY (124.6ms, 49%) on store_returns after join with date_dim is the dominant cost; early filtering on date_dim is already present (366 rows).

## Dispatcher Reasoning Trace
- Cost spine: HASH_GROUP_BY (124.6ms) → HASH_JOIN INNER on c_customer_sk (24.6ms) → HASH_JOIN LEFT (14.8ms) → TOP_N (5.0ms).
- No nested loops, no repeated scans, no set ops, no comma joins → families B, D, E, F less likely.
- Plan already uses CTE materialization (CTE_SCAN) and hash joins; DuckDB's CTE_INLINING strength makes further CTE transforms low-impact.
- Low importance (★) → lower probe count (5) with focus on A (early filtering) and C (aggregation pushdown) families.

## Equivalence Tier
- unordered

## Additional Intelligence
### AST Feature Detection

- **decorrelate**: 100% match (AGG_AVG, AGG_SUM, CORRELATED_SUB, CTE) (gap: CORRELATED_SUBQUERY_PARALYSIS) [CAUTION: MISSING_FILTER, ALREADY_DECORRELATED]  [SUPPORT: native_or_universal]
- **prefetch_fact_join**: 100% match (AGG_SUM, DATE_DIM, GROUP_BY, STAR_JOIN) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: MAX_2_CHAINS]  [SUPPORT: native_or_universal]
- **sf_shared_scan_decorrelate**: 100% match (AGG_AVG, CORRELATED_SUB, CTE, SCALAR_AGG_SUB_CTE) (gap: CORRELATED_SUBQUERY_PARALYSIS) [SUPPORT: portability_candidate; engines=snowflake]
- **dimension_cte_isolate**: 100% match (DATE_DIM, GROUP_BY, MULTI_TABLE_5+) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: CROSS_JOIN_3_DIMS, UNFILTERED_CTE]  [SUPPORT: native_or_universal]
- **self_join_decomposition**: 100% match (AGG_AVG, CTE, GROUP_BY) (gap: CROSS_CTE_PREDICATE_BLINDNESS)  [SUPPORT: native_or_universal]


## Probe Summary
5 probes fired, 1 passed validation, 1 showed speedup.

## BDA Table (all probes)

| Probe | Transform | Family | Status | Failure Category | Speedup | Top EXPLAIN Nodes | Model Description | SQL Patch | Error/Notes |
|-------|-----------|--------|--------|------------------|---------|-------------------|-------------------|-----------|-------------|
| p03 | prefetch_fact_join | A | FAIL | equivalence_fail | - | - | Create a CTE that pre-filters date_dim (d_year=2000) and store (s_state='SD'), then join with store_returns and customer in a staged pipeline to reduce rows before aggregation. | p03 | Synthetic semantic mismatch: Optimized query failed: Binder Error: Referenced column "s_state" not found in FROM clause! Candidate bindings: "s_store_sk", "ctr_store_sk", "c_first_name", "ctr_customer_sk", "c_first_sales_date_sk" LINE 1: ... = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk... ^ |
| p01 | aggregate_pushdown | C | NEUTRAL | none | 1.05x | SEQ_SCAN store_returns [558K rows] (1258ms); HASH_GROUP_BY [539K rows] (97ms) | Push aggregation below the date_dim join: pre-aggregate store_returns by sr_customer_sk, sr_store_sk, sr_returned_date_sk before joining with date_dim, then sum the pre-aggregated fees per customer/store. | p01 |  |
| p04 | decorrelate | B | WIN | none | 1.53x | SEQ_SCAN store_returns [558K rows] (1359ms); HASH_GROUP_BY [539K rows] (96ms) | Rewrite the correlated scalar subquery (SELECT AVG(ctr_total_return)*1.2 FROM customer_total_return ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) as a standalone CTE computing per-store averages, then JOIN in main WHERE clause. | p04 |  |
| p02 | channel_bitmap_aggregation | C | FAIL | equivalence_fail | - | - | Consolidate the two references to customer_total_return CTE (ctr1, ctr2) into a single CTE that computes both the per-store average and the per-customer/store total in one pass, eliminating the second CTE_SCAN. | p02 | Synthetic semantic mismatch: Optimized query failed: Binder Error: Values list "ctr1" does not have a column named "ctr_store_avg" LINE 1: ... AS ctr1, store, customer WHERE ctr1.ctr_total_return > ctr1.ctr_store_avg * 1.2 AND s_store_sk = ctr1.ctr_store_sk... ^ |
| p05 | sf_shared_scan_decorrelate | B | FAIL | syntax_error | - | - | Use Snowflake-style shared-scan decorrelation: create a CTE with store_returns+date_dim join, then compute per-store averages and per-customer/store totals in separate CTEs from that shared scan, then join in main query. | p05 | Retry failed: Failed to parse/apply PatchPlan |

## Worker SQL Patches

### p03: prefetch_fact_join (FAIL, n/a)
```sql
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk), filtered_store AS (SELECT s_store_sk FROM store WHERE s_state = 'SD') SELECT c_customer_id FROM customer_total_return AS ctr1, filtered_store AS s, customer WHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return) * 1.2 FROM customer_total_return AS ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100;

/* end query 1 in stream 0 using template query1.tpl */;
```

### p01: aggregate_pushdown (NEUTRAL, 1.05x)
```sql
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return AS ctr1, store, customer WHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return) * 1.2 FROM customer_total_return AS ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100;

/* end query 1 in stream 0 using template query1.tpl */;
```

### p04: decorrelate (WIN, 1.53x)
```sql
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk), store_averages AS (SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS avg_return FROM customer_total_return GROUP BY ctr_store_sk) SELECT c_customer_id FROM customer_total_return AS ctr1 JOIN store_averages AS sa ON ctr1.ctr_store_sk = sa.ctr_store_sk, store AS s, customer AS c WHERE s.s_store_sk = ctr1.ctr_store_sk AND c.c_customer_sk = ctr1.ctr_customer_sk AND s.s_state = 'SD' AND ctr1.ctr_total_return > sa.avg_return ORDER BY c_customer_id LIMIT 100;

/* end query 1 in stream 0 using template query1.tpl */;
```

### p02: channel_bitmap_aggregation (FAIL, n/a)
```sql
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return AS ctr1, store, customer WHERE ctr1.ctr_total_return > ctr1.ctr_store_avg * 1.2 AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100;

/* end query 1 in stream 0 using template query1.tpl */;
```

### p05: sf_shared_scan_decorrelate (FAIL, n/a)
```sql
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total AS ctr1 JOIN store_avg AS sa ON ctr1.ctr_store_sk = sa.ctr_store_sk JOIN store AS s ON s.s_store_sk = ctr1.ctr_store_sk JOIN customer AS c ON ctr1.ctr_customer_sk = c.c_customer_sk WHERE s.s_state = 'SD' AND ctr1.ctr_total_return > sa.avg_threshold ORDER BY c_customer_id LIMIT 100;

/* end query 1 in stream 0 using template query1.tpl */;
```
