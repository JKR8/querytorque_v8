## Role

You are a SQL optimization analyst. Your task is to analyze this query, identify the primary bottleneck, and design structural optimization targets.

For each target, describe the STRUCTURAL SHAPE of the optimized query using an IR node map (CTE names, FROM tables, WHERE conditions, GROUP BY, ORDER BY). A separate code-generation worker will convert your targets into executable patch plans.

Identify the primary bottleneck. Only provide secondary targets if they are distinct and high-confidence. **Quality > Quantity.**

## Query: query_35

**Dialect**: DUCKDB

```sql
-- start query 35 in stream 0 using template query35.tpl
select  
  ca_state,
  cd_gender,
  cd_marital_status,
  cd_dep_count,
  count(*) cnt1,
  max(cd_dep_count),
  sum(cd_dep_count),
  max(cd_dep_count),
  cd_dep_employed_count,
  count(*) cnt2,
  max(cd_dep_employed_count),
  sum(cd_dep_employed_count),
  max(cd_dep_employed_count),
  cd_dep_college_count,
  count(*) cnt3,
  max(cd_dep_college_count),
  sum(cd_dep_college_count),
  max(cd_dep_college_count)
 from
  customer c,customer_address ca,customer_demographics
 where
  c.c_current_addr_sk = ca.ca_address_sk and
  cd_demo_sk = c.c_current_cdemo_sk and 
  exists (select *
          from store_sales,date_dim
          where c.c_customer_sk = ss_customer_sk and
                ss_sold_date_sk = d_date_sk and
                d_year = 2001 and
                d_qoy < 4) and
   (exists (select *
            from web_sales,date_dim
            where c.c_customer_sk = ws_bill_customer_sk and
                  ws_sold_date_sk = d_date_sk and
                  d_year = 2001 and
                  d_qoy < 4) or 
    exists (select * 
            from catalog_sales,date_dim
            where c.c_customer_sk = cs_ship_customer_sk and
                  cs_sold_date_sk = d_date_sk and
                  d_year = 2001 and
                  d_qoy < 4))
 group by ca_state,
          cd_gender,
          cd_marital_status,
          cd_dep_count,
          cd_dep_employed_count,
          cd_dep_college_count
 order by ca_state,
          cd_gender,
          cd_marital_status,
          cd_dep_count,
          cd_dep_employed_count,
          cd_dep_college_count
 LIMIT 100;

-- end query 35 in stream 0 using template query35.tpl
```


## Current Execution Plan

```
Total execution time: 7543ms

TOP_N [100 rows, 2.4ms]
  HASH_GROUP_BY [58K rows, 65.9ms]
    FILTER [77K rows, 6.5ms]
      LEFT_DELIM_JOIN MARK on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [0 rows, 44.2ms]
        LEFT_DELIM_JOIN MARK on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [0 rows, 67.9ms]
          LEFT_DELIM_JOIN SEMI on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [0 rows, 393.0ms, 5%]
            HASH_JOIN INNER on cd_demo_sk = c_current_cdemo_sk [483K rows, 110.3ms, 1%]
              SEQ_SCAN  customer_demographics [1.9M of 30.7M rows, 93.5ms, 1%]  Filters: cd_demo_sk>=4 AND cd_demo_sk<=1920791
              HASH_JOIN INNER on c_current_addr_sk = ca_address_sk [500K rows, 20.7ms]
                SEQ_SCAN  customer [500K of 2.5M rows, 36.0ms]
                SEQ_SCAN  customer_address [250K of 750K rows, 9.7ms]
            HASH_JOIN SEMI on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [200K rows, 267.7ms, 4%]
              HASH_JOIN INNER on c_customer_sk = ss_customer_sk [2.9M rows, 545.9ms, 7%]
                DELIM_SCAN [0 rows]
                HASH_JOIN INNER on ss_sold_date_sk = d_date_sk [3.1M rows, 47.6ms]
                  SEQ_SCAN  store_sales [3.1M of 806.4M rows, 4297.6ms, 57%]
                  FILTER [274 rows]
                    SEQ_SCAN  date_dim [274 of 73K rows, 2.6ms]  Filters: d_year=2001, d_qoy<4
            HASH_GROUP_BY [483K rows, 75.8ms, 1%]
          HASH_JOIN MARK on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [200K rows, 20.6ms]
            HASH_JOIN INNER on c_customer_sk = ws_bill_customer_sk [324K rows, 82.7ms, 1%]
              DELIM_SCAN [0 rows]
              HASH_JOIN INNER on ws_sold_date_sk = d_date_sk [803K rows, 5.1ms]
                SEQ_SCAN  web_sales [803K of 201.5M rows, 1078.8ms, 14%]
                FILTER [274 rows]
                  SEQ_SCAN  date_dim [274 of 73K rows, 2.6ms]  Filters: d_year=2001, d_qoy<4
          HASH_GROUP_BY [200K rows, 11.6ms]
        HASH_JOIN MARK on c_customer_sk IS NOT DISTINCT FROM c_customer_sk [200K rows, 45.0ms]
          HASH_JOIN INNER on cs_ship_customer_sk = c_customer_sk [635K rows, 6.1ms]
            HASH_JOIN INNER on cs_sold_date_sk = d_date_sk [1.6M rows, 3.0ms]
              SEQ_SCAN  catalog_sales [1.6M of 403.2M rows, 170.9ms, 2%]
              FILTER [274 rows]
                SEQ_SCAN  date_dim [274 of 73K rows, 2.4ms]  Filters: d_year=2001, d_qoy<4
            DELIM_SCAN [0 rows]
        HASH_GROUP_BY [200K rows, 8.0ms]
```


## IR Structure (for patch targeting)

```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: customer c, customer_address ca, customer_demographics
    WHERE [8ec0dc781ff406d9]: c.c_current_addr_sk = ca.ca_address_sk AND cd_demo_sk = c.c_current_cdemo_sk AND EXISTS(SELECT * ...
    GROUP BY: ca_state, cd_gender, cd_marital_status, cd_dep_count, cd_dep_employed_count, cd_dep_college_count
    ORDER BY: ca_state, cd_gender, cd_marital_status, cd_dep_count, cd_dep_employed_count, cd_dep_college_count
S1 [OTHER_DDL]

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**Note**: Use `by_node_id` (e.g., "S0") and `by_anchor_hash` (16-char hex) from map above to target patch operations.


## Optimization Families

Review the 6 families below. Each has a proven gold example.

Choose up to **4 most relevant families** for this query based on:
- Query structure (CTEs, subqueries, joins, aggregations, set operations)
- Execution plan signals (WHERE placement, repeated scans, correlated subqueries)
- Problem signature (cardinality estimation errors, loops vs sets, filter ordering)



### Family A: Early Filtering (Predicate Pushback)
**Description**: Push small filters into CTEs early, reduce row count before expensive operations
**Speedup Range**: 1.3–4.0x (~35% of all wins)
**Use When**:
  1. Late WHERE filters on dimension tables
  2. Cascading CTEs with filters applied downstream
  3. Expensive joins after filters could be pushed earlier

**Gold Example**: `date_cte_isolate` (4.00x)



### Family B: Decorrelation (Sets Over Loops)
**Description**: Convert correlated subqueries to standalone CTEs with GROUP BY, eliminate per-row re-execution
**Speedup Range**: 2.4–2.9x (~15% of all wins)
**Use When**:
  1. Correlated subqueries in WHERE clause
  2. Scalar aggregates computed per outer row
  3. DELIM_SCAN in execution plan (indicates correlation)

**Gold Example**: `decorrelate` (2.92x)



### Family C: Aggregation Pushdown (Minimize Rows Touched)
**Description**: Aggregate before expensive joins when GROUP BY keys ⊇ join keys, reduce intermediate sizes
**Speedup Range**: 1.3–15.3x (~5% of all wins (high variance))
**Use When**:
  1. GROUP BY happens after large joins
  2. GROUP BY keys are subset of join keys
  3. Intermediate result size >> final result size

**Gold Example**: `aggregate_pushdown` (42.90x)



### Family D: Set Operation Optimization (Sets Over Loops)
**Description**: Replace INTERSECT/UNION-based patterns with EXISTS/NOT EXISTS, avoid full materialization
**Speedup Range**: 1.7–2.7x (~8% of all wins)
**Use When**:
  1. INTERSECT patterns between large sets
  2. UNION ALL with duplicate elimination
  3. Set operations materializing full intermediate results

**Gold Example**: `intersect_to_exists` (1.83x)



### Family E: Materialization / Prefetch (Don't Repeat Work)
**Description**: Extract repeated scans or pre-compute intermediate results for reuse across multiple consumers
**Speedup Range**: 1.3–6.2x (~18% of all wins)
**Use When**:
  1. Repeated scans of same table with different filters
  2. Dimension filters applied independently multiple times
  3. CTE referenced multiple times with implicit re-evaluation

**Gold Example**: `multi_dimension_prefetch` (2.71x)



### Family F: Join Transform (Right Shape First)
**Description**: Restructure join topology: convert comma joins to explicit INNER JOIN, optimize join order, eliminate self-joins via single-pass aggregation
**Speedup Range**: 1.8–8.6x (~19% of all wins)
**Use When**:
  1. Comma-separated joins (implicit cross joins) in FROM clause
  2. Self-joins scanning same table multiple times
  3. Dimension-fact join order suboptimal for predicate pushdown

**Gold Example**: `inner_join_conversion` (3.44x)



## Detected Patterns

### AST Feature Detection

- **composite_decorrelate_union**: 100% match (AGG_COUNT, AGG_SUM, DATE_DIM, EXISTS) (gap: CORRELATED_SUBQUERY_PARALYSIS)
- **or_to_union**: 100% match (AGG_SUM, DATE_DIM, GROUP_BY, OR_BRANCH) (gap: CROSS_COLUMN_OR_DECOMPOSITION) [CAUTION: MAX_3_BRANCHES, SAME_COL_OR]
- **prefetch_fact_join**: 100% match (AGG_SUM, DATE_DIM, GROUP_BY, STAR_JOIN) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: MAX_2_CHAINS]
- **dimension_cte_isolate**: 100% match (DATE_DIM, GROUP_BY, MULTI_TABLE_5+) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: CROSS_JOIN_3_DIMS, UNFILTERED_CTE]
- **multi_dimension_prefetch**: 75% match (AGG_SUM, DATE_DIM, GROUP_BY) (gap: CROSS_CTE_PREDICATE_BLINDNESS)
  Missing: CASE_EXPR


**Instruction**: Prioritize detected patterns above. If a high-confidence
pathology is detected, your primary target SHOULD address it.


## Worker Routing

Your targets will be routed to specialized workers:
- **W1 "Reducer"** (Families A, D): Cardinality reduction — early filtering, set operations
- **W2 "Unnester"** (Families B, C): Decorrelation, aggregation pushdown
- **W3 "Builder"** (Families F, E): Join restructuring, materialization/prefetch
- **W4 "Wildcard"** (Dynamic): Deep specialist — your **#1 target** gets maximum effort

The highest-relevance target always goes to W4. Design diverse targets across worker roles for maximum coverage.


## Your Task

Analyze this query against the 6 families above.

Identify the **primary bottleneck**. Only provide secondary targets if they are distinct and high-confidence. Quality > Quantity.

For each target (1 to 4):
1. Describe the bottleneck hypothesis
2. Design a TARGET IR node map showing what the optimized query SHOULD look like
3. Score relevance (0.0–1.0)
4. Recommend which gold example(s) a code-generation worker should use as reference


**Output format**:

```json
[
  {
    "family": "B",
    "transform": "shared_scan_decorrelate",
    "target_id": "t1",
    "relevance_score": 0.95,
    "hypothesis": "Correlated scalar subquery re-scans web_sales per row. Shared-scan variant: inner=outer table with same date filter.",
    "target_ir": "S0 [SELECT]\n  CTE: common_scan  (via Q1)\n    FROM: web_sales, date_dim\n    WHERE: d_date BETWEEN ... AND d_date_sk = ws_sold_date_sk\n  CTE: thresholds  (via Q2)\n    FROM: common_scan\n    GROUP BY: ws_item_sk\n  MAIN QUERY (via Q0)\n    FROM: common_scan cs, item, thresholds t\n    WHERE: i_manufact_id = 320 AND ... AND cs.ws_ext_discount_amt > t.threshold\n    ORDER BY: sum(ws_ext_discount_amt)",
    "recommended_examples": ["sf_shared_scan_decorrelate"]
  }
]
```

**Rules**:
- target_ir must follow the IR node map format (same as Section 4)
- target_ir describes the STRUCTURAL SHAPE of the optimized query (CTE names, FROM tables, WHERE conditions, GROUP BY, ORDER BY)
- recommended_examples: list gold example IDs the worker should use as reference patch template
- Each target should represent a DIFFERENT optimization strategy
- Rank by relevance_score (highest first)
- Output up to 4 targets

After JSON, provide analysis:

## Analysis
For each available family, explain relevance (HIGH / MEDIUM / LOW) in 1-2 sentences.
**Chosen families**: [list]
**Confidence**: High/Medium/Low
