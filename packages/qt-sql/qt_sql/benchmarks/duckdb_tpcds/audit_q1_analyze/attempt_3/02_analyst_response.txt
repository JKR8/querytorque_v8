## 1. STRUCTURAL BREAKDOWN

**customer_total_return CTE**: Computes total return amounts (sum of SR_FEE) for each customer-store combination in year 2000. Scans store_returns (~large fact table) joined with date_dim (~small dimension), outputs ~557K rows of customer-store aggregates.

**main_query**: Finds customers whose total return at a store exceeds 1.2x the average return for that store, limited to stores in South Dakota (SD). Processes ~539K rows from the CTE, joins with store (filtered by state) and customer tables, outputs 100 customer IDs ordered by ID.

## 2. BOTTLENECK IDENTIFICATION

**Dominant bottleneck**: Correlated subquery inside main_query's WHERE clause.

**Mechanism**: For each row in ctr1 (after joining with store and customer), the query executes a separate aggregation (AVG) over the entire CTE (ctr2) filtered by matching store_sk. This creates O(n²) computational complexity where each of ~539K rows triggers a full scan/aggregation of the 557K-row CTE. Even with DuckDB's potential optimizations, this correlation prevents efficient batch processing.

**Secondary issues**:
1. The CTE filters only by year 2000, but the main query further filters stores by state='SD'. This filter happens AFTER the CTE aggregation, missing early reduction opportunity.
2. The ORDER BY c_customer_id + LIMIT 100 suggests we could use early stopping if the data were sorted, but the current plan processes all rows before sorting.

## 3. PROPOSED OPTIMIZATION

### **Optimization 1: Decorrelate the subquery**
- **What**: Extract store-level averages into a separate CTE before the main query, then join instead of correlated filtering.
- **Why**: Eliminates O(n²) behavior by computing store averages once (O(n)) and using hash join (O(n)).
- **Risk**: Must ensure NULL handling for stores with no returns matches original semantics (both CTEs filter same date range).
- **Impact**: Significant (likely 2-5x).

### **Optimization 2: Push store state filter into CTE**
- **What**: Join store table early in the CTE to filter by state='SD' before aggregation.
- **Why**: Reduces CTE cardinality from ~557K to only SD stores before expensive GROUP BY and SUM.
- **Risk**: Changes semantics if store dimension has type 2 SCD issues (different states over time). Verify business logic.
- **Impact**: Moderate (depends on SD store count).

### **Optimization 3: Materialize CTE with store averages**
- **What**: Combine both aggregations (customer-store totals and store averages) in a single CTE scan using window functions.
- **Why**: Single pass over store_returns computes both metrics, avoiding repeated scans.
- **Risk**: Window function over 557K rows may have memory implications, but cheaper than correlated subquery.
- **Impact**: Significant.

## 4. FAILURE ANALYSIS

**Attempt 1 (WIN 1.85x)**: Likely basic decorrelation attempt but missed state filter pushdown, leaving CTE too large.

**Attempt 2 (WIN 1.58x)**: Possibly tried only materialize_cte without addressing correlation, getting limited benefit.

**Lesson**: Need combined approach addressing BOTH correlation AND early filtering for maximum impact.

## 5. RECOMMENDED STRATEGY

Implement **combined decorrelation with early filtering**:

1. Create a filtered_date CTE for year 2000 dates (small dimension prefetch).
2. Create a filtered_stores CTE for SD stores (small dimension prefetch).
3. Create a single CTE that joins store_returns with both filtered dimensions, computes:
   - Customer-store total returns (GROUP BY)
   - Store average returns (via window function or separate aggregation joined)
4. Main query simply filters where customer_total > 1.2 * store_average.

This addresses all three issues: correlation (via precomputed store averages), early filtering (via dimension CTEs), and single pass (consolidated CTE).

Implementation sketch:
```sql
WITH filtered_dates AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000),
     filtered_stores AS (SELECT s_store_sk FROM store WHERE s_state = 'SD'),
     store_returns_filtered AS (
         SELECT sr_customer_sk, sr_store_sk, sr_fee
         FROM store_returns sr
         INNER JOIN filtered_dates fd ON sr.sr_returned_date_sk = fd.d_date_sk
         INNER JOIN filtered_stores fs ON sr.sr_store_sk = fs.s_store_sk
     ),
     customer_store_totals AS (
         SELECT 
             sr_customer_sk AS ctr_customer_sk,
             sr_store_sk AS ctr_store_sk,
             SUM(sr_fee) AS ctr_total_return,
             AVG(SUM(sr_fee)) OVER (PARTITION BY sr_store_sk) AS store_avg_return
         FROM store_returns_filtered
         GROUP BY sr_customer_sk, sr_store_sk
     )
SELECT c_customer_id
FROM customer_store_totals ctr
INNER JOIN customer c ON ctr.ctr_customer_sk = c.c_customer_sk
WHERE ctr.ctr_total_return > ctr.store_avg_return * 1.2
ORDER BY c_customer_id
LIMIT 100;
```

## 6. EXAMPLE SELECTION

FAISS picks are relevant but incomplete. I recommend:

**decorrelate** (core pattern), **early_filter** (push state filter), **dimension_cte_isolate** (pre-filter date and store), **single_pass_aggregation** (combine metrics in one scan)

EXAMPLES: decorrelate, early_filter, dimension_cte_isolate, single_pass_aggregation