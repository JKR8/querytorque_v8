[SCENARIO: duckdb_embedded]
DuckDB runs in-process with configurable memory limit.
No remote spill â€” all data must fit in memory or use
out-of-core execution (significantly slower).

RESOURCE ENVELOPE:
  memory: Configurable via SET memory_limit (default: 80% of system RAM)
  compute: All available CPU cores (auto-parallelism)
  storage_io: Local SSD, columnar parquet/native storage

FAILURE DEFINITIONS:
  [FATAL] query_duration >120s
  [FATAL] out_of_memory any
  [WARNING] out_of_core_execution any

STRATEGY PRIORITIES (ordered):
  1. Reduce intermediate sizes to fit in memory_limit
  2. Exploit columnar scan efficiency (predicate pushdown)
  3. Minimize redundant scans (same table scanned multiple times)
  4. Push filters into CTEs (every CTE must have WHERE)
  5. Pre-aggregate before joins to reduce hash table sizes

AVOID (will fail on this envelope):
  - Cross-joining 3+ dimension CTEs (0.0076x regression)
  - OR splitting on same column (engine handles via BitmapOr equivalent)
  - Materializing EXISTS subqueries (0.14x regression)