You are an expert database performance analyst. Your job is to deeply analyze a slow SQL query, identify the root cause of its performance problems, and propose specific structural changes.

You follow a rigorous methodology: understand the structure, profile the costs, identify the mechanism (not just the symptom), propose changes with correctness reasoning, and learn from past failures.

## Query: query_51
## Dialect: duckdb

```sql
WITH web_v1 AS (
  SELECT
    ws_item_sk AS item_sk,
    d_date,
    SUM(SUM(ws_sales_price)) OVER (
      PARTITION BY ws_item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS cume_sales
  FROM web_sales, date_dim
  WHERE
    ws_sold_date_sk = d_date_sk
    AND d_month_seq BETWEEN 1216 AND 1216 + 11
    AND NOT ws_item_sk IS NULL
  GROUP BY
    ws_item_sk,
    d_date
), store_v1 AS (
  SELECT
    ss_item_sk AS item_sk,
    d_date,
    SUM(SUM(ss_sales_price)) OVER (
      PARTITION BY ss_item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS cume_sales
  FROM store_sales, date_dim
  WHERE
    ss_sold_date_sk = d_date_sk
    AND d_month_seq BETWEEN 1216 AND 1216 + 11
    AND NOT ss_item_sk IS NULL
  GROUP BY
    ss_item_sk,
    d_date
)
SELECT
  *
FROM (
  SELECT
    item_sk,
    d_date,
    web_sales,
    store_sales,
    MAX(web_sales) OVER (
      PARTITION BY item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS web_cumulative,
    MAX(store_sales) OVER (
      PARTITION BY item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS store_cumulative
  FROM (
    SELECT
      CASE WHEN NOT web.item_sk IS NULL THEN web.item_sk ELSE store.item_sk END AS item_sk,
      CASE WHEN NOT web.d_date IS NULL THEN web.d_date ELSE store.d_date END AS d_date,
      web.cume_sales AS web_sales,
      store.cume_sales AS store_sales
    FROM web_v1 AS web
    FULL OUTER JOIN store_v1 AS store
      ON (
        web.item_sk = store.item_sk AND web.d_date = store.d_date
      )
  ) AS x
) AS y
WHERE
  web_cumulative > store_cumulative
ORDER BY
  item_sk,
  d_date
LIMIT 100
```

## Query Structure (DAG)

### 1. web_v1
**Role**: CTE (Definition Order: 0)
**Stats**: 33% Cost | ~1k rows
**Flags**: GROUP_BY, WINDOW, ORDER_BY
**Outputs**: [item_sk, d_date, cume_sales] — ordered by d_date ASC
**Dependencies**: web_sales, date_dim (join)
**Joins**: ws_sold_date_sk = d_date_sk
**Filters**: d_month_seq BETWEEN 1216 AND 1216 + 11 | NOT ws_item_sk IS NULL
**Operators**: HASH_GROUP_BY, SEQ_SCAN[web_sales], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  ws_item_sk AS item_sk,
  d_date,
  SUM(SUM(ws_sales_price)) OVER (
    PARTITION BY ws_item_sk
    ORDER BY d_date
    rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
  ) AS cume_sales
FROM web_sales, date_dim
WHERE
  ws_sold_date_sk = d_date_sk
  AND d_month_seq BETWEEN 1216 AND 1216 + 11
  AND NOT ws_item_sk IS NULL
GROUP BY
  ws_item_sk,
  d_date
```

### 2. store_v1
**Role**: CTE (Definition Order: 0)
**Stats**: 33% Cost | ~1k rows
**Flags**: GROUP_BY, WINDOW, ORDER_BY
**Outputs**: [item_sk, d_date, cume_sales] — ordered by d_date ASC
**Dependencies**: store_sales, date_dim (join)
**Joins**: ss_sold_date_sk = d_date_sk
**Filters**: d_month_seq BETWEEN 1216 AND 1216 + 11 | NOT ss_item_sk IS NULL
**Operators**: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  ss_item_sk AS item_sk,
  d_date,
  SUM(SUM(ss_sales_price)) OVER (
    PARTITION BY ss_item_sk
    ORDER BY d_date
    rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
  ) AS cume_sales
FROM store_sales, date_dim
WHERE
  ss_sold_date_sk = d_date_sk
  AND d_month_seq BETWEEN 1216 AND 1216 + 11
  AND NOT ss_item_sk IS NULL
GROUP BY
  ss_item_sk,
  d_date
```

### 3. main_query
**Role**: Root / Output (Definition Order: 1)
**Stats**: 33% Cost | ~1k rows processed → 100 rows output
**Flags**: GROUP_BY, ORDER_BY, LIMIT(100)
**Outputs**: [*] — ordered by item_sk ASC, d_date ASC
**Dependencies**: web_v1 AS web (join), store_v1 AS store (join)
**Filters**: web_cumulative > store_cumulative
**Operators**: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[web_v1], SEQ_SCAN[store_v1]
**Key Logic (SQL)**:
```sql
SELECT
  *
FROM (
  SELECT
    item_sk,
    d_date,
    web_sales,
    store_sales,
    MAX(web_sales) OVER (
      PARTITION BY item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS web_cumulative,
    MAX(store_sales) OVER (
      PARTITION BY item_sk
      ORDER BY d_date
      rows BETWEEN UNBOUNDED preceding AND CURRENT ROW
    ) AS store_cumulative
  FROM (
    SELECT
...
```

### Edges
- web_v1 → main_query
- store_v1 → main_query


## Previous Optimization Attempts

- Attempt 1: **date_cte_isolate** → REGRESSION (0.87x)

## Reference Examples

**FAISS selected (by structural similarity):** deferred_window_aggregation, early_filter, shared_dimension_multi_channel

**All available gold examples:**

- **composite_decorrelate_union** (2.42xx) — Decorrelate multiple correlated EXISTS subqueries into pre-materialized DISTINCT
- **date_cte_isolate** (4.00xx) — Extract date filtering into a separate CTE to enable predicate pushdown and redu
- **decorrelate** (2.92xx) — Convert correlated subquery to separate CTE with GROUP BY, then JOIN
- **deferred_window_aggregation** (1.36xx) — When multiple CTEs each perform GROUP BY + WINDOW (cumulative sum), then are joi
- **dimension_cte_isolate** (1.93xx) — Pre-filter ALL dimension tables into CTEs before joining with fact table, not ju
- **early_filter** (4.00xx) — Filter dimension tables FIRST, then join to fact tables to reduce expensive join
- **intersect_to_exists** (1.83xx) — Convert INTERSECT subquery pattern to multiple EXISTS clauses for better join pl
- **materialize_cte** (1.37xx) — Extract repeated subquery patterns into a CTE to avoid recomputation
- **multi_date_range_cte** (2.35xx) — When query uses multiple date_dim aliases with different filters (d1, d2, d3), c
- **multi_dimension_prefetch** (2.71xx) — Pre-filter multiple dimension tables (date + store) into separate CTEs before jo
- **or_to_union** (3.17xx) — Split OR conditions on different columns into UNION ALL branches for better inde
- **prefetch_fact_join** (3.77xx) — Pre-filter dimension table into CTE, then pre-join with fact table in second CTE
- **pushdown** (2.11xx) — Push filters from outer query into CTEs/subqueries to reduce intermediate result
- **shared_dimension_multi_channel** (1.30xx) — Extract shared dimension filters (date, item, promotion) into CTEs when multiple
- **single_pass_aggregation** (4.47xx) — Consolidate multiple subqueries scanning the same table into a single CTE with c
- **union_cte_split** (1.36xx) — Split a generic UNION ALL CTE into specialized CTEs when the main query filters 

## Your Task

Analyze this query following these steps IN ORDER:

### 1. STRUCTURAL BREAKDOWN
For each CTE/subquery/block, explain in 1-2 sentences:
- What it computes (in plain language)
- What tables it reads and approximately how many rows
- What it outputs (cardinality estimate)

### 2. BOTTLENECK IDENTIFICATION
Using the DAG costs above, identify the dominant cost center.
Don't just name it — explain the MECHANISM:
- Is it a full table scan that could be filtered?
- Is it a sort for a window function that could be deferred?
- Is it a hash join on a large build side that could be pre-filtered?
- Is it scanning the same table multiple times when once would suffice?

### 3. PROPOSED OPTIMIZATION
Propose 1-3 specific structural changes. For EACH one:
- **What**: Exactly what to change (e.g., 'merge CTEs X and Y into one scan')
- **Why**: The performance mechanism (e.g., 'eliminates a 28M-row rescan of store_sales')
- **Risk**: What semantic constraint could break (e.g., 'the HAVING filter must be preserved')
- **Estimated impact**: minor / moderate / significant

### 4. FAILURE ANALYSIS
For each previous failed/regressed attempt, explain:
- WHY it failed (the specific mechanism)
- What constraint that teaches us for the next attempt

### 5. RECOMMENDED STRATEGY
Synthesize everything into a single recommended optimization approach.
Be specific enough that another engineer could implement it from your description.

### 6. EXAMPLE SELECTION
FAISS selected these examples: deferred_window_aggregation, early_filter, shared_dimension_multi_channel
Review the FAISS picks against the available examples above.
If you think different examples would be more relevant for this query,
list your preferred examples. Otherwise confirm the FAISS picks are good.

```
EXAMPLES: example_id_1, example_id_2, example_id_3
```

Use exact IDs from the available examples list above.
