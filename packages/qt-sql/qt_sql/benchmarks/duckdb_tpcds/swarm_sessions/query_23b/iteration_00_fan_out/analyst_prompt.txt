You are a senior query optimization architect. Your job is to deeply analyze a SQL query and produce a structured briefing for 4 specialist workers who will each write a different optimized version.

You are the ONLY call that sees all the data: EXPLAIN plans, logical-tree costs, full constraint list, global knowledge, and the complete example catalog. The workers will only see what YOU put in their briefings. Your output quality directly determines their success.

## Query: query_23b
## Dialect: duckdb

```sql
 1 | with frequent_ss_items as
 2 |  (select substr(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt
 3 |   from store_sales
 4 |       ,date_dim
 5 |       ,item
 6 |   where ss_sold_date_sk = d_date_sk
 7 |     and ss_item_sk = i_item_sk
 8 |     and d_year in (2000,2000 + 1,2000 + 2,2000 + 3)
 9 |   group by substr(i_item_desc,1,30),i_item_sk,d_date
10 |   having count(*) >4),
11 |  max_store_sales as
12 |  (select max(csales) tpcds_cmax
13 |   from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales
14 |         from store_sales
15 |             ,customer
16 |             ,date_dim 
17 |         where ss_customer_sk = c_customer_sk
18 |          and ss_sold_date_sk = d_date_sk
19 |          and d_year in (2000,2000+1,2000+2,2000+3)
20 |         group by c_customer_sk)),
21 |  best_ss_customer as
22 |  (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales
23 |   from store_sales
24 |       ,customer
25 |   where ss_customer_sk = c_customer_sk
26 |   group by c_customer_sk
27 |   having sum(ss_quantity*ss_sales_price) > (95/100.0) * (select
28 |   *
29 |  from max_store_sales))
30 |   select c_last_name,c_first_name,sales
31 |  from (select c_last_name,c_first_name,sum(cs_quantity*cs_list_price) sales
32 |         from catalog_sales
33 |             ,customer
34 |             ,date_dim 
35 |         where d_year = 2000 
36 |          and d_moy = 5 
37 |          and cs_sold_date_sk = d_date_sk 
38 |          and cs_item_sk in (select item_sk from frequent_ss_items)
39 |          and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer)
40 |          and cs_bill_customer_sk = c_customer_sk 
41 |        group by c_last_name,c_first_name
42 |       union all
43 |       select c_last_name,c_first_name,sum(ws_quantity*ws_list_price) sales
44 |        from web_sales
45 |            ,customer
46 |            ,date_dim 
47 |        where d_year = 2000 
48 |          and d_moy = 5 
49 |          and ws_sold_date_sk = d_date_sk 
50 |          and ws_item_sk in (select item_sk from frequent_ss_items)
51 |          and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer)
52 |          and ws_bill_customer_sk = c_customer_sk
53 |        group by c_last_name,c_first_name) 
54 |      order by c_last_name,c_first_name,sales
55 |  LIMIT 100;
```

## EXPLAIN ANALYZE Plan

```
Total execution time: 16199ms

CTE [0 rows, 0.2ms]
  FILTER [4,202 rows, 36.3ms]
    Expression: (count_star() > 4)
    HASH_GROUP_BY [13.8M rows, 7580.6ms, 47%]
      Aggregates: count_star()
      HASH_JOIN INNER on ss_item_sk = i_item_sk [16.6M rows, 85.4ms]
        HASH_JOIN INNER on ss_sold_date_sk = d_date_sk [16.6M rows, 110.7ms]
          SEQ_SCAN  store_sales [16.6M of 345.6M rows, 281.4ms, 2%]
          FILTER [1,098 rows]
            Expression: (d_date_sk BETWEEN 2450816 AND 2452642)
            SEQ_SCAN  date_dim [1,461 of 73K rows]  Filters: d_year>=2000 AND d_year<=2003
        SEQ_SCAN  item [102K rows, 11.2ms]
  CTE [0 rows]
    UNGROUPED_AGGREGATE [1 rows, 1.4ms]
      Aggregates: max(#0)
      HASH_GROUP_BY [472K rows, 638.3ms, 4%]
        Aggregates: sum(#1)
        HASH_JOIN INNER on ss_customer_sk = c_customer_sk [16.2M rows, 74.8ms]
          HASH_JOIN INNER on ss_sold_date_sk = d_date_sk [16.2M rows, 78.4ms]
            SEQ_SCAN  store_sales [16.2M of 345.6M rows, 327.4ms, 2%]
            FILTER [1,098 rows]
              Expression: (d_date_sk BETWEEN 2450816 AND 2452642)
              SEQ_SCAN  date_dim [1,461 of 73K rows]  Filters: d_year>=2000 AND d_year<=2003
          SEQ_SCAN  customer [500K of 2.5M rows, 0.5ms]
    TOP_N [5 rows]
      Top: 100
      Order By: unnamed_subquery.c_last_name ASC, unnamed_subquery.c_first_name ASC, unnamed_subquery.sales ASC
      UNION [0 rows]
        HASH_GROUP_BY [3 rows, 3.6ms]
          Aggregates: sum(#2)
          HASH_JOIN INNER on c_customer_sk = cs_bill_customer_sk [5 rows, 0.7ms]
            SEQ_SCAN  customer [167K of 2.5M rows, 3.2ms]  Filters: optional: Dynamic Filter (c_last_name)
            HASH_JOIN SEMI on cs_bill_customer_sk = #0 [5 rows, 0.9ms]
              HASH_JOIN RIGHT_SEMI on #0 = cs_item_sk [7,396 rows, 12.3ms]
                CTE_SCAN [4,202 rows]
                HASH_JOIN INNER on cs_sold_date_sk = d_date_sk [141K rows, 0.9ms]
                  SEQ_SCAN  catalog_sales [141K of 172.8M rows, 2.5ms]
                  FILTER [31 rows]
                    Expression: (d_date_sk BETWEEN 2450815 AND 2452653)
                    SEQ_SCAN  date_dim [31 of 73K rows]  Filters: d_year=2000, d_moy=5
              NESTED_LOOP_JOIN INNER on CAST(sum((CAST(ss_quantity AS DECIMAL(17,0)) * CAST(ss_sa... [171 rows, 13.4ms]
                HASH_GROUP_BY [496K rows, 852.7ms, 5%]
                  Aggregates: sum(#1)
                  HASH_JOIN INNER on ss_customer_sk = c_customer_sk [27.5M rows, 110.7ms]
                    SEQ_SCAN  store_sales [27.5M of 345.6M rows, 230.4ms, 1%]
                    SEQ_SCAN  customer [500K of 2.5M rows, 0.7ms]
                UNGROUPED_AGGREGATE [1 rows]
                  Aggregates: "first"(#0), count_star()
                  CTE_SCAN [1 rows]
        HASH_GROUP_BY [2 rows, 0.5ms]
          Aggregates: sum(#2)
          HASH_JOIN INNER on c_customer_sk = ws_bill_customer_sk [2 rows]
            FILTER [7,173 rows]
              Expression: (c_customer_sk BETWEEN 2 AND 499998)
              SEQ_SCAN  customer [7,173 of 2.5M rows, 0.4ms]  Filters: optional: Dynamic Filter (c_last_name)
            HASH_JOIN SEMI on ws_bill_customer_sk = #0 [2 rows, 2.3ms]
              HASH_JOIN RIGHT_SEMI on #0 = ws_item_sk [3,679 rows, 9.4ms]
                CTE_SCAN [4,202 rows]
                HASH_JOIN INNER on ws_sold_date_sk = d_date_sk [73K rows, 10.4ms]
                  SEQ_SCAN  web_sales [73K of 86.4M rows, 45.6ms]
                  FILTER [31 rows]
                    Expression: (d_date_sk BETWEEN 2450816 AND 2452642)
                    SEQ_SCAN  date_dim [31 of 73K rows]  Filters: d_year=2000, d_moy=5
              NESTED_LOOP_JOIN INNER on CAST(sum((CAST(ss_quantity AS DECIMAL(17,0)) * CAST(ss_sa... [171 rows, 15.6ms]
                HASH_GROUP_BY [496K rows, 1022.8ms, 6%]
                  Aggregates: sum(#1)
                  HASH_JOIN INNER on c_customer_sk = c_customer_sk [27.5M rows, 958.3ms, 6%]
                    SEQ_SCAN  store_sales [27.5M of 345.6M rows, 378.3ms, 2%]  Filters: ss_customer_sk>=2 AND ss_customer_sk<=499998
                    SEQ_SCAN  customer [500K of 2.5M rows, 0.9ms]  Filters: c_customer_sk>=2 AND c_customer_sk<=499998
                UNGROUPED_AGGREGATE [1 rows]
                  Aggregates: "first"(#0), count_star()
                  CTE_SCAN [1 rows]
```

**NOTE:** The EXPLAIN plan shows the PHYSICAL execution structure, which may differ significantly from the logical tree below. The optimizer may have already split CTEs, reordered joins, or pushed predicates. When the EXPLAIN and the logical tree disagree, the EXPLAIN is ground truth for what the optimizer is already doing.

DuckDB EXPLAIN ANALYZE reports **operator-exclusive** wall-clock time per node (children's time is NOT included in the parent's reported time). The percentage annotations are also exclusive. You can sum sibling nodes to get pipeline cost. logical-tree cost percentages are derived metrics that may not reflect actual execution time — use EXPLAIN timings as ground truth.

## Query Structure (Logic Tree)

```
QUERY: (single statement)
├── [CTE] frequent_ss_items  [=]  Cost: 25%  Rows: ~1K  — Identify store items that sell more than four times per day/description slice during 2000-2003 to define the frequent-item set.
│   ├── SCAN (store_sales, date_dim (join), item (join))
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── JOIN (ss_item_sk = i_item_sk)
│   ├── FILTER (d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3))
│   ├── AGG (GROUP BY)
│   └── OUTPUT (itemdesc, item_sk, solddate, cnt)
├── [CTE] max_store_sales  [=]  Cost: 25%  Rows: ~1K  — Compute the maximum customer store-sales amount over 2000-2003 as the benchmark for top-customer selection.
│   ├── SCAN (store_sales, customer, date_dim)
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── FILTER (d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3))
│   ├── AGG (GROUP BY)
│   └── OUTPUT (tpcds_cmax)
├── [CTE] best_ss_customer  [=]  Cost: 25%  Rows: ~1K  — Select customers whose cumulative store sales exceed 95% of the maximum customer store-sales benchmark.
│   ├── SCAN (store_sales, customer (join), max_store_sales (correlated subquery))
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (c_customer_sk, ssales)
└── [MAIN] main_query  [=]  Cost: 25%  Rows: ~1K  — Union May 2000 catalog and web sales for frequent items purchased by top store customers, then sum the combined sales amount.
    ├── SCAN (catalog_sales, customer, date_dim, web_sales, best_ss_customer, frequent_ss_items)
    ├── JOIN (cs_sold_date_sk = d_date_sk)
    ├── JOIN (cs_bill_customer_sk = c_customer_sk)
    ├── FILTER (d_year = 2000)
    ├── FILTER (d_moy = 5)
    ├── FILTER (+2 more)
    ├── AGG (GROUP BY)
    ├── SORT (c_last_name ASC, c_first_name ASC, sales ASC)
    └── OUTPUT (c_last_name, c_first_name, sales)
```

## Node Details

### 1. frequent_ss_items
**Role**: CTE (Definition Order: 0)
**Intent**: Identify store items that sell more than four times per day/description slice during 2000-2003 to define the frequent-item set.
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [itemdesc, item_sk, solddate, cnt]
**Dependencies**: store_sales, date_dim (join), item (join)
**Joins**: ss_sold_date_sk = d_date_sk | ss_item_sk = i_item_sk
**Filters**: d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
**Operators**: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[date_dim], SEQ_SCAN[item]
**Key Logic (SQL)**:
```sql
SELECT
  SUBSTRING(i_item_desc, 1, 30) AS itemdesc,
  i_item_sk AS item_sk,
  d_date AS solddate,
  COUNT(*) AS cnt
FROM store_sales, date_dim, item
WHERE
  ss_sold_date_sk = d_date_sk
  AND ss_item_sk = i_item_sk
  AND d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
GROUP BY
  SUBSTRING(i_item_desc, 1, 30),
  i_item_sk,
  d_date
HAVING
  COUNT(*) > 4
```

### 2. max_store_sales
**Role**: CTE (Definition Order: 0)
**Intent**: Compute the maximum customer store-sales amount over 2000-2003 as the benchmark for top-customer selection.
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [tpcds_cmax]
**Dependencies**: store_sales, customer, date_dim
**Joins**: ss_customer_sk = c_customer_sk | ss_sold_date_sk = d_date_sk
**Filters**: d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
**Operators**: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[customer], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  MAX(csales) AS tpcds_cmax
FROM (
  SELECT
    c_customer_sk,
    SUM(ss_quantity * ss_sales_price) AS csales
  FROM store_sales, customer, date_dim
  WHERE
    ss_customer_sk = c_customer_sk
    AND ss_sold_date_sk = d_date_sk
    AND d_year IN (2000, 2000 + 1, 2000 + 2, 2000 + 3)
  GROUP BY
    c_customer_sk
)
```

### 3. best_ss_customer
**Role**: CTE (Definition Order: 1)
**Intent**: Select customers whose cumulative store sales exceed 95% of the maximum customer store-sales benchmark.
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [c_customer_sk, ssales]
**Dependencies**: store_sales, customer (join), max_store_sales (correlated subquery)
**Joins**: ss_customer_sk = c_customer_sk
**Operators**: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[store_sales], SEQ_SCAN[customer], SEQ_SCAN[max_store_sales]
**Key Logic (SQL)**:
```sql
SELECT
  c_customer_sk,
  SUM(ss_quantity * ss_sales_price) AS ssales
FROM store_sales, customer
WHERE
  ss_customer_sk = c_customer_sk
GROUP BY
  c_customer_sk
HAVING
  SUM(ss_quantity * ss_sales_price) > (
    95 / 100.0
  ) * (
    SELECT
      *
    FROM max_store_sales
  )
```

### 4. main_query
**Role**: Root / Output (Definition Order: 2)
**Intent**: Union May 2000 catalog and web sales for frequent items purchased by top store customers, then sum the combined sales amount.
**Stats**: 25% Cost | ~1k rows processed → 100 rows output
**Flags**: GROUP_BY, ORDER_BY, LIMIT(100)
**Outputs**: [c_last_name, c_first_name, sales] — ordered by c_last_name ASC, c_first_name ASC, sales ASC
**Dependencies**: catalog_sales, customer, date_dim, web_sales, best_ss_customer, frequent_ss_items
**Joins**: cs_sold_date_sk = d_date_sk | cs_bill_customer_sk = c_customer_sk
**Filters**: d_year = 2000 | d_moy = 5 | cs_item_sk IN (SELECT item_sk FROM frequent_ss_items) | cs_bill_customer_sk IN (SELECT c_customer_sk FROM best_ss_customer)
**Operators**: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[catalog_sales], SEQ_SCAN[customer], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  c_last_name,
  c_first_name,
  sales
FROM (
  SELECT
    c_last_name,
    c_first_name,
    SUM(cs_quantity * cs_list_price) AS sales
  FROM catalog_sales, customer, date_dim
  WHERE
    d_year = 2000
    AND d_moy = 5
    AND cs_sold_date_sk = d_date_sk
    AND cs_item_sk IN (
      SELECT
        item_sk
      FROM frequent_ss_items
    )
    AND cs_bill_customer_sk IN (
...
```

### Edges
- max_store_sales → best_ss_customer
- best_ss_customer → main_query
- best_ss_customer → main_query
- frequent_ss_items → main_query
- frequent_ss_items → main_query


## Pre-Computed Semantic Intent

**Query intent:** Estimate May 2000 cross-channel demand from high-value store customers by summing catalog and web sales for items that were frequently sold in stores during 2000-2003.

START from this pre-computed intent. In your SEMANTIC_CONTRACT output, ENRICH it with: intersection/union semantics from JOIN types, aggregation function traps, NULL propagation paths, and filter dependencies. Do NOT re-derive what is already stated above.

## Aggregation Semantics Check

You MUST verify aggregation equivalence for any proposed restructuring:

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Returns NULL for 0-1 values. Changing group membership changes the result.
- `STDDEV_SAMP(x) FILTER (WHERE year=1999)` over a combined (1999,2000) group is NOT equivalent to `STDDEV_SAMP(x)` over only 1999 rows — FILTER still uses the combined group's membership for the stddev denominator.
- **AVG and STDDEV are NOT duplicate-safe**: if a join introduces row duplication, the aggregate result changes.
- When splitting a UNION ALL CTE with GROUP BY + aggregate, each split branch must preserve the exact GROUP BY columns and filter to the exact same row set as the original.
- **SAFE ALTERNATIVE**: If GROUP BY includes the discriminator column (e.g., d_year), each group is already partitioned. STDDEV_SAMP computed per-group is correct. You can then pivot using `MAX(CASE WHEN year = 1999 THEN year_total END) AS year_total_1999` because the GROUP BY guarantees exactly one row per (customer, year) — the MAX is just a row selector, not a real aggregation.

## Top 20 Tag-Matched Examples

### intersect_to_exists (1.83x)
**Description:** Convert INTERSECT subquery pattern to multiple EXISTS clauses for better join planning
**Principle:** Semi-Join Short-Circuit: replace INTERSECT with EXISTS to avoid full materialization and sorting. INTERSECT must compute complete result sets before intersecting; EXISTS stops at the first match per row, enabling semi-join optimizations.

### multi_intersect_exists_cte (2.39x)
**Description:** Convert cascading INTERSECT operations into correlated EXISTS subqueries with pre-materialized date and channel CTEs
**When NOT to apply:** Do not use when the INTERSECT operates on small result sets (< 1000 rows) where materialization cost is negligible. Also not applicable when the EXISTS correlation would be on non-indexed columns, as the correlated probe could be slower than the hash-based INTERSECT.

### shared_dimension_multi_channel (1.30x)
**Description:** Extract shared dimension filters (date, item, promotion) into CTEs when multiple channel CTEs (store/catalog/web) apply identical filters independently
**Principle:** Shared Dimension Extraction: when multiple channel CTEs (store/catalog/web) apply identical dimension filters, extract those shared filters into one CTE and reference it from each channel. Avoids redundant dimension scans.

### union_cte_split (1.36x)
**Description:** Split a generic UNION ALL CTE into specialized CTEs when the main query filters by year or discriminator - eliminates redundant scans
**Principle:** CTE Specialization: when a generic CTE is scanned multiple times with different filters (e.g., by year), split it into specialized CTEs that embed the filter in their definition. Each specialized CTE processes only its relevant subset, eliminating redundant scans.

### composite_decorrelate_union (2.42x)
**Description:** Decorrelate multiple correlated EXISTS subqueries into pre-materialized DISTINCT customer CTEs with a shared date filter, and replace OR(EXISTS a, EXISTS b) with UNION of key sets
**Principle:** Composite Decorrelation: when multiple correlated EXISTS share common filters, extract shared dimensions into a single CTE and decorrelate the EXISTS checks into pre-materialized key sets joined via UNION.

### multi_date_range_cte (2.35x)
**Description:** When query uses multiple date_dim aliases with different filters (d1, d2, d3), create separate CTEs for each date range and pre-join with fact tables
**Principle:** Early Selection per Alias: when a query joins the same dimension table multiple times with different filters (d1, d2, d3), create separate CTEs for each filter and pre-join with fact tables to reduce rows entering the main join.

### date_cte_isolate (4.00x)
**Description:** Extract date filtering into a separate CTE to enable predicate pushdown and reduce scans
**Principle:** Dimension Isolation: extract small dimension lookups into CTEs so they materialize once and subsequent joins probe a tiny hash table instead of rescanning.
**When NOT to apply:** Do not use when the optimizer already pushes date predicates effectively (e.g., simple equality filters on date columns in self-joins). Do not decompose an already-efficient existing CTE into sub-CTEs — this adds materialization overhead without reducing scans. Caused 0.49x regression on Q31 (DuckDB already optimized the date pushdown) and 0.71x on Q1 (decomposed a well-structured CTE into slower pieces).

### deferred_window_aggregation (1.36x)
**Description:** When multiple CTEs each perform GROUP BY + WINDOW (cumulative sum), then are joined with FULL OUTER JOIN followed by another WINDOW pass for NULL carry-forward: defer the WINDOW out of the CTEs, join daily totals, then compute cumulative sums once on the joined result. SUM() OVER() naturally skips NULLs, eliminating the need for a separate MAX() carry-forward window.
**Principle:** Deferred Aggregation: delay expensive operations (window functions) until after joins reduce the dataset. Computing window functions inside individual CTEs then joining is more expensive than joining first and computing windows once on the combined result.
**When NOT to apply:** Do not use when the CTE window function is referenced by other consumers besides the final join (the cumulative value is needed elsewhere). Do not use when the window function is not a monotonically accumulating SUM - e.g., AVG, COUNT, or non-monotonic window functions require separate computation. Only applies when the join is FULL OUTER and the carry-forward window is MAX/LAST_VALUE over a cumulative sum.

### materialize_cte (1.37x)
**Description:** Extract repeated subquery patterns into a CTE to avoid recomputation
**Principle:** Shared Materialization: extract repeated subquery patterns into CTEs to avoid recomputation. When the same logical check appears multiple times, compute it once and reference the result.
**When NOT to apply:** NEVER convert EXISTS or NOT EXISTS subqueries into materialized CTEs when the EXISTS is used as a filter (not a data source). EXISTS uses semi-join short-circuiting — the database stops scanning as soon as one match is found. Materializing into a CTE forces a full scan of the subquery table, destroying this optimization. Caused 0.14x on Q16 (7x slowdown — EXISTS on catalog_sales materialized into full CTE scan) and 0.54x on Q95 (EXISTS on web_sales forced full materialization).

### decorrelate (2.92x)
**Description:** Convert correlated subquery to separate CTE with GROUP BY, then JOIN
**Principle:** Decorrelation: convert correlated subqueries to standalone CTEs with GROUP BY, then JOIN. Correlated subqueries re-execute per outer row; a pre-computed CTE executes once.

### prefetch_fact_join (3.77x)
**Description:** Pre-filter dimension table into CTE, then pre-join with fact table in second CTE before joining other dimensions
**Principle:** Staged Join Pipeline: build a CTE chain that progressively reduces data — first CTE filters the dimension, second CTE pre-joins filtered dimension keys with the fact table, subsequent CTEs join remaining dimensions against the already-reduced fact set.
**When NOT to apply:** Do not use on queries with baseline runtime under 50ms — CTE materialization overhead dominates on fast queries. Do not use on window-function-dominated queries where filtering is not the bottleneck. Avoid on queries with 5+ table joins and complex inter-table predicates where forcing join order via CTEs prevents the optimizer from choosing a better plan. Caused 0.50x on Q25 (fast baseline query), 0.87x on Q51 (window-function bottleneck), and 0.77x on Q72 (complex multi-table join reordering).

### or_to_union (3.17x)
**Description:** Split OR conditions on different columns into UNION ALL branches for better index usage
**Principle:** OR-to-UNION Decomposition: split OR conditions on different columns into separate UNION ALL branches, each with a focused predicate. The optimizer can use different access paths per branch instead of a single scan with a complex filter.
**When NOT to apply:** Do not split OR when all branches filter the SAME column on the same table (e.g., t_hour >= 8 OR t_hour <= 17). This duplicates the entire fact table scan for each branch with no selectivity benefit. Only apply when OR conditions span DIFFERENT tables or fundamentally different column families. Also never split into more than 3 UNION branches — each branch rescans the fact table. Caused 0.59x on Q90 (same-column time range split doubled fact scans) and historically 0.23x-0.41x on queries with 9+ UNION branches.

### rollup_to_union_windowing (2.47x)
**Description:** Replace GROUP BY ROLLUP with explicit UNION ALL of pre-aggregated CTEs at each hierarchy level, combined with window functions for ranking
**When NOT to apply:** Do not use when ROLLUP generates all levels efficiently (small dimension tables, few groups) or when the query genuinely needs all possible grouping set combinations. Only beneficial when specific levels need different optimization paths.

### dimension_cte_isolate (1.93x)
**Description:** Pre-filter ALL dimension tables into CTEs before joining with fact table, not just date_dim
**Principle:** Early Selection: pre-filter dimension tables into CTEs returning only surrogate keys before joining with fact tables. Each dimension CTE is tiny, creating small hash tables that speed up the fact table probe.

### channel_bitmap_aggregation (6.24x)
**Description:** Consolidate repeated scans of the same fact table (one per time/channel bucket) into a single scan with CASE WHEN labels and conditional aggregation
**When NOT to apply:** Do not use when the number of distinct buckets exceeds 8 (diminishing returns from CASE evaluation overhead). Also not applicable when each subquery has structurally different joins or table references.

### DSB_SELF_JOIN_DECOMPOSITION (unknownx)
**Description:** Decompose many-to-many self-joins into multiple passes with
      intermediate materialization for better cardinality estimation.

### early_filter (4.00x)
**Description:** Filter dimension tables FIRST, then join to fact tables to reduce expensive joins
**Principle:** Early Selection: filter small dimension tables first, then join to large fact tables. This reduces the fact table scan to only rows matching the filter, rather than scanning all rows and filtering after the join.

### multi_dimension_prefetch (2.71x)
**Description:** Pre-filter multiple dimension tables (date + store) into separate CTEs before joining with fact table
**Principle:** Multi-Dimension Prefetch: when multiple dimension tables have selective filters, pre-filter ALL of them into CTEs before the fact table join. Combined selectivity compounds — each dimension CTE reduces the fact scan further.
**When NOT to apply:** Do not create dimension CTEs without a WHERE clause that actually reduces rows — an unfiltered dimension CTE is pure overhead (full scan + materialization for zero selectivity benefit). Avoid on queries with 5+ tables and complex inter-table predicates where forcing join order via CTEs prevents the optimizer from choosing a better plan. Caused 0.85x on Q67 (unfiltered dimension CTEs added overhead) and 0.77x on Q72 (forced suboptimal join ordering on complex multi-table query).

### pushdown (2.11x)
**Description:** Push filters from outer query into CTEs/subqueries to reduce intermediate result sizes
**Principle:** Scan Consolidation: when multiple subqueries scan the same table with similar patterns, consolidate them into CTEs that compute all needed aggregates in fewer passes. Reduces N scans to fewer scans.

### single_pass_aggregation (4.47x)
**Description:** Consolidate multiple subqueries scanning the same table into a single CTE with conditional aggregates
**Principle:** Single-Pass Aggregation: consolidate multiple scalar subqueries on the same table into one CTE using CASE expressions inside aggregate functions. Reduces N separate table scans to 1 pass.

## Optimization Principles (from benchmark history)

**Or To Union** (2.5x avg, 19 wins)
  Why: Converting OR to UNION ALL lets optimizer choose independent index paths per branch
  When: WHERE clause has OR conditions over different dimension keys (≤3 branches)
**Single Pass Aggregation** (2.5x avg, 5 wins)
  Why: Consolidating repeated scans into CASE aggregates reduces I/O from N scans to 1; Pre-joining filtered dimensions with fact table before aggregation reduces join input; Separate CTEs for each date alias avoids ambiguous multi-way date joins
  When: Query has repeated scans of the same fact table with different WHERE filters
**Dimension Cte Isolate** (2.1x avg, 7 wins)
  Why: Pre-filtering all dimension tables into CTEs avoids repeated full-table scans; Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows
  When: Query joins 2+ dimension tables that could each be pre-filtered independently
**Prefetch Fact Join** (1.8x avg, 18 wins)
  Why: Pre-joining filtered dimensions with fact table before aggregation reduces join input; Pre-filtering multiple dimension tables in parallel reduces join fan-out; Consolidating repeated scans into CASE aggregates reduces I/O from N scans to 1
  When: Query joins filtered dates/dims with large fact table; pre-join reduces probe size
**Date Cte Isolate** (1.8x avg, 48 wins)
  Why: Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows
  When: Query joins date_dim on multiple conditions (year, month, etc.) with fact tables

## Regression Examples

### regression_q74_pushdown: pushdown on q74 (0.68x)
**Anti-pattern:** When splitting a UNION CTE by year, you MUST remove or replace the original UNION CTE. Keeping both the split and original versions causes redundant materialization and extreme cardinality misestimates.
**Mechanism:** Created year-specific CTEs (store_sales_1999, store_sales_2000, etc.) but KEPT the original year_total union CTE alongside them. The optimizer materializes both the split versions and the original union, resulting in redundant computation. Projection cardinality estimates show 10^16x errors from the confused CTE graph.

### regression_q31_pushdown: pushdown on q31 (0.49x)
**Anti-pattern:** When creating filtered versions of existing CTEs, always REMOVE the original unfiltered CTEs. Keeping both causes redundant materialization and 1000x+ cardinality misestimates on self-joins.
**Mechanism:** Created both filtered (store_sales_agg, web_sales_agg) AND original (ss, ws) versions of the same aggregations. The query does a 6-way self-join matching quarterly patterns (Q1->Q2->Q3). Duplicate CTEs doubled materialization and confused the optimizer's cardinality estimates for the multi-self-join.

### regression_q51_date_cte_isolate: date_cte_isolate on q51 (0.87x)
**Anti-pattern:** Do not materialize running/cumulative window aggregates into CTEs before joins that filter based on those aggregates. The optimizer can co-optimize window evaluation and join filtering together.
**Mechanism:** Materialized cumulative window functions (SUM() OVER ORDER BY) into separate CTEs (web_v1, store_v1) before a FULL OUTER JOIN that filters on web_cumulative > store_cumulative. The original evaluates windows lazily during the join, co-optimizing window computation with the join filter. Materialization forces full window computation before filtering.

## Exploit Algorithm: Evidence-Based Gap Intelligence

The following YAML describes known optimizer gaps with detection rules, procedural exploit steps, and evidence. Use DETECT rules to match structural features of the query, then follow EXPLOIT_STEPS.

**ENGINE STRENGTHS**
1. **INTRA_SCAN_PREDICATE_PUSHDOWN**: Pushes WHERE filters directly into SEQ_SCAN. **Do NOT** create a CTE to push a filter already inside the scan node.
2. **SAME_COLUMN_OR**: OR on the same column uses a single scan. **Do NOT** split same-column ORs into UNION ALL — duplicates fact scans.
3. **HASH_JOIN_SELECTION**: Automatically selects hash joins. **Do NOT** restructure simple join orders — focus on input reduction.
4. **CTE_INLINING**: Single‑reference CTEs are inlined; multi‑reference may be materialized. **Do NOT** assume CTEs are optimization fences.
5. **COLUMNAR_PROJECTION**: Reads only referenced columns. **Do NOT** ignore column selection in CTEs — fewer columns = less memory.
6. **PARALLEL_AGGREGATION**: Parallel scans and efficient perfect‑hash GROUP BY. **Do NOT** restructure aggregations unless reducing input rows.
7. **EXISTS_SEMI_JOIN**: EXISTS uses semi‑join with early termination. **Do NOT** convert EXISTS to a materialized CTE — forces full scan.

**CORRECTNESS RULES**
- Row count must be identical.
- NULL handling semantics must be preserved.
- ORDER BY must be kept unless the query has no LIMIT/ORDER requirement.
- LIMIT must be preserved.

**OPTIMIZER GAPS**

## Gap: CORRELATED_SUBQUERY_PARALYSIS
Correlated subqueries re‑execute per outer row. Opportunity: decorrelate into CTEs that execute once.
**decorrelate** [W3] — HIGH reliability, 1 win, avg 2.92x  
Convert correlated subqueries to standalone CTEs with GROUP BY, then JOIN.
- ✓ Q1: 2.92x — pushed s_state='SD' filter early
- Guard: Preserve all WHERE filters. Check EXPLAIN — if hash join, optimizer already decorrelated.
**composite_decorrelate_union** [W3] — HIGH reliability, 1 win, avg 2.42x  
For multiple correlated EXISTS, extract shared dimensions once, decorrelate each into DISTINCT CTEs, then UNION.
- ✓ Q35: 2.42x — shared date filter CTE, each EXISTS became SELECT DISTINCT customer_sk
- Guard: (none)

## Gap: CROSS_COLUMN_OR_DECOMPOSITION
OR on different columns prevents optimal scan path. Opportunity: split into UNION ALL with focused predicates.
**or_to_union** [W2] — HIGH reliability, 1 win, avg 3.17x  
Split OR conditions on different columns into separate UNION ALL branches.
- ✓ Q15: 3.17x — three OR branches became UNION ALL
- ✗ Q90: 0.59x — split same‑column time range, doubled fact scans
- Guard: Max 3 UNION branches. Never split same‑column ORs. Avoid on self‑joins.

## Gap: CROSS_CTE_PREDICATE_BLINDNESS
Predicates aren’t pushed across CTE boundaries. Opportunity: pre‑filter dimensions into CTEs.
**date_cte_isolate** [W1] — HIGH reliability, 2 wins, avg 4.00x  
Extract date dimension lookups into CTE. Join instead of scalar subquery.
- ✓ Q6/Q11: 4.00x — date filter into CTE
- ✗ Q31: 0.49x — baseline <100ms, CTE overhead exceeded savings
- Guard: Skip if baseline <100ms. Don't decompose efficient existing CTEs.
**early_filter** [W1] — HIGH reliability, 2 wins, avg 4.00x  
Filter small dimension tables first, then join to fact tables.
- ✓ Q11/Q93: 4.00x — filtered reason table first
- Guard: (none)
**prefetch_fact_join** [W1] — HIGH reliability, 1 win, avg 3.77x  
Build CTE chain: filter dimension, pre‑join with fact, then join remaining dimensions.
- ✓ Q63: 3.77x — filtered date_dim first, pre‑joined with store_sales
- ✗ Q25: 0.50x — baseline <50ms, CTE overhead dominated
- Guard: Max 2 cascading fact‑table CTE chains.
**multi_dimension_prefetch** [W1] — HIGH reliability, 1 win, avg 2.71x  
Pre‑filter multiple dimension tables into CTEs before fact join.
- ✓ Q43: 2.71x — pre‑filtered date_dim and store
- ✗ Q67: 0.85x — unfiltered dimension CTEs added overhead
- Guard: Every CTE must have a WHERE clause.
**multi_date_range_cte** [W1] — HIGH reliability, 1 win, avg 2.35x  
When same dimension is joined multiple times, create separate filtered CTEs per alias.
- ✓ Q29: 2.35x — separate date CTEs for d1, d2, d3
- Guard: (none)
**pushdown** [W1] — HIGH reliability, 1 win, avg 2.11x  
Consolidate multiple subqueries scanning same table into a single CTE.
- ✓ Q9: 2.11x — 15+ scalar subqueries became one CTE
- Guard: (none)
**dimension_cte_isolate** [W1] — MEDIUM reliability, 1 win, avg 1.93x  
Pre‑filter dimension tables into CTEs returning only surrogate keys.
- ✓ Q26: 1.93x — isolated date, demographics, promotions
- ✗ Q26: 0.0076x — cross‑joined 3+ dimension CTEs, Cartesian explosion
- Guard: Never cross‑join 3+ dimension CTEs. Every CTE must have a WHERE clause.
**shared_dimension_multi_channel** [W1] — MEDIUM reliability, 1 win, avg 1.30x  
Extract shared dimension filters into common CTEs for multiple channels.
- ✓ Q80: 1.30x — shared date, item, promotion filters extracted once
- Guard: (none)

## Gap: REDUNDANT_SCAN_ELIMINATION
Repeated scans of same table waste I/O. Opportunity: consolidate into single scan.
**channel_bitmap_aggregation** [W2] — HIGH reliability, 1 win, avg 6.24x  
Consolidate repeated fact scans into one scan with CASE WHEN and conditional aggregation.
- ✓ Q88: 6.24x — 8 scans → 1 scan with CASE labels
- Guard: Not for >8 buckets or structurally different joins.
**single_pass_aggregation** [W2] — HIGH reliability, 1 win, avg 4.47x  
Consolidate scalar subqueries into one CTE with CASE inside aggregates.
- ✓ Q9: 4.47x — 15 subqueries → one CTE
- Guard: (none)

## Gap: UNION_CTE_SELF_JOIN_DECOMPOSITION
Generic CTEs scanned multiple times with different filters. Opportunity: split into specialized CTEs.
**rollup_to_union_windowing** [W4] — HIGH reliability, 1 win, avg 2.47x  
Replace GROUP BY ROLLUP with UNION ALL of pre‑aggregated CTEs per level.
- ✓ Q36: 2.47x — explicit UNION ALL allowed per‑level optimization
- Guard: Not when ROLLUP is efficient (small dimensions, few groups).
**union_cte_split** [W4] — MEDIUM reliability, 1 win, avg 1.36x  
Split generic CTE scanned multiple times into specialized CTEs that embed the filter.
- ✓ Q74: 1.36x — generic wswscs CTE split by year
- ✗ Q74: 0.49x — kept both original UNION and specialized CTEs
- Guard: Original UNION must be eliminated.

**STANDALONE TRANSFORMS**
**multi_intersect_exists_cte** [W3] — HIGH reliability, 1 win, avg 2.39x  
Convert cascading INTERSECT to correlated EXISTS with pre‑materialized date/channel CTEs.
- ✓ Q14: 2.39x — EXISTS short‑circuits vs. full materialization
- Guard: Not for small result sets (<1000 rows).
**intersect_to_exists** [W3] — MEDIUM reliability, 1 win, avg 1.83x  
Replace INTERSECT with EXISTS to avoid full materialization and sorting.
- ✓ Q14: 1.83x — semi‑join short‑circuit
- Guard: (none)
**materialize_cte** [W3] — MEDIUM reliability, 1 win, avg 1.37x  
Extract repeated subquery patterns into CTEs to avoid recomputation.
- ✓ Q95: 1.37x — multi‑warehouse order detection into CTE
- ✗ Q16: 0.14x — converted EXISTS to materialized CTE, forced full scan
- Guard: Never convert EXISTS/NOT EXISTS used as filter into materialized CTE.
**deferred_window_aggregation** [W3] — MEDIUM reliability, 1 win, avg 1.36x  
Delay window functions until after joins reduce the dataset.
- ✓ Q51: 1.36x — removed WINDOW from CTEs, computed SUM once after join
- Guard: Not when CTE window is referenced by multiple consumers.

**GLOBAL GUARD RAILS**
1. Never split same‑column ORs — engine handles natively. Caused 0.59x on Q90.
2. Never cross‑join 3+ dimension CTEs — Cartesian explosion. Caused 0.0076x on Q26.
3. Never convert EXISTS/NOT EXISTS used as filter into materialized CTE — destroys semi‑join short‑circuit. Caused 0.14x on Q16.
4. Max 3 UNION branches — 6+ duplicates fact scans. Caused 0.23x on Q13.
5. Skip transform if baseline <100ms — CTE overhead dominates. Caused 0.50x on Q25.
6. Every dimension CTE must have a WHERE clause — unfiltered CTE = pure overhead. Caused 0.85x on Q67.
7. Preserve all WHERE filters when decorrelating — missing filter causes cross‑product (0.34x).
8. Check EXPLAIN before decorrelating — if hash join, optimizer already decorrelated.
9. Avoid on self‑joins — each UNION branch re‑does the self‑join. Caused 0.51x.
10. Avoid on window‑function‑dominated queries — filtering not the bottleneck. Caused 0.87x on Q51.

## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 — prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY
  - Failure: Q76 — filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## Your Task

First, use a `<reasoning>` block for your internal analysis. This will be stripped before parsing. Work through these steps IN ORDER:

1. **CLASSIFY**: What structural archetype is this query?
   (channel-comparison self-join / correlated-aggregate filter / star-join with late dim filter / repeated fact scan / multi-channel UNION ALL / EXISTS-set operations / other)

2. **EXPLAIN PLAN ANALYSIS**: From the EXPLAIN ANALYZE output, identify:
   - Compute wall-clock ms per EXPLAIN node. Sum repeated operations (e.g., 2x store_sales joins = total cost). The EXPLAIN is ground truth, not the logical-tree cost percentages.
   - Which nodes consume >10% of runtime and WHY
   - Where row counts drop sharply (existing selectivity)
   - Where row counts DON'T drop (missed optimization opportunity)
   - Whether the optimizer already splits CTEs, pushes predicates, or performs transforms you might otherwise assign
   - Count scans per base table. If a fact table is scanned N times, a restructuring that reduces it to 1 scan saves (N-1)/N of that table's I/O cost. Prioritize transforms that reduce scan count on the largest tables.
   - Whether the CTE is materialized once and probed multiple times, or re-executed per reference

3. **GAP MATCHING**: Compare the EXPLAIN analysis to the Engine Profile gaps above. For each gap:
   - Does this query exhibit the gap? (e.g., is a predicate NOT pushed into a CTE? Is the same fact table scanned multiple times?)
   - Check the 'opportunity' — does this query's structure match?
   - Check 'what_didnt_work' and 'field_notes' — any disqualifiers for this query?
   - Also verify: is the optimizer ALREADY handling this well? (Check the Optimizer Strengths above — if the engine already does it, your transform adds overhead, not value.)

4. **AGGREGATION TRAP CHECK**: For every aggregate function in the query, verify: does my proposed restructuring change which rows participate in each group? STDDEV_SAMP, VARIANCE, PERCENTILE_CONT, CORR are grouping-sensitive. SUM, COUNT, MIN, MAX are grouping-insensitive (modulo duplicates). If the query uses FILTER clauses or conditional aggregation, verify equivalence explicitly.

5. **TRANSFORM SELECTION**: From the matched engine gaps, select transforms that exploit the specific gaps present in THIS query. Rank by expected value (rows affected × historical speedup from evidence). Select 4 that are structurally diverse — each attacking a different gap or bottleneck.
   REJECT tag-matched examples whose primary technique requires a structural feature this query lacks (e.g., reject intersect_to_exists if query has no INTERSECT; reject decorrelate if query has no correlated subquery). Tag matching is approximate — always verify structural applicability.

6. **LOGICAL TREE DESIGN**: For each worker's strategy, define the target logical tree topology. Verify that every node contract has exhaustive output columns by checking downstream references.
   CTE materialization matters for your design: a CTE referenced by 2+ consumers will likely be materialized (good — computed once, probed many). A CTE referenced once may be inlined (no materialization benefit from 'sharing'). Design shared CTEs only when multiple downstream nodes consume them. See CTE_INLINING in Engine Profile strengths.

Then produce the structured briefing in EXACTLY this format:

```
=== SHARED BRIEFING ===

SEMANTIC_CONTRACT: (80-150 tokens, cover ONLY:)
(a) One sentence of business intent (start from pre-computed intent if available).
(b) JOIN type semantics that constrain rewrites (INNER = intersection = all sides must match).
(c) Any aggregation function traps specific to THIS query.
(d) Any filter dependencies that a rewrite could break.
Do NOT repeat information already in ACTIVE_CONSTRAINTS or REGRESSION_WARNINGS.

BOTTLENECK_DIAGNOSIS:
[Which operation dominates cost and WHY (not just '50% cost').
Scan-bound vs join-bound vs aggregation-bound.
Cardinality flow (how many rows at each stage).
What the optimizer already handles well (don't re-optimize).
Whether logical-tree cost percentages are misleading.]

ACTIVE_CONSTRAINTS:
- [CORRECTNESS_CONSTRAINT_ID]: [Why it applies to this query, 1 line]
- [ENGINE_GAP_ID]: [Evidence from EXPLAIN that this gap is active]
(List all 4 correctness constraints + the 1-3 engine gaps that
are active for THIS query based on your EXPLAIN analysis.)

REGRESSION_WARNINGS:
1. [Pattern name] ([observed regression]):
   CAUSE: [What happened mechanistically]
   RULE: [Actionable avoidance rule for THIS query]
(If no regression warnings are relevant, write 'None applicable.')

=== WORKER 1 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 2 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 3 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 4 BRIEFING === (EXPLORATION WORKER)

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]
CONSTRAINT_OVERRIDE: [CONSTRAINT_ID or 'None']
OVERRIDE_REASONING: [Why this query's structure differs from the observed failure, or 'N/A']
EXPLORATION_TYPE: [constraint_relaxation | compound_strategy | novel_combination]

```

## Section Validation Checklist (MUST pass before final output)

Use this checklist to verify content quality, not just section presence:

### SHARED BRIEFING
- `SEMANTIC_CONTRACT`: 40-200 tokens and includes business intent, JOIN semantics, aggregation trap, and filter dependency.
- `BOTTLENECK_DIAGNOSIS`: states dominant mechanism, bound type (`scan-bound`/`join-bound`/`aggregation-bound`), cardinality flow, and what optimizer already handles well.
- `ACTIVE_CONSTRAINTS`: includes all 4 correctness IDs plus 1-3 active engine gaps with EXPLAIN evidence.
- `REGRESSION_WARNINGS`: either `None applicable.` or numbered entries with both `CAUSE:` and `RULE:`.

### WORKER N BRIEFING (N=1..4)
- `STRATEGY`: non-empty and unique across workers.
- `TARGET_LOGICAL_TREE`: explicit node chain (e.g., `a -> b -> c`).
- `NODE_CONTRACTS`: every logical tree node has a contract with `FROM`, `OUTPUT` (explicit columns), and `CONSUMERS`.
- `EXAMPLES`: 1-3 IDs per worker. Sharing an example across workers is allowed if each worker's EXAMPLE_ADAPTATION explains a different aspect to apply.
- `EXAMPLE_ADAPTATION`: for each example, states what to adapt and what to ignore for this worker's strategy.
- `HAZARD_FLAGS`: query-specific risks, not generic cautions.

### WORKER 4 EXPLORATION FIELDS
- Includes `CONSTRAINT_OVERRIDE`, `OVERRIDE_REASONING`, and `EXPLORATION_TYPE`.

## Transform Catalog

Select 4 transforms that are applicable to THIS query, maximizing structural diversity (each must attack a different part of the execution plan).

### Predicate Movement
- **global_predicate_pushdown**: Trace selective predicates from late in the CTE chain back to the earliest scan via join equivalences. Biggest win when a dimension filter is applied after a large intermediate materialization.
  Maps to examples: pushdown, early_filter, date_cte_isolate
- **transitive_predicate_propagation**: Infer predicates through join equivalence chains (A.key = B.key AND B.key = 5 -> A.key = 5). Especially across CTE boundaries where optimizers stop propagating.
  Maps to examples: early_filter, dimension_cte_isolate
- **null_rejecting_join_simplification**: When downstream WHERE rejects NULLs from the outer side of a LEFT JOIN, convert to INNER. Enables reordering and predicate pushdown. CHECK: does the query actually have LEFT/OUTER joins before assigning this.
  Maps to examples: (no direct gold example — novel transform)

### Join Restructuring
- **self_join_elimination**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, split into N pre-partitioned CTEs. Eliminates discriminator filtering and repeated hash probes on rows that don't match.
  Maps to examples: union_cte_split, shared_dimension_multi_channel
- **decorrelation**: Convert correlated EXISTS/IN/scalar subqueries to CTE + JOIN. CHECK: does the query actually have correlated subqueries before assigning this.
  Maps to examples: decorrelate, composite_decorrelate_union
- **aggregate_pushdown**: When GROUP BY follows a multi-table join but aggregation only uses columns from one side, push the GROUP BY below the join. CHECK: verify the join doesn't change row multiplicity for the aggregate (one-to-many breaks AVG/STDDEV).
  Maps to examples: (no direct gold example — novel transform)
- **late_attribute_binding**: When a dimension table is joined only to resolve display columns (names, descriptions) that aren't used in filters, aggregations, or join conditions, defer that join until after all filtering and aggregation is complete. Join on the surrogate key once against the final reduced result set. This eliminates N-1 dimension scans when the CTE references the dimension N times. CHECK: verify the deferred columns aren't used in WHERE, GROUP BY, or JOIN ON — only in the final SELECT.
  Maps to examples: dimension_cte_isolate (partial pattern), early_filter

### Scan Optimization
- **star_join_prefetch**: Pre-filter ALL dimension tables into CTEs, then probe fact table with the combined key intersection.
  Maps to examples: dimension_cte_isolate, multi_dimension_prefetch, prefetch_fact_join, date_cte_isolate
- **single_pass_aggregation**: Merge N subqueries on the same fact table into 1 scan with CASE/FILTER inside aggregates. CHECK: STDDEV_SAMP/VARIANCE are grouping-sensitive — FILTER over a combined group != separate per-group computation.
  Maps to examples: single_pass_aggregation, channel_bitmap_aggregation
- **scan_consolidation_pivot**: When a CTE is self-joined N times with each reference filtering to a different discriminator (e.g., year, channel), consolidate into fewer scans that GROUP BY the discriminator, then pivot rows to columns using MAX(CASE WHEN discriminator = X THEN agg_value END). This halves the fact scans and dimension joins. SAFE when GROUP BY includes the discriminator — each group is naturally partitioned, so aggregates like STDDEV_SAMP are computed correctly per-partition. The pivot MAX is just a row selector (one row per group), not a real aggregation.
  Maps to examples: single_pass_aggregation, union_cte_split

### Structural Transforms
- **union_consolidation**: Share dimension lookups across UNION ALL branches that scan different fact tables with the same dim joins.
  Maps to examples: shared_dimension_multi_channel
- **window_optimization**: Push filters before window functions when they don't affect the frame. Convert ROW_NUMBER + filter to LATERAL + LIMIT. Merge same-PARTITION windows into one sort pass.
  Maps to examples: deferred_window_aggregation
- **exists_restructuring**: Convert INTERSECT to EXISTS for semi-join short-circuit, or restructure complex EXISTS with shared CTEs. CHECK: does the query actually have INTERSECT or complex EXISTS.
  Maps to examples: intersect_to_exists, multi_intersect_exists_cte

## Strategy Selection Rules

1. **CHECK APPLICABILITY**: Each transform has a structural prerequisite (correlated subquery, UNION ALL CTE, LEFT JOIN, etc.). Verify the query actually has the prerequisite before assigning a transform. DO NOT assign decorrelation if there are no correlated subqueries.
2. **CHECK OPTIMIZER OVERLAP**: Read the EXPLAIN plan. If the optimizer already performs a transform (e.g., already splits a UNION CTE, already pushes a predicate), that transform will have marginal benefit. Note this in your reasoning and prefer transforms the optimizer is NOT already doing.
3. **MAXIMIZE DIVERSITY**: Each worker must attack a different part of the execution plan. Do not assign 'pushdown variant A' and 'pushdown variant B'. Assign transforms from different categories above.
4. **ASSESS RISK PER-QUERY**: Risk is a function of (transform x query complexity), not an inherent property of the transform. Decorrelation is low-risk on a simple EXISTS and high-risk on nested correlation inside a CTE. Assess per-assignment.
5. **COMPOSITION IS ALLOWED AND ENCOURAGED**: A strategy can combine 2-3 transforms from different categories (e.g., star_join_prefetch + scan_consolidation_pivot, or date_cte_isolate + early_filter + decorrelate). The TARGET_LOGICAL_TREE should reflect the combined structure. Compound strategies are often the source of the biggest wins.
6. **MINIMAL-CHANGE BASELINE**: If the EXPLAIN shows the optimizer already handles the primary bottleneck (e.g., already splits CTEs, already pushes predicates), consider assigning one worker as a minimal-change baseline: explicit JOINs only, no structural changes. This provides a regression-safe fallback.

Each worker gets 1-3 examples. If fewer than 2 examples genuinely match the worker's strategy, assign 1 and state 'No additional examples apply.' Do NOT pad with irrelevant examples — an irrelevant example is worse than no example because the worker will try to apply its pattern. No duplicate examples across workers. Use example IDs from the catalog above.

For TARGET_LOGICAL_TREE: Define the CTE structure you want produced. For NODE_CONTRACTS: Be exhaustive with OUTPUT columns — missing columns cause semantic breaks.

## Exploration Budget (Worker 4)

Workers 1-3 follow the engine profile's proven patterns. **Worker 4 is the EXPLORATION worker** with a different mandate:

Worker 4 MAY (in priority order — prefer higher-value exploration):
  (c) **PREFERRED**: Attempt a novel technique not listed in the engine profile, if the EXPLAIN plan reveals an optimizer blind spot not yet documented. This is the highest-value exploration — new discoveries expand the engine profile for all future queries.
  (b) Combine 2-3 transforms from different engine gaps into a compound strategy that hasn't been tested before. Medium value — tests interaction effects between known patterns.
  (a) Retry a technique from 'what_didnt_work', IF the structural context of THIS query differs materially from the observed failure — explain the structural difference in HAZARD_FLAGS. Lowest priority — only when the query structure clearly diverges from the failed case.

Worker 4 may NEVER violate correctness constraints (LITERAL_PRESERVATION, SEMANTIC_EQUIVALENCE, COMPLETE_OUTPUT, CTE_COLUMN_COMPLETENESS).

The exploration worker's output is tagged EXPLORATORY and tracked separately. Past failures documented in the engine profile are context-specific — they happened on specific queries with specific structures. Worker 4's job is to test whether those failures generalize or not. If Worker 4 discovers a new win, it becomes field intelligence for the engine profile.

## Output Consumption Spec

Each worker receives:
1. SHARED BRIEFING (SEMANTIC_CONTRACT + BOTTLENECK_DIAGNOSIS + ACTIVE_CONSTRAINTS + REGRESSION_WARNINGS)
2. Their specific WORKER N BRIEFING (STRATEGY + TARGET_LOGICAL_TREE + NODE_CONTRACTS + EXAMPLES + EXAMPLE_ADAPTATION + HAZARD_FLAGS)
3. Full before/after SQL for their assigned examples (retrieved by example ID)
4. The original query SQL (full, as reference)
5. Column completeness contract + output format spec

Workers do NOT see other workers' briefings.
Presentation order: briefing first (understanding), then examples (patterns), then original SQL (source), then output format (mechanics).