You are a senior SQL optimization architect for DuckDB v1.4.3. You have FULL FREEDOM to design your own approach — you are NOT constrained to any specific logical tree topology or CTE structure. Your job: diagnose WHY previous workers failed to reach 2.0x, identify unexplored optimization angles, and produce an optimized SQL rewrite that reaches the target.

Preserve defensive guards: if the original uses CASE WHEN x > 0 THEN y/x END around a division, keep it — guards prevent silent breakage. Strip benchmark comments (-- start query, -- end query) from output.

## Target: >=2.0x speedup

Your target is >=2.0x speedup on this query. This is the bar. Anything below 2.0x is a miss.

## Previous Optimization Attempts
Target: **>=2.0x** | 4 workers tried | none reached target

### W3: window_function_decorrelation → 1.0x ★ BEST [PASS, below target (1.0x)]
- **Examples**: SELF_JOIN_TO_WINDOW, CORRELATED_SUBQUERY_TO_WINDOW, single_pass_aggregation
- **Transforms**: decorrelate
- **Approach**: - SELF_JOIN_TO_WINDOW: Replace self-join for store averages with window function
- **Optimized SQL:**
```sql
WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000), store_filter AS (SELECT s_store_sk FROM store WHERE s_state = 'SD'), prefetched_returns AS (SELECT sr_customer_sk, sr_store_sk, sr_fee FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk), customer_totals_with_avg AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return, AVG(SUM(sr_fee)) OVER (PARTITION BY sr_store_sk) * 1.2 AS store_avg_threshold FROM prefetched_returns GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_totals_with_avg ctr1 JOIN store_filter ON ctr1.ctr_store_sk = s_store_sk JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk WHERE ctr1.ctr_total_return > ctr1.store_avg_threshold ORDER BY c_customer_id LIMIT 100
```

### W4: compound_prefetch_with_single_pass → 1.0x ★ BEST [PASS, below target (1.0x)]
- **Examples**: prefetch_fact_join, dimension_cte_isolate, decorrelate
- **Transforms**: prefetch_fact_join
- **Approach**: - prefetch_fact_join: Apply staged reduction (date filter → fact join → aggregat
- **Optimized SQL:**
```sql
WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000), returns_all_stores AS (SELECT sr_customer_sk, sr_store_sk, sr_fee FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk), customer_totals_all AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM returns_all_stores GROUP BY sr_customer_sk, sr_store_sk), store_avgs_all AS (SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS store_avg_threshold FROM customer_totals_all GROUP BY ctr_store_sk), store_filter AS (SELECT s_store_sk FROM store WHERE s_state = 'SD') SELECT c_customer_id FROM customer_totals_all AS ctr1 JOIN store_filter ON ctr1.ctr_store_sk = store_filter.s_store_sk JOIN store_avgs_all ON ctr1.ctr_store_sk = store_avgs_all.ctr_store_sk JOIN customer ON ctr1.ctr_customer_sk = customer.c_customer_sk WHERE ctr1.ctr_total_return > store_avgs_all.store_avg_threshold ORDER BY c_customer_id LIMIT 100
```

### W1: star_join_prefetch → 0.0x [ERROR]
- **Examples**: dimension_cte_isolate, prefetch_fact_join, date_cte_isolate
- **Transforms**: dimension_cte_isolate
- **Approach**: - dimension_cte_isolate: Apply pre-filtering of store and date into separate CTE
- **Error**: Original query failed: Catalog Error: Table with name store_returns does not exist!
Did you mean "sqlite_master"?

LINE 6: from store_returns
             ^
- **Optimized SQL:**
```sql
WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000), store_filter AS (SELECT s_store_sk FROM store WHERE s_state = 'SD'), prefetched_returns AS (SELECT sr_customer_sk, sr_store_sk, sr_fee FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk JOIN store_filter ON sr_store_sk = s_store_sk), customer_total_return_sd AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM prefetched_returns GROUP BY sr_customer_sk, sr_store_sk), customer_total_return_all AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk GROUP BY sr_customer_sk, sr_store_sk), store_avgs AS (SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS store_avg_threshold FROM customer_total_return_all GROUP BY ctr_store_sk) SELECT c_customer_id FROM customer_total_return_sd ctr1 JOIN store_avgs ON ctr1.ctr_store_sk = store_avgs.ctr_store_sk JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk WHERE ctr1.ctr_total_return > store_avgs.store_avg_threshold ORDER BY c_customer_id LIMIT 100
```
- **Planner Diagnosis:**
```
[EXPLAIN failed — planner rejected this SQL]
Error: Original query failed: Catalog Error: Table with name store_returns does not exist!
Did you mean "sqlite_master"?

LINE 6: from store_returns
             ^
```

### W2: cte_splitting_with_shared_dimension → 0.0x [ERROR]
- **Examples**: union_cte_split, shared_dimension_multi_channel, decorrelate
- **Transforms**: decorrelate
- **Approach**: - union_cte_split: Apply splitting of CTE into specialized versions (SD vs all s
- **Error**: Original query failed: Catalog Error: Table with name store_returns does not exist!
Did you mean "sqlite_master"?

LINE 6: from store_returns
             ^
- **Optimized SQL:**
```sql
WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_year = 2000), store_filter AS (SELECT s_store_sk FROM store WHERE s_state = 'SD'), returns_all_stores AS (SELECT sr_customer_sk, sr_store_sk, sr_fee FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk), customer_total_return_all AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM returns_all_stores GROUP BY sr_customer_sk, sr_store_sk), store_avgs_all AS (SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS store_avg_threshold FROM customer_total_return_all GROUP BY ctr_store_sk), returns_sd_stores AS (SELECT sr_customer_sk, sr_store_sk, sr_fee FROM store_returns JOIN date_filter ON sr_returned_date_sk = d_date_sk JOIN store_filter ON sr_store_sk = s_store_sk), customer_total_return_sd AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM returns_sd_stores GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return_sd AS ctr1 JOIN store_avgs_all ON ctr1.ctr_store_sk = store_avgs_all.ctr_store_sk JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk WHERE ctr1.ctr_total_return > store_avgs_all.store_avg_threshold ORDER BY c_customer_id LIMIT 100
```
- **Planner Diagnosis:**
```
[EXPLAIN failed — planner rejected this SQL]
Error: Original query failed: Catalog Error: Table with name store_returns does not exist!
Did you mean "sqlite_master"?

LINE 6: from store_returns
             ^
```


## Original Execution Plan (EXPLAIN ANALYZE)

Compare each candidate's plan (above) against this baseline.

```
┌─────────────────────────────────────┐
│┌───────────────────────────────────┐│
││    Query Profiling Information    ││
│└───────────────────────────────────┘│
└─────────────────────────────────────┘
EXPLAIN ANALYZE -- start query 1 in stream 0 using template query1.tpl with customer_total_return as (select sr_customer_sk as ctr_customer_sk ,sr_store_sk as ctr_store_sk ,sum(SR_FEE) as ctr_total_return from store_returns ,date_dim where sr_returned_date_sk = d_date_sk and d_year =2000 group by sr_customer_sk ,sr_store_sk)  select c_customer_id from customer_total_return ctr1 ,store ,customer where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2 from customer_total_return ctr2 where ctr1.ctr_store_sk = ctr2.ctr_store_sk) and s_store_sk = ctr1.ctr_store_sk and s_state = 'SD' and ctr1.ctr_customer_sk = c_customer_sk order by c_customer_id  LIMIT 100;  -- end query 1 in stream 0 using template query1.tpl 
┌────────────────────────────────────────────────┐
│┌──────────────────────────────────────────────┐│
││              Total Time: 0.113s              ││
│└──────────────────────────────────────────────┘│
└────────────────────────────────────────────────┘
┌───────────────────────┐
│         QUERY         │
└───────────┬───────────┘
┌───────────┴───────────┐
│    EXPLAIN_ANALYZE    │
│    ────────────────   │
│         0 rows        │
│        (0.00s)        │
└───────────┬───────────┘
┌───────────┴───────────┐
│          CTE          │
│    ────────────────   │
│       CTE Name:       │
│ customer_total_return │
│                       ├─────────────────────────────────────┐
│     Table Index: 0    │                                     │
│                       │                                     │
│         0 rows        │                                     │
│        (0.00s)        │                                     │
└───────────┬───────────┘                                     │
┌───────────┴───────────┐                         ┌───────────┴───────────┐
│       PROJECTION      │                         │         TOP_N         │
│    ────────────────   │                         │    ────────────────   │
│           #0          │                         │        Top: 100       │
│__internal_decompress_i│                         │                       │
│ ntegral_integer(#1, 1)│                         │       Order By:       │
│           #2          │                         │    tpcds_sf10.main    │
│                       │                         │       .customer       │
│                       │                         │   .c_customer_id ASC  │
│                       │                         │                       │
│      539,331 rows     │                         │        100 rows       │
│        (0.00s)        │                         │        (0.00s)        │
└───────────┬───────────┘                         └───────────┬───────────┘
┌───────────┴───────────┐                         ┌───────────┴───────────┐
│     HASH_GROUP_BY     │                         │       PROJECTION      │
│    ────────────────   │                         │    ────────────────   │
│        Groups:        │                         │           #0          │
│           #0          │                         │                       │
│           #1          │                         │                       │
│                       │                         │                       │
│      Aggregates:      │                         │                       │
│        sum(#2)        │                         │                       │
│                       │                         │                       │
│      539,331 rows     │                         │      61,974 rows      │
│        (0.19s)        │                         │        (0.00s)        │
└───────────┬───────────┘                         └───────────┬───────────┘
┌───────────┴───────────┐                         ┌───────────┴───────────┐
│       PROJECTION      │                         │         FILTER        │
│    ────────────────   │                         │    ────────────────   │
│     sr_customer_sk    │                         │ (CAST(ctr_total_return│
│      sr_store_sk      │                         │ AS DOUBLE) > SUBQUERY)│
│         sr_fee        │                         │                       │
│                       │                         │                       │
│      557,705 rows     │                         │      61,974 rows      │
│        (0.00s)        │                         │        (0.00s)        │
└───────────┬───────────┘                         └───────────┬───────────┘
┌───────────┴───────────┐                         ┌───────────┴───────────┐
│       PROJECTION      │                         │    LEFT_DELIM_JOIN    │
│    ────────────────   │                         │    ────────────────   │
│           #0          │                         │    Join Type: LEFT    │
│__internal_compress_int│                         │                       │
│ egral_utinyint(#1, 1) │                         │      Conditions:      │
│           #2          │                         │  ctr_store_sk IS NOT  │
│__internal_compress_int│                         │      DISTINCT FROM    ├──────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────┐
│  egral_usmallint(#3,  │                         │      ctr_store_sk     │                                                              │                                                                          │
│        2450820)       │                         │                       │                                                              │                                                                          │
│                       │                         │     Delim Index: 1    │                                                              │                                                                          │
│                       │                         │                       │                                                              │                                                                          │
│      557,705 rows     │                         │         0 rows        │                                                              │                                                                          │
... (80 more lines truncated)
```

## Semantic Contract (MUST preserve)

Find South Dakota customers whose total 2000 store-return fees at a given store exceed 120% of that store's average customer return total (across all customers returning to that store). JOIN semantics: INNER joins between CTE, store, and customer require all sides to match; the correlated subquery computes per-store average across ALL stores in the CTE, not just SD stores. Aggregation trap: The store average must be computed over the same 2000 date-filtered store_returns as the customer totals. Filter dependency: The s_state='SD' filter applies only to the store dimension for the main comparison, not to the store average calculation.

## Bottleneck Diagnosis

Scan-bound: store_returns TABLE_SCAN (0.71s, 63% of total) dominates. The CTE aggregates 557K rows to 539K rows (3% reduction). The store filter (s_state='SD') reduces CTE rows from 539K to 158K (71% reduction) but is applied AFTER CTE materialization. The optimizer already decorrelates the subquery efficiently (LEFT_DELIM_JOIN computes store averages once). Logical-tree cost percentages are misleading: the 50% cost shown for CTE underrepresents the actual 70% time spent on store_returns scan + aggregation.

## Engine Profile

*This is field intelligence gathered from 88 TPC-DS queries at SF1-SF10. Use it to guide your analysis but apply your own judgment — every query is different. Add to this knowledge if you observe something new.*

### Optimizer Strengths (DO NOT fight these)
- **INTRA_SCAN_PREDICATE_PUSHDOWN**: Pushes WHERE filters directly into SEQ_SCAN. Single-table predicates are applied at scan time, zero overhead.
- **SAME_COLUMN_OR**: OR on the SAME column (e.g., t_hour BETWEEN 8 AND 11 OR t_hour BETWEEN 16 AND 17) is handled in a single scan with range checks.
- **HASH_JOIN_SELECTION**: Selects hash joins automatically. Join ordering is generally sound for 2-4 table joins.
- **CTE_INLINING**: CTEs referenced once are typically inlined (treated as subquery). Multi-referenced CTEs may be materialized.
- **COLUMNAR_PROJECTION**: Only reads columns actually referenced. Unused columns have zero I/O cost.
- **PARALLEL_AGGREGATION**: Scans and aggregations parallelized across threads. PERFECT_HASH_GROUP_BY is highly efficient.
- **EXISTS_SEMI_JOIN**: EXISTS/NOT EXISTS uses semi-join with early termination — stops after first match per outer row.

### Optimizer Gaps (opportunities)
- **CROSS_CTE_PREDICATE_BLINDNESS**: Cannot push predicates from the outer query backward into CTE definitions.
  Opportunity: Move selective predicates INTO the CTE definition. Pre-filter dimensions/facts before they get materialized.
    + Q6/Q11: 4.00x — date filter moved into CTE
    + Q63: 3.77x — pre-joined filtered dates with fact table before other dims
    + Q93: 2.97x — dimension filter applied before LEFT JOIN chain
- **REDUNDANT_SCAN_ELIMINATION**: Cannot detect when the same fact table is scanned N times with similar filters across subquery boundaries.
  Opportunity: Consolidate N subqueries on the same table into 1 scan with CASE WHEN / FILTER() inside aggregates.
    + Q88: 6.28x — 8 time-bucket subqueries consolidated into 1 scan with 8 CASE branches
    + Q9: 4.47x — 15 separate store_sales scans consolidated into 1 scan with 5 CASE buckets
- **CORRELATED_SUBQUERY_PARALYSIS**: Cannot automatically decorrelate correlated aggregate subqueries into GROUP BY + JOIN.
  Opportunity: Convert correlated WHERE to CTE with GROUP BY on the correlation column, then JOIN back.
    + Q1: 2.92x — correlated AVG with store_sk correlation converted to GROUP BY store_sk + JOIN
- **CROSS_COLUMN_OR_DECOMPOSITION**: Cannot decompose OR conditions that span DIFFERENT columns into independent targeted scans.
  Opportunity: Split cross-column ORs into UNION ALL branches, each with a targeted single-column filter.
    + Q88: 6.28x — 8 time-bucket subqueries with distinct hour ranges (distinct access paths)
    + Q15: 3.17x — (zip OR state OR price) split to 3 targeted branches
    + Q10: 1.49x, Q45: 1.35x, Q41: 1.89x
- **LEFT_JOIN_FILTER_ORDER_RIGIDITY**: Cannot reorder LEFT JOINs to apply selective dimension filters before expensive fact table joins.
  Opportunity: Pre-filter the selective dimension into a CTE, then use the filtered result as the JOIN partner.
    + Q93: 2.97x — filtered reason dimension FIRST, then LEFT JOIN to returns then fact
    + Q80: 1.40x — dimension isolation before fact join
- **UNION_CTE_SELF_JOIN_DECOMPOSITION**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, the optimizer materializes the full UNION once and probes it N times, discarding most rows each time.
  Opportunity: Split the UNION ALL into N separate CTEs (one per discriminator value).
    + Q74: 1.36x — UNION of store/web sales split into separate year-partitioned CTEs

## Reference Examples

Pattern reference only — do not copy table/column names or literals.

### 1. decorrelate (2.92x)

**Principle:** Decorrelation: convert correlated subqueries to standalone CTEs with GROUP BY, then JOIN. Correlated subqueries re-execute per outer row; a pre-computed CTE executes once.

**BEFORE (slow):**
```sql
with customer_total_return as
(select sr_customer_sk as ctr_customer_sk
,sr_store_sk as ctr_store_sk
,sum(SR_FEE) as ctr_total_return
from store_returns
,date_dim
where sr_returned_date_sk = d_date_sk
and d_year =2000
group by sr_customer_sk
,sr_store_sk)
 select c_customer_id
from customer_total_return ctr1
,store
,customer
where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2
from customer_total_return ctr2
where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
and s_store_sk = ctr1.ctr_store_sk
and s_state = 'SD'
and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
 LIMIT 100;
```

**AFTER (fast):**
[filtered_returns]:
```sql
SELECT sr.sr_customer_sk, sr.sr_store_sk, sr.sr_fee FROM store_returns sr JOIN date_dim d ON sr.sr_returned_date_sk = d.d_date_sk JOIN store s ON sr.sr_store_sk = s.s_store_sk WHERE d.d_year = 2000 AND s.s_state = 'SD'
```
[customer_total_return]:
```sql
SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM filtered_returns GROUP BY sr_customer_sk, sr_store_sk
```
[store_avg_return]:
```sql
SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS avg_return_threshold FROM customer_total_return GROUP BY ctr_store_sk
```
[main_query]:
```sql
SELECT c.c_customer_id FROM customer_total_return ctr1 JOIN store_avg_return sar ON ctr1.ctr_store_sk = sar.ctr_store_sk JOIN customer c ON ctr1.ctr_customer_sk = c.c_customer_sk WHERE ctr1.ctr_total_return > sar.avg_return_threshold ORDER BY c.c_customer_id LIMIT 100
```

### 2. shared_dimension_multi_channel (1.30x)

**Principle:** Shared Dimension Extraction: when multiple channel CTEs (store/catalog/web) apply identical dimension filters, extract those shared filters into one CTE and reference it from each channel. Avoids redundant dimension scans.

**BEFORE (slow):**
```sql
with ssr as
 (select  s_store_id as store_id,
          sum(ss_ext_sales_price) as sales,
          sum(coalesce(sr_return_amt, 0)) as "returns",
          sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit
  from store_sales left outer join store_returns on
         (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),
     date_dim,
     store,
     item,
     promotion
 where ss_sold_date_sk = d_date_sk
       and d_date between cast('1998-08-28' as date) 
                  and (cast('1998-08-28' as date) + INTERVAL 30 DAY)
       and ss_store_sk = s_store_sk
       and ss_item_sk = i_item_sk
       and i_current_price > 50
       and ss_promo_sk = p_promo_sk
       and p_channel_tv = 'N'
 group by s_store_id)
 ,
 csr as
 (select  cp_catalog_page_id as catalog_page_id,
          sum(cs_ext_sales_price) as sales,
          sum(coalesce(cr_return_amount, 0)) as "returns",
          sum(cs_net_profit - coalesce(cr_net_loss, 0)) as profit
  from catalog_sales left outer join catalog_returns on
         (cs_item_sk = cr_item_sk and cs_order_number = cr_order_number),
     date_dim,
     catalog_page,
     item,
     promotion
 where cs_sold_date_sk = d_date_sk
       and d_date between cast('1998-08-28' as date)
                  and (cast('1998-08-28' as date) + INTERVAL 30 DAY)
        and cs_catalog_page_sk = cp_catalog_page_sk
       and cs_item_sk = i_item_sk
       and i_current_price > 50
       and cs_promo_sk = p_promo_sk
       and p_channel_tv = 'N'
group by cp_catalog_page_id)
 ,
 wsr as
 (select  web_site_id,
          sum(ws_ext_sales_price) as sales,
          sum(coalesce(wr_return_amt, 0)) as "returns",
          sum(ws_net_profit - coalesce(wr_net_loss, 0)) as profit
  from web_sales left outer join web_returns on
         (ws_item_sk = wr_item_sk and ws_order_number = wr_order_number),
     date_dim,
     web_site,
     item,
     promotion
 where ws_sold_date_sk = d_date_sk
       and d_date between cast('1998-08-28' as date)
                  and (cast('1998-08-28' as date) + INTERVAL 30 DAY)
        and ws_web_site_sk = web_site_sk
       and ws_item_sk = i_item_sk
       and i_current_price > 50
       and ws_promo_sk = p_promo_sk
       and p_channel_tv = 'N'
group by web_site_id)
  select channel
        , id
        , sum(sales) as sales
        , sum("returns") as "returns"
        , sum(profit) as profit
 from 
 (select 'store channel' as channel
        , 'store' || store_id as id
        , sales
        , "returns"
        , profit
 from   ssr
 union all
 select 'catalog channel' as channel
        , 'catalog_page' || catalog_page_id as id
        , sales
        , "returns"
        , profit
 from  csr
 union all
 select 'web channel' as channel
        , 'web_site' || web_site_id as id
        , sales
        , "returns"
        , profit
 from   wsr
 ) x
 group by rollup (channel, id)
 order by channel
         ,id
 LIMIT 100;
```

**AFTER (fast):**
[filtered_dates]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1998-08-28' AS DATE) AND (CAST('1998-08-28' AS DATE) + INTERVAL '30' DAY)
```
[filtered_items]:
```sql
SELECT i_item_sk FROM item WHERE i_current_price > 50
```
[filtered_promotions]:
```sql
SELECT p_promo_sk FROM promotion WHERE p_channel_tv = 'N'
```
[prefiltered_store_sales]:
```sql
SELECT ss_item_sk, ss_store_sk, ss_ticket_number, ss_ext_sales_price, ss_net_profit FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk JOIN filtered_items ON ss_item_sk = i_item_sk JOIN filtered_promotions ON ss_promo_sk = p_promo_sk
```
[prefiltered_web_sales]:
```sql
SELECT ws_item_sk, ws_web_site_sk, ws_order_number, ws_ext_sales_price, ws_net_profit FROM web_sales JOIN filtered_dates ON ws_sold_date_sk = d_date_sk JOIN filtered_items ON ws_item_sk = i_item_sk JOIN filtered_promotions ON ws_promo_sk = p_promo_sk
```
[ssr]:
```sql
SELECT s_store_id AS store_id, SUM(ss_ext_sales_price) AS sales, SUM(COALESCE(sr_return_amt, 0)) AS returns, SUM(ss_net_profit - COALESCE(sr_net_loss, 0)) AS profit FROM prefiltered_store_sales LEFT OUTER JOIN store_returns ON (ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number) JOIN store ON ss_store_sk = s_store_sk GROUP BY s_store_id
```
[wsr]:
```sql
SELECT web_site_id, SUM(ws_ext_sales_price) AS sales, SUM(COALESCE(wr_return_amt, 0)) AS returns, SUM(ws_net_profit - COALESCE(wr_net_loss, 0)) AS profit FROM prefiltered_web_sales LEFT OUTER JOIN web_returns ON (ws_item_sk = wr_item_sk AND ws_order_number = wr_order_number) JOIN web_site ON ws_web_site_sk = web_site_sk GROUP BY web_site_id
```

### 3. date_cte_isolate (4.00x)

**Principle:** Dimension Isolation: extract small dimension lookups into CTEs so they materialize once and subsequent joins probe a tiny hash table instead of rescanning.

**BEFORE (slow):**
```sql
select a.ca_state state, count(*) cnt
 from customer_address a
     ,customer c
     ,store_sales s
     ,date_dim d
     ,item i
 where       a.ca_address_sk = c.c_current_addr_sk
 	and c.c_customer_sk = s.ss_customer_sk
 	and s.ss_sold_date_sk = d.d_date_sk
 	and s.ss_item_sk = i.i_item_sk
 	and d.d_month_seq = 
 	     (select distinct (d_month_seq)
 	      from date_dim
               where d_year = 2002
 	        and d_moy = 3 )
 	and i.i_current_price > 1.2 * 
             (select avg(j.i_current_price) 
 	     from item j 
 	     where j.i_category = i.i_category)
 group by a.ca_state
 having count(*) >= 10
 order by cnt, a.ca_state
 LIMIT 100;
```

**AFTER (fast):**
[target_month]:
```sql
SELECT DISTINCT d_month_seq FROM date_dim WHERE d_year = 2000 AND d_moy = 1
```
[category_avg_price]:
```sql
SELECT i_category, AVG(i_current_price) * 1.2 AS avg_threshold FROM item GROUP BY i_category
```
[filtered_dates]:
```sql
SELECT d_date_sk FROM date_dim JOIN target_month ON d_month_seq = target_month.d_month_seq
```
[filtered_sales]:
```sql
SELECT ss_customer_sk, ss_item_sk FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk
```
[main_query]:
```sql
SELECT a.ca_state AS state, COUNT(*) AS cnt FROM customer_address a JOIN customer c ON a.ca_address_sk = c.c_current_addr_sk JOIN filtered_sales s ON c.c_customer_sk = s.ss_customer_sk JOIN item i ON s.ss_item_sk = i.i_item_sk JOIN category_avg_price cap ON i.i_category = cap.i_category WHERE i.i_current_price > cap.avg_threshold GROUP BY a.ca_state HAVING COUNT(*) >= 10 ORDER BY cnt, a.ca_state LIMIT 100
```

### 4. deferred_window_aggregation (1.36x)

**Principle:** Deferred Aggregation: delay expensive operations (window functions) until after joins reduce the dataset. Computing window functions inside individual CTEs then joining is more expensive than joining first and computing windows once on the combined result.

**BEFORE (slow):**
```sql
WITH web_v1 as (
select
  ws_item_sk item_sk, d_date,
  sum(sum(ws_sales_price))
      over (partition by ws_item_sk order by d_date rows between unbounded preceding and current row) cume_sales
from web_sales
    ,date_dim
where ws_sold_date_sk=d_date_sk
  and d_month_seq between 1216 and 1216+11
  and ws_item_sk is not NULL
group by ws_item_sk, d_date),
store_v1 as (
select
  ss_item_sk item_sk, d_date,
  sum(sum(ss_sales_price))
      over (partition by ss_item_sk order by d_date rows between unbounded preceding and current row) cume_sales
from store_sales
    ,date_dim
where ss_sold_date_sk=d_date_sk
  and d_month_seq between 1216 and 1216+11
  and ss_item_sk is not NULL
group by ss_item_sk, d_date)
 select *
from (select item_sk
     ,d_date
     ,web_sales
     ,store_sales
     ,max(web_sales)
         over (partition by item_sk order by d_date rows between unbounded preceding and current row) web_cumulative
     ,max(store_sales)
         over (partition by item_sk order by d_date rows between unbounded preceding and current row) store_cumulative
     from (select case when web.item_sk is not null then web.item_sk else store.item_sk end item_sk
                 ,case when web.d_date is not null then web.d_date else store.d_date end d_date
                 ,web.cume_sales web_sales
                 ,store.cume_sales store_sales
           from web_v1 web full outer join store_v1 store on (web.item_sk = store.item_sk
                                                          and web.d_date = store.d_date)
          )x )y
where web_cumulative > store_cumulative
order by item_sk
        ,d_date
 LIMIT 100;
```

**AFTER (fast):**
[web_daily]:
```sql
SELECT ws_item_sk AS item_sk, d_date, SUM(ws_sales_price) AS daily_sales FROM web_sales, date_dim WHERE ws_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1216 AND 1216 + 11 AND ws_item_sk IS NOT NULL GROUP BY ws_item_sk, d_date
```
[store_daily]:
```sql
SELECT ss_item_sk AS item_sk, d_date, SUM(ss_sales_price) AS daily_sales FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1216 AND 1216 + 11 AND ss_item_sk IS NOT NULL GROUP BY ss_item_sk, d_date
```
[main_query]:
```sql
SELECT * FROM (SELECT COALESCE(web.item_sk, store.item_sk) AS item_sk, COALESCE(web.d_date, store.d_date) AS d_date, SUM(web.daily_sales) OVER (PARTITION BY COALESCE(web.item_sk, store.item_sk) ORDER BY COALESCE(web.d_date, store.d_date) ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS web_cumulative, SUM(store.daily_sales) OVER (PARTITION BY COALESCE(web.item_sk, store.item_sk) ORDER BY COALESCE(web.d_date, store.d_date) ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS store_cumulative FROM web_daily web FULL OUTER JOIN store_daily store ON web.item_sk = store.item_sk AND web.d_date = store.d_date) y WHERE web_cumulative > store_cumulative ORDER BY item_sk, d_date LIMIT 100
```

### 5. intersect_to_exists (1.83x)

**Principle:** Semi-Join Short-Circuit: replace INTERSECT with EXISTS to avoid full materialization and sorting. INTERSECT must compute complete result sets before intersecting; EXISTS stops at the first match per row, enabling semi-join optimizations.

**BEFORE (slow):**
```sql
with  cross_items as
 (select i_item_sk ss_item_sk
 from item,
 (select iss.i_brand_id brand_id
     ,iss.i_class_id class_id
     ,iss.i_category_id category_id
 from store_sales
     ,item iss
     ,date_dim d1
 where ss_item_sk = iss.i_item_sk
   and ss_sold_date_sk = d1.d_date_sk
   and d1.d_year between 2000 AND 2000 + 2
 intersect 
 select ics.i_brand_id
     ,ics.i_class_id
     ,ics.i_category_id
 from catalog_sales
     ,item ics
     ,date_dim d2
 where cs_item_sk = ics.i_item_sk
   and cs_sold_date_sk = d2.d_date_sk
   and d2.d_year between 2000 AND 2000 + 2
 intersect
 select iws.i_brand_id
     ,iws.i_class_id
     ,iws.i_category_id
 from web_sales
     ,item iws
     ,date_dim d3
 where ws_item_sk = iws.i_item_sk
   and ws_sold_date_sk = d3.d_date_sk
   and d3.d_year between 2000 AND 2000 + 2)
 where i_brand_id = brand_id
      and i_class_id = class_id
      and i_category_id = category_id
),
 avg_sales as
 (select avg(quantity*list_price) average_sales
  from (select ss_quantity quantity
             ,ss_list_price list_price
       from store_sales
           ,date_dim
       where ss_sold_date_sk = d_date_sk
         and d_year between 2000 and 2000 + 2
       union all 
       select cs_quantity quantity 
             ,cs_list_price list_price
       from catalog_sales
           ,date_dim
       where cs_sold_date_sk = d_date_sk
         and d_year between 2000 and 2000 + 2 
       union all
       select ws_quantity quantity
             ,ws_list_price list_price
       from web_sales
           ,date_dim
       where ws_sold_date_sk = d_date_sk
         and d_year between 2000 and 2000 + 2) x)
  select channel, i_brand_id,i_class_id,i_category_id,sum(sales), sum(number_sales)
 from(
       select 'store' channel, i_brand_id,i_class_id
             ,i_category_id,sum(ss_quantity*ss_list_price) sales
             , count(*) number_sales
       from store_sales
           ,item
           ,date_dim
       where ss_item_sk in (select ss_item_sk from cross_items)
         and ss_item_sk = i_item_sk
         and ss_sold_date_sk = d_date_sk
         and d_year = 2000+2 
         and d_moy = 11
       group by i_brand_id,i_class_id,i_category_id
       having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)
       union all
       select 'catalog' channel, i_brand_id,i_class_id,i_category_id, sum(cs_quantity*cs_list_price) sales, count(*) number_sales
       from catalog_sales
           ,item
           ,date_dim
       where cs_item_sk in (select ss_item_sk from cross_items)
         and cs_item_sk = i_item_sk
         and cs_sold_date_sk = d_date_sk
         and d_year = 2000+2 
         and d_moy = 11
       group by i_brand_id,i_class_id,i_category_id
       having sum(cs_quantity*cs_list_price) > (select average_sales from avg_sales)
       union all
       select 'web' channel, i_brand_id,i_class_id,i_category_id, sum(ws_quantity*ws_list_price) sales , count(*) number_sales
       from web_sales
           ,item
           ,date_dim
       where ws_item_sk in (select ss_item_sk from cross_items)
         and ws_item_sk = i_item_sk
         and ws_sold_date_sk = d_date_sk
         and d_year = 2000+2
         and d_moy = 11
       group by i_brand_id,i_class_id,i_category_id
       having sum(ws_quantity*ws_list_price) > (select average_sales from avg_sales)
 ) y
 group by rollup (channel, i_brand_id,i_class_id,i_category_id)
 order by channel,i_brand_id,i_class_id,i_category_id
 LIMIT 100;
```

**AFTER (fast):**
[cross_items_flat]:
```sql
SELECT i.i_item_sk AS ss_item_sk FROM item i WHERE EXISTS (SELECT 1 FROM store_sales, item iss, date_dim d1 WHERE ss_item_sk = iss.i_item_sk AND ss_sold_date_sk = d1.d_date_sk AND d1.d_year BETWEEN 1999 AND 2001 AND iss.i_brand_id = i.i_brand_id AND iss.i_class_id = i.i_class_id AND iss.i_category_id = i.i_category_id) AND EXISTS (SELECT 1 FROM catalog_sales, item ics, date_dim d2 WHERE cs_item_sk = ics.i_item_sk AND cs_sold_date_sk = d2.d_date_sk AND d2.d_year BETWEEN 1999 AND 2001 AND ics.i_brand_id = i.i_brand_id AND ics.i_class_id = i.i_class_id AND ics.i_category_id = i.i_category_id) AND EXISTS (SELECT 1 FROM web_sales, item iws, date_dim d3 WHERE ws_item_sk = iws.i_item_sk AND ws_sold_date_sk = d3.d_date_sk AND d3.d_year BETWEEN 1999 AND 2001 AND iws.i_brand_id = i.i_brand_id AND iws.i_class_id = i.i_class_id AND iws.i_category_id = i.i_category_id)
```
[main_query]:
```sql
SELECT ... FROM ... WHERE i_item_sk IN (SELECT ss_item_sk FROM cross_items_flat) ...
```

### 6. multi_intersect_exists_cte (2.39x)

**BEFORE (slow):**
```sql
with cross_items as
 (select i_item_sk ss_item_sk
 from item,
 (select iss.i_brand_id brand_id, iss.i_class_id class_id, iss.i_category_id category_id
 from store_sales, item iss, date_dim d1
 where ss_item_sk = iss.i_item_sk and ss_sold_date_sk = d1.d_date_sk
   and d1.d_year between 2000 AND 2000 + 2
 intersect
 select ics.i_brand_id, ics.i_class_id, ics.i_category_id
 from catalog_sales, item ics, date_dim d2
 where cs_item_sk = ics.i_item_sk and cs_sold_date_sk = d2.d_date_sk
   and d2.d_year between 2000 AND 2000 + 2
 intersect
 select iws.i_brand_id, iws.i_class_id, iws.i_category_id
 from web_sales, item iws, date_dim d3
 where ws_item_sk = iws.i_item_sk and ws_sold_date_sk = d3.d_date_sk
   and d3.d_year between 2000 AND 2000 + 2)
 where i_brand_id = brand_id and i_class_id = class_id and i_category_id = category_id),
 avg_sales as
 (select avg(quantity*list_price) average_sales
  from (select ss_quantity quantity, ss_list_price list_price from store_sales, date_dim where ss_sold_date_sk = d_date_sk and d_year between 2000 and 2000 + 2
   union all select cs_quantity, cs_list_price from catalog_sales, date_dim where cs_sold_date_sk = d_date_sk and d_year between 2000 and 2000 + 2
   union all select ws_quantity, ws_list_price from web_sales, date_dim where ws_sold_date_sk = d_date_sk and d_year between 2000 and 2000 + 2) x)
 select channel, i_brand_id, i_class_id, i_category_id, sum(sales), sum(number_sales)
 from (
  select 'store' channel, i_brand_id, i_class_id, i_category_id, sum(ss_quantity*ss_list_price) sales, count(*) number_sales
  from store_sales, item, date_dim
  where ss_item_sk in (select ss_item_sk from cross_items) and ss_item_sk = i_item_sk and ss_sold_date_sk = d_date_sk and d_year = 2000+2 and d_moy = 11
  group by i_brand_id, i_class_id, i_category_id
  having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)
  union all
  select 'catalog' channel, i_brand_id, i_class_id, i_category_id, sum(cs_quantity*cs_list_price) sales, count(*) number_sales
  from catalog_sales, item, date_dim
  where cs_item_sk in (select ss_item_sk from cross_items) and cs_item_sk = i_item_sk and cs_sold_date_sk = d_date_sk and d_year = 2000+2 and d_moy = 11
  group by i_brand_id, i_class_id, i_category_id
  having sum(cs_quantity*cs_list_price) > (select average_sales from avg_sales)
  union all
  select 'web' channel, i_brand_id, i_class_id, i_category_id, sum(ws_quantity*ws_list_price) sales, count(*) number_sales
  from web_sales, item, date_dim
  where ws_item_sk in (select ss_item_sk from cross_items) and ws_item_sk = i_item_sk and ws_sold_date_sk = d_date_sk and d_year = 2000+2 and d_moy = 11
  group by i_brand_id, i_class_id, i_category_id
  having sum(ws_quantity*ws_list_price) > (select average_sales from avg_sales)
 ) y
 group by rollup (channel, i_brand_id, i_class_id, i_category_id)
 order by channel, i_brand_id, i_class_id, i_category_id
 LIMIT 100;
```

**AFTER (fast):**
[filtered_dates]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_year BETWEEN 2000 AND 2002
```
[cross_items]:
```sql
SELECT i_item_sk AS ss_item_sk, i_brand_id, i_class_id, i_category_id FROM item WHERE EXISTS (SELECT 1 FROM store_sales JOIN item iss ON ss_item_sk = iss.i_item_sk JOIN filtered_dates d1 ON ss_sold_date_sk = d1.d_date_sk WHERE iss.i_brand_id = item.i_brand_id AND iss.i_class_id = item.i_class_id AND iss.i_category_id = item.i_category_id) AND EXISTS (SELECT 1 FROM catalog_sales JOIN item ics ON cs_item_sk = ics.i_item_sk JOIN filtered_dates d2 ON cs_sold_date_sk = d2.d_date_sk WHERE ics.i_brand_id = item.i_brand_id AND ics.i_class_id = item.i_class_id AND ics.i_category_id = item.i_category_id) AND EXISTS (SELECT 1 FROM web_sales JOIN item iws ON ws_item_sk = iws.i_item_sk JOIN filtered_dates d3 ON ws_sold_date_sk = d3.d_date_sk WHERE iws.i_brand_id = item.i_brand_id AND iws.i_class_id = item.i_class_id AND iws.i_category_id = item.i_category_id)
```

## Correctness Invariants (HARD STOPS — non-negotiable)

These 4 constraints are absolute. Even with full creative freedom, you may NEVER violate these:

- **COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.
- **CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
- **LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.
- **SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## Aggregation Semantics Check (HARD STOP)

- STDDEV_SAMP/VARIANCE are grouping-sensitive — changing group membership changes the result.
- AVG and STDDEV are NOT duplicate-safe.
- FILTER over a combined group != separate per-group computation.
- Verify aggregation equivalence for ANY proposed restructuring.

## Regression Warnings

### regression_q1_decorrelate: decorrelate on q1 (0.71x)
**Anti-pattern:** Do not pre-aggregate GROUP BY results into CTEs when the query uses them in a correlated comparison (e.g., customer return > 1.2x store average). The optimizer can compute aggregates incrementally with filter pushdown; materialization loses this.
**Mechanism:** Pre-computed customer_total_return (GROUP BY customer, store) and store_avg_return (GROUP BY store) as separate CTEs. The original correlated subquery computed the per-store average incrementally during the customer scan, filtering as it goes. Materializing forces full aggregation of ALL stores before any filtering.

### regression_q51_date_cte_isolate: date_cte_isolate on q51 (0.87x)
**Anti-pattern:** Do not materialize running/cumulative window aggregates into CTEs before joins that filter based on those aggregates. The optimizer can co-optimize window evaluation and join filtering together.
**Mechanism:** Materialized cumulative window functions (SUM() OVER ORDER BY) into separate CTEs (web_v1, store_v1) before a FULL OUTER JOIN that filters on web_cumulative > store_cumulative. The original evaluates windows lazily during the join, co-optimizing window computation with the join filter. Materialization forces full window computation before filtering.

### regression_q25_date_cte_isolate: date_cte_isolate on q25 (0.5x)
**Anti-pattern:** Do not pre-filter and materialize fact tables when the query has 3+ fact table joins. The optimizer needs freedom to reorder multi-way fact joins and push filters across them.
**Mechanism:** Pre-filtered and joined store_sales to date CTE BEFORE the 3-way fact table join (store_sales <- store_returns <- catalog_sales). By materializing the date-filtered store_sales early, the optimizer loses the ability to push filters across the multi-way fact join and reorder joins optimally.


## Original SQL

```sql
with customer_total_return as
(select sr_customer_sk as ctr_customer_sk
,sr_store_sk as ctr_store_sk
,sum(SR_FEE) as ctr_total_return
from store_returns
,date_dim
where sr_returned_date_sk = d_date_sk
and d_year =2000
group by sr_customer_sk
,sr_store_sk)
 select c_customer_id
from customer_total_return ctr1
,store
,customer
where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2
from customer_total_return ctr2
where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
and s_store_sk = ctr1.ctr_store_sk
and s_state = 'SD'
and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
 LIMIT 100;
```

## Your Task — Self-Directed Retry

Work through these 3 steps in a `<reasoning>` block, then output your optimized SQL:

1. **DIAGNOSE**: Why did the best worker achieve 1.0x instead of the 2.0x target? What do the EXPLAIN plans reveal about the actual execution bottleneck?
2. **IDENTIFY**: What optimization angles are still unexplored? What did the empirical results reveal that couldn't have been known before seeing the execution plans?
3. **REWRITE**: Produce optimized SQL that exploits the angles you identified. You may build on the best foundation or start fresh.

## Rewrite Checklist (must pass before final SQL)

- Verify output schema matches the Column Completeness Contract (same columns, same names, same order).
- Keep all semantic invariants from `Correctness Invariants` (including join/null behavior).
- Verify aggregation equivalence: same rows participate in each group, same aggregate semantics.
- Preserve all literals exactly (numbers, strings, date values).
- Apply `Hazard Flags` as hard guards against known failure modes.

### Column Completeness Contract

Your `main_query` component MUST produce **exactly** these output columns (same names, same order):

  1. `c_customer_id`

Do NOT add, remove, or rename any output columns. The result set schema must be identical to the original query.

## Output Format

Your response has **two parts** in order:

### Part 1: Modified Logic Tree

Show what changed using change markers. Generate the tree BEFORE writing SQL.

Change markers:
- `[+]` — New component added
- `[-]` — Component removed
- `[~]` — Component modified (describe what changed)
- `[=]` — Unchanged (no children needed)
- `[!]` — Structural change (e.g. CTE → subquery)

### Part 2: Component Payload JSON

```json
{
  "spec_version": "1.0",
  "dialect": "<dialect>",
  "rewrite_rules": [
    {"id": "R1", "type": "<transform_name>", "description": "<what changed>", "applied_to": ["<component_id>"]}
  ],
  "statements": [{
    "target_table": null,
    "change": "modified",
    "components": {
      "<cte_name>": {
        "type": "cte",
        "change": "modified",
        "sql": "<complete SQL for this CTE body>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<upstream_id>"]}
      },
      "main_query": {
        "type": "main_query",
        "change": "modified",
        "sql": "<final SELECT>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<cte_name>"]}
      }
    },
    "reconstruction_order": ["<cte_name>", "main_query"],
    "assembly_template": "WITH <cte_name> AS ({<cte_name>}) {main_query}"
  }],
  "macros": {},
  "frozen_blocks": [],
  "validation_checks": []
}
```

### Rules
- **Tree first, always.** Generate the Logic Tree before writing any SQL
- **One component at a time.** When writing SQL for component X, treat others as opaque interfaces
- **No ellipsis.** Every `sql` value must be complete, executable SQL
- **Frozen blocks are copy-paste.** Large CASE-WHEN lookups must be verbatim
- **Validate interfaces.** Verify every `consumes` reference exists in upstream `outputs`
- Only include components you **changed or added** — set unchanged components to `"change": "unchanged"` with `"sql": ""`
- `main_query` output columns must match the Column Completeness Contract above
- `reconstruction_order`: topological order of components for assembly

After the JSON, explain the mechanism:

```
Changes: <1-2 sentences: what structural change + the expected mechanism>
Expected speedup: <estimate>
```

Now output your Logic Tree and Component Payload JSON: