## 1. STRUCTURAL BREAKDOWN

**Innermost subquery (dw1)**: Computes aggregated sales (price × quantity) for store sales over a 12-month period (month_seq 1206-1217). Joins store_sales (fact table) with date_dim (date filter), store, and item (dimensions). Uses ROLLUP to generate hierarchical subtotals across 8 grouping dimensions. Processes ~5.4M fact rows with dimensions, outputs aggregated rows for all grouping combinations.

**Middle subquery (dw2)**: Computes rank within each i_category partition based on sumsales DESC. Takes dw1's aggregated result, applies window function to rank sales performance per category across all ROLLUP levels (including subtotal rows where some dimensions are NULL).

**Outer query**: Filters to top 100 ranks per category, orders by all 10 columns (including grouping dimensions and sumsales), then limits to 100 total rows. Final output shows top performers within each category across multiple aggregation levels.

## 2. BOTTLENECK IDENTIFICATION

The dominant cost center is the **large ROLLUP aggregation on 5.4M fact rows joined with 3 dimension tables**. The MECHANISM causing slowness is:

1. **Unfiltered dimension joins**: All three dimension tables (date_dim, store, item) are joined in their entirety before any aggregation, though only date_dim has a filter predicate.
2. **Excessive ROLLUP cardinality**: ROLLUP on 8 columns generates 2^8 = 256 grouping sets. With 5.4M input rows, this creates massive intermediate results before the window function.
3. **Late filtering**: The rank filter (rk <= 100) and LIMIT 100 are applied AFTER computing the entire ROLLUP and window function, wasting computation on rows that won't make the final cut.

The window function itself partitions by i_category and sorts by sumsales DESC, which requires a sort of the entire ROLLUP result per category. This is expensive but a symptom of the large input, not the root cause.

## 3. PROPOSED OPTIMIZATION

**Optimization 1: Pre-filter dimensions into CTEs before join**
- **What**: Extract filtered date_dim (month_seq filter), store (no filter), and item (no filter) into separate CTEs before joining with store_sales.
- **Why**: DuckDB can build smaller hash tables for dimension joins. The date_dim filter reduces dimension size early, and separate CTEs help the optimizer push predicates.
- **Risk**: Must ensure all needed columns are preserved (d_year, d_qoy, d_moy from date_dim). No semantic change.
- **Estimated impact**: Moderate (1.3-1.5x)

**Optimization 2: Push rank filtering into subquery**
- **What**: Convert RANK() window to use QUALIFY clause or move filter earlier: `QUALIFY RANK() OVER (PARTITION BY i_category ORDER BY sumsales DESC) <= 100`
- **Why**: DuckDB's optimizer may push the rank filter down, reducing the amount of data sorted and processed by the window function. Currently processes all ROLLUP rows, then filters 99%+ away.
- **Risk**: QUALIFY syntax may not be supported or may change semantics with NULL groups from ROLLUP. Must test thoroughly.
- **Estimated impact**: Significant (2-3x if optimization works)

**Optimization 3: Eliminate unnecessary ROLLUP columns for ranking**
- **What**: The business question likely wants ranking at the most granular level (product-store-month), but ROLLUP creates aggregated rows (like category totals) that also get ranked. Consider if business logic requires ranking aggregated subtotals.
- **Why**: If only granular rankings matter, remove ROLLUP or filter out aggregated rows (where any grouping column is NULL) before ranking.
- **Risk**: Changes semantics if business needs category/subcategory rankings.
- **Estimated impact**: Major (3-5x reduction in rows to rank)

## 4. FAILURE ANALYSIS

**Previous attempt: date_cte_isolate → REGRESSION (0.85x)**
- **Why it failed**: Isolating date_dim into a CTE likely prevented join reordering or predicate pushdown that DuckDB's optimizer could do with the original explicit joins. The date filter (d_month_seq BETWEEN) is already selective; forcing materialization may have added overhead.
- **Constraint learned**: DuckDB's optimizer handles simple dimension filtering well in explicit joins. Structural changes must preserve the optimizer's ability to reorder joins based on statistics.

## 5. RECOMMENDED STRATEGY

Implement a **two-phase filtering strategy**:

1. **Create filtered fact CTE**: First join store_sales with ONLY date_dim (filtered) to reduce fact rows early using the selective date predicate. This leverages date_dim's small size and selective filter.
   
2. **Materialize dimension CTEs**: Create CTEs for store and item with only needed columns (store_id, item category/brand/etc.) to serve as dimension lookups.

3. **Join filtered fact with dimensions**: Join the reduced fact set with store and item CTEs.

4. **Apply QUALIFY for early rank filtering**: Use DuckDB's QUALIFY clause to filter during window computation rather than after.

The key insight: **date_dim is the most selective filter and should be applied BEFORE other dimension joins**. Store and item have no filters, so joining them early adds cost without reducing rows.

Implementation sketch:
```sql
WITH filtered_dates AS (
  SELECT d_date_sk, d_year, d_qoy, d_moy
  FROM date_dim
  WHERE d_month_seq BETWEEN 1206 AND 1206 + 11
),
filtered_sales AS (
  SELECT ss_sales_price, ss_quantity, ss_item_sk, ss_store_sk,
         d.d_year, d.d_qoy, d.d_moy
  FROM store_sales ss
  JOIN filtered_dates d ON ss.ss_sold_date_sk = d.d_date_sk
),
item_dim AS (
  SELECT i_item_sk, i_category, i_class, i_brand, i_product_name
  FROM item
),
store_dim AS (
  SELECT s_store_sk, s_store_id
  FROM store
)
SELECT i_category, i_class, i_brand, i_product_name,
       d_year, d_qoy, d_moy, s_store_id, sumsales,
       RANK() OVER (PARTITION BY i_category ORDER BY sumsales DESC) AS rk
FROM (
  SELECT i_category, i_class, i_brand, i_product_name,
         d_year, d_qoy, d_moy, s_store_id,
         SUM(COALESCE(ss_sales_price * ss_quantity, 0)) AS sumsales
  FROM filtered_sales fs
  JOIN item_dim i ON fs.ss_item_sk = i.i_item_sk
  JOIN store_dim s ON fs.ss_store_sk = s.s_store_sk
  GROUP BY ROLLUP(i_category, i_class, i_brand, i_product_name,
                  d_year, d_qoy, d_moy, s_store_id)
) dw1
QUALIFY rk <= 100
ORDER BY i_category, i_class, i_brand, i_product_name,
         d_year, d_qoy, d_moy, s_store_id, sumsales, rk
LIMIT 100;
```

## 6. EXAMPLE SELECTION

**FAISS picks**: deferred_window_aggregation, prefetch_fact_join, decorrelate

**My selection**: 
- **prefetch_fact_join** (most relevant): Pre-filtering date_dim and pre-joining with fact table matches our strategy.
- **early_filter**: Directly applicable - filter date_dim first before other joins.
- **dimension_cte_isolate**: Creating CTEs for store and item dimensions isolates them for cleaner optimization.

**Why not deferred_window_aggregation**: The window function depends on the ROLLUP aggregation result, so it cannot be deferred past joins. The window is the final computation step.

**Why not decorrelate**: No correlated subqueries in this query.

EXAMPLES: prefetch_fact_join, early_filter, dimension_cte_isolate