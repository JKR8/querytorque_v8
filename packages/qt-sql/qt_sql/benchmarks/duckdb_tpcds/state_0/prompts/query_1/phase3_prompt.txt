You are a SQL query rewrite engine.

Your goal: rewrite the complete SQL query to maximize execution speed
while preserving exact semantic equivalence (same rows, same columns,
same ordering).

You will receive the full query, its DAG structure showing how CTEs and
subqueries connect, cost analysis per node, and suggested rewrite patterns.
You may restructure the query freely: create new CTEs, merge existing ones,
push filters across node boundaries, or decompose subqueries.

## Query: query_1

```sql
WITH customer_total_return AS (
  SELECT
    sr_customer_sk AS ctr_customer_sk,
    sr_store_sk AS ctr_store_sk,
    SUM(SR_FEE) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE
    sr_returned_date_sk = d_date_sk AND d_year = 2000
  GROUP BY
    sr_customer_sk,
    sr_store_sk
)
SELECT
  c_customer_id
FROM customer_total_return AS ctr1, store, customer
WHERE
  ctr1.ctr_total_return > (
    SELECT
      AVG(ctr_total_return) * 1.2
    FROM customer_total_return AS ctr2
    WHERE
      ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'SD'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY
  c_customer_id
LIMIT 100
```

## DAG Topology

```sql
-- DAG TOPOLOGY
-- Depth 0:
--   customer_total_return (cte, 50% cost) [GROUP_BY]
--     outputs: [ctr_customer_sk, ctr_store_sk, ctr_total_return]
-- Depth 1:
--   main_query (main, 50% cost) [GROUP_BY, CORRELATED] ← reads [customer_total_return, customer_total_return]
--     outputs: [c_customer_id]
-- Edges:
--   customer_total_return → main_query
--   customer_total_return → main_query
```

## Performance Profile

**customer_total_return**: 50% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, SEQ_SCAN[store_returns], SEQ_SCAN[date_dim]
**main_query**: 50% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[customer_total_return], SEQ_SCAN[store], SEQ_SCAN[customer]

## Suggested Rewrite Strategy

Phase 2 analysis identified these optimization opportunities:

- **customer_total_return** → apply **pushdown**
  Heuristic: 50.0% cost, flags=['GROUP_BY']
- **main_query** → apply **decorrelate**
  Heuristic: 50.0% cost, flags=['GROUP_BY', 'CORRELATED']

## Reference Example: decorrelate

### BEFORE (slow)
```sql
[customer_total_return] CORRELATED:
SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return
FROM store_returns, date_dim
WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
GROUP BY sr_customer_sk, sr_store_sk

[main_query]:
SELECT c_customer_id FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (SELECT avg(ctr_total_return)*1.2 FROM customer_total_return ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk)
  AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id LIMIT 100
```

**Key insight:** Push s_state='SD' filter EARLY into first CTE. Compute average as SEPARATE CTE with GROUP BY (NOT window function). Join on average instead of correlated subquery.

### AFTER (fast)
[filtered_returns]:
```sql
SELECT sr.sr_customer_sk, sr.sr_store_sk, sr.sr_fee FROM store_returns sr JOIN date_dim d ON sr.sr_returned_date_sk = d.d_date_sk JOIN store s ON sr.sr_store_sk = s.s_store_sk WHERE d.d_year = 2000 AND s.s_state = 'SD'
```
[customer_total_return]:
```sql
SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(sr_fee) AS ctr_total_return FROM filtered_returns GROUP BY sr_customer_sk, sr_store_sk
```
[store_avg_return]:
```sql
SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS avg_return_threshold FROM customer_total_return GROUP BY ctr_store_sk
```
[main_query]:
```sql
SELECT c.c_customer_id FROM customer_total_return ctr1 JOIN store_avg_return sar ON ctr1.ctr_store_sk = sar.ctr_store_sk JOIN customer c ON ctr1.ctr_customer_sk = c.c_customer_sk WHERE ctr1.ctr_total_return > sar.avg_return_threshold ORDER BY c.c_customer_id LIMIT 100
```

## Constraints

### CRITICAL — Correctness Guards (top of sandwich)

**SEMANTIC_EQUIVALENCE**
The rewritten query MUST return exactly the same rows, columns, and
ordering as the original. This is the prime directive.

**LITERAL_PRESERVATION**
Keep all literal values (dates, strings, numbers) exactly as they appear in
the original SQL. Do not round, truncate, or reformat them.

### HIGH — Performance and Style Rules (middle of sandwich)

**NO_UNFILTERED_DIM_CTE**
When creating a new CTE that scans a dimension table, include at least one
filter predicate. Never materialize an entire dimension without a WHERE clause.

**OR_TO_UNION_LIMIT**
When converting OR predicates to UNION ALL, limit to 4 branches maximum.
Beyond 4, the UNION overhead exceeds the OR scan cost for most planners.

**EXPLICIT_JOINS**
Convert comma-separated implicit joins to explicit JOIN ... ON syntax.
This gives the optimizer better join-order freedom.

### CRITICAL — Correctness Guards (bottom of sandwich)

**KEEP_EXISTS_AS_EXISTS**
Preserve EXISTS/NOT EXISTS subqueries as-is. Do not convert them to
IN/NOT IN or to JOINs — this risks NULL-handling semantic changes.

**COMPLETE_OUTPUT**
The rewritten query must output ALL columns from the original SELECT.
Never drop, rename, or reorder output columns.

## Output

Return the complete rewritten SQL query. The query must be syntactically
valid and ready to execute.

```sql
-- Your rewritten query here
```

After the SQL, briefly explain what you changed:

```
Changes: <1-2 sentence summary of the rewrite>
Expected speedup: <estimate>
```

Now output your rewritten SQL: