## 1. STRUCTURAL BREAKDOWN

**frequent_ss_items**: Finds store items sold frequently (>4 times) on specific dates within 2000-2003. Scans store_sales (fact), date_dim, and item (dimension). Outputs ~1k rows of (item_sk, solddate, itemdesc, count).

**max_store_sales**: Computes the maximum total sales amount per customer within 2000-2003 from store_sales. Scans store_sales, customer, and date_dim. Outputs a single row with one scalar value.

**best_ss_customer**: Identifies customers whose total store sales (across all time) exceed 95% of the maximum found in max_store_sales. Scans store_sales and customer. Outputs ~1k rows of qualifying customer_sk.

**main_query**: Aggregates sales from catalog_sales and web_sales for May 2000, filtered to frequent items and best customers. Scans two large fact tables (catalog_sales, web_sales) plus customer and date_dim. Outputs 100 rows of customer names with sales totals.

## 2. BOTTLENECK IDENTIFICATION

The dominant cost center is the **triple scanning of store_sales** with different join patterns and aggregation levels:

1. **frequent_ss_items**: Full scan of store_sales with joins to date_dim and item
2. **max_store_sales**: Full scan of store_sales with joins to date_dim and customer  
3. **best_ss_customer**: Full scan of store_sales with join to customer

**Mechanism**: Each CTE performs a separate full table scan on the massive store_sales table (billions of rows) with overlapping date filters (years 2000-2003). The redundant scans are expensive because:
- Same date range filtering applied three times independently
- Large hash joins rebuilt three times
- No predicate pushdown optimization between CTEs

Additionally, the **best_ss_customer** CTE has a semantic inconsistency: it compares customers' lifetime sales (no date filter) against a maximum computed from 2000-2003 only. This causes an unnecessary full scan where a filtered scan could suffice if business logic permits.

## 3. PROPOSED OPTIMIZATION

### Change 1: Single-pass store_sales aggregation with multiple grouping sets
**What**: Merge frequent_ss_items and max_store_sales into one CTE that scans store_sales once with date filtering, computing both item-date aggregates and customer aggregates.

**Why**: Eliminates two redundant scans of store_sales (~28M rows each). Single pass computes both aggregates simultaneously.

**Risk**: Must ensure grouping logic remains correct - frequent_ss_items groups by item+date, max_store_sales groups by customer.

**Impact**: Significant (reduces store_sales scans from 3 to 1)

### Change 2: Decouple best_ss_customer from date filtering inconsistency
**What**: Modify best_ss_customer to filter store_sales by the same 2000-2003 date range as max_store_sales, making the comparison semantically consistent.

**Why**: Enables reuse of the pre-aggregated customer totals from Change 1, eliminating the third store_sales scan entirely.

**Risk**: Changes business logic - now compares customer sales from 2000-2003 against maximum from same period. Must verify if lifetime comparison was intentional.

**Impact**: Moderate to significant (eliminates another full scan)

### Change 3: Pre-materialize date_dim CTE for shared filtering
**What**: Extract date_dim filtering for 2000-2003 into a reusable CTE, then join fact tables to this pre-filtered dimension.

**Why**: Reduces repeated date_dim scans and enables better predicate pushdown to fact tables.

**Risk**: Minimal - preserves exact same filtering logic.

**Impact**: Moderate (reduces dimension table processing)

## 4. RECOMMENDED STRATEGY

Implement a **single-pass store_sales aggregation CTE** that computes both item-date frequencies and customer sales totals for 2000-2003 in one scan. Then derive frequent_ss_items and best_ss_customer from this pre-aggregated data. This structural change:
1. Scans store_sales once instead of three times
2. Pre-filters date range at the earliest possible stage  
3. Allows max_store_sales to be computed from already-aggregated customer totals
4. Makes best_ss_customer filter consistent with the date range

Implementation sketch:
```sql
WITH store_sales_agg AS (
  SELECT
    ss_item_sk,
    ss_customer_sk,
    d_date,
    COUNT(*) as item_date_cnt,
    SUM(ss_quantity * ss_sales_price) as customer_sales
  FROM store_sales
  JOIN date_dim ON ss_sold_date_sk = d_date_sk
  WHERE d_year IN (2000, 2001, 2002, 2003)
  GROUP BY GROUPING SETS (
    (ss_item_sk, d_date),
    (ss_customer_sk)
  )
),
frequent_ss_items AS (
  SELECT ss_item_sk as item_sk, d_date as solddate, ...
  FROM store_sales_agg
  WHERE item_date_cnt > 4
    AND ss_item_sk IS NOT NULL
  JOIN item ON ss_item_sk = i_item_sk
),
max_store_sales AS (
  SELECT MAX(customer_sales) as tpcds_cmax
  FROM store_sales_agg
  WHERE ss_customer_sk IS NOT NULL
),
...
```

## 5. EXAMPLE SELECTION

EXAMPLES: single_pass_aggregation, shared_dimension_multi_channel, pushdown

**Rationale**: 
- **single_pass_aggregation** directly addresses the triple store_sales scan by consolidating multiple aggregations
- **shared_dimension_multi_channel** applies because catalog_sales and web_sales share the same date/customer filters
- **pushdown** is crucial for moving date filters into the earliest CTEs

The FAISS-selected examples (intersect_to_exists, union_cte_split) are less relevant since this query doesn't use INTERSECT and already has a well-structured UNION ALL.