You are an expert database performance analyst. Your job is to deeply analyze a slow SQL query, identify the root cause of its performance problems, and propose specific structural changes.

You follow a rigorous methodology: understand the structure, profile the costs, identify the mechanism (not just the symptom), propose changes with correctness reasoning, and learn from past failures.

## Query: query_1
## Dialect: duckdb

```sql
WITH sd_stores AS (
  SELECT
    s_store_sk
  FROM store
  WHERE
    s_state = 'SD'
), store_customer_returns AS (
  SELECT
    sr_customer_sk AS ctr_customer_sk,
    sr_store_sk AS ctr_store_sk,
    SUM(SR_FEE) AS ctr_total_return,
    AVG(SUM(SR_FEE)) OVER (PARTITION BY sr_store_sk) AS store_avg_return
  FROM store_returns
  JOIN date_dim
    ON sr_returned_date_sk = d_date_sk
  JOIN sd_stores
    ON sr_store_sk = sd_stores.s_store_sk
  WHERE
    d_year = 2000
  GROUP BY
    sr_customer_sk,
    sr_store_sk
)
SELECT
  c_customer_id
FROM store_customer_returns AS ctr1
JOIN customer
  ON ctr1.ctr_customer_sk = c_customer_sk
WHERE
  ctr1.ctr_total_return > ctr1.store_avg_return * 1.2
ORDER BY
  c_customer_id
LIMIT 100
```

## Query Structure (DAG)

### 1. sd_stores
**Role**: CTE (Definition Order: 0)
**Stats**: 0% Cost | ~35 rows
**Outputs**: [s_store_sk]
**Dependencies**: store
**Filters**: s_state = 'SD'
**Operators**: SEQ_SCAN[store]
**Key Logic (SQL)**:
```sql
SELECT
  s_store_sk
FROM store
WHERE
  s_state = 'SD'
```

### 2. store_customer_returns
**Role**: CTE (Definition Order: 1)
**Stats**: 89% Cost | ~557k rows
**Flags**: GROUP_BY, WINDOW
**Outputs**: [ctr_customer_sk, ctr_store_sk, ctr_total_return, store_avg_return]
**Dependencies**: store_returns, date_dim (join), sd_stores (join)
**Filters**: d_year = 2000
**Operators**: SEQ_SCAN[store_returns], SEQ_SCAN[date_dim], HASH_GROUP_BY, HASH_JOIN, LEFT_DELIM_JOIN
**Key Logic (SQL)**:
```sql
SELECT
  sr_customer_sk AS ctr_customer_sk,
  sr_store_sk AS ctr_store_sk,
  SUM(SR_FEE) AS ctr_total_return,
  AVG(SUM(SR_FEE)) OVER (PARTITION BY sr_store_sk) AS store_avg_return
FROM store_returns
JOIN date_dim
  ON sr_returned_date_sk = d_date_sk
JOIN sd_stores
  ON sr_store_sk = sd_stores.s_store_sk
WHERE
  d_year = 2000
GROUP BY
  sr_customer_sk,
  sr_store_sk
```

### 3. main_query
**Role**: Root / Output (Definition Order: 2)
**Stats**: 8% Cost | ~539k rows processed → 100 rows output
**Flags**: GROUP_BY, ORDER_BY, LIMIT(100)
**Outputs**: [c_customer_id] — ordered by c_customer_id ASC
**Dependencies**: store_customer_returns AS ctr1 (join), customer (join)
**Filters**: ctr1.ctr_total_return > ctr1.store_avg_return * 1.2
**Operators**: SEQ_SCAN[customer], CTE_SCAN, COLUMN_DATA_SCAN, CTE_SCAN, SEQ_SCAN[DELIM_SCAN]
**Key Logic (SQL)**:
```sql
SELECT
  c_customer_id
FROM store_customer_returns AS ctr1
JOIN customer
  ON ctr1.ctr_customer_sk = c_customer_sk
WHERE
  ctr1.ctr_total_return > ctr1.store_avg_return * 1.2
ORDER BY
  c_customer_id
LIMIT 100
```

### Edges
- sd_stores → store_customer_returns
- store_customer_returns → main_query


## Reference Examples

**FAISS selected (by structural similarity):** decorrelate, deferred_window_aggregation, shared_dimension_multi_channel

**All available gold examples:**

- **composite_decorrelate_union** (2.42xx) — Decorrelate multiple correlated EXISTS subqueries into pre-materialized DISTINCT
- **date_cte_isolate** (4.00xx) — Extract date filtering into a separate CTE to enable predicate pushdown and redu
- **decorrelate** (2.92xx) — Convert correlated subquery to separate CTE with GROUP BY, then JOIN
- **deferred_window_aggregation** (1.36xx) — When multiple CTEs each perform GROUP BY + WINDOW (cumulative sum), then are joi
- **dimension_cte_isolate** (1.93xx) — Pre-filter ALL dimension tables into CTEs before joining with fact table, not ju
- **early_filter** (4.00xx) — Filter dimension tables FIRST, then join to fact tables to reduce expensive join
- **intersect_to_exists** (1.83xx) — Convert INTERSECT subquery pattern to multiple EXISTS clauses for better join pl
- **materialize_cte** (1.37xx) — Extract repeated subquery patterns into a CTE to avoid recomputation
- **multi_date_range_cte** (2.35xx) — When query uses multiple date_dim aliases with different filters (d1, d2, d3), c
- **multi_dimension_prefetch** (2.71xx) — Pre-filter multiple dimension tables (date + store) into separate CTEs before jo
- **or_to_union** (3.17xx) — Split OR conditions on different columns into UNION ALL branches for better inde
- **prefetch_fact_join** (3.77xx) — Pre-filter dimension table into CTE, then pre-join with fact table in second CTE
- **pushdown** (2.11xx) — Push filters from outer query into CTEs/subqueries to reduce intermediate result
- **shared_dimension_multi_channel** (1.30xx) — Extract shared dimension filters (date, item, promotion) into CTEs when multiple
- **single_pass_aggregation** (4.47xx) — Consolidate multiple subqueries scanning the same table into a single CTE with c
- **union_cte_split** (1.36xx) — Split a generic UNION ALL CTE into specialized CTEs when the main query filters 

## Your Task

Analyze this query following these steps IN ORDER:

### 1. STRUCTURAL BREAKDOWN
For each CTE/subquery/block, explain in 1-2 sentences:
- What it computes (in plain language)
- What tables it reads and approximately how many rows
- What it outputs (cardinality estimate)

### 2. BOTTLENECK IDENTIFICATION
Using the DAG costs above, identify the dominant cost center.
Don't just name it — explain the MECHANISM:
- Is it a full table scan that could be filtered?
- Is it a sort for a window function that could be deferred?
- Is it a hash join on a large build side that could be pre-filtered?
- Is it scanning the same table multiple times when once would suffice?

### 3. PROPOSED OPTIMIZATION
Propose 1-3 specific structural changes. For EACH one:
- **What**: Exactly what to change (e.g., 'merge CTEs X and Y into one scan')
- **Why**: The performance mechanism (e.g., 'eliminates a 28M-row rescan of store_sales')
- **Risk**: What semantic constraint could break (e.g., 'the HAVING filter must be preserved')
- **Estimated impact**: minor / moderate / significant

### 5. RECOMMENDED STRATEGY
Synthesize everything into a single recommended optimization approach.
Be specific enough that another engineer could implement it from your description.

### 6. EXAMPLE SELECTION
FAISS selected these examples: decorrelate, deferred_window_aggregation, shared_dimension_multi_channel
Review the FAISS picks against the available examples above.
If you think different examples would be more relevant for this query,
list your preferred examples. Otherwise confirm the FAISS picks are good.

```
EXAMPLES: example_id_1, example_id_2, example_id_3
```

Use exact IDs from the available examples list above.
