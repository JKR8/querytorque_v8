You are a SQL rewrite engine for PostgreSQL v16.11-0ubuntu0.24.04.1). Follow the Target Logical Tree structure below. Your job is to write correct, executable SQL for each node — not to decide whether to restructure. Preserve exact semantic equivalence (same rows, same columns, same ordering). Preserve defensive guards: if the original uses CASE WHEN x > 0 THEN y/x END around a division, keep it — even when a WHERE clause makes the zero case unreachable. Guards prevent silent breakage if filters change upstream. Strip benchmark comments (-- start query, -- end query) from your output.

## Semantic Contract (MUST preserve)

This query finds total sales in Oct 1998 from catalog and web channels for items that sold frequently in 1998 (same item sold >4 times per day) and customers whose lifetime spending exceeds 95% of the maximum customer spend in 1998. All joins are INNER (must match). Aggregates are COUNT(*) and SUM(quantity*price) with HAVING filters; no sensitive aggregates. The UNION ALL branches depend on the same CTE results; breaking correlation in best_ss_customer must preserve the 95% threshold logic.

## Target Logical Tree + Node Contracts

Build your rewrite following this CTE structure. Each node's OUTPUT list is exhaustive — your SQL must produce exactly those columns.

TARGET_LOGICAL_TREE:
frequent_ss_items -> max_store_sales -> threshold_value -> best_ss_customer -> main_union
NODE_CONTRACTS:
  frequent_ss_items: (same as original, but with explicit JOINs)
  max_store_sales: (same as original)
  threshold_value:
    FROM: max_store_sales
    OUTPUT: tpcds_cmax * (95/100.0) as threshold
    EXPECTED_ROWS: 1
    CONSUMERS: best_ss_customer
  best_ss_customer:
    FROM: store_sales JOIN customer ON ss_customer_sk = c_customer_sk
    WHERE: c_birth_year BETWEEN 1934 AND 1940
    GROUP BY: c_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as ssales
    HAVING: ssales > (SELECT threshold FROM threshold_value)
    OUTPUT: c_customer_sk, ssales
    EXPECTED_ROWS: 24K
    CONSUMERS: main_union
  main_union: (same as Worker 1)

NODE_CONTRACTS:
frequent_ss_items: (same as original, but with explicit JOINs)
  max_store_sales: (same as original)
  threshold_value:
    FROM: max_store_sales
    OUTPUT: tpcds_cmax * (95/100.0) as threshold
    EXPECTED_ROWS: 1
    CONSUMERS: best_ss_customer
  best_ss_customer:
    FROM: store_sales JOIN customer ON ss_customer_sk = c_customer_sk
    WHERE: c_birth_year BETWEEN 1934 AND 1940
    GROUP BY: c_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as ssales
    HAVING: ssales > (SELECT threshold FROM threshold_value)
    OUTPUT: c_customer_sk, ssales
    EXPECTED_ROWS: 24K
    CONSUMERS: main_union
  main_union: (same as Worker 1)

## Hazard Flags (avoid these specific risks)

- Must use MATERIALIZED on threshold_value CTE to prevent re-inlining of correlation.
- Ensure threshold calculation uses exact same formula: tpcds_cmax * (95/100.0).

## Regression Warnings (observed failures on similar queries)

1. OR to UNION ALL (0.21x regression):
   CAUSE: Splitting OR conditions into UNION ALL blocked bitmap index scans.
   RULE: Do not split OR conditions; this query has no OR conditions.
2. EXISTS to IN (0.50x regression):
   CAUSE: Converting EXISTS to IN changed NULL semantics and blocked hash anti-joins.
   RULE: Do not convert EXISTS/IN; this query uses IN (subquery) which is already optimal.

## Constraints (analyst-filtered for this query)

- COMPLETE_OUTPUT: Query outputs single SUM(sales) column; must preserve.
- CTE_COLUMN_COMPLETENESS: Downstream references need item_sk from frequent_ss_items and c_customer_sk from best_ss_customer.
- LITERAL_PRESERVATION: All filter values (1998, 81-100, 'Children','Men','Sports', 11-21, 1934-1940, 95/100.0) must remain exact.
- SEMANTIC_EQUIVALENCE: Must return same sum of sales.
- COMMA_JOIN_WEAKNESS: All joins are comma-separated; EXPLAIN shows hash/nested loop but cardinality estimation suffers.
- CORRELATED_SUBQUERY_PARALYSIS: best_ss_customer HAVING references max_store_sales as correlated scalar subquery.
- CROSS_CTE_PREDICATE_BLINDNESS: store_sales scanned 3 times with similar date_dim joins (d_year=1998).

## Example Adaptation Notes

For each example: what to apply to your rewrite, and what to ignore.

- inline_decorrelate_materialized: Apply decomposition into 3 CTEs: dimension filter (customer), fact filter (store_sales), and threshold computation. Use MATERIALIZED to prevent inlining. Ignore the non-equi join aspect.
- early_filter_decorrelate: Push dimension filters into CTE definitions and pre-compute threshold in separate CTE. Ignore the early filtering of fact table (

## Reference Examples

Pattern reference only — do not copy table/column names or literals.

### 1. inline_decorrelate_materialized (timeout_rescue)

**Principle:** Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs — (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data — then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.

**BEFORE (slow):**
```sql
select  sum(cs_ext_discount_amt)  as "excess discount amount"
from
   catalog_sales
   ,item
   ,date_dim
where
(i_manufact_id in (1, 78, 97, 516, 521)
or i_manager_id BETWEEN 25 and 54)
and i_item_sk = cs_item_sk
and d_date between '1999-03-07' and
        cast('1999-03-07' as date) + interval '90 day'
and d_date_sk = cs_sold_date_sk
and cs_ext_discount_amt
     > (
         select
            1.3 * avg(cs_ext_discount_amt)
         from
            catalog_sales
           ,date_dim
         where
              cs_item_sk = i_item_sk
          and d_date between '1999-03-07' and
                             cast('1999-03-07' as date) + interval '90 day'
          and d_date_sk = cs_sold_date_sk
          and cs_list_price between 16 and 45
          and cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01
      )
order by sum(cs_ext_discount_amt)
limit 100;
```

**AFTER (fast):**
```sql
WITH filtered_items AS MATERIALIZED (
    SELECT i_item_sk
    FROM item
    WHERE i_manufact_id IN (1, 78, 97, 516, 521)
       OR i_manager_id BETWEEN 25 AND 54
),
date_filtered_sales AS MATERIALIZED (
    SELECT cs.cs_item_sk, cs.cs_ext_discount_amt,
           cs.cs_list_price, cs.cs_sales_price
    FROM catalog_sales cs
    JOIN date_dim d ON d.d_date_sk = cs.cs_sold_date_sk
    WHERE d.d_date BETWEEN '1999-03-07' AND cast('1999-03-07' as date) + interval '90 day'
),
item_avg_discount AS MATERIALIZED (
    SELECT dfs.cs_item_sk,
           1.3 * avg(dfs.cs_ext_discount_amt) AS threshold
    FROM date_filtered_sales dfs
    JOIN filtered_items fi ON fi.i_item_sk = dfs.cs_item_sk
    WHERE dfs.cs_list_price BETWEEN 16 AND 45
      AND dfs.cs_sales_price / dfs.cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01
    GROUP BY dfs.cs_item_sk
)
SELECT sum(dfs.cs_ext_discount_amt) AS "excess discount amount"
FROM date_filtered_sales dfs
JOIN item_avg_discount iad ON iad.cs_item_sk = dfs.cs_item_sk
WHERE dfs.cs_ext_discount_amt > iad.threshold
ORDER BY 1
LIMIT 100;
```

### 2. early_filter_decorrelate (1.13x)

**Principle:** Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.

**BEFORE (slow):**
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         sr_reason_sk AS ctr_reason_sk,
         SUM(SR_REFUNDED_CASH) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk
    AND d_year = 2001
    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer, customer_demographics
WHERE ctr1.ctr_total_return > (
    SELECT AVG(ctr_total_return) * 1.2
    FROM customer_total_return ctr2
    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state IN ('MI', 'NC', 'WI')
  AND ctr1.ctr_customer_sk = c_customer_sk
  AND c_current_cdemo_sk = cd_demo_sk
  AND cd_marital_status IN ('W', 'W')
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

**AFTER (fast):**
```sql
WITH customer_total_return AS (
    SELECT sr_customer_sk AS ctr_customer_sk,
           sr_store_sk AS ctr_store_sk,
           sr_reason_sk AS ctr_reason_sk,
           SUM(SR_REFUNDED_CASH) AS ctr_total_return
    FROM store_returns
    JOIN date_dim ON sr_returned_date_sk = d_date_sk
    JOIN store ON sr_store_sk = s_store_sk
    WHERE d_year = 2001
      AND s_state IN ('MI', 'NC', 'WI')
      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
),
store_thresholds AS (
    SELECT ctr_store_sk,
           AVG(ctr_total_return) * 1.2 AS avg_limit
    FROM customer_total_return
    GROUP BY ctr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1
JOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk
JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk
JOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk
JOIN store s ON ctr1.ctr_store_sk = s.s_store_sk
WHERE ctr1.ctr_total_return > st.avg_limit
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s.s_state IN ('MI', 'NC', 'WI')
  AND cd_marital_status = 'W'
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

## Original SQL

```sql
with frequent_ss_items as
 (select substring(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt
  from store_sales
      ,date_dim
      ,item
  where ss_sold_date_sk = d_date_sk
    and ss_item_sk = i_item_sk
    and d_year = 1998
    and i_manager_id BETWEEN 81 and 100
     AND i_category IN ('Children', 'Men', 'Sports')
  group by substring(i_item_desc,1,30),i_item_sk,d_date
  having count(*) >4),
 max_store_sales as
 (select max(csales) tpcds_cmax
  from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales
        from store_sales
            ,customer
            ,date_dim
        where ss_customer_sk = c_customer_sk
         and ss_sold_date_sk = d_date_sk
         and d_year = 1998
         and ss_wholesale_cost BETWEEN 11 AND 21
        group by c_customer_sk) tmp1),
 best_ss_customer as
 (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales
  from store_sales
      ,customer
  where ss_customer_sk = c_customer_sk
  and c_birth_year BETWEEN 1934 AND 1940
  group by c_customer_sk
  having sum(ss_quantity*ss_sales_price) > (95/100.0) * (select
  *
from
 max_store_sales))
  select  sum(sales)
 from (select cs_quantity*cs_list_price sales
       from catalog_sales
           ,date_dim
       where d_year = 1998
         and d_moy = 10
         and cs_sold_date_sk = d_date_sk
         and cs_item_sk in (select item_sk from frequent_ss_items)
         and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer)
         and cs_wholesale_cost BETWEEN 11 AND 21
      union all
      select ws_quantity*ws_list_price sales
       from web_sales
           ,date_dim
       where d_year = 1998
         and d_moy = 10
         and ws_sold_date_sk = d_date_sk
         and ws_item_sk in (select item_sk from frequent_ss_items)
         and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer)
         and ws_wholesale_cost BETWEEN 11 AND 21) tmp2
 limit 100;
```

## Per-Rewrite Configuration (SET LOCAL)

You have two optimization levers: SQL rewrite AND per-query configuration.
After writing your rewrite, analyze its execution profile and emit SET LOCAL
commands that fix planner-level bottlenecks specific to YOUR rewrite.

Memory budget: shared_buffers=128MB, effective_cache_size=4GB
Global work_mem: 4MB (per-operation)
Active connections: ~1 (work_mem headroom: safe up to 16MB per-op)
Storage: HDD (random_page_cost=4.0)
Parallel capacity: max_parallel_workers=8, per_gather=2

SET LOCAL permissions:
  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem

### Tunable Parameters (whitelist — only these are allowed)

- **effective_cache_size** (1024MB–65536MB): Advisory: how much OS cache to expect (MB). Safe to set aggressively.
- **enable_hashjoin** (on | off): Enable hash join plan type.
- **enable_mergejoin** (on | off): Enable merge join plan type.
- **enable_nestloop** (on | off): Enable nested-loop join plan type.
- **enable_seqscan** (on | off): Enable sequential scan plan type.
- **from_collapse_limit** (1–20): Max FROM items before subqueries stop being flattened.
- **geqo_threshold** (2–20): Number of FROM items that triggers genetic query optimizer.
- **hash_mem_multiplier** (1.0–10.0): Multiplier applied to work_mem for hash-based operations.
- **jit** (on | off): Enable JIT compilation.
- **jit_above_cost** (0.0–1000000.0): Query cost above which JIT is activated.
- **join_collapse_limit** (1–20): Max FROM items before planner stops trying all join orders.
- **max_parallel_workers_per_gather** (0–8): Max parallel workers per Gather node.
- **parallel_setup_cost** (0.0–10000.0): Planner estimate of cost to launch parallel workers.
- **parallel_tuple_cost** (0.0–1.0): Planner estimate of cost to transfer a tuple to parallel worker.
- **random_page_cost** (1.0–10.0): Planner estimate of cost of a random page fetch (1.0 = SSD, 4.0 = HDD).
- **work_mem** (64MB–2048MB): Memory for sorts/hashes per operation (MB). Allocated PER-OPERATION, not per-query. Count hash/sort ops in EXPLAIN before sizing.

### Rules
- Every SET LOCAL MUST cite a specific EXPLAIN node your rewrite creates/changes
- work_mem is PER-OPERATION: count hash/sort ops in your rewrite before sizing
- random_page_cost: ONLY change if your rewrite creates index-favorable access patterns
- Empty is valid: if your rewrite has no planner bottleneck, emit no SET LOCAL
- Stay within the resource envelope bounds above

### SET LOCAL Syntax
Include SET LOCAL commands in the `runtime_config` array field of your JSON output.
If no config changes help, omit the field or use an empty array.

## Rewrite Checklist (must pass before final SQL)

- Follow every node in `TARGET_LOGICAL_TREE` and produce each `NODE_CONTRACT` output column exactly.
- Keep all semantic invariants from `Semantic Contract` and `Constraints` (including join/null behavior).
- Preserve all literals and the exact final output schema/order.
- Apply `Hazard Flags` and `Regression Warnings` as hard guards against known failure modes.

### Column Completeness Contract

Your `main_query` component MUST produce **exactly** these output columns (same names, same order):

  1. `SUM(sales)`

Do NOT add, remove, or rename any output columns. The result set schema must be identical to the original query.

## Original Query Structure

This is the current query structure. All nodes are `[=]` (unchanged). Your modified Logic Tree below should show which nodes you changed.

```
QUERY: (single statement)
├── [CTE] frequent_ss_items  [=]  Cost: 0%  Rows: ~6K
│   ├── SCAN (store_sales, date_dim (join), item (join))
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── JOIN (ss_item_sk = i_item_sk)
│   ├── FILTER (d_year = 1998)
│   ├── FILTER (i_manager_id BETWEEN 81 AND 100)
│   ├── FILTER (+1 more)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (itemdesc, item_sk, solddate, cnt)
├── [CTE] max_store_sales  [=]  Cost: 0%  Rows: ~1K
│   ├── SCAN (store_sales, customer, date_dim)
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── FILTER (d_year = 1998)
│   ├── FILTER (ss_wholesale_cost BETWEEN 11 AND 21)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (tpcds_cmax)
├── [CTE] best_ss_customer  [=]  Cost: 56%  Rows: ~4.5M
│   ├── SCAN (store_sales, customer (join), max_store_sales (correlated subquery))
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── FILTER (c_birth_year BETWEEN 1934 AND 1940)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (c_customer_sk, ssales)
└── [MAIN] main_query  [=]  Cost: 39%  Rows: ~24K
    ├── SCAN (catalog_sales, date_dim, web_sales, best_ss_customer, frequent_ss_items)
    ├── JOIN (cs_sold_date_sk = d_date_sk)
    ├── FILTER (d_year = 1998)
    ├── FILTER (d_moy = 10)
    ├── FILTER (+3 more)
    ├── AGG (GROUP BY)
    └── OUTPUT (SUM(sales))
```

## Output Format

Your response has **two parts** in order:

### Part 1: Modified Logic Tree

Show what changed using change markers. Generate the tree BEFORE writing SQL.

Change markers:
- `[+]` — New component added
- `[-]` — Component removed
- `[~]` — Component modified (describe what changed)
- `[=]` — Unchanged (no children needed)
- `[!]` — Structural change (e.g. CTE → subquery)

### Part 2: Component Payload JSON

```json
{
  "spec_version": "1.0",
  "dialect": "<dialect>",
  "rewrite_rules": [
    {"id": "R1", "type": "<transform_name>", "description": "<what changed>", "applied_to": ["<component_id>"]}
  ],
  "statements": [{
    "target_table": null,
    "change": "modified",
    "components": {
      "<cte_name>": {
        "type": "cte",
        "change": "modified",
        "sql": "<complete SQL for this CTE body>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<upstream_id>"]}
      },
      "main_query": {
        "type": "main_query",
        "change": "modified",
        "sql": "<final SELECT>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<cte_name>"]}
      }
    },
    "reconstruction_order": ["<cte_name>", "main_query"],
    "assembly_template": "WITH <cte_name> AS ({<cte_name>}) {main_query}"
  }],
  "macros": {},
  "frozen_blocks": [],
  "runtime_config": ["SET LOCAL work_mem = '512MB'"],
  "validation_checks": []
}
```

### Rules
- **Tree first, always.** Generate the Logic Tree before writing any SQL
- **One component at a time.** When writing SQL for component X, treat others as opaque interfaces
- **No ellipsis.** Every `sql` value must be complete, executable SQL
- **Frozen blocks are copy-paste.** Large CASE-WHEN lookups must be verbatim
- **Validate interfaces.** Verify every `consumes` reference exists in upstream `outputs`
- Only include components you **changed or added** — set unchanged components to `"change": "unchanged"` with `"sql": ""`
- `main_query` output columns must match the Column Completeness Contract above
- `runtime_config`: SET LOCAL commands for PostgreSQL. Omit or use empty array if not needed
- `reconstruction_order`: topological order of components for assembly

After the JSON, explain the mechanism:

```
Changes: <1-2 sentences: what structural change + the expected mechanism>
Expected speedup: <estimate>
```

Now output your Logic Tree and Component Payload JSON: