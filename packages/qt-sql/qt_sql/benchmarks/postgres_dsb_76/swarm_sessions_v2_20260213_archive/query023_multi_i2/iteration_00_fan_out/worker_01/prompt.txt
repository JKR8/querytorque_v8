You are a SQL rewrite engine for PostgreSQL v16.11-0ubuntu0.24.04.1). Follow the Target Logical Tree structure below. Your job is to write correct, executable SQL for each node — not to decide whether to restructure. Preserve exact semantic equivalence (same rows, same columns, same ordering). Preserve defensive guards: if the original uses CASE WHEN x > 0 THEN y/x END around a division, keep it — even when a WHERE clause makes the zero case unreachable. Guards prevent silent breakage if filters change upstream. Strip benchmark comments (-- start query, -- end query) from your output.

## Semantic Contract (MUST preserve)

This query computes total sales from catalog and web channels for May 1999, filtered to items that were frequently sold in stores during 1999 (with manager and category constraints) and customers whose store sales exceed 95% of the maximum per-customer store sales in 1999 (with wholesale cost and birth year constraints). All joins are INNER (intersection) except the HAVING subquery which is a scalar correlated threshold. Aggregates are SUM and COUNT, which are duplicate-insensitive, allowing safe restructure. The query depends on multiple filters on date_dim (year, month), item (manager_id, category), customer (birth_year), and wholesale_cost ranges; any rewrite must preserve these exact literal values.

## Target Logical Tree + Node Contracts

Build your rewrite following this CTE structure. Each node's OUTPUT list is exhaustive — your SQL must produce exactly those columns.

TARGET_LOGICAL_TREE:
store_sales_enriched -> frequent_ss_items, max_store_sales, best_ss_customer -> main_union
NODE_CONTRACTS:
  store_sales_enriched:
    FROM: store_sales
      INNER JOIN date_dim ON ss_sold_date_sk = d_date_sk AND d_year = 1999
      INNER JOIN item ON ss_item_sk = i_item_sk AND i_manager_id BETWEEN 44 AND 63 AND i_category IN ('Men', 'Music', 'Sports')
      LEFT JOIN customer ON ss_customer_sk = c_customer_sk AND c_birth_year BETWEEN 1987 AND 1993
    WHERE: (none additional)
    GROUP BY: (none)
    AGGREGATE: (none)
    OUTPUT: ss_item_sk, ss_sold_date_sk, d_date, i_item_desc, ss_customer_sk, c_birth_year, ss_quantity, ss_sales_price, ss_wholesale_cost
    EXPECTED_ROWS: ~300K (combined rows from all store_sales scans)
    CONSUMERS: frequent_ss_items, max_store_sales, best_ss_customer
  frequent_ss_items:
    FROM: store_sales_enriched
    WHERE: (all rows qualify due to INNER JOIN filters)
    GROUP BY: SUBSTRING(i_item_desc FROM 1 FOR 30), ss_item_sk, d_date
    AGGREGATE: COUNT(*) as cnt
    HAVING: COUNT(*) > 4
    OUTPUT: SUBSTRING(i_item_desc FROM 1 FOR 30) as itemdesc, ss_item_sk as item_sk, d_date as solddate, cnt
    EXPECTED_ROWS: 2
    CONSUMERS: main_union (2 references)
  max_store_sales:
    FROM: store_sales_enriched
    WHERE: ss_wholesale_cost BETWEEN 26 AND 36
    GROUP BY: ss_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as csales
    OUTPUT: MAX(csales) as tpcds_cmax
    EXPECTED_ROWS: 1 (scalar)
    CONSUMERS: best_ss_customer
  best_ss_customer:
    FROM: store_sales_enriched
    WHERE: c_birth_year BETWEEN 1987 AND 1993
    GROUP BY: ss_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as ssales
    HAVING: SUM(ss_quantity * ss_sales_price) > (95/100.0) * (SELECT tpcds_cmax FROM max_store_sales)
    OUTPUT: ss_customer_sk as c_customer_sk, ssales
    EXPECTED_ROWS: 5504
    CONSUMERS: main_union (2 references)

NODE_CONTRACTS:
store_sales_enriched:
    FROM: store_sales
      INNER JOIN date_dim ON ss_sold_date_sk = d_date_sk AND d_year = 1999
      INNER JOIN item ON ss_item_sk = i_item_sk AND i_manager_id BETWEEN 44 AND 63 AND i_category IN ('Men', 'Music', 'Sports')
      LEFT JOIN customer ON ss_customer_sk = c_customer_sk AND c_birth_year BETWEEN 1987 AND 1993
    WHERE: (none additional)
    GROUP BY: (none)
    AGGREGATE: (none)
    OUTPUT: ss_item_sk, ss_sold_date_sk, d_date, i_item_desc, ss_customer_sk, c_birth_year, ss_quantity, ss_sales_price, ss_wholesale_cost
    EXPECTED_ROWS: ~300K (combined rows from all store_sales scans)
    CONSUMERS: frequent_ss_items, max_store_sales, best_ss_customer
  frequent_ss_items:
    FROM: store_sales_enriched
    WHERE: (all rows qualify due to INNER JOIN filters)
    GROUP BY: SUBSTRING(i_item_desc FROM 1 FOR 30), ss_item_sk, d_date
    AGGREGATE: COUNT(*) as cnt
    HAVING: COUNT(*) > 4
    OUTPUT: SUBSTRING(i_item_desc FROM 1 FOR 30) as itemdesc, ss_item_sk as item_sk, d_date as solddate, cnt
    EXPECTED_ROWS: 2
    CONSUMERS: main_union (2 references)
  max_store_sales:
    FROM: store_sales_enriched
    WHERE: ss_wholesale_cost BETWEEN 26 AND 36
    GROUP BY: ss_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as csales
    OUTPUT: MAX(csales) as tpcds_cmax
    EXPECTED_ROWS: 1 (scalar)
    CONSUMERS: best_ss_customer
  best_ss_customer:
    FROM: store_sales_enriched
    WHERE: c_birth_year BETWEEN 1987 AND 1993
    GROUP BY: ss_customer_sk
    AGGREGATE: SUM(ss_quantity * ss_sales_price) as ssales
    HAVING: SUM(ss_quantity * ss_sales_price) > (95/100.0) * (SELECT tpcds_cmax FROM max_store_sales)
    OUTPUT: ss_customer_sk as c_customer_sk, ssales
    EXPECTED_ROWS: 5504
    CONSUMERS: main_union (2 references)

## Hazard Flags (avoid these specific risks)

- LEFT JOIN on customer may introduce NULLs for rows that don't match birth year filter, affecting counts for frequent_ss_items (but those rows are filtered out by INNER JOIN on item/date, so safe).
- Materializing store_sales_enriched may be large (~300K rows); ensure work_mem sufficient for hash aggregates.

## Regression Warnings (observed failures on similar queries)

1. OR to UNION ALL (observed regression 0.21x):
   CAUSE: Splitting OR conditions into UNION ALL branches prevented bitmap index combination.
   RULE: Do not split OR conditions; this query has no OR conditions.
2. EXISTS to IN/NOT IN (observed regression 0.50x):
   CAUSE: Converting EXISTS to IN blocked semi-join short-circuit.
   RULE: Do not convert EXISTS; this query uses IN subqueries, not EXISTS.
3. CTE blocking parallelism (observed regression 0.67x):
   CAUSE: Materialized CTEs prevented parallel table scans.
   RULE: Avoid materializing CTEs that scan large fact tables if parallelism is needed; store_sales scans are already parallelized in EXPLAIN.

## Constraints (analyst-filtered for this query)

- COMPLETE_OUTPUT: The final output is a single SUM(sales) column; must preserve exactly.
- CTE_COLUMN_COMPLETENESS: Each CTE must output all columns referenced by downstream nodes (item_sk, c_customer_sk, etc.).
- LITERAL_PRESERVATION: Must keep all filter literals (1999, 5, 44, 63, 'Men','Music','Sports', 26, 36, 1987, 1993, 95/100.0).
- SEMANTIC_EQUIVALENCE: Must return same sum of sales across catalog and web channels.
- COMMA_JOIN_WEAKNESS: Query uses comma-separated joins (FROM store_sales, date_dim, item). EXPLAIN shows hash joins but cardinality estimation may suffer.
- CROSS_CTE_PREDICATE_BLINDNESS: store_sales scanned 3 times with overlapping filters; EXPLAIN shows separate scans for frequent_ss_items, max_store_sales, and best_ss_customer.

## Example Adaptation Notes

For each example: what to apply to your rewrite, and what to ignore.

- pg_self_join_decomposition: apply the pattern of materializing the store_sales scan once and deriving multiple aggregates; ignore the self-join aspect (this query has no self-join).
- pg_materialized_dimension_fact_prefilter: apply the staged reduction by pre-joining store_sales with all dimensions; ignore the non-equi join aspect.
- single_pass_aggregation: apply the idea of computing multiple aggregates from a single scan; ignore the CASE/FILTER pivoting (we'll use separate GROUP BYs).

## Reference Examples

Pattern reference only — do not copy table/column names or literals.

### 1. pg_self_join_decomposition (3.93x)

**Principle:** Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.

**BEFORE (slow):**
```sql
select 
	s_store_name,
	i_item_desc,
	sc.revenue,
	i_current_price,
	i_wholesale_cost,
	i_brand
 from store, item,
     (select ss_store_sk, avg(revenue) as ave
	from
	    (select  ss_store_sk, ss_item_sk,
		     sum(ss_sales_price) as revenue
		from store_sales, date_dim
		where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11
   and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01
		group by ss_store_sk, ss_item_sk) sa
	group by ss_store_sk) sb,
     (select  ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
	from store_sales, date_dim
	where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11
  and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01
	group by ss_store_sk, ss_item_sk) sc
 where sb.ss_store_sk = sc.ss_store_sk and
       sc.revenue <= 0.1 * sb.ave and
       s_store_sk = sc.ss_store_sk and
       i_item_sk = sc.ss_item_sk
       and i_manager_id BETWEEN 32 and 36
       and s_state in ('TN','TX','VA')
 order by s_store_name, i_item_desc
limit 100;
```

**AFTER (fast):**
[date_filter]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224
```
[store_sales_revenue]:
```sql
SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk
```
[store_avg_revenue]:
```sql
SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk
```
[filtered_store]:
```sql
SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36
```
[main_query]:
```sql
SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100
```

### 2. pg_materialized_dimension_fact_prefilter (2.68x)

**Principle:** Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.

**BEFORE (slow):**
```sql
select  i_item_desc
      ,w_warehouse_name
      ,d1.d_week_seq
      ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo
      ,sum(case when p_promo_sk is not null then 1 else 0 end) promo
      ,count(*) total_cnt
from catalog_sales
join inventory on (cs_item_sk = inv_item_sk)
join warehouse on (w_warehouse_sk=inv_warehouse_sk)
join item on (i_item_sk = cs_item_sk)
join customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)
join household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)
join date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)
join date_dim d2 on (inv_date_sk = d2.d_date_sk)
join date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)
left outer join promotion on (cs_promo_sk=p_promo_sk)
left outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)
where d1.d_week_seq = d2.d_week_seq
  and inv_quantity_on_hand < cs_quantity
  and d3.d_date > d1.d_date + interval '3 day'
  and hd_buy_potential = '501-1000'
  and d1.d_year = 1998
  and cd_marital_status = 'M'
  and cd_dep_count between 9 and 11
  and i_category IN ('Home', 'Men', 'Music')
  and cs_wholesale_cost BETWEEN 34 AND 54
group by i_item_desc,w_warehouse_name,d1.d_week_seq
order by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq
limit 100;
```

**AFTER (fast):**
[filtered_date]:
```sql
SELECT d_date_sk, d_date, d_week_seq FROM date_dim WHERE d_year = 1998
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc FROM item WHERE i_category IN ('Home', 'Men', 'Music')
```
[filtered_cd]:
```sql
SELECT cd_demo_sk FROM customer_demographics WHERE cd_marital_status = 'M' AND cd_dep_count BETWEEN 9 AND 11
```
[filtered_hd]:
```sql
SELECT hd_demo_sk FROM household_demographics WHERE hd_buy_potential = '501-1000'
```
[cs_filtered]:
```sql
SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk, cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost, cs_order_number FROM catalog_sales WHERE cs_wholesale_cost BETWEEN 34 AND 54
```
[main_query]:
```sql
SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq, SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo, SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo, COUNT(*) AS total_cnt FROM cs_filtered cs JOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk JOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk JOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk JOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk JOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk JOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk LEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk LEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk AND cr.cr_order_number = cs.cs_order_number WHERE d1.d_week_seq = d2.d_week_seq AND inv.inv_quantity_on_hand < cs.cs_quantity AND d3.d_date > d1.d_date + INTERVAL '3 day' GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq LIMIT 100
```

## Original SQL

```sql
with frequent_ss_items as
 (select substring(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt
  from store_sales
      ,date_dim
      ,item
  where ss_sold_date_sk = d_date_sk
    and ss_item_sk = i_item_sk
    and d_year = 1999
    and i_manager_id BETWEEN 44 and 63
     AND i_category IN ('Men', 'Music', 'Sports')
  group by substring(i_item_desc,1,30),i_item_sk,d_date
  having count(*) >4),
 max_store_sales as
 (select max(csales) tpcds_cmax
  from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales
        from store_sales
            ,customer
            ,date_dim
        where ss_customer_sk = c_customer_sk
         and ss_sold_date_sk = d_date_sk
         and d_year = 1999
         and ss_wholesale_cost BETWEEN 26 AND 36
        group by c_customer_sk) tmp1),
 best_ss_customer as
 (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales
  from store_sales
      ,customer
  where ss_customer_sk = c_customer_sk
  and c_birth_year BETWEEN 1987 AND 1993
  group by c_customer_sk
  having sum(ss_quantity*ss_sales_price) > (95/100.0) * (select
  *
from
 max_store_sales))
  select  sum(sales)
 from (select cs_quantity*cs_list_price sales
       from catalog_sales
           ,date_dim
       where d_year = 1999
         and d_moy = 5
         and cs_sold_date_sk = d_date_sk
         and cs_item_sk in (select item_sk from frequent_ss_items)
         and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer)
         and cs_wholesale_cost BETWEEN 26 AND 36
      union all
      select ws_quantity*ws_list_price sales
       from web_sales
           ,date_dim
       where d_year = 1999
         and d_moy = 5
         and ws_sold_date_sk = d_date_sk
         and ws_item_sk in (select item_sk from frequent_ss_items)
         and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer)
         and ws_wholesale_cost BETWEEN 26 AND 36) tmp2
 limit 100;
```

## Per-Rewrite Configuration (SET LOCAL)

You have two optimization levers: SQL rewrite AND per-query configuration.
After writing your rewrite, analyze its execution profile and emit SET LOCAL
commands that fix planner-level bottlenecks specific to YOUR rewrite.

Memory budget: shared_buffers=128MB, effective_cache_size=4GB
Global work_mem: 4MB (per-operation)
Active connections: ~1 (work_mem headroom: safe up to 16MB per-op)
Storage: HDD (random_page_cost=4.0)
Parallel capacity: max_parallel_workers=8, per_gather=2

SET LOCAL permissions:
  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem

### Tunable Parameters (whitelist — only these are allowed)

- **effective_cache_size** (1024MB–65536MB): Advisory: how much OS cache to expect (MB). Safe to set aggressively.
- **enable_hashjoin** (on | off): Enable hash join plan type.
- **enable_mergejoin** (on | off): Enable merge join plan type.
- **enable_nestloop** (on | off): Enable nested-loop join plan type.
- **enable_seqscan** (on | off): Enable sequential scan plan type.
- **from_collapse_limit** (1–20): Max FROM items before subqueries stop being flattened.
- **geqo_threshold** (2–20): Number of FROM items that triggers genetic query optimizer.
- **hash_mem_multiplier** (1.0–10.0): Multiplier applied to work_mem for hash-based operations.
- **jit** (on | off): Enable JIT compilation.
- **jit_above_cost** (0.0–1000000.0): Query cost above which JIT is activated.
- **join_collapse_limit** (1–20): Max FROM items before planner stops trying all join orders.
- **max_parallel_workers_per_gather** (0–8): Max parallel workers per Gather node.
- **parallel_setup_cost** (0.0–10000.0): Planner estimate of cost to launch parallel workers.
- **parallel_tuple_cost** (0.0–1.0): Planner estimate of cost to transfer a tuple to parallel worker.
- **random_page_cost** (1.0–10.0): Planner estimate of cost of a random page fetch (1.0 = SSD, 4.0 = HDD).
- **work_mem** (64MB–2048MB): Memory for sorts/hashes per operation (MB). Allocated PER-OPERATION, not per-query. Count hash/sort ops in EXPLAIN before sizing.

### Rules
- Every SET LOCAL MUST cite a specific EXPLAIN node your rewrite creates/changes
- work_mem is PER-OPERATION: count hash/sort ops in your rewrite before sizing
- random_page_cost: ONLY change if your rewrite creates index-favorable access patterns
- Empty is valid: if your rewrite has no planner bottleneck, emit no SET LOCAL
- Stay within the resource envelope bounds above

### SET LOCAL Syntax
Include SET LOCAL commands in the `runtime_config` array field of your JSON output.
If no config changes help, omit the field or use an empty array.

## Rewrite Checklist (must pass before final SQL)

- Follow every node in `TARGET_LOGICAL_TREE` and produce each `NODE_CONTRACT` output column exactly.
- Keep all semantic invariants from `Semantic Contract` and `Constraints` (including join/null behavior).
- Preserve all literals and the exact final output schema/order.
- Apply `Hazard Flags` and `Regression Warnings` as hard guards against known failure modes.

### Column Completeness Contract

Your `main_query` component MUST produce **exactly** these output columns (same names, same order):

  1. `SUM(sales)`

Do NOT add, remove, or rename any output columns. The result set schema must be identical to the original query.

## Original Query Structure

This is the current query structure. All nodes are `[=]` (unchanged). Your modified Logic Tree below should show which nodes you changed.

```
QUERY: (single statement)
├── [CTE] frequent_ss_items  [=]  Cost: 0%  Rows: ~4K
│   ├── SCAN (store_sales, date_dim (join), item (join))
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── JOIN (ss_item_sk = i_item_sk)
│   ├── FILTER (d_year = 1999)
│   ├── FILTER (i_manager_id BETWEEN 44 AND 63)
│   ├── FILTER (+1 more)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (itemdesc, item_sk, solddate, cnt)
├── [CTE] max_store_sales  [=]  Cost: 0%  Rows: ~1K
│   ├── SCAN (store_sales, customer, date_dim)
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── JOIN (ss_sold_date_sk = d_date_sk)
│   ├── FILTER (d_year = 1999)
│   ├── FILTER (ss_wholesale_cost BETWEEN 26 AND 36)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (tpcds_cmax)
├── [CTE] best_ss_customer  [=]  Cost: 53%  Rows: ~2.2M
│   ├── SCAN (store_sales, customer (join), max_store_sales (correlated subquery))
│   ├── JOIN (ss_customer_sk = c_customer_sk)
│   ├── FILTER (c_birth_year BETWEEN 1987 AND 1993)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (c_customer_sk, ssales)
└── [MAIN] main_query  [=]  Cost: 41%  Rows: ~10K
    ├── SCAN (catalog_sales, date_dim, web_sales, best_ss_customer, frequent_ss_items)
    ├── JOIN (cs_sold_date_sk = d_date_sk)
    ├── FILTER (d_year = 1999)
    ├── FILTER (d_moy = 5)
    ├── FILTER (+3 more)
    ├── AGG (GROUP BY)
    └── OUTPUT (SUM(sales))
```

## Output Format

Your response has **two parts** in order:

### Part 1: Modified Logic Tree

Show what changed using change markers. Generate the tree BEFORE writing SQL.

Change markers:
- `[+]` — New component added
- `[-]` — Component removed
- `[~]` — Component modified (describe what changed)
- `[=]` — Unchanged (no children needed)
- `[!]` — Structural change (e.g. CTE → subquery)

### Part 2: Component Payload JSON

```json
{
  "spec_version": "1.0",
  "dialect": "<dialect>",
  "rewrite_rules": [
    {"id": "R1", "type": "<transform_name>", "description": "<what changed>", "applied_to": ["<component_id>"]}
  ],
  "statements": [{
    "target_table": null,
    "change": "modified",
    "components": {
      "<cte_name>": {
        "type": "cte",
        "change": "modified",
        "sql": "<complete SQL for this CTE body>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<upstream_id>"]}
      },
      "main_query": {
        "type": "main_query",
        "change": "modified",
        "sql": "<final SELECT>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<cte_name>"]}
      }
    },
    "reconstruction_order": ["<cte_name>", "main_query"],
    "assembly_template": "WITH <cte_name> AS ({<cte_name>}) {main_query}"
  }],
  "macros": {},
  "frozen_blocks": [],
  "runtime_config": ["SET LOCAL work_mem = '512MB'"],
  "validation_checks": []
}
```

### Rules
- **Tree first, always.** Generate the Logic Tree before writing any SQL
- **One component at a time.** When writing SQL for component X, treat others as opaque interfaces
- **No ellipsis.** Every `sql` value must be complete, executable SQL
- **Frozen blocks are copy-paste.** Large CASE-WHEN lookups must be verbatim
- **Validate interfaces.** Verify every `consumes` reference exists in upstream `outputs`
- Only include components you **changed or added** — set unchanged components to `"change": "unchanged"` with `"sql": ""`
- `main_query` output columns must match the Column Completeness Contract above
- `runtime_config`: SET LOCAL commands for PostgreSQL. Omit or use empty array if not needed
- `reconstruction_order`: topological order of components for assembly

After the JSON, explain the mechanism:

```
Changes: <1-2 sentences: what structural change + the expected mechanism>
Expected speedup: <estimate>
```

Now output your Logic Tree and Component Payload JSON: