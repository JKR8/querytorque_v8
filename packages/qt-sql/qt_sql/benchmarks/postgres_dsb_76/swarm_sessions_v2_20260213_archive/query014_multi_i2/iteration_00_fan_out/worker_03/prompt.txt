You are a SQL rewrite engine for PostgreSQL v16.11-0ubuntu0.24.04.1). Follow the Target Logical Tree structure below. Your job is to write correct, executable SQL for each node — not to decide whether to restructure. Preserve exact semantic equivalence (same rows, same columns, same ordering). Preserve defensive guards: if the original uses CASE WHEN x > 0 THEN y/x END around a division, keep it — even when a WHERE clause makes the zero case unreachable. Guards prevent silent breakage if filters change upstream. Strip benchmark comments (-- start query, -- end query) from your output.

## Semantic Contract (MUST preserve)

This query compares store channel sales for items that sold across all three channels (store, catalog, web) in consecutive years (1999 vs 2000) for the same week (week 49-50). It filters to Electronics/Shoes/Sports categories, manager_id 42-51, and wholesale_cost 76-96. JOIN semantics are strict INNER intersections: items must appear in all three sales channels (INTERSECT), and year comparisons require matching brand/class/category. Aggregation uses SUM and COUNT which are duplicate-insensitive, but AVG in the threshold CTE must preserve exact row sets from the UNION ALL branches. Filter dependencies: date_dim filters for specific years and week_seq must remain correlated to their respective year subqueries.

## Target Logical Tree + Node Contracts

Build your rewrite following this CTE structure. Each node's OUTPUT list is exhaustive — your SQL must produce exactly those columns.

TARGET_LOGICAL_TREE:
cross_items -> avg_sales -> base_aggregation -> year_split -> final_join
NODE_CONTRACTS:
  cross_items:
    FROM: (original INTERSECT structure)
    OUTPUT: ss_item_sk
    EXPECTED_ROWS: 4,735
    CONSUMERS: base_aggregation
  avg_sales:
    FROM: (original UNION ALL structure)
    OUTPUT: average_sales
    EXPECTED_ROWS: 1
    CONSUMERS: year_split
  base_aggregation:
    FROM: store_sales 
          JOIN item ON ss_item_sk = i_item_sk 
          JOIN date_dim ON ss_sold_date_sk = d_date_sk 
          JOIN cross_items ON ss_item_sk = cross_items.ss_item_sk
    WHERE: i_category IN ('Electronics', 'Shoes', 'Sports') 
           AND i_manager_id BETWEEN 42 AND 51 
           AND ss_wholesale_cost BETWEEN 76 AND 96
           AND d_year IN (1999, 2000)
    OUTPUT: i_brand_id, i_class_id, i_category_id, 
            d_year,
            d_week_seq,
            ss_quantity * ss_list_price AS sales_amount,
            ss_quantity,
            ss_list_price
    EXPECTED_ROWS: ~10K (estimate)
    CONSUMERS: year_split
  year_split:
    FROM: base_aggregation
    GROUP BY: i_brand_id, i_class_id, i_category_id, d_year, d_week_seq
    HAVING: SUM(ss_quantity * ss_list_price) > (SELECT average_sales FROM avg_sales)
    OUTPUT: i_brand_id, i_class_id, i_category_id, 
            d_year,
            d_week_seq,
            SUM(ss_quantity * ss_list_price) AS sales,
            COUNT(*) AS number_sales
    EXPECTED_ROWS: ~200
    CONSUMERS: final_join
  final_join:
    FROM: (SELECT 'store' AS channel, * FROM year_split WHERE d_year = 2000 AND d_week_seq = (SELECT d_week_seq FROM date_dim WHERE d_year=2000 AND d_moy=12 AND d_dom=12)) AS this_year
          INNER JOIN
          (SELECT 'store' AS channel, * FROM year_split WHERE d_year = 1999 AND d_week_seq = (SELECT d_week_seq FROM date_dim WHERE d_year=1999 AND d_moy=12 AND d_dom=12)) AS last_year
          USING (i_brand_id, i_class_id, i_category_id)
    OUTPUT: this_year.channel AS ty_channel, this_year.i_brand_id AS ty_brand, ... (all 12 columns)
    EXPECTED_ROWS: 55
    CONSUMERS: final output

NODE_CONTRACTS:
cross_items:
    FROM: (original INTERSECT structure)
    OUTPUT: ss_item_sk
    EXPECTED_ROWS: 4,735
    CONSUMERS: base_aggregation
  avg_sales:
    FROM: (original UNION ALL structure)
    OUTPUT: average_sales
    EXPECTED_ROWS: 1
    CONSUMERS: year_split
  base_aggregation:
    FROM: store_sales 
          JOIN item ON ss_item_sk = i_item_sk 
          JOIN date_dim ON ss_sold_date_sk = d_date_sk 
          JOIN cross_items ON ss_item_sk = cross_items.ss_item_sk
    WHERE: i_category IN ('Electronics', 'Shoes', 'Sports') 
           AND i_manager_id BETWEEN 42 AND 51 
           AND ss_wholesale_cost BETWEEN 76 AND 96
           AND d_year IN (1999, 2000)
    OUTPUT: i_brand_id, i_class_id, i_category_id, 
            d_year,
            d_week_seq,
            ss_quantity * ss_list_price AS sales_amount,
            ss_quantity,
            ss_list_price
    EXPECTED_ROWS: ~10K (estimate)
    CONSUMERS: year_split
  year_split:
    FROM: base_aggregation
    GROUP BY: i_brand_id, i_class_id, i_category_id, d_year, d_week_seq
    HAVING: SUM(ss_quantity * ss_list_price) > (SELECT average_sales FROM avg_sales)
    OUTPUT: i_brand_id, i_class_id, i_category_id, 
            d_year,
            d_week_seq,
            SUM(ss_quantity * ss_list_price) AS sales,
            COUNT(*) AS number_sales
    EXPECTED_ROWS: ~200
    CONSUMERS: final_join
  final_join:
    FROM: (SELECT 'store' AS channel, * FROM year_split WHERE d_year = 2000 AND d_week_seq = (SELECT d_week_seq FROM date_dim WHERE d_year=2000 AND d_moy=12 AND d_dom=12)) AS this_year
          INNER JOIN
          (SELECT 'store' AS channel, * FROM year_split WHERE d_year = 1999 AND d_week_seq = (SELECT d_week_seq FROM date_dim WHERE d_year=1999 AND d_moy=12 AND d_dom=12)) AS last_year
          USING (i_brand_id, i_class_id, i_category_id)
    OUTPUT: this_year.channel AS ty_channel, this_year.i_brand_id AS ty_brand, ... (all 12 columns)
    EXPECTED_ROWS: 55
    CONSUMERS: final output

## Hazard Flags (avoid these specific risks)

- The week_seq subquery must be evaluated after aggregation, which may be less efficient than pushing it into the base scan.
- base_aggregation CTE may be large (~10K rows) and materialization overhead could outweigh benefit.

## Regression Warnings (observed failures on similar queries)

1. CTE blocking parallelism (observed regression in Q055):
   CAUSE: Materialized CTEs executed single-threaded, preventing parallel scans of large fact tables.
   RULE: Avoid wrapping large fact table scans in CTEs; keep them in main query for parallel execution.
2. EXISTS conversion regression (observed 0.50x on Q069):
   CAUSE: Converting INTERSECT to EXISTS changed semi-join to anti-join plan, losing hash optimization.
   RULE: Test EXISTS conversion cautiously; verify the optimizer chooses efficient semi-join.

## Constraints (analyst-filtered for this query)

- COMPLETE_OUTPUT: Must preserve all 12 output columns exactly.
- CTE_COLUMN_COMPLETENESS: New CTEs must include all columns referenced downstream (e.g., ss_item_sk, average_sales, i_brand_id, i_class_id, i_category_id).
- LITERAL_PRESERVATION: All filter values (1999, 2000, 12, 42, 51, 76, 96, category list) must remain exact.
- SEMANTIC_EQUIVALENCE: Result rows must match exactly.
- COMMA_JOIN_WEAKNESS: All CTEs use comma-style implicit joins causing cardinality estimation issues.
- CROSS_CTE_PREDICATE_BLINDNESS: Same store_sales+date_dim+item scan pattern repeated 4 times with minor filter variations.

## Example Adaptation Notes

For each example: what to apply to your rewrite, and what to ignore.

- pg_self_join_decomposition: Materialize the base store_sales+item+date_dim scan once in base_aggregation CTE, then derive both year aggregates from it.
- shared_dimension_multi_channel: Not applicable (only store channel). Ignore multi-channel aspect.

## Reference Examples

Pattern reference only — do not copy table/column names or literals.

### 1. pg_self_join_decomposition (3.93x)

**Principle:** Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.

**BEFORE (slow):**
```sql
select 
	s_store_name,
	i_item_desc,
	sc.revenue,
	i_current_price,
	i_wholesale_cost,
	i_brand
 from store, item,
     (select ss_store_sk, avg(revenue) as ave
	from
	    (select  ss_store_sk, ss_item_sk,
		     sum(ss_sales_price) as revenue
		from store_sales, date_dim
		where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11
   and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01
		group by ss_store_sk, ss_item_sk) sa
	group by ss_store_sk) sb,
     (select  ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
	from store_sales, date_dim
	where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11
  and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01
	group by ss_store_sk, ss_item_sk) sc
 where sb.ss_store_sk = sc.ss_store_sk and
       sc.revenue <= 0.1 * sb.ave and
       s_store_sk = sc.ss_store_sk and
       i_item_sk = sc.ss_item_sk
       and i_manager_id BETWEEN 32 and 36
       and s_state in ('TN','TX','VA')
 order by s_store_name, i_item_desc
limit 100;
```

**AFTER (fast):**
[date_filter]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224
```
[store_sales_revenue]:
```sql
SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk
```
[store_avg_revenue]:
```sql
SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk
```
[filtered_store]:
```sql
SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36
```
[main_query]:
```sql
SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100
```

## Original SQL

```sql
with  cross_items as
 (select i_item_sk ss_item_sk
 from item,
 (select iss.i_brand_id brand_id
     ,iss.i_class_id class_id
     ,iss.i_category_id category_id
 from store_sales
     ,item iss
     ,date_dim d1
 where ss_item_sk = iss.i_item_sk
   and ss_sold_date_sk = d1.d_date_sk
   and d1.d_year between 1999 AND 1999 + 2
   and i_category IN ('Electronics', 'Shoes', 'Sports')
   and i_manager_id BETWEEN 42 and 51
   and ss_wholesale_cost BETWEEN 76 AND 96
intersect
 select ics.i_brand_id
     ,ics.i_class_id
     ,ics.i_category_id
 from catalog_sales
     ,item ics
     ,date_dim d2
 where cs_item_sk = ics.i_item_sk
   and cs_sold_date_sk = d2.d_date_sk
   and d2.d_year between 1999 AND 1999 + 2
   and i_category IN ('Electronics', 'Shoes', 'Sports')
   and i_manager_id BETWEEN 42 and 51
   and cs_wholesale_cost BETWEEN 76 AND 96
intersect
 select iws.i_brand_id
     ,iws.i_class_id
     ,iws.i_category_id
 from web_sales
     ,item iws
     ,date_dim d3
 where ws_item_sk = iws.i_item_sk
   and ws_sold_date_sk = d3.d_date_sk
   and ws_wholesale_cost BETWEEN 76 AND 96
   and d3.d_year between 1999 AND 1999 + 2) x
 where i_brand_id = brand_id
      and i_class_id = class_id
      and i_category_id = category_id
      and i_category IN ('Electronics', 'Shoes', 'Sports')
      and i_manager_id BETWEEN 42 and 51
),
 avg_sales as
(select avg(quantity*list_price) average_sales
  from (select ss_quantity quantity
             ,ss_list_price list_price
       from store_sales
           ,date_dim
       where ss_sold_date_sk = d_date_sk
         and d_year between 1999 and 1999 + 2
         and ss_wholesale_cost BETWEEN 76 AND 96
       union all
       select cs_quantity quantity
             ,cs_list_price list_price
       from catalog_sales
           ,date_dim
       where cs_sold_date_sk = d_date_sk
         and d_year between 1999 and 1999 + 2
         and cs_wholesale_cost BETWEEN 76 AND 96
       union all
       select ws_quantity quantity
             ,ws_list_price list_price
       from web_sales
           ,date_dim
       where ws_sold_date_sk = d_date_sk
        and ws_wholesale_cost BETWEEN 76 AND 96
         and d_year between 1999 and 1999 + 2) x)
  select  this_year.channel ty_channel
                           ,this_year.i_brand_id ty_brand
                           ,this_year.i_class_id ty_class
                           ,this_year.i_category_id ty_category
                           ,this_year.sales ty_sales
                           ,this_year.number_sales ty_number_sales
                           ,last_year.channel ly_channel
                           ,last_year.i_brand_id ly_brand
                           ,last_year.i_class_id ly_class
                           ,last_year.i_category_id ly_category
                           ,last_year.sales ly_sales
                           ,last_year.number_sales ly_number_sales
 from
 (select 'store' channel, i_brand_id,i_class_id,i_category_id
        ,sum(ss_quantity*ss_list_price) sales, count(*) number_sales
 from store_sales
     ,item
     ,date_dim
 where ss_item_sk in (select ss_item_sk from cross_items)
   and ss_item_sk = i_item_sk
   and ss_sold_date_sk = d_date_sk
   and d_week_seq = (select d_week_seq
                     from date_dim
                     where d_year = 1999 + 1
                       and d_moy = 12
                       and d_dom = 12)
   and i_category IN ('Electronics', 'Shoes', 'Sports')
   and i_manager_id BETWEEN 42 and 51
   and ss_wholesale_cost BETWEEN 76 AND 96
 group by i_brand_id,i_class_id,i_category_id
 having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)) this_year,
 (select 'store' channel, i_brand_id,i_class_id
        ,i_category_id, sum(ss_quantity*ss_list_price) sales, count(*) number_sales
 from store_sales
     ,item
     ,date_dim
 where ss_item_sk in (select ss_item_sk from cross_items)
   and ss_item_sk = i_item_sk
   and ss_sold_date_sk = d_date_sk
   and d_week_seq = (select d_week_seq
                     from date_dim
                     where d_year = 1999
                       and d_moy = 12
                       and d_dom = 12)
   and i_category IN ('Electronics', 'Shoes', 'Sports')
   and ss_wholesale_cost BETWEEN 76 AND 96
   and i_manager_id BETWEEN 42 and 51
group by i_brand_id,i_class_id,i_category_id
 having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)) last_year
 where this_year.i_brand_id= last_year.i_brand_id
   and this_year.i_class_id = last_year.i_class_id
   and this_year.i_category_id = last_year.i_category_id
 order by this_year.channel, this_year.i_brand_id, this_year.i_class_id, this_year.i_category_id
 limit 100;
```

## Per-Rewrite Configuration (SET LOCAL)

You have two optimization levers: SQL rewrite AND per-query configuration.
After writing your rewrite, analyze its execution profile and emit SET LOCAL
commands that fix planner-level bottlenecks specific to YOUR rewrite.

Memory budget: shared_buffers=128MB, effective_cache_size=4GB
Global work_mem: 4MB (per-operation)
Active connections: ~1 (work_mem headroom: safe up to 16MB per-op)
Storage: HDD (random_page_cost=4.0)
Parallel capacity: max_parallel_workers=8, per_gather=2

SET LOCAL permissions:
  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem

### Tunable Parameters (whitelist — only these are allowed)

- **effective_cache_size** (1024MB–65536MB): Advisory: how much OS cache to expect (MB). Safe to set aggressively.
- **enable_hashjoin** (on | off): Enable hash join plan type.
- **enable_mergejoin** (on | off): Enable merge join plan type.
- **enable_nestloop** (on | off): Enable nested-loop join plan type.
- **enable_seqscan** (on | off): Enable sequential scan plan type.
- **from_collapse_limit** (1–20): Max FROM items before subqueries stop being flattened.
- **geqo_threshold** (2–20): Number of FROM items that triggers genetic query optimizer.
- **hash_mem_multiplier** (1.0–10.0): Multiplier applied to work_mem for hash-based operations.
- **jit** (on | off): Enable JIT compilation.
- **jit_above_cost** (0.0–1000000.0): Query cost above which JIT is activated.
- **join_collapse_limit** (1–20): Max FROM items before planner stops trying all join orders.
- **max_parallel_workers_per_gather** (0–8): Max parallel workers per Gather node.
- **parallel_setup_cost** (0.0–10000.0): Planner estimate of cost to launch parallel workers.
- **parallel_tuple_cost** (0.0–1.0): Planner estimate of cost to transfer a tuple to parallel worker.
- **random_page_cost** (1.0–10.0): Planner estimate of cost of a random page fetch (1.0 = SSD, 4.0 = HDD).
- **work_mem** (64MB–2048MB): Memory for sorts/hashes per operation (MB). Allocated PER-OPERATION, not per-query. Count hash/sort ops in EXPLAIN before sizing.

### Rules
- Every SET LOCAL MUST cite a specific EXPLAIN node your rewrite creates/changes
- work_mem is PER-OPERATION: count hash/sort ops in your rewrite before sizing
- random_page_cost: ONLY change if your rewrite creates index-favorable access patterns
- Empty is valid: if your rewrite has no planner bottleneck, emit no SET LOCAL
- Stay within the resource envelope bounds above

### SET LOCAL Syntax
Include SET LOCAL commands in the `runtime_config` array field of your JSON output.
If no config changes help, omit the field or use an empty array.

## Rewrite Checklist (must pass before final SQL)

- Follow every node in `TARGET_LOGICAL_TREE` and produce each `NODE_CONTRACT` output column exactly.
- Keep all semantic invariants from `Semantic Contract` and `Constraints` (including join/null behavior).
- Preserve all literals and the exact final output schema/order.
- Apply `Hazard Flags` and `Regression Warnings` as hard guards against known failure modes.

### Column Completeness Contract

Your `main_query` component MUST produce **exactly** these output columns (same names, same order):

  1. `ty_channel`
  2. `ty_brand`
  3. `ty_class`
  4. `ty_category`
  5. `ty_sales`
  6. `ty_number_sales`
  7. `ly_channel`
  8. `ly_brand`
  9. `ly_class`
  10. `ly_category`
  11. `ly_sales`
  12. `ly_number_sales`

Do NOT add, remove, or rename any output columns. The result set schema must be identical to the original query.

## Original Query Structure

This is the current query structure. All nodes are `[=]` (unchanged). Your modified Logic Tree below should show which nodes you changed.

```
QUERY: (single statement)
├── [CTE] avg_sales  [=]  Cost: 18%  Rows: ~840K
│   ├── SCAN (web_sales, date_dim, store_sales, catalog_sales)
│   ├── JOIN (ws_sold_date_sk = d_date_sk)
│   ├── FILTER (ws_wholesale_cost BETWEEN 76 AND 96)
│   ├── FILTER (d_year BETWEEN 1999 AND 1999 + 2)
│   ├── UNION
│   └── OUTPUT (average_sales)
├── [CTE] cross_items  [=]  Cost: 0%  Rows: ~1K
│   ├── SCAN (item, web_sales (join), item AS iws (join), date_dim AS d3 (join), store_sales (join), item AS iss (join), date_dim AS d1 (join), catalog_sales (join), item AS ics (join), date_dim AS d2 (join))
│   ├── JOIN (i_brand_id = brand_id)
│   ├── JOIN (i_class_id = class_id)
│   ├── JOIN (+1 more)
│   ├── FILTER (i_category IN ('Electronics', 'Shoes', 'Sports'))
│   ├── FILTER (i_manager_id BETWEEN 42 AND 51)
│   └── OUTPUT (ss_item_sk)
└── [MAIN] main_query  [=]  Cost: 71%  Rows: ~2.4M
    ├── SCAN (store_sales, item, date_dim, avg_sales, cross_items)
    ├── JOIN (this_year.i_brand_id = last_year.i_brand_id)
    ├── JOIN (this_year.i_class_id = last_year.i_class_id)
    ├── JOIN (+1 more)
    ├── AGG (GROUP BY)
    ├── SORT (this_year.channel ASC, this_year.i_brand_id ASC, this_year.i_class_id ASC, this_year.i_category_id ASC)
    └── OUTPUT (ty_channel, ty_brand, ty_class, ty_category, ty_sales, ty_number_sales, ly_channel, ly_brand, ...)
```

## Output Format

Your response has **two parts** in order:

### Part 1: Modified Logic Tree

Show what changed using change markers. Generate the tree BEFORE writing SQL.

Change markers:
- `[+]` — New component added
- `[-]` — Component removed
- `[~]` — Component modified (describe what changed)
- `[=]` — Unchanged (no children needed)
- `[!]` — Structural change (e.g. CTE → subquery)

### Part 2: Component Payload JSON

```json
{
  "spec_version": "1.0",
  "dialect": "<dialect>",
  "rewrite_rules": [
    {"id": "R1", "type": "<transform_name>", "description": "<what changed>", "applied_to": ["<component_id>"]}
  ],
  "statements": [{
    "target_table": null,
    "change": "modified",
    "components": {
      "<cte_name>": {
        "type": "cte",
        "change": "modified",
        "sql": "<complete SQL for this CTE body>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<upstream_id>"]}
      },
      "main_query": {
        "type": "main_query",
        "change": "modified",
        "sql": "<final SELECT>",
        "interfaces": {"outputs": ["col1", "col2"], "consumes": ["<cte_name>"]}
      }
    },
    "reconstruction_order": ["<cte_name>", "main_query"],
    "assembly_template": "WITH <cte_name> AS ({<cte_name>}) {main_query}"
  }],
  "macros": {},
  "frozen_blocks": [],
  "runtime_config": ["SET LOCAL work_mem = '512MB'"],
  "validation_checks": []
}
```

### Rules
- **Tree first, always.** Generate the Logic Tree before writing any SQL
- **One component at a time.** When writing SQL for component X, treat others as opaque interfaces
- **No ellipsis.** Every `sql` value must be complete, executable SQL
- **Frozen blocks are copy-paste.** Large CASE-WHEN lookups must be verbatim
- **Validate interfaces.** Verify every `consumes` reference exists in upstream `outputs`
- Only include components you **changed or added** — set unchanged components to `"change": "unchanged"` with `"sql": ""`
- `main_query` output columns must match the Column Completeness Contract above
- `runtime_config`: SET LOCAL commands for PostgreSQL. Omit or use empty array if not needed
- `reconstruction_order`: topological order of components for assembly

After the JSON, explain the mechanism:

```
Changes: <1-2 sentences: what structural change + the expected mechanism>
Expected speedup: <estimate>
```

Now output your Logic Tree and Component Payload JSON: