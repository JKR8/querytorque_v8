{
  "explain_plan_text": "Total execution time: 116.0ms\nPlanning time: 1.7ms\n\n-> Limit  (rows=0 loops=1 time=116.0ms)\n  -> Sort  (rows=0 loops=1 time=116.0ms)\n     Sort Method: quicksort  Space: 25kB (Memory)\n    -> Aggregate  (rows=0 loops=1 time=115.9ms)\n      -> Sort  (rows=0 loops=1 time=115.9ms)\n         Sort Method: quicksort  Space: 25kB (Memory)\n        -> Subquery Scan my_revenue  (rows=0 loops=1 time=115.9ms)\n          -> Aggregate  (rows=0 loops=1 time=115.9ms)\n            -> Aggregate  (rows=0 loops=1 time=0.0ms)\n              -> Index Only Scan on date_dim date_dim_2  (rows=0 loops=1 time=0.0ms)\n                 Index Cond: ((d_year = 1998) AND (d_moy = 1))\n            -> Aggregate  (rows=0 loops=1 time=0.0ms)\n              -> Index Only Scan on date_dim date_dim_3  (rows=0 loops=1 time=0.0ms)\n                 Index Cond: ((d_year = 1998) AND (d_moy = 1))\n            -> Nested Loop Inner  (rows=0 loops=1 time=115.9ms)\n              -> Nested Loop Inner  (rows=0 loops=1 time=115.9ms)\n                -> Nested Loop Inner  (rows=0 loops=1 time=115.9ms)\n                  -> Nested Loop Inner  (rows=69 loops=1 time=115.7ms)\n                    -> Unique  (rows=69 loops=1 time=114.7ms)\n                      -> Sort  (rows=70 loops=1 time=114.7ms)\n                         Sort Method: quicksort  Space: 28kB (Memory)\n                        -> Gather  (rows=70 loops=1 time=114.6ms)\n                           Workers: 2/2 launched\n                          -> Nested Loop Inner  (rows=23 loops=3 time=39.7ms)\n                            -> Hash Join Inner  (rows=134 loops=3 time=38.5ms)\n                               Hash Cond: (catalog_sales.cs_item_sk = item.i_item_sk)\n                              -> Nested Loop Inner  (rows=19K loops=3 time=36.8ms)\n                                -> Index Only Scan on date_dim date_dim_1  (rows=10 loops=3 time=1.8ms)\n                                   Index Cond: ((d_year = 1998) AND (d_moy = 1))\n                                -> Append  (rows=1,792 loops=31 time=3.2ms)\n                                  -> Index Scan on catalog_sales  (rows=1,031 loops=31 time=1.5ms)\n                                     Filter: ((cs_wholesale_cost >= '35'::numeric) AND (cs_wholesale_cost <= '65'::numeric))\n                                     Index Cond: (cs_sold_date_sk = date_dim_1.d_date_sk)\n                                     Rows Removed by Filter: 2,814\n                                  -> Index Scan on web_sales  (rows=761 loops=31 time=1.6ms)\n                                     Filter: ((ws_wholesale_cost >= '35'::numeric) AND (ws_wholesale_cost <= '65'::numeric))\n                                     Index Cond: (ws_sold_date_sk = date_dim_1.d_date_sk)\n                                     Rows Removed by Filter: 2,295\n                              -> Hash  (rows=1,454 loops=1 time=0.4ms)\n                                -> Index Only Scan on item  (rows=1,454 loops=1 time=0.3ms)\n                                   Index Cond: ((i_category = 'Electronics'::bpchar) AND (i_class = 'personal'::bpchar))\n                            -> Index Scan on customer  (rows=0 loops=403 time=0.0ms)\n                               Filter: ((c_birth_year >= 1928) AND (c_birth_year <= 1941))\n                               Index Cond: (c_customer_sk = catalog_sales.cs_bill_customer_sk)\n                               Rows Removed by Filter: 1\n                    -> Index Scan on customer_address  (rows=1 loops=69 time=0.0ms)\n                       Index Cond: (ca_address_sk = customer.c_current_addr_sk)\n                  -> Index Scan on store  (rows=0 loops=69 time=0.0ms)\n                     Filter: ((customer_address.ca_county)::text = (s_county)::text)\n                     Index Cond: ((s_state = customer_address.ca_state) AND (s_state = ANY ('{AR,CO,IA,IL,KY,NC,NM,NY,PA,TX}'::bpc...\n                -> Bitmap Heap Scan on store_sales  (rows=0 loops=1 time=0.0ms)\n                   Filter: ((ss_wholesale_cost >= '35'::numeric) AND (ss_wholesale_cost <= '65'::numeric))\n                   Recheck Cond: (ss_customer_sk = customer.c_customer_sk)\n                  -> Bitmap Index Scan on _dta_index_store_sales_6_1333579789__k4_k1_16  (rows=0 loops=1 time=0.0ms)\n                     Index Cond: (ss_customer_sk = customer.c_customer_sk)\n              -> Index Scan on date_dim  (rows=0 loops=1 time=0.0ms)\n                 Filter: ((d_month_seq >= $0) AND (d_month_seq <= $1))\n                 Index Cond: (d_date_sk = store_sales.ss_sold_date_sk)",
  "plan_scanner_text": "Plan diversity: 16 distinct plans, 6 plan changers | HIGH\nBaseline joins: 6x Nested Loop, 1x Hash Join\n  JOIN_TYPE_TRAP (19 combos): no_nestloop: ['Nested Loop(Inner)'] \u2192 []\n  JOIN_ORDER_TRAP (10 combos): table order unstable\n  SCAN_TYPE_TRAP (18 combos): scan methods fragile\n  MEMORY_SENSITIVITY: plan shape changes with more memory\nPlan changers: no_nestloop, no_hashjoin, force_merge, force_nestloop, no_parallel, ssd_costs",
  "global_knowledge": {
    "dataset": "postgresql_dsb",
    "last_updated": "2026-02-08T20:24:49.880702",
    "source_runs": [
      "swarm_batch_20260208_142643"
    ],
    "principles": [
      {
        "id": "or_to_union",
        "name": "Or To Union",
        "what": "Applied or_to_union achieving 73.35x speedup",
        "why": "Converting OR to UNION ALL lets optimizer choose independent index paths per branch",
        "when": "WHERE clause has OR conditions over different dimension keys (\u22643 branches)",
        "when_not": "Caused regression on query013_spj_spj, query085_agg, query085_spj_spj, query091_spj_spj, query101_agg (worst: 0.05x)",
        "verified_speedups": [
          73.35242111461433,
          66.32527638993845,
          53.10837425657921,
          3.067018811694743,
          1.7390934770856203,
          1.6973457487323271,
          1.170427997752986
        ],
        "avg_speedup": 28.637,
        "queries": [
          "query010_multi",
          "query013_agg",
          "query023_multi",
          "query054_multi",
          "query065_multi",
          "query072_spj_spj",
          "query080_multi"
        ],
        "transforms": [
          "date_cte_isolate",
          "dimension_cte_isolate",
          "or_to_union",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "decorrelate",
        "name": "Decorrelate",
        "what": "Applied decorrelate, date_cte_isolate, dimension_cte_isolate achieving 122.29x speedup",
        "why": "Correlated subqueries re-execute per outer row; converting to JOIN eliminates per-row overhead; Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering all dimension tables into CTEs avoids repeated full-table scans",
        "when": "Query has correlated subquery in WHERE or SELECT that references outer table",
        "when_not": "Caused regression on query030_multi, query038_multi, query054_multi, query058_multi, query072_spj_spj, query083_multi, query087_multi (worst: 0.00x)",
        "verified_speedups": [
          122.28628549405371,
          108.83433493033179,
          107.74576725495528,
          11.623200099186304,
          10.946612330947332,
          10.835117003229474,
          4.630291368154034,
          4.103093014813743,
          2.126719235185333,
          1.9521396747351196,
          1.9242588792425155,
          1.7574278136172428,
          1.7413608791729855,
          1.52735815246076,
          1.5244777783663208,
          1.3253961714080202
        ],
        "avg_speedup": 24.68,
        "queries": [
          "query039_multi",
          "query054_multi",
          "query059_multi",
          "query065_multi",
          "query081_multi",
          "query087_multi"
        ],
        "transforms": [
          "date_cte_isolate",
          "decorrelate",
          "dimension_cte_isolate",
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "prefetch_fact_join",
        "name": "Prefetch Fact Join",
        "what": "Applied prefetch_fact_join: Created separate pre-filtered CTEs for dimensions (date_dim, item) and fact subsets (store_sales with wholesale_cost filter), materialized distinct item_sk from frequent_ss_items, and converted IN subqueries to explicit joins to enable better plan optimization.",
        "why": "Pre-joining filtered dimensions with fact table before aggregation reduces join input",
        "when": "Query joins filtered dates/dims with large fact table; pre-join reduces probe size",
        "when_not": "Caused regression on query013_spj_spj, query025_agg, query031_multi, query050_spj_spj, query072_spj_spj, query083_multi, query085_agg, query087_multi, query101_agg (worst: 0.04x)",
        "verified_speedups": [
          64.55244269378329,
          3.3232686420331894,
          1.9096524241678212,
          1.1183033701998488
        ],
        "avg_speedup": 17.726,
        "queries": [
          "query023_multi",
          "query050_agg",
          "query080_multi"
        ],
        "transforms": [
          "prefetch_fact_join"
        ]
      },
      {
        "id": "date_cte_isolate",
        "name": "Date Cte Isolate",
        "what": "Applied date_cte_isolate, multi_dimension_prefetch achieving 11.23x speedup",
        "why": "Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering multiple dimension tables in parallel reduces join fan-out",
        "when": "Query joins date_dim on multiple conditions (year, month, etc.) with fact tables",
        "when_not": "Caused regression on many queries (worst: 0.00x); Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "verified_speedups": [
          11.232139433447843,
          9.62164454268518,
          5.230518436383954,
          4.427864781544116,
          4.332563879765709,
          4.1871005597949456,
          3.804719650483733,
          3.664518718324099,
          3.5183672412141727,
          3.280025955833233,
          2.020200575455194,
          1.8388338665963029,
          1.8219644169344114,
          1.820888647355756,
          1.5791616753126991,
          1.433334918594728,
          1.3943311758574517,
          1.3639086733889336,
          1.2724570762873264,
          1.2289723178206347,
          1.2090317549808018,
          1.1992900217595654,
          1.1873395682650838,
          1.180915193084758,
          1.1772966331778911,
          1.1760558717497558,
          1.1758970086549476,
          1.1715073534764526,
          1.1648213087268702,
          1.1404043526791572,
          1.1179640420855672,
          1.1119870666322185,
          1.1116793782034127,
          1.1084531937353757,
          1.1048729871954481
        ],
        "avg_speedup": 2.469,
        "queries": [
          "query010_multi",
          "query019_agg",
          "query019_spj_spj",
          "query025_spj_spj",
          "query027_agg",
          "query027_spj_spj",
          "query040_agg",
          "query040_spj_spj",
          "query050_agg",
          "query069_multi",
          "query072_agg",
          "query080_multi",
          "query099_agg",
          "query099_spj_spj",
          "query100_spj_spj",
          "query101_agg",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "transforms": [
          "date_cte_isolate",
          "dimension_cte_isolate",
          "multi_date_range_cte",
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "pushdown",
        "name": "Pushdown",
        "what": "Applied pushdown: Created separate CTEs for filtered stores and two date ranges, then computed the two periods' aggregations independently with explicit JOINs and early filter pushdown.",
        "why": "Pushing predicates closer to table scans reduces data volume in upper operators",
        "when": "WHERE predicates reference columns from tables deep in the join tree",
        "when_not": "Caused regression on query014_multi, query027_spj_spj, query031_multi, query058_multi, query083_multi, query085_spj_spj, query087_multi, query091_agg (worst: 0.19x)",
        "verified_speedups": [
          4.6799802539183615,
          2.877932543501555,
          1.9303353662366844,
          1.1778171040495835,
          1.1003399162251937
        ],
        "avg_speedup": 2.353,
        "queries": [
          "query019_agg",
          "query059_multi",
          "query072_agg",
          "query080_multi",
          "query094_multi"
        ],
        "transforms": [
          "pushdown"
        ]
      },
      {
        "id": "unknown",
        "name": "Unknown",
        "what": "Applied unknown: Isolated date_dim subquery into a materialized CTE and created pre-filtered dimension CTEs for item and customer before joining with fact tables. Converted implicit joins to explicit JOIN syntax.",
        "why": "",
        "when": "",
        "when_not": "",
        "verified_speedups": [
          2.524130607367415,
          1.8413157512267129,
          1.5782496133672843,
          1.3090493738395756,
          1.3023183424416436,
          1.2228752599892374
        ],
        "avg_speedup": 1.63,
        "queries": [
          "query018_spj_spj",
          "query019_agg",
          "query058_multi",
          "query084_agg",
          "query099_spj_spj",
          "query102_agg"
        ],
        "transforms": []
      },
      {
        "id": "dimension_cte_isolate",
        "name": "Dimension Cte Isolate",
        "what": "Applied dimension_cte_isolate achieving 1.91x speedup",
        "why": "Pre-filtering all dimension tables into CTEs avoids repeated full-table scans",
        "when": "Query joins 2+ dimension tables that could each be pre-filtered independently",
        "when_not": "Caused regression on many queries (worst: 0.00x); PostgreSQL's CTE optimization fence behavior can prevent predicate pushdown",
        "verified_speedups": [
          1.908835330896292,
          1.598830782308037,
          1.3077128182680382
        ],
        "avg_speedup": 1.605,
        "queries": [
          "query072_spj_spj",
          "query084_agg",
          "query084_spj_spj"
        ],
        "transforms": [
          "dimension_cte_isolate"
        ]
      },
      {
        "id": "multi_dimension_prefetch",
        "name": "Multi Dimension Prefetch",
        "what": "Applied multi_dimension_prefetch achieving 2.10x speedup",
        "why": "Pre-filtering multiple dimension tables in parallel reduces join fan-out",
        "when": "Query references multiple dimension tables (date + store, date + item, etc.)",
        "when_not": "Caused regression on many queries (worst: 0.05x); Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "verified_speedups": [
          2.102927380757268,
          1.9057258662371936,
          1.4104156816846927,
          1.405997493540603,
          1.148035192530136
        ],
        "avg_speedup": 1.595,
        "queries": [
          "query084_agg",
          "query084_spj_spj"
        ],
        "transforms": [
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      }
    ],
    "anti_patterns": [
      {
        "id": "regression_date_cte_isolate",
        "name": "Regression: Date Cte Isolate",
        "mechanism": "PostgreSQL's CTE optimization fence behavior prevents pushdown of filters across CTE boundaries, forcing full scans of large fact tables.",
        "observed_regressions": [
          0.09,
          0.1,
          0.13,
          0.14,
          0.15,
          0.17,
          0.17,
          0.21,
          0.32,
          0.33,
          0.48,
          0.53,
          0.54,
          0.56,
          0.57,
          0.57,
          0.58,
          0.58,
          0.59,
          0.61,
          0.62,
          0.68,
          0.69,
          0.7,
          0.7,
          0.73,
          0.73,
          0.74,
          0.77,
          0.78,
          0.78,
          0.8,
          0.8,
          0.81,
          0.81,
          0.82,
          0.82,
          0.83,
          0.85,
          0.88,
          0.88,
          0.89,
          0.9,
          0.9,
          0.91,
          0.93,
          0.93
        ],
        "queries": [
          "query010_multi",
          "query013_agg",
          "query013_spj_spj",
          "query018_agg",
          "query018_spj_spj",
          "query019_spj_spj",
          "query025_agg",
          "query027_agg",
          "query027_spj_spj",
          "query031_multi",
          "query038_multi",
          "query040_spj_spj",
          "query050_spj_spj",
          "query064_multi",
          "query069_multi",
          "query072_agg",
          "query085_agg",
          "query085_spj_spj",
          "query087_multi",
          "query091_agg",
          "query091_spj_spj",
          "query094_multi",
          "query100_agg",
          "query101_agg",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "avoid_when": "Applying date_cte_isolate to queries similar to the above list"
      },
      {
        "id": "regression_decorrelate",
        "name": "Regression: Decorrelate",
        "mechanism": "Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "observed_regressions": [
          0.0,
          0.04,
          0.06,
          0.06,
          0.26,
          0.31,
          0.34,
          0.53,
          0.6,
          0.76,
          0.85
        ],
        "queries": [
          "query030_multi",
          "query038_multi",
          "query054_multi",
          "query058_multi",
          "query072_spj_spj",
          "query083_multi",
          "query087_multi"
        ],
        "avoid_when": "Applying decorrelate to queries similar to query030_multi, query038_multi, query054_multi, query058_multi, query072_spj_spj, query083_multi, query087_multi"
      },
      {
        "id": "regression_or_to_union",
        "name": "Regression: Or To Union",
        "mechanism": "Column resolution errors from CTEs that only select subset of columns, then reference missing columns in main query WHERE clause.",
        "observed_regressions": [
          0.05,
          0.2,
          0.45,
          0.45,
          0.49,
          0.68
        ],
        "queries": [
          "query013_spj_spj",
          "query085_agg",
          "query085_spj_spj",
          "query091_spj_spj",
          "query101_agg"
        ],
        "avoid_when": "Applying or_to_union to queries similar to query013_spj_spj, query085_agg, query085_spj_spj, query091_spj_spj, query101_agg"
      },
      {
        "id": "regression_prefetch_fact_join",
        "name": "Regression: Prefetch Fact Join",
        "mechanism": "CTEs materialized by default in PostgreSQL, creating temporary tables that eliminated index usage and increased I/O.",
        "observed_regressions": [
          0.47,
          0.58,
          0.68
        ],
        "queries": [
          "query025_agg",
          "query087_multi",
          "query101_agg"
        ],
        "avoid_when": "Applying prefetch_fact_join to queries similar to query025_agg, query087_multi, query101_agg"
      },
      {
        "id": "regression_pushdown",
        "name": "Regression: Pushdown",
        "mechanism": "PostgreSQL's optimizer treats CTEs as optimization fences, preventing pushdown of filters across CTE boundaries.",
        "observed_regressions": [
          0.19,
          0.27,
          0.3,
          0.53,
          0.61,
          0.62,
          0.64,
          0.65
        ],
        "queries": [
          "query014_multi",
          "query027_spj_spj",
          "query031_multi",
          "query058_multi",
          "query083_multi",
          "query085_spj_spj",
          "query087_multi",
          "query091_agg"
        ],
        "avoid_when": "Applying pushdown to queries similar to query014_multi, query027_spj_spj, query031_multi, query058_multi, query083_multi, query085_spj_spj, query087_multi, query091_agg"
      },
      {
        "id": "regression_single_pass_aggregation",
        "name": "Regression: Single Pass Aggregation",
        "mechanism": "Only addressed dimension table filtering, missing the core bottleneck: multi-way self-join pattern forces repeated scans of aggregated data.",
        "observed_regressions": [
          0.62,
          0.67
        ],
        "queries": [
          "query031_multi"
        ],
        "avoid_when": "Applying single_pass_aggregation to queries similar to query031_multi"
      },
      {
        "id": "regression_unknown_regression",
        "name": "Regression: Unknown Regression",
        "mechanism": "Triple INTERSECT in cross_items performs expensive set operations on large intermediate results before applying item filters.",
        "observed_regressions": [
          0.78
        ],
        "queries": [
          "query014_multi"
        ],
        "avoid_when": "Applying unknown_regression to queries similar to query014_multi"
      },
      {
        "id": "error_execution",
        "name": "Error Pattern: Execution",
        "mechanism": "DISTINCT is not implemented for window functions",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query094_multi"
        ],
        "avoid_when": "Watch for execution errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "error_timeout",
        "name": "Error Pattern: Timeout",
        "mechanism": "canceling statement due to statement timeout",
        "observed_regressions": [
          0.0,
          0.0
        ],
        "queries": [
          "query014_multi",
          "query085_spj_spj"
        ],
        "avoid_when": "Watch for timeout errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "error_unknown",
        "name": "Error Pattern: Unknown",
        "mechanism": "operator does not exist: integer = character \u2014 type mismatch from incorrect join column references",
        "observed_regressions": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query013_spj_spj",
          "query014_multi",
          "query019_agg",
          "query023_multi",
          "query025_spj_spj",
          "query030_multi",
          "query031_multi",
          "query054_multi",
          "query058_multi",
          "query064_multi",
          "query072_agg",
          "query075_multi",
          "query085_agg",
          "query085_spj_spj",
          "query091_agg",
          "query094_multi",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "avoid_when": "Watch for unknown errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "semantic_mismatch_date_cte_isolate",
        "name": "Semantic Mismatch: Date Cte Isolate",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query040_agg",
          "query075_multi",
          "query099_agg",
          "query100_agg"
        ],
        "avoid_when": "Applying date_cte_isolate to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_decorrelate",
        "name": "Semantic Mismatch: Decorrelate",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query014_multi",
          "query059_multi",
          "query075_multi"
        ],
        "avoid_when": "Applying decorrelate to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_multi_dimension_prefetch",
        "name": "Semantic Mismatch: Multi Dimension Prefetch",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query084_agg"
        ],
        "avoid_when": "Applying multi_dimension_prefetch to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_or_to_union",
        "name": "Semantic Mismatch: Or To Union",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query100_agg"
        ],
        "avoid_when": "Applying or_to_union to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_prefetch_fact_join",
        "name": "Semantic Mismatch: Prefetch Fact Join",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query075_multi"
        ],
        "avoid_when": "Applying prefetch_fact_join to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_union_cte_split",
        "name": "Semantic Mismatch: Union Cte Split",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0
        ],
        "queries": [
          "query075_multi"
        ],
        "avoid_when": "Applying union_cte_split to queries where semantic equivalence is hard to verify"
      }
    ]
  },
  "semantic_intents": null,
  "matched_examples": [
    {
      "id": "pg_dimension_prefetch_star",
      "name": "Dimension Pre-filter with Explicit JOINs (PostgreSQL)",
      "description": "On multi-channel UNION queries with comma-separated implicit joins, pre-filter dimension tables (date, item, promotion) into CTEs and convert to explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates and join ordering from explicit JOINs with pre-materialized small dimension results.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "benchmark_queries": [
        "DSB Q080_multi"
      ],
      "verified_speedup": "3.32x",
      "principle": "Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.",
      "transforms": [
        "date_cte_isolate",
        "early_filter"
      ],
      "original_sql": "with ssr as\n (select  s_store_id as store_id,\n          sum(ss_ext_sales_price) as sales,\n          sum(coalesce(sr_return_amt, 0)) as returns,\n          sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit\n  from store_sales left outer join store_returns on\n         (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),\n     date_dim,\n     store,\n     item,\n     promotion\n where ss_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n       and ss_store_sk = s_store_sk\n       and ss_item_sk = i_item_sk\n       and i_current_price > 50\n       and ss_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and ss_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\n group by s_store_id)\n ,\n csr as\n (select  cp_catalog_page_id as catalog_page_id,\n          sum(cs_ext_sales_price) as sales,\n          sum(coalesce(cr_return_amount, 0)) as returns,\n          sum(cs_net_profit - coalesce(cr_net_loss, 0)) as profit\n  from catalog_sales left outer join catalog_returns on\n         (cs_item_sk = cr_item_sk and cs_order_number = cr_order_number),\n     date_dim,\n     catalog_page,\n     item,\n     promotion\n where cs_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n        and cs_catalog_page_sk = cp_catalog_page_sk\n       and cs_item_sk = i_item_sk\n       and i_current_price > 50\n       and cs_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and cs_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\ngroup by cp_catalog_page_id)\n ,\n wsr as\n (select  web_site_id,\n          sum(ws_ext_sales_price) as sales,\n          sum(coalesce(wr_return_amt, 0)) as returns,\n          sum(ws_net_profit - coalesce(wr_net_loss, 0)) as profit\n  from web_sales left outer join web_returns on\n         (ws_item_sk = wr_item_sk and ws_order_number = wr_order_number),\n     date_dim,\n     web_site,\n     item,\n     promotion\n where ws_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n        and ws_web_site_sk = web_site_sk\n       and ws_item_sk = i_item_sk\n       and i_current_price > 50\n       and ws_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and ws_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\ngroup by web_site_id)\n  select  channel\n        , id\n        , sum(sales) as sales\n        , sum(returns) as returns\n        , sum(profit) as profit\n from\n (select 'store channel' as channel\n        , 'store' || store_id as id\n        , sales\n        , returns\n        , profit\n from   ssr\n union all\n select 'catalog channel' as channel\n        , 'catalog_page' || catalog_page_id as id\n        , sales\n        , returns\n        , profit\n from  csr\n union all\n select 'web channel' as channel\n        , 'web_site' || web_site_id as id\n        , sales\n        , returns\n        , profit\n from   wsr\n ) x\n group by rollup (channel, id)\n order by channel\n         ,id\n limit 100;",
      "optimized_sql": "WITH filtered_date AS (SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1998-08-23' AS DATE) AND CAST('1998-08-23' AS DATE) + INTERVAL '30 DAY'), filtered_item AS (SELECT i_item_sk FROM item WHERE i_current_price > 50 AND i_category IN ('Jewelry', 'Music')), filtered_promotion AS (SELECT p_promo_sk FROM promotion WHERE p_channel_email = 'Y' AND p_channel_tv = 'Y' AND p_channel_radio = 'N' AND p_channel_press = 'N' AND p_channel_event = 'Y'), ssr AS (SELECT s_store_id AS store_id, SUM(ss_ext_sales_price) AS sales, SUM(COALESCE(sr_return_amt, 0)) AS returns, SUM(ss_net_profit - COALESCE(sr_net_loss, 0)) AS profit FROM store_sales LEFT OUTER JOIN store_returns ON (ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number) INNER JOIN filtered_date ON ss_sold_date_sk = filtered_date.d_date_sk INNER JOIN store ON ss_store_sk = s_store_sk INNER JOIN filtered_item ON ss_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ss_promo_sk = filtered_promotion.p_promo_sk WHERE ss_wholesale_cost BETWEEN 63 AND 78 GROUP BY s_store_id), csr AS (SELECT cp_catalog_page_id AS catalog_page_id, SUM(cs_ext_sales_price) AS sales, SUM(COALESCE(cr_return_amount, 0)) AS returns, SUM(cs_net_profit - COALESCE(cr_net_loss, 0)) AS profit FROM catalog_sales LEFT OUTER JOIN catalog_returns ON (cs_item_sk = cr_item_sk AND cs_order_number = cr_order_number) INNER JOIN filtered_date ON cs_sold_date_sk = filtered_date.d_date_sk INNER JOIN catalog_page ON cs_catalog_page_sk = cp_catalog_page_sk INNER JOIN filtered_item ON cs_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON cs_promo_sk = filtered_promotion.p_promo_sk WHERE cs_wholesale_cost BETWEEN 63 AND 78 GROUP BY cp_catalog_page_id), wsr AS (SELECT web_site_id, SUM(ws_ext_sales_price) AS sales, SUM(COALESCE(wr_return_amt, 0)) AS returns, SUM(ws_net_profit - COALESCE(wr_net_loss, 0)) AS profit FROM web_sales LEFT OUTER JOIN web_returns ON (ws_item_sk = wr_item_sk AND ws_order_number = wr_order_number) INNER JOIN filtered_date ON ws_sold_date_sk = filtered_date.d_date_sk INNER JOIN web_site ON ws_web_site_sk = web_site_sk INNER JOIN filtered_item ON ws_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ws_promo_sk = filtered_promotion.p_promo_sk WHERE ws_wholesale_cost BETWEEN 63 AND 78 GROUP BY web_site_id) SELECT channel, id, SUM(sales) AS sales, SUM(returns) AS returns, SUM(profit) AS profit FROM (SELECT 'store channel' AS channel, 'store' || store_id AS id, sales, returns, profit FROM ssr UNION ALL SELECT 'catalog channel' AS channel, 'catalog_page' || catalog_page_id AS id, sales, returns, profit FROM csr UNION ALL SELECT 'web channel' AS channel, 'web_site' || web_site_id AS id, sales, returns, profit FROM wsr) AS x GROUP BY ROLLUP (channel, id) ORDER BY channel, id LIMIT 100",
      "example": {
        "opportunity": "DIMENSION_PREFETCH_STAR",
        "input_slice": "with ssr as\n (select s_store_id as store_id,\n         sum(ss_ext_sales_price) as sales,\n         sum(coalesce(sr_return_amt, 0)) as returns,\n         sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit\n  from store_sales left outer join store_returns on\n       (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),\n     date_dim, store, item, promotion\n where ss_sold_date_sk = d_date_sk\n   and d_date between '1998-08-23' and '1998-08-23'::date + interval '30 day'\n   and ss_store_sk = s_store_sk\n   and ss_item_sk = i_item_sk\n   and i_current_price > 50\n   and ss_promo_sk = p_promo_sk\n   and p_channel_email = 'Y' and p_channel_tv = 'Y'\n   and p_channel_radio = 'N' and p_channel_press = 'N' and p_channel_event = 'Y'\n   and ss_wholesale_cost BETWEEN 63 AND 78\n   and i_category IN ('Jewelry', 'Music')\n group by s_store_id)\n-- ... csr and wsr CTEs similar ...\nselect channel, id, sum(sales), sum(returns), sum(profit)\nfrom (...) x group by rollup(channel, id) order by channel, id limit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "date_cte_isolate + early_filter",
              "nodes": {
                "filtered_date": "SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1998-08-23' AS DATE) AND CAST('1998-08-23' AS DATE) + INTERVAL '30 DAY'",
                "filtered_item": "SELECT i_item_sk FROM item WHERE i_current_price > 50 AND i_category IN ('Jewelry', 'Music')",
                "filtered_promotion": "SELECT p_promo_sk FROM promotion WHERE p_channel_email = 'Y' AND p_channel_tv = 'Y' AND p_channel_radio = 'N' AND p_channel_press = 'N' AND p_channel_event = 'Y'",
                "ssr": "SELECT s_store_id AS store_id, SUM(ss_ext_sales_price) AS sales, SUM(COALESCE(sr_return_amt, 0)) AS returns, SUM(ss_net_profit - COALESCE(sr_net_loss, 0)) AS profit FROM store_sales LEFT OUTER JOIN store_returns ON (ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number) INNER JOIN filtered_date ON ss_sold_date_sk = filtered_date.d_date_sk INNER JOIN store ON ss_store_sk = s_store_sk INNER JOIN filtered_item ON ss_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ss_promo_sk = filtered_promotion.p_promo_sk WHERE ss_wholesale_cost BETWEEN 63 AND 78 GROUP BY s_store_id",
                "csr": "SELECT cp_catalog_page_id AS catalog_page_id, SUM(cs_ext_sales_price) AS sales, SUM(COALESCE(cr_return_amount, 0)) AS returns, SUM(cs_net_profit - COALESCE(cr_net_loss, 0)) AS profit FROM catalog_sales LEFT OUTER JOIN catalog_returns ON (cs_item_sk = cr_item_sk AND cs_order_number = cr_order_number) INNER JOIN filtered_date ON cs_sold_date_sk = filtered_date.d_date_sk INNER JOIN catalog_page ON cs_catalog_page_sk = cp_catalog_page_sk INNER JOIN filtered_item ON cs_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON cs_promo_sk = filtered_promotion.p_promo_sk WHERE cs_wholesale_cost BETWEEN 63 AND 78 GROUP BY cp_catalog_page_id",
                "wsr": "SELECT web_site_id, SUM(ws_ext_sales_price) AS sales, SUM(COALESCE(wr_return_amt, 0)) AS returns, SUM(ws_net_profit - COALESCE(wr_net_loss, 0)) AS profit FROM web_sales LEFT OUTER JOIN web_returns ON (ws_item_sk = wr_item_sk AND ws_order_number = wr_order_number) INNER JOIN filtered_date ON ws_sold_date_sk = filtered_date.d_date_sk INNER JOIN web_site ON ws_web_site_sk = web_site_sk INNER JOIN filtered_item ON ws_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ws_promo_sk = filtered_promotion.p_promo_sk WHERE ws_wholesale_cost BETWEEN 63 AND 78 GROUP BY web_site_id",
                "main_query": "SELECT channel, id, SUM(sales) AS sales, SUM(returns) AS returns, SUM(profit) AS profit FROM (SELECT 'store channel' AS channel, 'store' || store_id AS id, sales, returns, profit FROM ssr UNION ALL SELECT 'catalog channel' AS channel, 'catalog_page' || catalog_page_id AS id, sales, returns, profit FROM csr UNION ALL SELECT 'web channel' AS channel, 'web_site' || web_site_id AS id, sales, returns, profit FROM wsr) AS x GROUP BY ROLLUP (channel, id) ORDER BY channel, id LIMIT 100"
              },
              "node_contracts": {
                "filtered_date": [
                  "d_date_sk"
                ],
                "filtered_item": [
                  "i_item_sk"
                ],
                "filtered_promotion": [
                  "p_promo_sk"
                ],
                "ssr": [
                  "store_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "csr": [
                  "catalog_page_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "wsr": [
                  "web_site_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "main_query": [
                  "channel",
                  "id",
                  "sales",
                  "returns",
                  "profit"
                ]
              },
              "data_flow": "filtered_date, filtered_item, filtered_promotion -> ssr, csr, wsr -> main_query",
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ROLLUP"
              ],
              "expected_speedup": "3.3x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Multi-Dimension Prefetch (PG) \u2014 pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax for PostgreSQL optimizer join-order freedom. Even partial transformation helps when one branch dominates runtime. Here: date (30/73K), item (2 categories), promotion (5 filters) all become CTEs; comma joins converted to INNER JOIN.",
        "pattern_detection": "Look for multi-channel queries (store/catalog/web) using comma-separated implicit joins with shared dimension filters (date range, item category, promotion flags). These queries benefit from extracting shared dimension filters into CTEs and converting to explicit JOINs."
      }
    },
    {
      "id": "pg_self_join_decomposition",
      "name": "Self-Join Decomposition (PostgreSQL)",
      "description": "Eliminate duplicate fact table scans in self-join patterns by computing the aggregation ONCE in a CTE and deriving both per-item and per-store averages from the same materialized result. PostgreSQL materializes CTEs by default, making this extremely effective.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "benchmark_queries": [
        "DSB Q065_multi"
      ],
      "verified_speedup": "3.93x",
      "principle": "Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.",
      "transforms": [
        "materialize_cte"
      ],
      "original_sql": "select \n\ts_store_name,\n\ti_item_desc,\n\tsc.revenue,\n\ti_current_price,\n\ti_wholesale_cost,\n\ti_brand\n from store, item,\n     (select ss_store_sk, avg(revenue) as ave\n\tfrom\n\t    (select  ss_store_sk, ss_item_sk,\n\t\t     sum(ss_sales_price) as revenue\n\t\tfrom store_sales, date_dim\n\t\twhere ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11\n   and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01\n\t\tgroup by ss_store_sk, ss_item_sk) sa\n\tgroup by ss_store_sk) sb,\n     (select  ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n\tfrom store_sales, date_dim\n\twhere ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11\n  and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01\n\tgroup by ss_store_sk, ss_item_sk) sc\n where sb.ss_store_sk = sc.ss_store_sk and\n       sc.revenue <= 0.1 * sb.ave and\n       s_store_sk = sc.ss_store_sk and\n       i_item_sk = sc.ss_item_sk\n       and i_manager_id BETWEEN 32 and 36\n       and s_state in ('TN','TX','VA')\n order by s_store_name, i_item_desc\nlimit 100;",
      "optimized_sql": "WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1213 + 11), store_sales_revenue AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON store_sales.ss_sold_date_sk = date_filter.d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01 GROUP BY ss_store_sk, ss_item_sk), store_avg_revenue AS (SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk), filtered_store AS (SELECT s_store_sk, s_store_name, s_state FROM store WHERE s_state IN ('TN', 'TX', 'VA')), filtered_item AS (SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand, i_manager_id FROM item WHERE i_manager_id BETWEEN 32 AND 36) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100",
      "example": {
        "opportunity": "SELF_JOIN_DECOMPOSITION",
        "input_slice": "select s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand\nfrom store, item,\n  (select ss_store_sk, avg(revenue) as ave\n   from (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n         from store_sales, date_dim\n         where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224\n           and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48\n         group by ss_store_sk, ss_item_sk) sa\n   group by ss_store_sk) sb,\n  (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n   from store_sales, date_dim\n   where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224\n     and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48\n   group by ss_store_sk, ss_item_sk) sc\nwhere sb.ss_store_sk = sc.ss_store_sk\n  and sc.revenue <= 0.1 * sb.ave\n  and s_store_sk = sc.ss_store_sk\n  and i_item_sk = sc.ss_item_sk\n  and i_manager_id BETWEEN 32 AND 36\n  and s_state in ('TN','TX','VA')\norder by s_store_name, i_item_desc\nlimit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "materialize_cte",
              "nodes": {
                "date_filter": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224",
                "store_sales_revenue": "SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk",
                "store_avg_revenue": "SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk",
                "filtered_store": "SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')",
                "filtered_item": "SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36",
                "main_query": "SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering"
              ],
              "expected_speedup": "3.9x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Shared Materialization (PG) \u2014 when the same fact+dimension scan appears multiple times, materialize it once as a CTE and reference it from each consumer. PostgreSQL CTE materialization guarantees single execution. Here: store_sales+date_dim scanned twice with identical predicates becomes one materialized CTE, reused for both per-item revenue and per-store averages. Combined with dimension pre-filtering to reduce I/O.",
        "pattern_detection": "Look for queries where the same fact table + date_dim join appears in two or more subqueries with identical WHERE predicates. The subqueries typically compute per-group aggregates at different granularities (e.g., per-item vs per-store)."
      }
    },
    {
      "id": "early_filter_decorrelate",
      "name": "Early Filter + Decorrelate",
      "database": "postgres",
      "verified_speedup": "1.13x",
      "principle": "Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.",
      "benchmark": {
        "dataset": "dsb_sf10",
        "query": "query001",
        "original_time_s": 13.43,
        "optimized_time_s": 11.85
      },
      "ast_flags": {
        "kb_patterns_detected": [
          {
            "id": "CORRELATED_TO_CTE",
            "name": "Correlated Subquery to Pre-computed CTE",
            "trigger": "WHERE col > (SELECT AVG/SUM/COUNT FROM ... WHERE correlated)"
          },
          {
            "id": "DATE_CTE_ISOLATION",
            "name": "Date CTE Isolation",
            "trigger": "date_dim joined with d_year/d_qoy/d_month filter, fact table present"
          }
        ],
        "structural_patterns": [
          "CTE with correlated subquery reference (ctr1 -> ctr2)",
          "Dimension filter (s_state) applied AFTER CTE in main query",
          "AVG aggregate in correlated subquery with GROUP BY correlation key"
        ]
      },
      "original_sql": "WITH customer_total_return AS (\n  SELECT sr_customer_sk AS ctr_customer_sk,\n         sr_store_sk AS ctr_store_sk,\n         sr_reason_sk AS ctr_reason_sk,\n         SUM(SR_REFUNDED_CASH) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year = 2001\n    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer, customer_demographics\nWHERE ctr1.ctr_total_return > (\n    SELECT AVG(ctr_total_return) * 1.2\n    FROM customer_total_return ctr2\n    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk\n  )\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state IN ('MI', 'NC', 'WI')\n  AND ctr1.ctr_customer_sk = c_customer_sk\n  AND c_current_cdemo_sk = cd_demo_sk\n  AND cd_marital_status IN ('W', 'W')\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100",
      "optimized_sql": "WITH customer_total_return AS (\n    SELECT sr_customer_sk AS ctr_customer_sk,\n           sr_store_sk AS ctr_store_sk,\n           sr_reason_sk AS ctr_reason_sk,\n           SUM(SR_REFUNDED_CASH) AS ctr_total_return\n    FROM store_returns\n    JOIN date_dim ON sr_returned_date_sk = d_date_sk\n    JOIN store ON sr_store_sk = s_store_sk\n    WHERE d_year = 2001\n      AND s_state IN ('MI', 'NC', 'WI')\n      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n),\nstore_thresholds AS (\n    SELECT ctr_store_sk,\n           AVG(ctr_total_return) * 1.2 AS avg_limit\n    FROM customer_total_return\n    GROUP BY ctr_store_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1\nJOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk\nJOIN customer ON ctr1.ctr_customer_sk = c_customer_sk\nJOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk\nJOIN store s ON ctr1.ctr_store_sk = s.s_store_sk\nWHERE ctr1.ctr_total_return > st.avg_limit\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s.s_state IN ('MI', 'NC', 'WI')\n  AND cd_marital_status = 'W'\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100",
      "pg_blind_spots": [
        "Cannot push filters into materialized CTEs",
        "Correlated subqueries may execute row-by-row on large CTEs"
      ],
      "input": {
        "description": "CTE aggregates fact table, main query has correlated subquery for AVG threshold, dimension filter applied late",
        "sql": "WITH customer_total_return AS (\n  SELECT sr_customer_sk AS ctr_customer_sk,\n         sr_store_sk AS ctr_store_sk,\n         sr_reason_sk AS ctr_reason_sk,\n         SUM(SR_REFUNDED_CASH) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year = 2001\n    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer, customer_demographics\nWHERE ctr1.ctr_total_return > (\n    SELECT AVG(ctr_total_return) * 1.2\n    FROM customer_total_return ctr2\n    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk\n  )\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state IN ('MI', 'NC', 'WI')\n  AND ctr1.ctr_customer_sk = c_customer_sk\n  AND c_current_cdemo_sk = cd_demo_sk\n  AND cd_marital_status IN ('W', 'W')\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100"
      },
      "output": {
        "description": "Push dimension filter INTO CTE, decorrelate AVG into separate CTE, JOIN on threshold",
        "sql": "WITH customer_total_return AS (\n    SELECT sr_customer_sk AS ctr_customer_sk,\n           sr_store_sk AS ctr_store_sk,\n           sr_reason_sk AS ctr_reason_sk,\n           SUM(SR_REFUNDED_CASH) AS ctr_total_return\n    FROM store_returns\n    JOIN date_dim ON sr_returned_date_sk = d_date_sk\n    JOIN store ON sr_store_sk = s_store_sk\n    WHERE d_year = 2001\n      AND s_state IN ('MI', 'NC', 'WI')\n      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n),\nstore_thresholds AS (\n    SELECT ctr_store_sk,\n           AVG(ctr_total_return) * 1.2 AS avg_limit\n    FROM customer_total_return\n    GROUP BY ctr_store_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1\nJOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk\nJOIN customer ON ctr1.ctr_customer_sk = c_customer_sk\nJOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk\nJOIN store s ON ctr1.ctr_store_sk = s.s_store_sk\nWHERE ctr1.ctr_total_return > st.avg_limit\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s.s_state IN ('MI', 'NC', 'WI')\n  AND cd_marital_status = 'W'\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100"
      },
      "transforms_applied": [
        "early_filter: Push s_state filter INTO CTE before aggregation",
        "decorrelate: Convert correlated AVG subquery to separate CTE with GROUP BY",
        "join_rewrite: Replace correlated lookup with JOIN on pre-computed threshold"
      ],
      "key_insight": "Principle: Early Selection + Decorrelation \u2014 push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN. Here: dimension filters pushed into CTEs, AVG threshold pre-computed and JOINed."
    },
    {
      "id": "pg_materialized_dimension_fact_prefilter",
      "name": "MATERIALIZED Dimension + Fact Pre-filter (PostgreSQL)",
      "description": "Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then join with explicit JOIN syntax. On queries with expensive non-equi joins (inventory quantity < sales quantity, week_seq correlation), reducing both dimension AND fact table sizes before the join dramatically cuts the search space. The MATERIALIZED keyword on PG12+ forces early execution of each CTE.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "benchmark_queries": [
        "DSB Q072_agg"
      ],
      "verified_speedup": "2.68x",
      "principle": "Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.",
      "transforms": [
        "early_filter",
        "date_cte_isolate"
      ],
      "original_sql": "select  i_item_desc\n      ,w_warehouse_name\n      ,d1.d_week_seq\n      ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo\n      ,sum(case when p_promo_sk is not null then 1 else 0 end) promo\n      ,count(*) total_cnt\nfrom catalog_sales\njoin inventory on (cs_item_sk = inv_item_sk)\njoin warehouse on (w_warehouse_sk=inv_warehouse_sk)\njoin item on (i_item_sk = cs_item_sk)\njoin customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)\njoin household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)\njoin date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)\njoin date_dim d2 on (inv_date_sk = d2.d_date_sk)\njoin date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)\nleft outer join promotion on (cs_promo_sk=p_promo_sk)\nleft outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)\nwhere d1.d_week_seq = d2.d_week_seq\n  and inv_quantity_on_hand < cs_quantity\n  and d3.d_date > d1.d_date + interval '3 day'\n  and hd_buy_potential = '501-1000'\n  and d1.d_year = 1998\n  and cd_marital_status = 'M'\n  and cd_dep_count between 9 and 11\n  and i_category IN ('Home', 'Men', 'Music')\n  and cs_wholesale_cost BETWEEN 34 AND 54\ngroup by i_item_desc,w_warehouse_name,d1.d_week_seq\norder by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq\nlimit 100;",
      "optimized_sql": "WITH filtered_date AS MATERIALIZED (\n  SELECT d_date_sk, d_date, d_week_seq\n  FROM date_dim\n  WHERE d_year = 1998\n),\nfiltered_item AS MATERIALIZED (\n  SELECT i_item_sk, i_item_desc\n  FROM item\n  WHERE i_category IN ('Home', 'Men', 'Music')\n),\nfiltered_cd AS MATERIALIZED (\n  SELECT cd_demo_sk\n  FROM customer_demographics\n  WHERE cd_marital_status = 'M'\n    AND cd_dep_count BETWEEN 9 AND 11\n),\nfiltered_hd AS MATERIALIZED (\n  SELECT hd_demo_sk\n  FROM household_demographics\n  WHERE hd_buy_potential = '501-1000'\n),\ncs_filtered AS MATERIALIZED (\n  SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk,\n         cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost,\n         cs_order_number\n  FROM catalog_sales\n  WHERE cs_wholesale_cost BETWEEN 34 AND 54\n)\nSELECT i.i_item_desc,\n       w.w_warehouse_name,\n       d1.d_week_seq,\n       SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo,\n       SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo,\n       COUNT(*) AS total_cnt\nFROM cs_filtered cs\nJOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk\nJOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk\nJOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk\nJOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk\nJOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk\nJOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk\nJOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk\nJOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk\nLEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk\nLEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk \n  AND cr.cr_order_number = cs.cs_order_number\nWHERE d1.d_week_seq = d2.d_week_seq\n  AND inv.inv_quantity_on_hand < cs.cs_quantity\n  AND d3.d_date > d1.d_date + INTERVAL '3 day'\nGROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq\nORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq\nLIMIT 100;",
      "example": {
        "opportunity": "MATERIALIZED_DIMENSION_FACT_PREFILTER",
        "input_slice": "select i_item_desc, w_warehouse_name, d1.d_week_seq,\n  sum(case when p_promo_sk is null then 1 else 0 end) no_promo,\n  sum(case when p_promo_sk is not null then 1 else 0 end) promo,\n  count(*) total_cnt\nfrom catalog_sales\njoin inventory on (cs_item_sk = inv_item_sk)\njoin warehouse on (w_warehouse_sk=inv_warehouse_sk)\njoin item on (i_item_sk = cs_item_sk)\njoin customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)\njoin household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)\njoin date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)\njoin date_dim d2 on (inv_date_sk = d2.d_date_sk)\njoin date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)\nleft outer join promotion on (cs_promo_sk=p_promo_sk)\nleft outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)\nwhere d1.d_week_seq = d2.d_week_seq\n  and inv_quantity_on_hand < cs_quantity\n  and d3.d_date > d1.d_date + interval '3 day'\n  and hd_buy_potential = '501-1000'\n  and d1.d_year = 1998\n  and cd_marital_status = 'M'\n  and cd_dep_count between 9 and 11\n  and i_category IN ('Home', 'Men', 'Music')\n  and cs_wholesale_cost BETWEEN 34 AND 54\ngroup by i_item_desc,w_warehouse_name,d1.d_week_seq\norder by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq\nlimit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "early_filter",
              "nodes": {
                "filtered_date": "SELECT d_date_sk, d_date, d_week_seq FROM date_dim WHERE d_year = 1998",
                "filtered_item": "SELECT i_item_sk, i_item_desc FROM item WHERE i_category IN ('Home', 'Men', 'Music')",
                "filtered_cd": "SELECT cd_demo_sk FROM customer_demographics WHERE cd_marital_status = 'M' AND cd_dep_count BETWEEN 9 AND 11",
                "filtered_hd": "SELECT hd_demo_sk FROM household_demographics WHERE hd_buy_potential = '501-1000'",
                "cs_filtered": "SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk, cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost, cs_order_number FROM catalog_sales WHERE cs_wholesale_cost BETWEEN 34 AND 54",
                "main_query": "SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq, SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo, SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo, COUNT(*) AS total_cnt FROM cs_filtered cs JOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk JOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk JOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk JOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk JOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk JOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk LEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk LEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk AND cr.cr_order_number = cs.cs_order_number WHERE d1.d_week_seq = d2.d_week_seq AND inv.inv_quantity_on_hand < cs.cs_quantity AND d3.d_date > d1.d_date + INTERVAL '3 day' GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering",
                "same join semantics"
              ],
              "expected_speedup": "2.7x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Staged Reduction for Non-Equi Joins \u2014 when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join to shrink the search space. MATERIALIZED on PG12+ forces early execution. Here: fact table CTE removes ~70% of catalog_sales rows, dimension CTEs reduce date (365/73K), item (3 categories), and demographics to tiny sets \u2014 all before the expensive inventory non-equi join.",
        "pattern_detection": "Look for multi-table star-schema queries with: (1) expensive non-equi joins (quantity comparisons, date arithmetic), (2) a range filter on the fact table that removes >50% of rows, (3) multiple dimension filters. The combination of MATERIALIZED dimension CTEs + fact table pre-filter works when the non-equi joins dominate cost."
      }
    },
    {
      "id": "inline_decorrelate_materialized",
      "name": "Inline Correlated Subquery to Materialized CTEs",
      "database": "postgres",
      "verified_speedup": "timeout_rescue",
      "principle": "Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs \u2014 (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data \u2014 then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.",
      "benchmark": {
        "dataset": "dsb_sf5",
        "query": "query032",
        "original_time_s": "timeout (>300s)",
        "optimized_time_s": 0.66
      },
      "ast_flags": {
        "kb_patterns_detected": [
          {
            "id": "INLINE_CORRELATED_SCALAR",
            "name": "Inline Correlated Scalar Subquery",
            "trigger": "WHERE col > (SELECT agg_func(col) FROM fact_table WHERE fact.key = outer.key)"
          },
          {
            "id": "FLAT_FROM_NO_CTE",
            "name": "Flat FROM clause without CTEs",
            "trigger": "FROM fact_table, dim1, dim2 WHERE ... AND col > (SELECT ...)"
          }
        ],
        "structural_patterns": [
          "No CTEs in original \u2014 flat FROM with implicit joins",
          "Correlated scalar subquery in WHERE with aggregate (avg/sum/count)",
          "Subquery re-scans fact table with date range filter",
          "Outer query joins fact \u00d7 dimension \u00d7 date_dim"
        ]
      },
      "original_sql": "select  sum(cs_ext_discount_amt)  as \"excess discount amount\"\nfrom\n   catalog_sales\n   ,item\n   ,date_dim\nwhere\n(i_manufact_id in (1, 78, 97, 516, 521)\nor i_manager_id BETWEEN 25 and 54)\nand i_item_sk = cs_item_sk\nand d_date between '1999-03-07' and\n        cast('1999-03-07' as date) + interval '90 day'\nand d_date_sk = cs_sold_date_sk\nand cs_ext_discount_amt\n     > (\n         select\n            1.3 * avg(cs_ext_discount_amt)\n         from\n            catalog_sales\n           ,date_dim\n         where\n              cs_item_sk = i_item_sk\n          and d_date between '1999-03-07' and\n                             cast('1999-03-07' as date) + interval '90 day'\n          and d_date_sk = cs_sold_date_sk\n          and cs_list_price between 16 and 45\n          and cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n      )\norder by sum(cs_ext_discount_amt)\nlimit 100;",
      "optimized_sql": "WITH filtered_items AS MATERIALIZED (\n    SELECT i_item_sk\n    FROM item\n    WHERE i_manufact_id IN (1, 78, 97, 516, 521)\n       OR i_manager_id BETWEEN 25 AND 54\n),\ndate_filtered_sales AS MATERIALIZED (\n    SELECT cs.cs_item_sk, cs.cs_ext_discount_amt,\n           cs.cs_list_price, cs.cs_sales_price\n    FROM catalog_sales cs\n    JOIN date_dim d ON d.d_date_sk = cs.cs_sold_date_sk\n    WHERE d.d_date BETWEEN '1999-03-07' AND cast('1999-03-07' as date) + interval '90 day'\n),\nitem_avg_discount AS MATERIALIZED (\n    SELECT dfs.cs_item_sk,\n           1.3 * avg(dfs.cs_ext_discount_amt) AS threshold\n    FROM date_filtered_sales dfs\n    JOIN filtered_items fi ON fi.i_item_sk = dfs.cs_item_sk\n    WHERE dfs.cs_list_price BETWEEN 16 AND 45\n      AND dfs.cs_sales_price / dfs.cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n    GROUP BY dfs.cs_item_sk\n)\nSELECT sum(dfs.cs_ext_discount_amt) AS \"excess discount amount\"\nFROM date_filtered_sales dfs\nJOIN item_avg_discount iad ON iad.cs_item_sk = dfs.cs_item_sk\nWHERE dfs.cs_ext_discount_amt > iad.threshold\nORDER BY 1\nLIMIT 100;",
      "pg_blind_spots": [
        "PG re-executes correlated scalar subquery per outer row \u2014 O(N*M) scans",
        "PG underestimates CTE cardinality (estimated 7 rows, actual 5,021) causing nested loop instead of hash join",
        "Without MATERIALIZED keyword, PG may inline CTE back into correlated form"
      ],
      "input": {
        "description": "Flat FROM (fact \u00d7 dim \u00d7 date) with inline correlated scalar subquery computing per-item discount threshold. Subquery re-scans the same fact table with date filter.",
        "sql": "select  sum(cs_ext_discount_amt)  as \"excess discount amount\"\nfrom\n   catalog_sales\n   ,item\n   ,date_dim\nwhere\n(i_manufact_id in (1, 78, 97, 516, 521)\nor i_manager_id BETWEEN 25 and 54)\nand i_item_sk = cs_item_sk\nand d_date between '1999-03-07' and\n        cast('1999-03-07' as date) + interval '90 day'\nand d_date_sk = cs_sold_date_sk\nand cs_ext_discount_amt\n     > (\n         select\n            1.3 * avg(cs_ext_discount_amt)\n         from\n            catalog_sales\n           ,date_dim\n         where\n              cs_item_sk = i_item_sk\n          and d_date between '1999-03-07' and\n                             cast('1999-03-07' as date) + interval '90 day'\n          and d_date_sk = cs_sold_date_sk\n          and cs_list_price between 16 and 45\n          and cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n      )\norder by sum(cs_ext_discount_amt)\nlimit 100;"
      },
      "output": {
        "description": "3 MATERIALIZED CTEs: (1) filtered dimension keys, (2) date-filtered fact rows, (3) per-item threshold via GROUP BY. Final query JOINs threshold CTE \u2014 hash join replaces per-row subquery.",
        "sql": "WITH filtered_items AS MATERIALIZED (\n    SELECT i_item_sk\n    FROM item\n    WHERE i_manufact_id IN (1, 78, 97, 516, 521)\n       OR i_manager_id BETWEEN 25 AND 54\n),\ndate_filtered_sales AS MATERIALIZED (\n    SELECT cs.cs_item_sk, cs.cs_ext_discount_amt,\n           cs.cs_list_price, cs.cs_sales_price\n    FROM catalog_sales cs\n    JOIN date_dim d ON d.d_date_sk = cs.cs_sold_date_sk\n    WHERE d.d_date BETWEEN '1999-03-07' AND cast('1999-03-07' as date) + interval '90 day'\n),\nitem_avg_discount AS MATERIALIZED (\n    SELECT dfs.cs_item_sk,\n           1.3 * avg(dfs.cs_ext_discount_amt) AS threshold\n    FROM date_filtered_sales dfs\n    JOIN filtered_items fi ON fi.i_item_sk = dfs.cs_item_sk\n    WHERE dfs.cs_list_price BETWEEN 16 AND 45\n      AND dfs.cs_sales_price / dfs.cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n    GROUP BY dfs.cs_item_sk\n)\nSELECT sum(dfs.cs_ext_discount_amt) AS \"excess discount amount\"\nFROM date_filtered_sales dfs\nJOIN item_avg_discount iad ON iad.cs_item_sk = dfs.cs_item_sk\nWHERE dfs.cs_ext_discount_amt > iad.threshold\nORDER BY 1\nLIMIT 100;"
      },
      "transforms_applied": [
        "decorrelate: Convert inline correlated scalar subquery (WHERE col > SELECT avg(...) WHERE key = outer.key) to MATERIALIZED CTE with GROUP BY + JOIN",
        "early_filter: Pre-filter dimension table (item) into separate MATERIALIZED CTE",
        "date_cte_isolate: Pre-filter fact table by date range into MATERIALIZED CTE, reuse for both threshold computation and final query"
      ],
      "key_insight": "Principle: Inline Decorrelation \u2014 when WHERE has a correlated scalar subquery that re-scans the fact table per outer row, decompose into 3 MATERIALIZED CTEs: (1) dimension filter, (2) date-filtered fact rows, (3) per-key aggregate threshold. The final query JOINs the threshold CTE, replacing O(N*M) correlated scans with a single hash join. CRITICAL: use AS MATERIALIZED on PostgreSQL to prevent the optimizer from inlining CTEs back into the original correlated form."
    },
    {
      "id": "pg_date_cte_explicit_join",
      "name": "Date CTE with Explicit JOIN Conversion (PostgreSQL)",
      "description": "Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "benchmark_queries": [
        "DSB Q099_agg"
      ],
      "verified_speedup": "2.28x",
      "principle": "Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.",
      "transforms": [
        "date_cte_isolate"
      ],
      "original_sql": "select \n   substring(w_warehouse_name,1,20)\n  ,sm_type\n  ,cc_name\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30 ) then 1 else 0 end)  as \"30 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 30) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 60) then 1 else 0 end )  as \"31-60 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 60) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 90) then 1 else 0 end)  as \"61-90 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 90) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 120) then 1 else 0 end)  as \"91-120 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk  > 120) then 1 else 0 end)  as \">120 days\"\nfrom\n   catalog_sales\n  ,warehouse\n  ,ship_mode\n  ,call_center\n  ,date_dim\nwhere\nd_month_seq between 1193 and 1193 + 23\nand cs_ship_date_sk   = d_date_sk\nand cs_warehouse_sk   = w_warehouse_sk\nand cs_ship_mode_sk   = sm_ship_mode_sk\nand cs_call_center_sk = cc_call_center_sk\nand cs_list_price between 271 and 300\nand sm_type = 'REGULAR'\nand cc_class = 'small'\nand w_gmt_offset = -5\ngroup by\n   substring(w_warehouse_name,1,20)\n  ,sm_type\n  ,cc_name\norder by substring(w_warehouse_name,1,20)\n        ,sm_type\n        ,cc_name\nlimit 100;",
      "optimized_sql": "WITH filtered_dates AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1193 + 23)\nSELECT SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name, SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS \"30 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 30) AND (cs_ship_date_sk - cs_sold_date_sk <= 60) THEN 1 ELSE 0 END) AS \"31-60 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 60) AND (cs_ship_date_sk - cs_sold_date_sk <= 90) THEN 1 ELSE 0 END) AS \"61-90 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 90) AND (cs_ship_date_sk - cs_sold_date_sk <= 120) THEN 1 ELSE 0 END) AS \"91-120 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 120) THEN 1 ELSE 0 END) AS \">120 days\" FROM catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk WHERE cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5 GROUP BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name ORDER BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name LIMIT 100",
      "example": {
        "opportunity": "DATE_CTE_EXPLICIT_JOIN",
        "input_slice": "select substring(w_warehouse_name,1,20), sm_type, cc_name,\n  sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30) then 1 else 0 end) as \"30 days\",\n  sum(case when ... > 30 and ... <= 60 then 1 else 0 end) as \"31-60 days\",\n  sum(case when ... > 60 and ... <= 90 then 1 else 0 end) as \"61-90 days\",\n  sum(case when ... > 90 and ... <= 120 then 1 else 0 end) as \"91-120 days\",\n  sum(case when ... > 120 then 1 else 0 end) as \">120 days\"\nfrom catalog_sales, warehouse, ship_mode, call_center, date_dim\nwhere d_month_seq between 1193 and 1216\n  and cs_ship_date_sk = d_date_sk\n  and cs_warehouse_sk = w_warehouse_sk\n  and cs_ship_mode_sk = sm_ship_mode_sk\n  and cs_call_center_sk = cc_call_center_sk\n  and cs_list_price between 271 and 300\n  and sm_type = 'REGULAR' and cc_class = 'small' and w_gmt_offset = -5\ngroup by substring(w_warehouse_name,1,20), sm_type, cc_name\norder by 1, 2, 3 limit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "date_cte_isolate",
              "nodes": {
                "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1216",
                "main_query": "SELECT SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name, SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS \"30 days\", ... FROM catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk WHERE cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5 GROUP BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name ORDER BY 1, 2, 3 LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same CASE bucketing",
                "same aggregation"
              ],
              "expected_speedup": "2.3x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Dimension Isolation + Explicit Joins \u2014 materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma joins to explicit JOIN syntax. On PostgreSQL, both are required: the CTE reduces probe size, while explicit JOINs give the optimizer join-order freedom. Here: date_dim (730 from 73K rows) becomes a CTE hash table that catalog_sales probes; comma joins converted to explicit INNER JOIN.",
        "pattern_detection": "Look for star-schema queries with comma-separated joins including date_dim with a month_seq/year/date range filter. The query should NOT contain EXISTS/NOT EXISTS or INTERSECT/EXCEPT (those patterns are harmed by CTE isolation on PostgreSQL)."
      }
    }
  ],
  "all_available_examples": [
    {
      "id": "pg_date_cte_explicit_join",
      "speedup": "2.28x",
      "description": "Isolate a selective date_dim filter into a CTE AND convert all comma-separated j"
    },
    {
      "id": "pg_dimension_prefetch_star",
      "speedup": "3.32x",
      "description": "On multi-channel UNION queries with comma-separated implicit joins, pre-filter d"
    },
    {
      "id": "early_filter_decorrelate",
      "speedup": "1.13x",
      "description": ""
    },
    {
      "id": "inline_decorrelate_materialized",
      "speedup": "timeout_rescue",
      "description": ""
    },
    {
      "id": "pg_materialized_dimension_fact_prefilter",
      "speedup": "2.68x",
      "description": "Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then "
    },
    {
      "id": "pg_self_join_decomposition",
      "speedup": "3.93x",
      "description": "Eliminate duplicate fact table scans in self-join patterns by computing the aggr"
    }
  ],
  "engine_profile": {
    "engine": "postgresql",
    "version_tested": "14.3+",
    "profile_type": "engine_profile",
    "briefing_note": "This is field intelligence gathered from 53 DSB queries at SF5-SF10. PostgreSQL is a fundamentally different optimizer than DuckDB \u2014 it has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here. Use this to guide your analysis but apply your own judgment \u2014 every query is different. Add to this knowledge if you observe something new.",
    "strengths": [
      {
        "id": "BITMAP_OR_SCAN",
        "summary": "Multi-branch OR conditions on indexed columns are handled via BitmapOr \u2014 a single fact table scan with bitmap combination. Extremely efficient.",
        "field_note": "NEVER split OR conditions into UNION ALL branches on PostgreSQL. BitmapOr is categorically faster. We saw 0.21x on Q085 and 0.26x on Q091 \u2014 each UNION branch forced a full 7-table join + fact scan that BitmapOr avoids. The only conceivable case for OR-to-UNION on PG is when branches reference completely different tables, and even then it's risky."
      },
      {
        "id": "SEMI_JOIN_EXISTS",
        "summary": "EXISTS/NOT EXISTS uses semi-join with early termination. Stops scanning after the first match per outer row.",
        "field_note": "NEVER convert EXISTS to IN/NOT IN or to materialized CTEs with SELECT DISTINCT. The semi-join stops after first match \u2014 materializing forces a full DISTINCT scan of million-row fact tables. We saw 0.50x on Q069 (3 DISTINCT CTEs vs 3 semi-joins) and 0.86x on Q010 (UNION ALL CTE without dedup vs OR'd EXISTS short-circuits). Also: NOT IN has NULL-handling semantics that can block hash anti-join optimization."
      },
      {
        "id": "INNER_JOIN_REORDERING",
        "summary": "PostgreSQL freely reorders INNER JOINs based on estimated selectivity. The cost model works well for explicit JOIN...ON syntax.",
        "field_note": "Don't restructure INNER JOIN orders \u2014 the optimizer handles this well. Focus on queries where JOIN type (LEFT) prevents reordering, or where comma-joins confuse the cost model (see COMMA_JOIN_WEAKNESS gap)."
      },
      {
        "id": "INDEX_ONLY_SCAN",
        "summary": "When an index covers all requested columns, PostgreSQL reads only the index without touching the heap.",
        "field_note": "Dimension table lookups are already fast via index-only scans. Pre-filtering small dimensions (<10K rows) into CTEs adds materialization overhead with minimal benefit."
      },
      {
        "id": "PARALLEL_QUERY_EXECUTION",
        "summary": "PostgreSQL parallelizes large scans and aggregations across worker processes with partial aggregation finalization.",
        "field_note": "Large fact table scans are already parallelized. Restructuring into CTEs may reduce parallelism opportunities because CTE materialization is single-threaded."
      },
      {
        "id": "JIT_COMPILATION",
        "summary": "PostgreSQL JIT-compiles complex expressions and tuple deforming for long-running queries.",
        "field_note": "Complex WHERE expressions have low per-row overhead due to JIT. Simplifying expressions for performance is usually unnecessary."
      }
    ],
    "gaps": [
      {
        "id": "COMMA_JOIN_WEAKNESS",
        "priority": "HIGH",
        "what": "Implicit comma-separated FROM tables (FROM t1, t2, t3 WHERE t1.id = t2.id) are treated as cross products initially. The cost model is significantly weaker on comma-joins than on explicit JOIN...ON syntax.",
        "why": "The planner's join search space is less constrained with comma-joins. Explicit JOINs provide structural hints that help the optimizer find better plans faster, especially for 5+ table joins.",
        "opportunity": "Convert comma-joins to explicit JOIN...ON syntax. This alone can unlock 2-3x improvements. Best when combined with date_cte_isolate.",
        "what_worked": [
          "Q080: 3.32x \u2014 comma-joins to explicit JOINs + date CTE on multi-channel UNION query",
          "Q099: 2.28x \u2014 same pattern on star schema",
          "Q054: 1.14x \u2014 JOIN conversion alone"
        ],
        "what_didnt_work": [],
        "field_notes": [
          "Look for FROM t1, t2, t3 WHERE ... syntax. 5+ comma-separated tables is the sweet spot.",
          "EXPLAIN will show unexpected join orders or high-cost nested loops when comma-joins confuse the cost model.",
          "The win usually comes from explicit JOINs + CTE together, not CTE alone. date_cte_isolate without JOIN conversion is often neutral or harmful.",
          "This is our most reliable PG optimization \u2014 convert the implicit syntax and the optimizer rewards you.",
          "Validate at target scale \u2014 SF5 wins don't predict SF10 on PG (Q027 went from 9.62x to 0.97x)."
        ]
      },
      {
        "id": "CORRELATED_SUBQUERY_PARALYSIS",
        "priority": "HIGH",
        "what": "Cannot automatically decorrelate complex correlated subqueries. Correlated scalar subqueries with aggregates are executed as nested-loop with repeated evaluation.",
        "why": "Same limitation as DuckDB \u2014 correlation requires recognizing GROUP BY + JOIN equivalence. PostgreSQL does basic decorrelation for simple IN/EXISTS but fails on complex aggregate correlations.",
        "opportunity": "Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.",
        "what_worked": [
          "Q092: 4428x \u2014 timeout recovery. Unbounded correlated subquery converted to explicit JOIN.",
          "Q032: 391x \u2014 same pattern, timeout to sub-second."
        ],
        "what_didnt_work": [],
        "field_notes": [
          "Look for WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key) patterns.",
          "EXPLAIN will show SubPlan or nested-loop with repeated subquery execution if the optimizer failed to decorrelate.",
          "These are often the queries that time out \u2014 if a DSB query runs >10s, check for correlated scalar subqueries first.",
          "Simple IN/EXISTS correlation is already handled by PG's semi-join optimization \u2014 only complex aggregate correlations need manual decorrelation.",
          "CRITICAL: when decorrelating, preserve ALL filters from the original subquery in the new CTE.",
          "Validate at target scale \u2014 decorrelation wins are usually robust across scales, but verify on SF10."
        ]
      },
      {
        "id": "NON_EQUI_JOIN_INPUT_BLINDNESS",
        "priority": "HIGH",
        "what": "Cannot pre-filter fact tables before non-equi join operations (date arithmetic, range comparisons, quantity < quantity). Non-equi joins fall back to nested-loop, which is O(N*M).",
        "why": "Hash joins require equi-conditions. Non-equi joins fall back to nested-loop, which processes all input rows. The optimizer cannot recognize that reducing N or M via pre-filtering would dramatically reduce cost.",
        "opportunity": "Reduce fact table input size via filtered CTE before the non-equi join.",
        "what_worked": [
          "Q072: 2.68x \u2014 pre-filtered catalog_sales by wholesale_cost range before non-equi quantity comparison with inventory. Reduced nested-loop input by ~70%."
        ],
        "what_didnt_work": [
          "Q013: 0.79x \u2014 pre-filtered with UNION/OR superset (loose filter). CTE fence blocked dimension predicate pushdown."
        ],
        "field_notes": [
          "Look for non-equi join conditions: >, <, BETWEEN, date arithmetic, quantity comparisons.",
          "EXPLAIN will show nested-loop join with high row estimates on both sides.",
          "A simple range filter on the fact table (e.g., wholesale_cost BETWEEN 34 AND 54) works well. A union/OR superset filter does NOT \u2014 it materializes too many rows.",
          "Only pre-filter when one side of the non-equi join is a large fact table. Small dimension tables (<10K rows) don't benefit.",
          "The CTE fence cost is negligible vs the non-equi join savings when the filter is tight.",
          "Validate at target scale \u2014 non-equi join cost grows super-linearly, so wins tend to hold or improve at larger scales."
        ]
      },
      {
        "id": "CTE_MATERIALIZATION_FENCE",
        "priority": "MEDIUM",
        "what": "PostgreSQL materializes CTEs by default (multi-referenced) or by choice (AS MATERIALIZED). This creates a hard optimization fence \u2014 no predicate pushdown from outer query into CTE. This makes CTE-based strategies a double-edged sword on PG.",
        "why": "CTE is computed and stored in memory/temp before the outer query executes. Any WHERE clause filters in the outer query cannot be pushed back into the CTE definition. Single-reference CTEs may be inlined in PG 12+, but multi-referenced CTEs are always materialized.",
        "opportunity": "Use materialization STRATEGICALLY: materialize when the CTE is expensive and reused multiple times. Avoid CTEs that fence off predicate pushdown for single-use cases.",
        "what_worked": [
          "Q065: 1.95x \u2014 strategic materialization prevented redundant fact table scan multiplication"
        ],
        "what_didnt_work": [
          "Q031: 0.74x \u2014 CTE fence blocked predicate pushdown that worked in original",
          "Q038: 0.77x \u2014 date_cte_isolate added fence that blocked INTERSECT optimization",
          "Q064: 0.65x \u2014 duplicated 18-table CTE body to push filters inside. NEVER do this \u2014 computing an 18-table join twice is always worse than computing once and filtering."
        ],
        "field_notes": [
          "NEVER duplicate a CTE body to push a filter inside when the CTE contains 5+ table joins. Filter the materialized result with WHERE, don't recompute.",
          "Do NOT use the AS MATERIALIZED keyword on CTEs. Write plain CTEs: 'name AS (SELECT ...)'. PG auto-materializes when beneficial. Forcing materialization on small dimension CTEs (<1000 rows) adds temp-table I/O overhead (0.69x observed on Q080).",
          "CTE fence + EXISTS = disaster. If the query uses EXISTS/NOT EXISTS, a CTE that fences off the semi-join optimization is actively harmful.",
          "CTE fence + INTERSECT/EXCEPT = harmful. Set operations handle their inputs efficiently inline. A CTE fence per branch adds overhead.",
          "A CTE result referenced 2+ times is materialized once, probed many \u2014 this IS the valid use case for CTEs on PG.",
          "When a date_cte_isolate CTE is applied to UNION ALL branches, apply to ALL branches or NONE. Partial application creates asymmetric plans."
        ]
      },
      {
        "id": "CROSS_CTE_PREDICATE_BLINDNESS",
        "priority": "MEDIUM",
        "what": "Same gap as DuckDB but WORSE on PostgreSQL because CTE materialization fence makes it more impactful. Predicates in the outer WHERE cannot propagate into materialized CTEs.",
        "why": "Even single-reference CTEs may be materialized (version-dependent). The optimizer does not trace data lineage through CTE boundaries.",
        "opportunity": "Same as DuckDB: pre-filter into CTE definition. But be more cautious \u2014 only when the CTE is clearly suboptimal.",
        "what_worked": [
          "Q080: 3.32x \u2014 date filter + comma-join conversion (the combo is key)",
          "Q099: 2.28x \u2014 date CTE with explicit JOIN"
        ],
        "what_didnt_work": [
          "Q027: 0.97x \u2014 won at SF5 (9.62x) but neutral at SF10. Cost model unreliability across scale.",
          "Q031: 0.55x \u2014 over-decomposed an already-efficient query"
        ],
        "field_notes": [
          "Convert comma-joins to explicit JOINs simultaneously \u2014 the CTE alone often isn't enough on PG.",
          "EXPLAIN will show sequential scan on dimension table without index condition \u2014 that's the signal.",
          "Don't use this on queries with INTERSECT, EXCEPT, or set operations \u2014 the CTE fence blocks set operation optimization.",
          "If the query already returns quickly (<100ms), the CTE materialization overhead can negate any savings.",
          "Validate at target scale \u2014 SF5 wins don't reliably predict SF10 on PostgreSQL."
        ]
      }
    ],
    "set_local_config_intel": {
      "briefing_note": "Field intelligence from programmatic EXPLAIN analysis + 6-step interleaved benchmark on 21 DSB queries at SF10 (PG 14.3). SET LOCAL configs are per-query session-scoped tuning that reverts on COMMIT. Config alone delivered 6.8x on Q100 and 2.3x additive on Q014. Avg additive across all configured queries: 1.31x.",
      "rules": [
        {
          "id": "SORT_SPILL_WORK_MEM",
          "trigger": "EXPLAIN shows Sort Space Type = 'Disk'",
          "config": "work_mem sized by sort/hash op count: <=2 ops \u2192 1GB, 3-5 \u2192 512MB, 6-10 \u2192 256MB, 10+ \u2192 128MB",
          "evidence": "Q100_agg: 6.82x additive (1390ms \u2192 204ms). Sort spills to disk eliminated. Q050_agg: 1.07x. Q014: 1.61x (template with work_mem=256MB).",
          "risk": "LOW. work_mem is per-operation, not per-query. Count sort+hash ops in EXPLAIN before sizing."
        },
        {
          "id": "JIT_OVERHEAD_DISABLE",
          "trigger": "JIT total > 5% of exec time, or >2% when exec < 10s, or absolute JIT > 500ms",
          "config": "SET LOCAL jit = 'off'",
          "evidence": "Q010: 1.07x additive (53ms \u2192 50ms, 582ms JIT was 6.8% of 8.5s exec). Q094: 1.05x. Minor but consistent lift on medium queries.",
          "risk": "LOW. JIT helps long analytical queries but overhead dominates on <30s queries."
        },
        {
          "id": "PARALLEL_COST_REDUCTION",
          "trigger": "EXPLAIN shows Workers Launched < Workers Planned",
          "config": "SET LOCAL parallel_setup_cost = '100.0'; SET LOCAL parallel_tuple_cost = '0.001'",
          "evidence": "Q081: 1.09x additive (502ms \u2192 444ms). Encourages parallelism where planner thinks it helps.",
          "risk": "LOW. Only reduces cost thresholds \u2014 planner still decides. Does not force workers."
        },
        {
          "id": "FORCED_PARALLELISM_DANGER",
          "trigger": "NEVER force max_parallel_workers_per_gather on queries that run < 500ms",
          "config": "Do NOT set max_parallel_workers_per_gather on fast queries",
          "evidence": "Q039: 7.34x REGRESSION (244ms \u2192 1792ms). Worker startup + coordination overhead dominates when the query is already fast. Template config with forced parallelism caused 9.5x regression.",
          "risk": "CRITICAL. Only set max_parallel_workers_per_gather when EXPLAIN shows large unparallelized SeqScans (>500K rows) with no Gather node above. Never on queries < 500ms."
        },
        {
          "id": "HASH_MEM_MULTIPLIER_CAUTION",
          "trigger": "EXPLAIN shows Hash Batches > 1",
          "config": "SET LOCAL hash_mem_multiplier = min(8, batches/2)",
          "evidence": "Q064: 0.56x REGRESSION (1901ms \u2192 3368ms). High multiplier (8.0) caused planner to choose a worse hash join plan. Approach with caution.",
          "risk": "MEDIUM-HIGH. Can cause plan regression if multiplier is too aggressive. Test values 2.0-4.0 first."
        },
        {
          "id": "EFFECTIVE_CACHE_SIZE_ADVISORY",
          "trigger": "Shared Read Blocks > 10x Shared Hit Blocks (cold buffer ratio), or Temp I/O > 1000 blocks",
          "config": "SET LOCAL effective_cache_size = '48GB'",
          "evidence": "Q087: 0.99x \u2014 neutral alone but safe. Usually combined with work_mem. Q100: part of 6.82x combo.",
          "risk": "LOW. Advisory only \u2014 tells planner how much OS cache to expect. Does not allocate memory."
        }
      ],
      "key_findings": [
        "Config tuning is ADDITIVE to SQL rewrite \u2014 not a substitute. Best results combine good rewrite + targeted config.",
        "work_mem for sort spills is the single biggest config lever on PostgreSQL (Q100: 6.8x).",
        "Forced parallelism (max_parallel_workers_per_gather) is DANGEROUS on fast queries. Use parallel cost reduction instead \u2014 it nudges the planner without forcing overhead.",
        "hash_mem_multiplier > 4.0 can cause plan regressions. Start conservative (2.0) and test.",
        "JIT overhead is consistent but small (1.05-1.07x). Worth disabling on queries < 30s.",
        "Template configs (one-size-fits-all) are inferior to EXPLAIN-driven configs. Q039 proved this."
      ]
    },
    "scale_sensitivity_warning": "PostgreSQL optimizations validated at SF5 do NOT reliably predict SF10 behavior. 7 queries that won at SF5 regressed at SF10 (Q027 9.62x at SF5 but 0.97x at SF10). Always validate at target scale. Cost estimates are overconfident on sample data."
  },
  "constraints": [
    {
      "id": "COMPLETE_OUTPUT",
      "severity": "CRITICAL",
      "description": "The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns.",
      "constraint_rules": [
        {
          "rule": "ALL_COLUMNS_PRESENT",
          "description": "Every column in the original SELECT list must appear in the rewritten SELECT list."
        },
        {
          "rule": "NO_COLUMN_RENAME",
          "description": "Column aliases must be preserved exactly. If the original says 'AS total_sales', the rewrite must use the same alias."
        },
        {
          "rule": "PRESERVE_COLUMN_ORDER",
          "description": "Columns must appear in the same order as the original SELECT list."
        }
      ],
      "prompt_instruction": "The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original."
    },
    {
      "id": "CTE_COLUMN_COMPLETENESS",
      "severity": "CRITICAL",
      "description": "When creating or modifying a CTE, its SELECT list MUST include ALL columns that downstream nodes reference. Check the Node Contracts and Downstream Usage sections before writing any CTE.",
      "failure_rate": "Caused 54% of all execution errors (7 of 13 failures)",
      "observed_failures": [
        {
          "query": "Q21",
          "error": "prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q76",
          "error": "filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q24",
          "error": "filtered_base CTE omits s_state, i_current_price, i_manager_id, i_units, i_size needed by downstream CTEs",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q64",
          "error": "filtered_store_sales CTE omits ss_sold_date_sk needed for JOIN in cross_sales CTE",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q60",
          "error": "ss/ws/cs CTEs reference item.i_item_sk and item.i_category in WHERE but item table not joined in CTE",
          "type": "MISSING_TABLE_IN_CTE"
        },
        {
          "query": "Q13",
          "error": "filtered_store_sales CTE references hd_demo_sk, cd_demo_sk from tables not joined in the CTE",
          "type": "MISSING_TABLE_IN_CTE"
        },
        {
          "query": "Q2",
          "error": "Ambiguous d_date_sk and d_week_seq columns between CTE and re-joined date_dim",
          "type": "AMBIGUOUS_COLUMN_REF"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CHECK_DOWNSTREAM_REFS",
          "description": "Before writing a CTE, check the Downstream Usage section. Every column listed in downstream_refs for that node MUST appear in the CTE's SELECT list."
        },
        {
          "rule": "CHECK_JOIN_COLUMNS",
          "description": "If a downstream node JOINs on a column from this CTE (e.g., ON cte.d_date_sk = ...), that column MUST be in the CTE's SELECT."
        },
        {
          "rule": "CHECK_TABLE_PRESENCE",
          "description": "If a CTE's WHERE clause references columns from a table, that table MUST be in the CTE's FROM/JOIN clause."
        }
      ],
      "prompt_instruction": "CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error."
    },
    {
      "id": "LITERAL_PRESERVATION",
      "severity": "CRITICAL",
      "description": "All literal values (strings, numbers, dates) from the original query MUST be preserved EXACTLY in the rewrite",
      "failure_rate": "100% of Q2-Q16 failures were caused by hallucinated literals",
      "observed_failures": [
        {
          "query": "Q2",
          "original": "d_year = 2001, d_year = 2002",
          "hallucinated": "d_year = 1998, d_year = 1999",
          "type": "YEAR_HALLUCINATION"
        },
        {
          "query": "Q7",
          "original": "cd_gender = 'M', cd_marital_status = 'S', d_year = 2000",
          "hallucinated": "cd_gender = 'F', cd_marital_status = 'W', d_year = 2001",
          "type": "MULTIPLE_LITERAL_HALLUCINATION"
        },
        {
          "query": "Q10",
          "original": "d_year = 2002, ca_county IN ('Rush County', 'Toole County', 'Jefferson County', 'Dona Ana County', 'La Porte County')",
          "hallucinated": "d_year = 2001, ca_county IN ('Storey County', 'Marquette County', 'Warren County', 'Cochran County', 'Kandiyohi County')",
          "type": "YEAR_AND_STRING_HALLUCINATION"
        },
        {
          "query": "Q13",
          "original": "cd_marital_status = 'M', cd_education_status = 'Advanced Degree'",
          "hallucinated": "cd_marital_status = 'D', cd_education_status = 'Unknown'",
          "type": "STRING_HALLUCINATION"
        },
        {
          "query": "Q16",
          "original": "ca_state = 'GA', cc_county = 'Williamson County', d_date BETWEEN '2002-02-01' AND '2002-04-02'",
          "hallucinated": "ca_state = 'WV', cc_county IN ('Ziebach County', ...), d_date BETWEEN '2002-4-01' AND ...",
          "type": "STATE_COUNTY_DATE_HALLUCINATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "EXACT_STRING_MATCH",
          "description": "String literals in WHERE clauses must be copied character-for-character",
          "examples": [
            "'M' not 'F'",
            "'GA' not 'WV'",
            "'Rush County' not 'Storey County'"
          ]
        },
        {
          "rule": "EXACT_NUMBER_MATCH",
          "description": "Numeric literals (years, amounts, counts) must be copied exactly",
          "examples": [
            "2000 not 2001",
            "2002 not 2001",
            "100.00 not 150.00"
          ]
        },
        {
          "rule": "EXACT_DATE_MATCH",
          "description": "Date literals must be copied exactly, including format",
          "examples": [
            "'2002-02-01' not '2002-4-01'"
          ]
        },
        {
          "rule": "EXACT_LIST_MATCH",
          "description": "IN lists must contain the exact same values in the same order",
          "examples": [
            "IN ('TX', 'OH', 'TX') not IN ('SD', 'KS', 'MI')"
          ]
        }
      ],
      "prompt_instruction": "CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED."
    },
    {
      "id": "PG_OR_TO_UNION_BLOCK",
      "severity": "CRITICAL",
      "engine": "postgresql",
      "description": "NEVER apply or_to_union on PostgreSQL when OR branches share the same fact table joins. PostgreSQL handles OR via BitmapOr index scans in a single pass. UNION ALL forces N separate fact table scans.",
      "failure_rate": "2/2 queries regressed severely (0.21x-0.26x)",
      "observed_failures": [
        {
          "query": "DSB Q085_agg",
          "speedup": "0.21x",
          "original": "7-table join with 2 nested OR blocks (demographics + geography)",
          "rewrite": "3 UNION ALL branches, each repeating the full 7-table join",
          "problem": "3x scans of web_sales + web_returns fact tables. Original did 1 scan with BitmapOr."
        },
        {
          "query": "DSB Q091_spj_spj",
          "speedup": "0.26x",
          "original": "7-table join with 2-branch OR on demographics",
          "rewrite": "7 CTEs + 2 branch data CTEs + UNION ALL",
          "problem": "Over-decomposed into 7+ CTEs. Doubled catalog_returns scan. CTE materialization overhead."
        }
      ],
      "constraint_rules": [
        {
          "rule": "BLOCK_ON_POSTGRESQL",
          "description": "Do not use or_to_union transform on PostgreSQL databases",
          "rationale": "PostgreSQL BitmapOr handles OR predicates in a single scan. UNION ALL always causes multi-scan regression."
        },
        {
          "rule": "SHARED_FACT_TABLE_CHECK",
          "description": "If all OR branches scan the same fact table with same joins, never split to UNION",
          "rationale": "The only valid case for OR\u2192UNION is when branches reference different tables or indexes."
        }
      ],
      "when_or_to_union_helps_on_pg": [
        "Almost never on PostgreSQL",
        "Only when OR branches reference completely different tables"
      ],
      "when_or_to_union_hurts_on_pg": [
        "Always when branches share the same fact table (web_sales, store_sales, catalog_sales)",
        "Always when OR is on filter predicates (demographics, geography)",
        "Always when combined with CTE over-decomposition"
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER use OR\u2192UNION on PostgreSQL. PostgreSQL handles OR conditions efficiently via BitmapOr index scans in a single pass. Converting OR to UNION ALL forces multiple scans of fact tables and causes catastrophic regressions (0.21x-0.26x observed on DSB benchmark). Keep the original OR structure."
    },
    {
      "id": "SEMANTIC_EQUIVALENCE",
      "severity": "CRITICAL",
      "description": "The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive.",
      "constraint_rules": [
        {
          "rule": "SAME_ROWS",
          "description": "The rewritten query must produce the same set of rows as the original. No rows may be added or removed."
        },
        {
          "rule": "SAME_COLUMNS",
          "description": "The rewritten query must return the same columns in the same order with the same names and data types."
        },
        {
          "rule": "SAME_ORDERING",
          "description": "If the original query has an ORDER BY clause, the rewritten query must preserve the same ordering."
        }
      ],
      "prompt_instruction": "The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set \u2014 even by one row, one column, or a different sort order \u2014 is WRONG and will be REJECTED."
    },
    {
      "id": "KEEP_EXISTS_AS_EXISTS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Prefer preserving EXISTS/NOT EXISTS subqueries. Converting to IN/NOT IN risks NULL-handling changes; converting to JOINs risks duplicate rows.",
      "observed_failures": [
        {
          "problem": "Converting NOT EXISTS to NOT IN changes behavior when the subquery column contains NULLs. NOT IN with NULLs returns no rows.",
          "type": "NULL_SEMANTIC_CHANGE"
        },
        {
          "problem": "Converting EXISTS to JOIN can produce duplicate rows when the subquery matches multiple rows per outer row.",
          "type": "DUPLICATE_ROW_INTRODUCTION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "AVOID_EXISTS_TO_IN",
          "description": "Avoid converting EXISTS/NOT EXISTS to IN/NOT IN unless the subquery column is provably NOT NULL (has a NOT NULL constraint or is a primary key)."
        },
        {
          "rule": "EXISTS_TO_JOIN_NEEDS_DISTINCT",
          "description": "Converting EXISTS to JOIN requires SELECT DISTINCT or GROUP BY to prevent row duplication when the subquery matches multiple rows per outer row."
        }
      ],
      "override_conditions": [
        "The subquery join column has a NOT NULL constraint or is a primary key (safe for IN conversion)",
        "The subquery returns at most 1 row per outer row (1:1 relationship, safe for JOIN)",
        "EXISTS is converted to JOIN + DISTINCT/GROUP BY to explicitly handle duplicates"
      ],
      "prompt_instruction": "DEFAULT: Preserve EXISTS/NOT EXISTS as-is. NOT EXISTS\u2192NOT IN breaks with NULLs; EXISTS\u2192JOIN can duplicate rows. HOWEVER: if the join column is NOT NULL (PK or explicit constraint), EXISTS\u2192IN is safe. If the subquery is 1:1 with the outer query, EXISTS\u2192JOIN is safe. The exploration worker MAY convert EXISTS with written proof of NULL safety or 1:1 cardinality."
    },
    {
      "id": "NO_CROSS_JOIN_DIMENSIONS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid CROSS JOINing dimension tables into a single CTE. The Cartesian product can explode row counts and prevent index use on fact tables.",
      "failure_rate": "Caused 0.0076x regression on Q080 (132x slower) when 3 dimensions were cross-joined",
      "observed_failures": [
        {
          "query": "Q080_multi",
          "regression": "0.0076x (57ms -> 7500ms)",
          "broken_rewrite": "filtered_dims AS (SELECT d_date_sk, i_item_sk, p_promo_sk FROM date_dim CROSS JOIN item CROSS JOIN promotion WHERE ...)",
          "problem": "CROSS JOIN created 120K-row CTE (30 \u00d7 200 \u00d7 20), then 3-key join prevented index use on fact tables.",
          "type": "CROSS_JOIN_DIMENSION_EXPLOSION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_SEPARATE_DIMENSION_CTES",
          "description": "Each dimension table should generally be its own CTE with its own filter. Combining via CROSS JOIN risks Cartesian explosion."
        }
      ],
      "override_conditions": [
        "Only 2 dimensions are joined (not 3+) AND the product is <1000 rows",
        "The dimensions share a foreign key (not a true Cartesian \u2014 it's a filtered JOIN)",
        "The combined CTE replaces N separate semi-joins with 1 multi-key join on the fact table"
      ],
      "prompt_instruction": "DEFAULT: Keep each dimension as a SEPARATE CTE (filtered_date, filtered_item, etc.). Cross-joining 3 dimensions caused 0.0076x on Q080 (30\u00d7200\u00d720 = 120K rows). HOWEVER: joining exactly 2 small dimensions (<1000 row product) via a foreign key (not Cartesian) may be acceptable if it reduces total join count on the fact table. The exploration worker MAY attempt a 2-dimension join with size estimate. Never cross-join 3+ dimensions."
    },
    {
      "id": "NO_MATERIALIZE_EXISTS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid converting EXISTS/NOT EXISTS subqueries into materialized CTEs with full table scans. EXISTS uses semi-join short-circuiting which is typically more efficient.",
      "failure_rate": "Caused 0.14x and 0.54x regressions (7x and 2x slowdowns)",
      "observed_failures": [
        {
          "query": "Q16",
          "regression": "0.14x (18ms -> 126ms)",
          "original": "EXISTS (SELECT * FROM catalog_sales cs2 WHERE cs1.cs_order_number = cs2.cs_order_number AND cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)",
          "broken_rewrite": "WITH multi_warehouse_orders AS (SELECT DISTINCT cs_order_number FROM catalog_sales GROUP BY cs_order_number HAVING MIN(cs_warehouse_sk) <> MAX(cs_warehouse_sk))",
          "type": "EXISTS_TO_FULL_SCAN_CTE"
        },
        {
          "query": "Q95",
          "regression": "0.54x (390ms -> 728ms)",
          "original": "EXISTS(SELECT 1 FROM ws_wh WHERE ws_wh.ws_order_number = ws1.ws_order_number)",
          "broken_rewrite": "WITH multi_warehouse_orders AS (SELECT DISTINCT ws_order_number FROM ws_wh)",
          "type": "EXISTS_TO_MATERIALIZED_DISTINCT"
        }
      ],
      "observed_successes": [
        {
          "query": "Q14",
          "speedup": "1.83x",
          "context": "intersect_to_exists: INTERSECT converted to EXISTS for semi-join short-circuit. Shows EXISTS restructuring CAN help when applied in the right direction."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_EXISTS_SEMI_JOIN",
          "description": "EXISTS and NOT EXISTS use semi-join optimization that short-circuits after finding the first match. Converting to materialized CTEs usually forces a full scan."
        },
        {
          "rule": "AVOID_FULL_TABLE_DISTINCT_CTE",
          "description": "Avoid creating CTEs like SELECT DISTINCT key FROM large_table to replace EXISTS. The CTE scans the entire table; EXISTS can stop after one match."
        }
      ],
      "override_conditions": [
        "The EXISTS subquery is correlated and executed many times (optimizer fails to decorrelate it)",
        "The CTE would be small (<10K rows) and probed multiple times, amortizing materialization cost",
        "The EXISTS is inside a UNION ALL branch where each branch re-executes the same correlated subquery"
      ],
      "prompt_instruction": "DEFAULT: Keep EXISTS/NOT EXISTS as-is \u2014 semi-join short-circuiting is usually faster than materialization. Converting to CTEs caused 0.14x on Q16 and 0.54x on Q95. HOWEVER: if the correlated EXISTS is executed many times and the optimizer fails to decorrelate it, materializing into a small CTE (<10K rows) probed via JOIN may help. The exploration worker MAY attempt this with reasoning about correlation frequency and CTE size."
    },
    {
      "id": "NO_MATERIALIZED_KEYWORD_PG",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "On PostgreSQL, do not add the AS MATERIALIZED keyword to CTEs unless a gold example explicitly uses it. PG 12+ already materializes CTEs referenced more than once. Adding AS MATERIALIZED on small CTEs prevents the optimizer from inlining them, adding temp-table I/O overhead.",
      "failure_rate": "Caused 0.69x regression on Q080",
      "observed_failures": [
        {
          "query": "Q080_multi",
          "regression": "0.69x (57ms -> 83ms)",
          "broken_rewrite": "filtered_date AS MATERIALIZED (SELECT d_date_sk FROM date_dim WHERE ...)",
          "problem": "MATERIALIZED keyword forced temp-table spill for tiny CTEs (30 rows), adding overhead that exceeded filtering benefit.",
          "type": "UNNECESSARY_MATERIALIZATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_EXPLICIT_MATERIALIZED",
          "description": "Do not add the AS MATERIALIZED keyword. Write plain CTEs: 'name AS (SELECT ...)'. PostgreSQL will materialize them automatically when beneficial."
        }
      ],
      "prompt_instruction": "Do NOT use the AS MATERIALIZED keyword on CTEs. Write plain CTEs: 'name AS (SELECT ...)'. PostgreSQL automatically materializes CTEs when beneficial. Forcing materialization on small dimension CTEs (< 1000 rows) adds temp-table I/O overhead that causes regressions (0.69x observed). The proven gold examples use plain CTEs without MATERIALIZED."
    },
    {
      "id": "NO_UNFILTERED_DIMENSION_CTE",
      "severity": "HIGH",
      "description": "Never create a 'filtered' dimension CTE that has no WHERE clause. A CTE that selects all rows from a dimension table is pure materialization overhead with zero filtering benefit.",
      "failure_rate": "Caused 0.85x regression on Q67",
      "observed_failures": [
        {
          "query": "Q67",
          "regression": "0.85x (4509ms -> 5291ms)",
          "broken_rewrite": "filtered_stores AS (SELECT s_store_sk, s_store_id FROM store), filtered_items AS (SELECT i_item_sk, i_category, i_class, i_brand, i_product_name FROM item)",
          "problem": "Both CTEs select ALL rows - no WHERE clause, no filtering. Pure overhead.",
          "type": "UNFILTERED_DIMENSION_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CTE_MUST_FILTER",
          "description": "Every dimension CTE you create MUST have a WHERE clause that reduces the row count. If a dimension table has no filter to apply, do NOT extract it into a CTE."
        },
        {
          "rule": "COLUMN_PROJECTION_IS_NOT_FILTERING",
          "description": "Selecting a subset of columns (SELECT a, b FROM table) is NOT filtering. The CTE still materializes all rows. Only a WHERE clause reduces rows."
        }
      ],
      "prompt_instruction": "Every CTE you create must include a WHERE clause that actually reduces row count. Selecting fewer columns is not filtering \u2014 the CTE still materializes every row. If a dimension table has no predicate to push down, leave it as a direct join in the main query instead of wrapping it in a CTE."
    },
    {
      "id": "OR_TO_UNION_GUARD",
      "severity": "HIGH",
      "overridable": true,
      "description": "Guard rails for or_to_union: branches should have different access paths (not same column) and be limited to 3 or fewer.",
      "observed_failures": [
        {
          "query": "Q90",
          "regression": "0.59x (16ms -> 27ms)",
          "original": "WHERE t.t_hour BETWEEN 10 AND 11 OR t.t_hour BETWEEN 16 AND 17",
          "broken_rewrite": "UNION ALL of two separate web_sales scans (one for AM hours, one for PM hours)",
          "problem": "Doubles the fact table scan. The OR on t_hour is trivial for the optimizer - it just checks two ranges on one column.",
          "type": "UNION_SAME_COLUMN_OR"
        },
        {
          "query": "Q13",
          "regression": "0.23x",
          "problem": "9 UNION branches from nested OR expansion (3 conditions x 3 values) caused 9x fact table scans.",
          "type": "UNION_BRANCH_EXPLOSION"
        },
        {
          "query": "Q48",
          "regression": "0.41x",
          "problem": "9 UNION branches from nested OR expansion caused severe regression from multiplied fact table scans.",
          "type": "UNION_BRANCH_EXPLOSION"
        }
      ],
      "observed_successes": [
        {
          "query": "Q88",
          "speedup": "6.28x",
          "context": "8 time-bucket subqueries on store_sales, each filtering distinct hour ranges via different WHERE clauses. Branches access genuinely different row subsets."
        },
        {
          "query": "Q10",
          "speedup": "1.49x",
          "context": "OR across different dimension table lookups creating distinct access paths."
        },
        {
          "query": "Q45",
          "speedup": "1.35x",
          "context": "OR conditions reference different tables/subqueries."
        }
      ],
      "constraint_rules": [
        {
          "rule": "OR_TO_UNION_REQUIRES_DIFFERENT_PATHS",
          "description": "or_to_union is most beneficial when OR conditions create fundamentally different access paths (e.g., across different tables or between a correlated subquery and a direct filter). Same-column ORs on trivial ranges are usually handled efficiently by the optimizer as a single scan."
        },
        {
          "rule": "OR_TO_UNION_PREFER_3_OR_FEWER",
          "description": "Prefer 3 or fewer UNION ALL branches. Nested ORs that expand into 9+ combinations are almost always harmful. 4-5 branches may be acceptable if each accesses genuinely different row subsets."
        }
      ],
      "override_conditions": [
        "Branches access genuinely different row subsets (different WHERE predicates, not just same-column ranges)",
        "Total branch count stays at 4-5 or fewer (not Cartesian expansion of nested ORs)",
        "EXPLAIN shows the fact table is already scanned N times in baseline, so splitting does not increase scan count",
        "Each branch filters to <20% of fact table rows (high selectivity per branch)"
      ],
      "prompt_instruction": "DEFAULT: Prefer 3 or fewer UNION ALL branches with different access paths per branch. Same-column ORs on simple ranges are usually handled efficiently by the optimizer. Nested ORs that expand into 4+ branches (e.g., 3 x 3 = 9 combinations) caused 0.23x-0.41x regressions. HOWEVER: or_to_union achieved 6.28x on Q88 where branches had genuinely different row subsets. The exploration worker MAY try 4-5 branches if each branch has distinct access paths and high selectivity. Provide reasoning."
    },
    {
      "id": "OR_TO_UNION_SELF_JOIN",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid or_to_union on queries with self-joins. Splitting OR conditions on self-joined tables can create multiple independent scans that cannot share the self-join optimization.",
      "observed_failures": [
        {
          "query": "Q23",
          "regression": "0.51x",
          "problem": "Self-join on store_sales was split into separate UNION branches, each requiring its own full self-join, doubling execution time.",
          "type": "SELF_JOIN_SPLIT"
        }
      ],
      "constraint_rules": [
        {
          "rule": "AVOID_OR_TO_UNION_ON_SELF_JOINS",
          "description": "If a query contains a self-join (same table aliased twice), or_to_union is risky because the self-join must typically remain in a single query block to share the scan."
        }
      ],
      "override_conditions": [
        "The OR conditions are on a column NOT involved in the self-join predicate",
        "The self-join aliases have independent WHERE filters that make each branch selective",
        "EXPLAIN shows the self-join is already executed multiple times in baseline"
      ],
      "prompt_instruction": "DEFAULT: Avoid or_to_union when the query contains a self-join (same table with different aliases). Splitting forces each branch to independently perform the self-join (observed 0.51x on Q23). HOWEVER: if the OR conditions target a column not involved in the self-join predicate, or if each alias already has independent selective filters, splitting may still help. The exploration worker MAY attempt this with written reasoning about why the structural context differs from Q23."
    },
    {
      "id": "PG_CTE_DUPLICATION_BLOCK",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "NEVER duplicate a CTE body to push a filter inside when the CTE contains 5+ table joins. If a CTE is referenced N times in the original, the optimized version must reference it at most N times. Duplicating expensive CTEs forces the full join to execute multiple times.",
      "failure_rate": "1/1 query regressed severely (0.65x)",
      "observed_failures": [
        {
          "query": "DSB Q064_multi",
          "speedup": "0.65x",
          "original": "cross_sales CTE (18-table join) referenced twice with year filter in WHERE",
          "rewrite": "Duplicated cross_sales into cross_sales_1998 and cross_sales_1999, each with year pushed inside",
          "problem": "18-table join executed TWICE instead of once. Original computed it once and self-joined with year filter."
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_CTE_BODY_DUPLICATION",
          "description": "If a CTE has 5+ table joins, never duplicate its body to push a filter inside",
          "rationale": "The CTE computation cost dominates. Computing it twice always exceeds the filter benefit."
        },
        {
          "rule": "REUSE_MATERIALIZED_CTE",
          "description": "PostgreSQL materializes CTEs. Reuse the materialized result with WHERE filters instead of duplicating.",
          "rationale": "CTE materialization means the result is computed once and stored. Filter on the stored result, don't recompute."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER duplicate a CTE body to push a single-column filter inside. If the original has one CTE referenced multiple times, keep it as one CTE and filter in the WHERE clause. PostgreSQL materializes CTEs, so computing an expensive multi-table join twice is always worse than computing once and filtering. Observed 0.65x regression on 18-table CTE duplication."
    },
    {
      "id": "PG_EXISTS_TO_IN_BLOCK",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "NEVER convert EXISTS/NOT EXISTS correlated subqueries into IN/NOT IN with materialized CTEs on PostgreSQL. PostgreSQL's semi-join optimization for EXISTS uses early termination (stops after first match). Materializing into CTEs forces full DISTINCT aggregation of fact tables.",
      "failure_rate": "2/2 queries regressed (0.50x-0.86x)",
      "observed_failures": [
        {
          "query": "DSB Q069_multi",
          "speedup": "0.50x",
          "original": "EXISTS/NOT EXISTS subqueries against store_sales, web_sales, catalog_sales",
          "rewrite": "3 CTEs with DISTINCT customer_sk, then IN/NOT IN checks",
          "problem": "DISTINCT on multi-million-row fact tables is expensive. EXISTS semi-join stops after first match per row."
        },
        {
          "query": "DSB Q010_multi",
          "speedup": "0.86x",
          "original": "EXISTS against store_sales, OR'd EXISTS against web_sales/catalog_sales",
          "rewrite": "store_customers CTE + web_or_catalog_customers UNION ALL CTE",
          "problem": "UNION ALL without deduplication creates massive CTE. Original OR'd EXISTS short-circuits independently."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PRESERVE_EXISTS_SEMIJOIN",
          "description": "Keep EXISTS/NOT EXISTS as correlated subqueries on PostgreSQL",
          "rationale": "PostgreSQL converts EXISTS to efficient semi-join with early termination. CTE materialization loses this."
        },
        {
          "rule": "NO_DISTINCT_FACT_CTE",
          "description": "Never materialize SELECT DISTINCT customer_sk FROM fact_table into a CTE",
          "rationale": "DISTINCT on fact tables (millions of rows) is always expensive. Semi-join avoids the full scan."
        },
        {
          "rule": "NO_NOT_IN_REPLACEMENT",
          "description": "Never replace NOT EXISTS with NOT IN on PostgreSQL",
          "rationale": "NOT IN has NULL-handling semantics that can block hash anti-join optimization."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER convert EXISTS/NOT EXISTS to IN/NOT IN with materialized CTEs. PostgreSQL uses efficient semi-join with early termination for EXISTS. Materializing DISTINCT keys from fact tables forces full scans and loses the early-termination benefit. Observed 0.50x-0.86x regressions."
    },
    {
      "id": "REMOVE_REPLACED_CTES",
      "severity": "HIGH",
      "description": "When creating replacement CTEs, always remove the original CTEs from the WITH clause. Leaving dead/unused CTEs causes unnecessary materialization overhead.",
      "failure_rate": "Contributed to 0.49x and 0.68x regressions",
      "observed_failures": [
        {
          "query": "Q31",
          "regression": "0.49x (99ms -> 201ms)",
          "problem": "Created new store_sales_agg and web_sales_agg CTEs but left the original ss and ws CTEs in the WITH clause. Both old and new CTEs coexist, wasting materialization.",
          "type": "DEAD_CTE_OVERHEAD"
        },
        {
          "query": "Q74",
          "regression": "0.68x (493ms -> 724ms)",
          "problem": "Created 4 new year-specific CTEs but left the original year_total, year_total_store, year_total_web CTEs. Total of 8 CTEs instead of 4.",
          "type": "DEAD_CTE_OVERHEAD"
        }
      ],
      "constraint_rules": [
        {
          "rule": "REPLACE_NOT_APPEND",
          "description": "When your rewrite replaces a CTE with a new version, the original CTE node must be removed or overwritten. Do not define both the old and new CTE."
        }
      ],
      "prompt_instruction": "When creating replacement CTEs, overwrite the original by using the same node_id in your rewrite_sets, or ensure the original is removed from the WITH clause. Every CTE in the final query should be actively used \u2014 dead CTEs still get materialized and waste resources (caused 0.49x on Q31, 0.68x on Q74)."
    },
    {
      "id": "UNION_CTE_SPLIT_MUST_REPLACE",
      "severity": "HIGH",
      "description": "When splitting a UNION into separate CTEs, the original UNION must be eliminated. Creating CTEs that duplicate the UNION branches while keeping the original UNION doubles the work.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "UNION branches were extracted into CTEs but the original UNION ALL remained in the main query, causing each branch to be computed twice.",
          "type": "DUPLICATE_UNION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CTE_SPLIT_REPLACES_UNION",
          "description": "When applying union_cte_split, the final query must reference the CTEs instead of the original UNION. The total number of UNION ALL operations should not increase."
        }
      ],
      "prompt_instruction": "When applying union_cte_split (splitting UNION into CTEs), the original UNION must be eliminated from the main query. The main query should reference the split CTEs, not duplicate the UNION branches. If the rewritten query has more UNION ALL operations than the original, the rewrite is incorrect."
    },
    {
      "id": "DECORRELATE_MUST_FILTER_FIRST",
      "severity": "MEDIUM",
      "description": "When decorrelating a subquery into a JOIN, the replacement JOIN must include a selective filter. A decorrelation that produces an unfiltered cross-product is worse than the original correlated subquery.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "Correlated subquery was converted to JOIN without carrying over the original WHERE filters, producing a much larger intermediate result than the correlated version.",
          "type": "UNFILTERED_DECORRELATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "DECORRELATE_PRESERVES_FILTERS",
          "description": "When converting a correlated subquery to a JOIN + GROUP BY CTE, all WHERE conditions from the original subquery must be preserved in the CTE or JOIN condition. The replacement must not produce more rows than the original correlated subquery."
        }
      ],
      "prompt_instruction": "When decorrelating a correlated subquery into a JOIN, ensure all original WHERE filters are preserved in the replacement CTE or JOIN condition. A decorrelation without selective filters creates a cross-product that is larger than the original per-row correlated execution. The replacement CTE must filter to at most the same cardinality as the original subquery."
    },
    {
      "id": "DIMENSION_CTE_SAME_COLUMN_OR",
      "severity": "MEDIUM",
      "description": "Do not extract dimension CTE filters when the WHERE clause has OR conditions on the same column. Same-column ORs are efficiently handled by the optimizer in a single scan; CTE extraction adds overhead without benefit.",
      "observed_failures": [
        {
          "query": "Q37",
          "regression": "0.89x",
          "problem": "OR conditions on item.i_current_price ranges were extracted into separate CTEs, adding CTE materialization overhead without improving selectivity.",
          "type": "SAME_COLUMN_OR_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "KEEP_SAME_COLUMN_OR_INLINE",
          "description": "When OR conditions filter the same column (e.g., i_current_price BETWEEN X AND Y OR i_current_price BETWEEN A AND B), keep them inline in WHERE. Only extract dimension CTEs when filters span different columns or tables."
        }
      ],
      "prompt_instruction": "Do not create dimension CTEs to isolate OR conditions that filter the same column. The optimizer handles same-column ORs efficiently in a single scan. Only apply dimension_cte_isolate when filters span different columns or different dimension tables."
    },
    {
      "id": "EARLY_FILTER_CTE_BEFORE_CHAIN",
      "severity": "MEDIUM",
      "description": "Early filter CTEs must be referenced by the main query chain. An orphaned CTE that pre-filters data but is never joined back into the main query wastes materialization effort.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "Early filter CTEs were created but not referenced in subsequent JOINs, resulting in wasted CTE materialization plus the original unfiltered joins remaining.",
          "type": "ORPHANED_FILTER_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "FILTER_CTE_MUST_BE_REFERENCED",
          "description": "Every early_filter CTE must be referenced by at least one downstream CTE or the main query. If a filter CTE is created, the original unfiltered table reference must be replaced with the CTE reference."
        }
      ],
      "prompt_instruction": "When creating an early_filter CTE, ensure it is actually referenced in the main query chain. The original unfiltered table reference must be replaced with the CTE reference. Do not create CTEs that filter a table if the main query still joins the original unfiltered table \u2014 this adds overhead without benefit."
    },
    {
      "id": "EXPLICIT_JOINS",
      "severity": "MEDIUM",
      "description": "Convert comma-separated implicit joins to explicit JOIN ... ON syntax. This gives the optimizer better join-order freedom.",
      "constraint_rules": [
        {
          "rule": "PREFER_EXPLICIT_JOIN",
          "description": "When the original query uses comma-separated tables with WHERE conditions for joining, convert to explicit JOIN ... ON syntax."
        }
      ],
      "prompt_instruction": "Convert comma-separated implicit joins to explicit JOIN ... ON syntax. This gives the optimizer better join-order freedom."
    },
    {
      "id": "MIN_BASELINE_THRESHOLD",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Be conservative with CTE-based transforms on queries with very short baseline runtimes. CTE materialization overhead can dominate when the query is already fast.",
      "failure_rate": "Caused 0.14x-0.59x regressions on queries under 50ms",
      "observed_failures": [
        {
          "query": "Q25",
          "regression": "0.50x (31ms -> 62ms)",
          "baseline_ms": 31,
          "transform": "prefetch_fact_join with 6 CTEs",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        },
        {
          "query": "Q90",
          "regression": "0.59x (16ms -> 27ms)",
          "baseline_ms": 16,
          "transform": "multi_dimension_prefetch with 4 CTEs + UNION ALL",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        },
        {
          "query": "Q16",
          "regression": "0.14x (18ms -> 126ms)",
          "baseline_ms": 18,
          "transform": "materialize_cte with 2 full-scan CTEs",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CHECK_BASELINE_RUNTIME",
          "description": "If the Execution Plan shows estimated or actual runtime under 50ms, prefer minimal rewrites. DuckDB already optimizes simple star-join patterns efficiently."
        }
      ],
      "override_conditions": [
        "The transform reduces scan count (e.g., 3 scans \u2192 1 scan) even on a fast query",
        "The query is a component of a larger pipeline where cumulative savings matter",
        "The transform simplifies the query structure without adding CTEs (e.g., pushdown, decorrelate)"
      ],
      "prompt_instruction": "DEFAULT: If baseline is under 100ms, prefer minimal rewrites. CTE materialization overhead (hash tables, intermediate storage) can exceed filtering benefit on fast queries. HOWEVER: transforms that reduce scan count without adding CTEs (pushdown, decorrelate) may still help. The exploration worker MAY attempt structural changes on fast queries if the transform is scan-reducing, not CTE-adding."
    },
    {
      "id": "PG_DATE_CTE_CAUTION",
      "severity": "MEDIUM",
      "engine": "postgresql",
      "description": "date_cte_isolate on PostgreSQL is a double-edged sword. It helps on star-schema queries with explicit JOINs (3.32x, 2.28x wins) but HURTS on queries with EXISTS/NOT EXISTS, INTERSECT/EXCEPT, or when the CTE fence blocks predicate pushdown. Only use when converting comma-joins to explicit JOINs simultaneously.",
      "failure_rate": "7/29 queries regressed when date_cte_isolate was applied (24% regression rate)",
      "observed_wins": [
        {
          "query": "DSB Q080_multi",
          "speedup": "3.32x",
          "pattern": "Multi-channel UNION with comma joins \u2192 explicit JOINs + date CTE"
        },
        {
          "query": "DSB Q099_agg",
          "speedup": "2.28x",
          "pattern": "Star schema with comma joins \u2192 explicit JOINs + date CTE"
        },
        {
          "query": "DSB Q040_agg",
          "speedup": "1.22x",
          "pattern": "Star schema + date range CTE"
        },
        {
          "query": "DSB Q072_spj_spj",
          "speedup": "1.14x",
          "pattern": "3 date_dim instances consolidated into 1 CTE"
        }
      ],
      "observed_failures": [
        {
          "query": "DSB Q069_multi",
          "speedup": "0.50x",
          "pattern": "EXISTS subqueries \u2192 CTE + IN (killed semi-join)"
        },
        {
          "query": "DSB Q031_multi",
          "speedup": "0.55x",
          "pattern": "Already had efficient CTEs, CTE fence blocked pushdown"
        },
        {
          "query": "DSB Q087_multi",
          "speedup": "0.59x",
          "pattern": "EXCEPT branches, broad customer CTE"
        },
        {
          "query": "DSB Q038_multi",
          "speedup": "0.85x",
          "pattern": "INTERSECT branches, CTE overhead"
        },
        {
          "query": "DSB Q010_multi",
          "speedup": "0.86x",
          "pattern": "EXISTS \u2192 CTE + IN conversion"
        },
        {
          "query": "DSB Q014_multi",
          "speedup": "0.87x",
          "pattern": "Inconsistent application, scalar subquery overhead"
        },
        {
          "query": "DSB Q058_multi",
          "speedup": "0.89x",
          "pattern": "Inconsistent application across branches"
        }
      ],
      "constraint_rules": [
        {
          "rule": "REQUIRE_EXPLICIT_JOIN_CONVERSION",
          "description": "Only use date_cte_isolate when ALSO converting comma-separated joins to explicit JOINs",
          "rationale": "The win comes from explicit JOINs + CTE together, not CTE alone."
        },
        {
          "rule": "BLOCK_WITH_EXISTS",
          "description": "Never use date_cte_isolate on queries containing EXISTS/NOT EXISTS subqueries",
          "rationale": "CTE materialization kills semi-join early termination."
        },
        {
          "rule": "BLOCK_WITH_SET_OPS",
          "description": "Never use date_cte_isolate on queries with INTERSECT/EXCEPT",
          "rationale": "Each set operation branch handles date_dim efficiently inline. CTE adds overhead."
        },
        {
          "rule": "CONSISTENT_APPLICATION",
          "description": "If applying to multi-branch queries (UNION ALL), apply to ALL branches or NONE",
          "rationale": "Partial application creates asymmetric plans that confuse the optimizer."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: date_cte_isolate is ONLY beneficial when combined with converting comma-joins to explicit JOINs on star-schema queries. DO NOT use it on queries with EXISTS/NOT EXISTS (kills semi-join), INTERSECT/EXCEPT (adds CTE overhead per branch), or when the query already has efficient CTEs. If applying to UNION ALL branches, apply to ALL or NONE."
    },
    {
      "id": "PG_LOOSE_PREFILTER_BLOCK",
      "severity": "MEDIUM",
      "engine": "postgresql",
      "description": "Do not pre-filter a fact table into a CTE with a UNION/OR superset filter when the actual WHERE clause has correlated conditions. However, a fact table CTE with a simple range filter IS beneficial when the query has expensive non-equi joins (quantity comparisons, date arithmetic) that dominate cost.",
      "failure_rate": "1/2 queries: 1 regression (0.79x), 1 win (2.68x)",
      "observed_failures": [
        {
          "query": "DSB Q013_agg",
          "speedup": "0.79x",
          "original": "store_sales joined with 5 dims, correlated OR predicates on price/profit ranges",
          "rewrite": "filtered_ss CTE with union of all price/profit ranges (loose filter), then join dims",
          "problem": "Loose CTE filter is a superset of correlated OR conditions. CTE fence blocks dimension predicate pushdown."
        }
      ],
      "observed_wins": [
        {
          "query": "DSB Q072_agg",
          "speedup": "2.68x",
          "original": "catalog_sales joined with inventory (non-equi: inv_quantity < cs_quantity), 3x date_dim, item, cd, hd",
          "rewrite": "MATERIALIZED CTEs for all dims + fact table (cs_wholesale_cost BETWEEN 34 AND 54)",
          "why_it_worked": "Non-equi joins (quantity comparison, week_seq correlation) dominate cost. Reducing fact table by ~70% before these joins shrinks the search space. The CTE fence cost is negligible vs the non-equi join savings."
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_FACT_CTE_WITH_CORRELATED_OR",
          "description": "Do not pre-filter fact tables into CTEs when the WHERE clause has correlated OR conditions",
          "rationale": "OR conditions create complex predicate interactions that the optimizer handles better inline. A union of all OR branches is a superset that materializes too many rows."
        },
        {
          "rule": "FACT_CTE_OK_WITH_NON_EQUI_JOINS",
          "description": "A fact table CTE with a simple range filter IS beneficial when the query has expensive non-equi joins (e.g., quantity < quantity, date arithmetic comparisons)",
          "rationale": "Non-equi joins cannot use hash/merge join efficiently. Reducing input size before these joins has outsized impact. The CTE fence cost is negligible compared to non-equi join savings."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: Do NOT pre-filter fact tables into CTEs when the WHERE has correlated OR conditions (the union/superset filter materializes too many rows, 0.79x regression). BUT DO pre-filter fact tables when the query has expensive non-equi joins (quantity comparisons, date arithmetic) - reducing the fact table before these joins is highly effective (2.68x win on Q072)."
    },
    {
      "id": "PREFETCH_MULTI_FACT_CHAIN",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Prefer limiting cascading fact-table CTEs to 2. Each additional CTE materializes a large intermediate result.",
      "observed_failures": [
        {
          "query": "Q4",
          "regression": "0.78x",
          "problem": "3 cascading fact-table CTEs (store_sales -> catalog_sales -> web_sales) created excessive intermediate materialization.",
          "type": "FACT_CHAIN_OVERHEAD"
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_2_OR_FEWER_FACT_CTES",
          "description": "When pre-joining fact tables with filtered dimensions in CTEs, 2 cascading fact CTEs is safe. A third adds risk of excessive materialization."
        }
      ],
      "override_conditions": [
        "Each fact CTE has highly selective filters (<5% of rows survive), keeping intermediate sizes small",
        "The 3rd CTE reads from a dimension-filtered result, not a raw fact table",
        "The query already has 3+ separate fact table scans in baseline \u2014 chaining cannot be worse"
      ],
      "prompt_instruction": "DEFAULT: Limit to 2 cascading fact-table CTEs. A 3rd CTE caused 0.78x on Q4 from excessive materialization. HOWEVER: if each CTE applies highly selective filters (<5% row survival), the intermediate results stay small. The exploration worker MAY try a 3-CTE chain if filters are selective and baseline already has 3+ separate scans."
    },
    {
      "id": "SINGLE_PASS_AGGREGATION_LIMIT",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Prefer limiting single-pass aggregation to 8 CASE branches. Beyond 8, CASE evaluation overhead may reduce benefit.",
      "observed_failures": [
        {
          "query": "Q88",
          "note": "8 CASE branches (time slices) was the maximum tested that still showed improvement (6.28x). More branches are untested, not proven harmful.",
          "type": "CASE_BRANCH_LIMIT"
        }
      ],
      "observed_successes": [
        {
          "query": "Q88",
          "speedup": "6.28x",
          "context": "8 CASE branches consolidating 8 separate time-bucket subqueries into a single scan."
        },
        {
          "query": "Q9",
          "speedup": "4.47x",
          "context": "5 CASE branches consolidating repeated store_sales scans."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_8_OR_FEWER_CASE_BRANCHES",
          "description": "When consolidating repeated scans into CASE WHEN aggregates, 8 or fewer branches is well-tested. More branches are untested territory."
        }
      ],
      "override_conditions": [
        "The original query has 9-12 repeated scans on the same fact table (high consolidation value)",
        "Each CASE branch is a simple equality check (low per-row overhead)",
        "The fact table is large (>1M rows) so scan reduction dominates CASE evaluation cost"
      ],
      "prompt_instruction": "DEFAULT: Use at most 8 CASE branches for single_pass_aggregation (tested up to 8 at 6.28x on Q88). HOWEVER: 9-12 branches with simple equality checks on large fact tables may still net positive. The exploration worker MAY try 9-12 branches if the scan reduction value is high. Beyond 12 branches is not recommended."
    }
  ],
  "regression_warnings": [],
  "strategy_leaderboard": null,
  "query_archetype": null,
  "resource_envelope": "Memory budget: shared_buffers=128MB, effective_cache_size=4GB\nGlobal work_mem: 4MB (per-operation)\nActive connections: ~1 (work_mem headroom: safe up to 16MB per-op)\nStorage: HDD (random_page_cost=4.0)\nParallel capacity: max_parallel_workers=8, per_gather=2\n\nSET LOCAL permissions:\n  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem",
  "exploit_algorithm_text": "**SQL Optimizer Swarm: PostgreSQL Focus**\n\n## 1. ENGINE STRENGTHS\n1. **BITMAP_OR_SCAN**: Handles multi-branch ORs on indexed columns via single scan with bitmap combination. **Do NOT** split OR conditions into UNION ALL branches.\n2. **SEMI_JOIN_EXISTS**: EXISTS/NOT EXISTS uses semi-join with early termination. **Do NOT** convert EXISTS to IN/NOT IN or materializing CTEs.\n3. **INNER_JOIN_REORDERING**: Freely reorders INNER JOINs based on selectivity. **Do NOT** manually restructure INNER JOIN orders.\n4. **INDEX_ONLY_SCAN**: Reads only index when covering all requested columns. **Do NOT** pre-filter small dimensions into CTEs for index-only scans.\n5. **PARALLEL_QUERY_EXECUTION**: Parallelizes large scans/aggregations across workers. **Do NOT** restructure into CTEs that block parallelism.\n6. **JIT_COMPILATION**: JIT-compiles complex expressions for long queries. **Do NOT** simplify expressions for per-row overhead.\n\n## 2. CORRECTNESS RULES\n- Preserve exact row count \u2014 no filtering/duplication.\n- Maintain NULL semantics in WHERE/ON conditions.\n- Do not add/remove ORDER BY unless proven safe.\n- Preserve LIMIT semantics \u2014 no result set expansion.\n\n## 3. OPTIMIZER GAPS\n\n### COMMA_JOIN_WEAKNESS\nPostgreSQL's comma joins confuse cardinality estimation. Opportunity: Convert to explicit JOINs with pre-filtered CTEs.\n\n**pg_dimension_prefetch_star** [W1] \u2014 HIGH reliability, 1 win, avg 3.32x\nPre-filter selective dimensions into CTEs; convert comma joins to explicit JOIN syntax.\n- \u2713 Q080: 3.32x \u2014 date, item, promotion CTEs + explicit joins\n\n**pg_date_cte_explicit_join** [W1] \u2014 HIGH reliability, 1 win, avg 2.28x\nMaterialize selective dimension filters into CTEs AND convert comma joins to explicit JOIN.\n- \u2713 Q099: 2.28x \u2014 date_dim CTE + explicit join syntax\n\n### CORRELATED_SUBQUERY_PARALYSIS\nCorrelated scalar subqueries re-execute per outer row. Opportunity: Decorate via MATERIALIZED CTEs.\n\n**inline_decorrelate_materialized** [W3] \u2014 HIGH reliability, 1 win, avg 461.92x\nDecompose correlated scalar subquery into 3 MATERIALIZED CTEs: dimension filter, fact filter, per-key aggregate.\n- Guard: Use AS MATERIALIZED on CTEs to prevent inlining.\n\n**early_filter_decorrelate** [W3] \u2014 LOW reliability, 1 win, avg 1.13x\nPush dimension filters into CTE definitions; pre-compute thresholds in separate CTEs.\n- Guard: Limited benefit; use only when early filtering is significant.\n\n### CROSS_CTE_PREDICATE_BLINDNESS\nSame fact+dimension scan appears multiple times. Opportunity: Materialize once and reuse.\n\n**pg_self_join_decomposition** [W1] \u2014 HIGH reliability, 1 win, avg 3.93x\nMaterialize identical fact+dimension scan once as CTE; derive aggregates from single result.\n- \u2713 Q065: 3.93x \u2014 store_sales+date_dim scanned once, reused\n\n### NON_EQUI_JOIN_INPUT_BLINDNESS\nExpensive non-equi joins lack pre-filtering. Opportunity: Reduce both sides via MATERIALIZED CTEs.\n\n**pg_materialized_dimension_fact_prefilter** [W2] \u2014 HIGH reliability, 1 win, avg 2.68x\nStage reduction: shrink BOTH dimension and fact tables via MATERIALIZED CTEs before non-equi join.\n- \u2713 Q072: 2.68x \u2014 fact CTE removed 70% rows, dimension CTEs tiny\n\n## 4. STANDALONE TRANSFORMS\n*(none)*\n\n## 5. GLOBAL GUARD RAILS\n1. Never split OR conditions into UNION ALL \u2014 caused 0.21x on Q085.\n2. Never convert EXISTS to IN/NOT IN or materializing CTEs \u2014 caused 0.50x on Q069.\n3. Never restructure INNER JOIN orders \u2014 optimizer handles reordering.\n4. Avoid CTEs for small dimension lookups (<10K rows) \u2014 index-only scans are faster.\n5. Avoid CTEs that block parallel execution \u2014 materialization is single-threaded.\n6. Use AS MATERIALIZED when decorrelating \u2014 prevents optimizer inlining.\n7. Skip transforms if baseline <100ms \u2014 overhead exceeds savings.\n8. Preserve efficient existing CTEs \u2014 don't decompose working patterns.\n9. Verify NULL semantics in NOT IN conversions \u2014 can block hash anti-joins.\n10. Maintain ROLLUP/window pushdown \u2014 CTEs can prevent optimizations."
}