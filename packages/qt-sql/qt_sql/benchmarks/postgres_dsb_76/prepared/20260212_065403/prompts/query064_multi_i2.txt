You are a senior query optimization architect. Your job is to deeply analyze a SQL query and produce a structured briefing for 4 specialist workers who will each write a different optimized version.

You are the ONLY call that sees all the data: EXPLAIN plans, logical-tree costs, full constraint list, global knowledge, and the complete example catalog. The workers will only see what YOU put in their briefings. Your output quality directly determines their success.

## Query: query064_multi_i2
## Dialect: postgres

```sql
  1 | with cs_ui as
  2 |  (select cs_item_sk
  3 |         ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund
  4 |   from catalog_sales
  5 |       ,catalog_returns
  6 |   where cs_item_sk = cr_item_sk
  7 |     and cs_order_number = cr_order_number
  8 |     and cs_wholesale_cost BETWEEN 76 AND 96
  9 |    group by cs_item_sk
 10 |   having sum(cs_ext_list_price)>2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)),
 11 | cross_sales as
 12 |  (select i_product_name product_name
 13 |      ,i_item_sk item_sk
 14 |      ,s_store_name store_name
 15 |      ,s_zip store_zip
 16 |      ,ad1.ca_street_number b_street_number
 17 |      ,ad1.ca_street_name b_street_name
 18 |      ,ad1.ca_city b_city
 19 |      ,ad1.ca_zip b_zip
 20 |      ,ad2.ca_street_number c_street_number
 21 |      ,ad2.ca_street_name c_street_name
 22 |      ,ad2.ca_city c_city
 23 |      ,ad2.ca_zip c_zip
 24 |      ,d1.d_year as syear
 25 |      ,d2.d_year as fsyear
 26 |      ,d3.d_year s2year
 27 |      ,count(*) cnt
 28 |      ,sum(ss_wholesale_cost) s1
 29 |      ,sum(ss_list_price) s2
 30 |      ,sum(ss_coupon_amt) s3
 31 |   FROM   store_sales
 32 |         ,store_returns
 33 |         ,cs_ui
 34 |         ,date_dim d1
 35 |         ,date_dim d2
 36 |         ,date_dim d3
 37 |         ,store
 38 |         ,customer
 39 |         ,customer_demographics cd1
 40 |         ,customer_demographics cd2
 41 |         ,promotion
 42 |         ,household_demographics hd1
 43 |         ,household_demographics hd2
 44 |         ,customer_address ad1
 45 |         ,customer_address ad2
 46 |         ,income_band ib1
 47 |         ,income_band ib2
 48 |         ,item
 49 |   WHERE  ss_store_sk = s_store_sk AND
 50 |          ss_sold_date_sk = d1.d_date_sk AND
 51 |          ss_customer_sk = c_customer_sk AND
 52 |          ss_cdemo_sk= cd1.cd_demo_sk AND
 53 |          ss_hdemo_sk = hd1.hd_demo_sk AND
 54 |          ss_addr_sk = ad1.ca_address_sk and
 55 |          ss_item_sk = i_item_sk and
 56 |          ss_item_sk = sr_item_sk and
 57 |          ss_ticket_number = sr_ticket_number and
 58 |          ss_item_sk = cs_ui.cs_item_sk and
 59 |          c_current_cdemo_sk = cd2.cd_demo_sk AND
 60 |          c_current_hdemo_sk = hd2.hd_demo_sk AND
 61 |          c_current_addr_sk = ad2.ca_address_sk and
 62 |          c_first_sales_date_sk = d2.d_date_sk and
 63 |          c_first_shipto_date_sk = d3.d_date_sk and
 64 |          ss_promo_sk = p_promo_sk and
 65 |          hd1.hd_income_band_sk = ib1.ib_income_band_sk and
 66 |          hd2.hd_income_band_sk = ib2.ib_income_band_sk and
 67 |          cd1.cd_marital_status <> cd2.cd_marital_status and
 68 |          i_current_price between 77 and 77 + 10
 69 |          and p_channel_email = 'Y'
 70 |          and p_channel_tv = 'Y'
 71 |          and p_channel_radio = 'Y'
 72 |          and ad2.ca_state in ('KS','OH','VA')
 73 |          and ss_wholesale_cost BETWEEN 76 AND 96
 74 |          and cd1.cd_marital_status in ('S', 'S', 'S')
 75 |          and cd1.cd_education_status in ('Advanced Degree', '4 yr Degree', '4 yr Degree')
 76 |          and cd2.cd_marital_status in ('S', 'S', 'S')
 77 |          and cd2.cd_education_status in ('Advanced Degree', '4 yr Degree', '4 yr Degree')
 78 | group by i_product_name
 79 |        ,i_item_sk
 80 |        ,s_store_name
 81 |        ,s_zip
 82 |        ,ad1.ca_street_number
 83 |        ,ad1.ca_street_name
 84 |        ,ad1.ca_city
 85 |        ,ad1.ca_zip
 86 |        ,ad2.ca_street_number
 87 |        ,ad2.ca_street_name
 88 |        ,ad2.ca_city
 89 |        ,ad2.ca_zip
 90 |        ,d1.d_year
 91 |        ,d2.d_year
 92 |        ,d3.d_year
 93 | )
 94 | select cs1.product_name
 95 |      ,cs1.store_name
 96 |      ,cs1.store_zip
 97 |      ,cs1.b_street_number
 98 |      ,cs1.b_street_name
 99 |      ,cs1.b_city
100 |      ,cs1.b_zip
101 |      ,cs1.c_street_number
102 |      ,cs1.c_street_name
103 |      ,cs1.c_city
104 |      ,cs1.c_zip
105 |      ,cs1.syear
106 |      ,cs1.cnt
107 |      ,cs1.s1 as s11
108 |      ,cs1.s2 as s21
109 |      ,cs1.s3 as s31
110 |      ,cs2.s1 as s12
111 |      ,cs2.s2 as s22
112 |      ,cs2.s3 as s32
113 |      ,cs2.syear
114 |      ,cs2.cnt
115 | from cross_sales cs1,cross_sales cs2
116 | where cs1.item_sk=cs2.item_sk and
117 |      cs1.syear = 2000 and
118 |      cs2.syear = 2000 + 1 and
119 |      cs2.cnt <= cs1.cnt and
120 |      cs1.store_name = cs2.store_name and
121 |      cs1.store_zip = cs2.store_zip
122 | order by cs1.product_name
123 |        ,cs1.store_name
124 |        ,cs2.cnt
125 |        ,cs1.s1
126 |        ,cs2.s1
127 | ;
```

## EXPLAIN ANALYZE Plan

```
Total execution time: 3199.8ms
Planning time: 96.5ms

-> Sort  (rows=0 loops=1 time=3199.8ms)
   Sort Method: quicksort  Space: 25kB (Memory)
  -> Aggregate  (rows=0 loops=1 time=3199.8ms)
    -> Incremental Sort  (rows=0 loops=1 time=3199.8ms)
      -> Nested Loop Inner  (rows=0 loops=1 time=3199.8ms)
        -> Nested Loop Inner  (rows=0 loops=1 time=3199.8ms)
          -> Nested Loop Inner  (rows=0 loops=1 time=3199.8ms)
             Join Filter: (cd1.cd_marital_status <> cd2.cd_marital_status)
            -> Nested Loop Inner  (rows=10 loops=1 time=3199.7ms)
              -> Nested Loop Inner  (rows=74 loops=1 time=3199.1ms)
                -> Nested Loop Inner  (rows=1,156 loops=1 time=3192.9ms)
                  -> Nested Loop Inner  (rows=1,168 loops=1 time=3190.8ms)
                    -> Nested Loop Inner  (rows=1,210 loops=1 time=3187.6ms)
                      -> Nested Loop Inner  (rows=1,211 loops=1 time=3177.3ms)
                        -> Nested Loop Inner  (rows=1,211 loops=1 time=3174.6ms)
                          -> Nested Loop Inner  (rows=22K loops=1 time=3028.2ms)
                            -> Nested Loop Inner  (rows=23K loops=1 time=2918.8ms)
                              -> Nested Loop Inner  (rows=23K loops=1 time=2872.9ms)
                                -> Nested Loop Inner  (rows=23K loops=1 time=2842.9ms)
                                  -> Nested Loop Inner  (rows=24K loops=1 time=2812.3ms)
                                     Join Filter: (item.i_item_sk = store_sales.ss_item_sk)
                                    -> Nested Loop Inner  (rows=158K loops=1 time=1468.1ms)
                                      -> Merge Join Inner  (rows=604 loops=1 time=1397.2ms)
                                         Merge Cond: (catalog_sales.cs_item_sk = item.i_item_sk)
                                        -> Aggregate  (rows=59K loops=1 time=1365.8ms)
                                           Filter: (sum(catalog_sales.cs_ext_list_price) > ('2'::numeric * sum(((catalog_returns.cr_refunded_cash + ...
                                           Rows Removed by Filter: 9,430
                                          -> Gather Merge  (rows=101K loops=1 time=1293.8ms)
                                             Workers: 2/2 launched
                                            -> Aggregate  (rows=34K loops=3 time=1249.9ms)
                                              -> Sort  (rows=86K loops=3 time=1210.4ms)
                                                 Sort Method: external merge  Space: 3568kB (Disk)
                                                -> Hash Join Inner  (rows=86K loops=3 time=1180.0ms)
                                                   Hash Cond: ((catalog_returns.cr_item_sk = catalog_sales.cs_item_sk) AND (catalog_returns.cr_order_number = c...
                                                  -> Seq Scan on catalog_returns  (rows=719K loops=3 time=110.3ms)
                                                  -> Hash  (rows=573K loops=3 time=834.4ms)
                                                     Batches: 32  Memory: 3744kB
                                                    -> Seq Scan on catalog_sales  (rows=573K loops=3 time=732.3ms)
                                                       Filter: ((cs_wholesale_cost >= '76'::numeric) AND (cs_wholesale_cost <= '96'::numeric))
                                                       Rows Removed by Filter: 4.2M
                                        -> Index Scan on item  (rows=1,071 loops=1 time=28.1ms)
                                           Filter: ((i_current_price >= '77'::numeric) AND (i_current_price <= '87'::numeric))
                                           Rows Removed by Filter: 101K
                                      -> Index Only Scan on store_returns  (rows=261 loops=604 time=0.1ms)
                                         Index Cond: (sr_item_sk = item.i_item_sk)
                                    -> Index Scan on store_sales  (rows=0 loops=157863 time=0.0ms)
                                       Filter: ((ss_wholesale_cost >= '76'::numeric) AND (ss_wholesale_cost <= '96'::numeric))
                                       Index Cond: ((ss_ticket_number = store_returns.sr_ticket_number) AND (ss_item_sk = store_returns.sr_item_sk))
                                       Rows Removed by Filter: 1
                                  -> Index Only Scan on date_dim d1  (rows=1 loops=23565 time=0.0ms)
                                     Index Cond: (d_date_sk = store_sales.ss_sold_date_sk)
                                -> Index Scan on store  (rows=1 loops=22981 time=0.0ms)
                                   Index Cond: (s_store_sk = store_sales.ss_store_sk)
                              -> Index Scan on household_demographics hd1  (rows=1 loops=22712 time=0.0ms)
                                 Index Cond: (hd_demo_sk = store_sales.ss_hdemo_sk)
                            -> Index Scan on customer_address ad1  (rows=1 loops=22550 time=0.0ms)
                               Index Cond: (ca_address_sk = store_sales.ss_addr_sk)
                          -> Index Scan on customer_demographics cd1  (rows=0 loops=22483 time=0.0ms)
                             Filter: ((cd_marital_status = ANY ('{S,S,S}'::bpchar[])) AND (cd_education_status = ANY ('{"Advanced Degr...
                             Index Cond: (cd_demo_sk = store_sales.ss_cdemo_sk)
                             Rows Removed by Filter: 1
                        -> Index Only Scan on income_band ib1  (rows=1 loops=1211 time=0.0ms)
                           Index Cond: (ib_income_band_sk = hd1.hd_income_band_sk)
                      -> Index Scan on customer  (rows=1 loops=1211 time=0.0ms)
                         Index Cond: (c_customer_sk = store_sales.ss_customer_sk)
                    -> Index Only Scan on date_dim d2  (rows=1 loops=1210 time=0.0ms)
                       Index Cond: (d_date_sk = customer.c_first_sales_date_sk)
                  -> Index Only Scan on date_dim d3  (rows=1 loops=1168 time=0.0ms)
                     Index Cond: (d_date_sk = customer.c_first_shipto_date_sk)
                -> Index Scan on customer_address ad2  (rows=0 loops=1156 time=0.0ms)
                   Filter: (ca_state = ANY ('{KS,OH,VA}'::bpchar[]))
                   Index Cond: (ca_address_sk = customer.c_current_addr_sk)
                   Rows Removed by Filter: 1
              -> Index Scan on promotion  (rows=0 loops=74 time=0.0ms)
                 Filter: ((p_channel_email = 'Y'::bpchar) AND (p_channel_tv = 'Y'::bpchar) AND (p_channel_radio = 'Y'::bpc...
                 Index Cond: (p_promo_sk = store_sales.ss_promo_sk)
                 Rows Removed by Filter: 1
            -> Index Scan on customer_demographics cd2  (rows=0 loops=10 time=0.0ms)
               Filter: ((cd_marital_status = ANY ('{S,S,S}'::bpchar[])) AND (cd_education_status = ANY ('{"Advanced Degr...
               Index Cond: (cd_demo_sk = customer.c_current_cdemo_sk)
               Rows Removed by Filter: 1
          -> Index Scan on household_demographics hd2  (rows=0 loops=1 time=0.0ms)
             Index Cond: (hd_demo_sk = customer.c_current_hdemo_sk)
        -> Index Only Scan on income_band ib2  (rows=0 loops=1 time=0.0ms)
           Index Cond: (ib_income_band_sk = hd2.hd_income_band_sk)
  -> Nested Loop Inner  (rows=0 loops=1 time=3179.1ms)
     Join Filter: ((cs2.cnt <= cs1.cnt) AND (cs1.item_sk = cs2.item_sk) AND ((cs1.store_name)::text = (cs2.store_na...
    -> CTE Scan cs1 (CTE: cross_sales)  (rows=0 loops=1 time=3179.1ms)
       Filter: (syear = 2000)
    -> CTE Scan cs2 (CTE: cross_sales)  (rows=0 loops=1 time=0.0ms)
       Filter: (syear = 2001)
```

**NOTE:** The EXPLAIN plan shows the PHYSICAL execution structure, which may differ significantly from the logical tree below. The optimizer may have already split CTEs, reordered joins, or pushed predicates. When the EXPLAIN and the logical tree disagree, the EXPLAIN is ground truth for what the optimizer is already doing.

Use EXPLAIN ANALYZE timings as ground truth. logical-tree cost percentages are derived metrics that may not reflect actual execution time.

## Plan-Space Scanner Intelligence

Plan diversity: 66 distinct plans, 13 plan changers | HIGH
Baseline joins: 17x Nested Loop, 1x Merge Join, 1x Hash Join
  JOIN_TYPE_TRAP (89 combos): no_nestloop: ['Nested Loop(Inner)'] → []
  JOIN_ORDER_TRAP (92 combos): table order unstable
  SCAN_TYPE_TRAP (92 combos): scan methods fragile
  MEMORY_SENSITIVITY: plan shape changes with more memory
Plan changers: no_nestloop, no_hashjoin, no_mergejoin, no_seqscan, force_hash, force_merge, force_nestloop, work_mem_256mb, work_mem_1gb, no_parallel, max_parallel, ssd_costs, ssd_plus_mem

postgres_optimizer_workflow:
  description: "Step-by-step logic for an LLM to apply PostgreSQL tuning flags based on DSB benchmark findings."

  step_1_apply_baseline_globals:
    description: "Apply to ALL queries. Based on Findings SF-001 (Cost Model) and SF-007 (Memory)."
    rationale: "Default random_page_cost is too high for SSDs; work_mem defaults are too low for star-schemas."
    commands:
      - "SET random_page_cost = 1.1;"
      - "SET work_mem = '256MB';"
      - "SET effective_cache_size = '4GB';"

  step_2_analyze_and_apply_conditionals:
    logic: "Check query structure and intent to apply specific overrides."
    branches:
      - case: "Low Latency / Point Lookup"
        condition: "Query filters on PKs or highly selective indexes; expected runtime < 100ms."
        finding_ref: "SF-006 (JIT overhead)"
        action: "Prepend: SET jit = off;"

      - case: "Heavy Aggregation / Star Scan"
        condition: "Query involves large fact table scans and GROUP BY."
        finding_ref: "SF-005, SF-010 (Conservative Parallelism)"
        action: "Force parallelism. Prepend: SET max_parallel_workers_per_gather = 4; SET min_parallel_table_scan_size = '8kB';"

      - case: "Complex Multi-Join"
        condition: "Query contains > 5 joins where the textual order implies a specific logical flow."
        finding_ref: "SF-004 (Join Order Trap)"
        action: "Respect author order. Prepend: SET join_collapse_limit = 1;"

  step_3_safety_constraints:
    description: "Critical anti-patterns to avoid. Based on Findings SF-002 and SF-009."
    rules:
      - "NEVER generate 'SET enable_nestloop = off;' (Risks 184x regression on lookups)."
      - "NEVER generate 'SET enable_hashjoin = off;' for large analytic queries (Structural bottleneck)."

  step_4_fallback_strategy:
    description: "If the query remains slow (1-10s range) despite baseline."
    finding_ref: "SF-003, SF-008"
    action: "Suggest trying: SET enable_mergejoin = off; (can force efficient hash/nested loops in rare 'trap' scenarios)."


## Query Structure (Logic Tree)

```
QUERY: (single statement)
├── [CTE] cs_ui  [=]  Cost: 29%  Rows: ~2.2M
│   ├── SCAN (catalog_sales, catalog_returns (join))
│   ├── JOIN (cs_item_sk = cr_item_sk)
│   ├── JOIN (cs_order_number = cr_order_number)
│   ├── FILTER (cs_wholesale_cost BETWEEN 76 AND 96)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (cs_item_sk, sale, refund)
├── [CTE] cross_sales  [=]  Cost: 27%  Rows: ~157K
│   ├── SCAN (store_sales, store_returns (join), cs_ui (join), date_dim AS d1 (join), date_dim AS d2 (join), date_dim AS d3 (join), store (join), customer (join), customer_demographics AS cd1 (join), customer_demographics AS cd2 (join), promotion (join), household_demographics AS hd1 (join), household_demographics AS hd2 (join), customer_address AS ad1 (join), customer_address AS ad2 (join), income_band AS ib1 (join), income_band AS ib2 (join), item (join))
│   ├── JOIN (ss_store_sk = s_store_sk)
│   ├── JOIN (ss_sold_date_sk = d1.d_date_sk)
│   ├── JOIN (+16 more)
│   ├── FILTER (cd1.cd_marital_status <> cd2.cd_marital_status)
│   ├── FILTER (i_current_price BETWEEN 77 AND 77 + 10)
│   ├── FILTER (+9 more)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (product_name, item_sk, store_name, store_zip, b_street_number, b_street_name, b_city, b_zip, ...)
└── [MAIN] main_query  [=]  Cost: 42%
    ├── SCAN (cross_sales AS cs1 (join), cross_sales AS cs2 (join))
    ├── JOIN (cs1.item_sk = cs2.item_sk)
    ├── JOIN (cs1.store_name = cs2.store_name)
    ├── JOIN (+1 more)
    ├── FILTER (cs1.syear = 2000)
    ├── FILTER (cs2.syear = 2000 + 1)
    ├── FILTER (+1 more)
    ├── AGG (GROUP BY)
    ├── SORT (cs1.product_name ASC, cs1.store_name ASC, cs2.cnt ASC, cs1.s1 ASC, cs2.s1 ASC)
    └── OUTPUT (product_name, store_name, store_zip, b_street_number, b_street_name, b_city, b_zip, c_street_number, ...)
```

## Node Details

### 1. cs_ui
**Role**: CTE (Definition Order: 0)
**Stats**: 29% Cost | ~2.2M rows
**Flags**: GROUP_BY
**Outputs**: [cs_item_sk, sale, refund]
**Dependencies**: catalog_sales, catalog_returns (join)
**Joins**: cs_item_sk = cr_item_sk | cs_order_number = cr_order_number
**Filters**: cs_wholesale_cost BETWEEN 76 AND 96
**Operators**: SEQ_SCAN[catalog_returns], SEQ_SCAN[catalog_sales]
**Key Logic (SQL)**:
```sql
SELECT
  cs_item_sk,
  SUM(cs_ext_list_price) AS sale,
  SUM(cr_refunded_cash + cr_reversed_charge + cr_store_credit) AS refund
FROM catalog_sales, catalog_returns
WHERE
  cs_item_sk = cr_item_sk
  AND cs_order_number = cr_order_number
  AND cs_wholesale_cost BETWEEN 76 AND 96
GROUP BY
  cs_item_sk
HAVING
  SUM(cs_ext_list_price) > 2 * SUM(cr_refunded_cash + cr_reversed_charge + cr_store_credit)
```

### 2. cross_sales
**Role**: CTE (Definition Order: 1)
**Stats**: 27% Cost | ~157k rows
**Flags**: GROUP_BY
**Outputs**: [product_name, item_sk, store_name, store_zip, b_street_number, b_street_name, b_city, b_zip, c_street_number, c_street_name, ...]
**Dependencies**: store_sales, store_returns (join), cs_ui (join), date_dim AS d1 (join), date_dim AS d2 (join), date_dim AS d3 (join), store (join), customer (join), customer_demographics AS cd1 (join), customer_demographics AS cd2 (join), promotion (join), household_demographics AS hd1 (join), household_demographics AS hd2 (join), customer_address AS ad1 (join), customer_address AS ad2 (join), income_band AS ib1 (join), income_band AS ib2 (join), item (join)
**Joins**: ss_store_sk = s_store_sk | ss_sold_date_sk = d1.d_date_sk | ss_customer_sk = c_customer_sk | ss_cdemo_sk = cd1.cd_demo_sk | ss_hdemo_sk = hd1.hd_demo_sk | ss_addr_sk = ad1.ca_address_sk | ss_item_sk = i_item_sk | ss_item_sk = sr_item_sk | ss_ticket_number = sr_ticket_number | ss_item_sk = cs_ui.cs_item_sk | c_current_cdemo_sk = cd2.cd_demo_sk | c_current_hdemo_sk = hd2.hd_demo_sk | c_current_addr_sk = ad2.ca_address_sk | c_first_sales_date_sk = d2.d_date_sk | c_first_shipto_date_sk = d3.d_date_sk | ss_promo_sk = p_promo_sk | hd1.hd_income_band_sk = ib1.ib_income_band_sk | hd2.hd_income_band_sk = ib2.ib_income_band_sk
**Filters**: cd1.cd_marital_status <> cd2.cd_marital_status | i_current_price BETWEEN 77 AND 77 + 10 | p_channel_email = 'Y' | p_channel_tv = 'Y' | p_channel_radio = 'Y' | ad2.ca_state IN ('KS', 'OH', 'VA') | ss_wholesale_cost BETWEEN 76 AND 96 | cd1.cd_marital_status IN ('S', 'S', 'S') | cd1.cd_education_status IN ('Advanced Degree', '4 yr Degree', '4 yr Degree') | cd2.cd_marital_status IN ('S', 'S', 'S') | cd2.cd_education_status IN ('Advanced Degree', '4 yr Degree', '4 yr Degree')
**Operators**: SEQ_SCAN[item], SEQ_SCAN[store_returns], SEQ_SCAN[store_sales], SEQ_SCAN[date_dim], SEQ_SCAN[store]
**Key Logic (SQL)**:
```sql
SELECT
  i_product_name AS product_name,
  i_item_sk AS item_sk,
  s_store_name AS store_name,
  s_zip AS store_zip,
  ad1.ca_street_number AS b_street_number,
  ad1.ca_street_name AS b_street_name,
  ad1.ca_city AS b_city,
  ad1.ca_zip AS b_zip,
  ad2.ca_street_number AS c_street_number,
  ad2.ca_street_name AS c_street_name,
  ad2.ca_city AS c_city,
  ad2.ca_zip AS c_zip,
  d1.d_year AS syear,
  d2.d_year AS fsyear,
  d3.d_year AS s2year,
  COUNT(*) AS cnt,
  SUM(ss_wholesale_cost) AS s1,
  SUM(ss_list_price) AS s2,
  SUM(ss_coupon_amt) AS s3
...
```

### 3. main_query
**Role**: Root / Output (Definition Order: 2)
**Stats**: 42% Cost | ~0 rows
**Flags**: GROUP_BY, ORDER_BY
**Outputs**: [product_name, store_name, store_zip, b_street_number, b_street_name, b_city, b_zip, c_street_number, c_street_name, c_city, ...] — ordered by cs1.product_name ASC, cs1.store_name ASC, cs2.cnt ASC, cs1.s1 ASC, cs2.s1 ASC
**Dependencies**: cross_sales AS cs1 (join), cross_sales AS cs2 (join)
**Joins**: cs1.item_sk = cs2.item_sk | cs1.store_name = cs2.store_name | cs1.store_zip = cs2.store_zip
**Filters**: cs1.syear = 2000 | cs2.syear = 2000 + 1 | cs2.cnt <= cs1.cnt
**Operators**: SEQ_SCAN[CTE Scan], SEQ_SCAN[CTE Scan]
**Key Logic (SQL)**:
```sql
SELECT
  cs1.product_name,
  cs1.store_name,
  cs1.store_zip,
  cs1.b_street_number,
  cs1.b_street_name,
  cs1.b_city,
  cs1.b_zip,
  cs1.c_street_number,
  cs1.c_street_name,
  cs1.c_city,
  cs1.c_zip,
  cs1.syear,
  cs1.cnt,
  cs1.s1 AS s11,
  cs1.s2 AS s21,
  cs1.s3 AS s31,
  cs2.s1 AS s12,
  cs2.s2 AS s22,
  cs2.s3 AS s32,
...
```

### Edges
- cs_ui → cross_sales
- cross_sales → main_query
- cross_sales → main_query


## Aggregation Semantics Check

You MUST verify aggregation equivalence for any proposed restructuring:

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Returns NULL for 0-1 values. Changing group membership changes the result.
- `STDDEV_SAMP(x) FILTER (WHERE year=1999)` over a combined (1999,2000) group is NOT equivalent to `STDDEV_SAMP(x)` over only 1999 rows — FILTER still uses the combined group's membership for the stddev denominator.
- **AVG and STDDEV are NOT duplicate-safe**: if a join introduces row duplication, the aggregate result changes.
- When splitting a UNION ALL CTE with GROUP BY + aggregate, each split branch must preserve the exact GROUP BY columns and filter to the exact same row set as the original.
- **SAFE ALTERNATIVE**: If GROUP BY includes the discriminator column (e.g., d_year), each group is already partitioned. STDDEV_SAMP computed per-group is correct. You can then pivot using `MAX(CASE WHEN year = 1999 THEN year_total END) AS year_total_1999` because the GROUP BY guarantees exactly one row per (customer, year) — the MAX is just a row selector, not a real aggregation.

## Top 6 Tag-Matched Examples

### pg_dimension_prefetch_star (3.32x)
**Description:** On multi-channel UNION queries with comma-separated implicit joins, pre-filter dimension tables (date, item, promotion) into CTEs and convert to explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates and join ordering from explicit JOINs with pre-materialized small dimension results.
**Principle:** Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.

### pg_materialized_dimension_fact_prefilter (2.68x)
**Description:** Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then join with explicit JOIN syntax. On queries with expensive non-equi joins (inventory quantity < sales quantity, week_seq correlation), reducing both dimension AND fact table sizes before the join dramatically cuts the search space. The MATERIALIZED keyword on PG12+ forces early execution of each CTE.
**Principle:** Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.

### early_filter_decorrelate (1.13x)
**Principle:** Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.

### pg_self_join_decomposition (3.93x)
**Description:** Eliminate duplicate fact table scans in self-join patterns by computing the aggregation ONCE in a CTE and deriving both per-item and per-store averages from the same materialized result. PostgreSQL materializes CTEs by default, making this extremely effective.
**Principle:** Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.

### inline_decorrelate_materialized (timeout_rescuex)
**Principle:** Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs — (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data — then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.

### pg_date_cte_explicit_join (2.28x)
**Description:** Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.
**Principle:** Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.

## Optimization Principles (from benchmark history)

**Or To Union** (28.6x avg, 7 wins)
  Why: Converting OR to UNION ALL lets optimizer choose independent index paths per branch
  When: WHERE clause has OR conditions over different dimension keys (≤3 branches)
**Decorrelate** (24.7x avg, 6 wins)
  Why: Correlated subqueries re-execute per outer row; converting to JOIN eliminates per-row overhead; Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering all dimension tables into CTEs avoids repeated full-table scans
  When: Query has correlated subquery in WHERE or SELECT that references outer table
**Prefetch Fact Join** (17.7x avg, 3 wins)
  Why: Pre-joining filtered dimensions with fact table before aggregation reduces join input
  When: Query joins filtered dates/dims with large fact table; pre-join reduces probe size
**Date Cte Isolate** (2.5x avg, 19 wins)
  Why: Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering multiple dimension tables in parallel reduces join fan-out
  When: Query joins date_dim on multiple conditions (year, month, etc.) with fact tables
**Pushdown** (2.4x avg, 5 wins)
  Why: Pushing predicates closer to table scans reduces data volume in upper operators
  When: WHERE predicates reference columns from tables deep in the join tree

## Exploit Algorithm: Evidence-Based Gap Intelligence

The following YAML describes known optimizer gaps with detection rules, procedural exploit steps, and evidence. Use DETECT rules to match structural features of the query, then follow EXPLOIT_STEPS.

**SQL Optimizer Swarm: PostgreSQL Focus**

## 1. ENGINE STRENGTHS
1. **BITMAP_OR_SCAN**: Handles multi-branch ORs on indexed columns via single scan with bitmap combination. **Do NOT** split OR conditions into UNION ALL branches.
2. **SEMI_JOIN_EXISTS**: EXISTS/NOT EXISTS uses semi-join with early termination. **Do NOT** convert EXISTS to IN/NOT IN or materializing CTEs.
3. **INNER_JOIN_REORDERING**: Freely reorders INNER JOINs based on selectivity. **Do NOT** manually restructure INNER JOIN orders.
4. **INDEX_ONLY_SCAN**: Reads only index when covering all requested columns. **Do NOT** pre-filter small dimensions into CTEs for index-only scans.
5. **PARALLEL_QUERY_EXECUTION**: Parallelizes large scans/aggregations across workers. **Do NOT** restructure into CTEs that block parallelism.
6. **JIT_COMPILATION**: JIT-compiles complex expressions for long queries. **Do NOT** simplify expressions for per-row overhead.

## 2. CORRECTNESS RULES
- Preserve exact row count — no filtering/duplication.
- Maintain NULL semantics in WHERE/ON conditions.
- Do not add/remove ORDER BY unless proven safe.
- Preserve LIMIT semantics — no result set expansion.

## 3. OPTIMIZER GAPS

### COMMA_JOIN_WEAKNESS
PostgreSQL's comma joins confuse cardinality estimation. Opportunity: Convert to explicit JOINs with pre-filtered CTEs.

**pg_dimension_prefetch_star** [W1] — HIGH reliability, 1 win, avg 3.32x
Pre-filter selective dimensions into CTEs; convert comma joins to explicit JOIN syntax.
- ✓ Q080: 3.32x — date, item, promotion CTEs + explicit joins

**pg_date_cte_explicit_join** [W1] — HIGH reliability, 1 win, avg 2.28x
Materialize selective dimension filters into CTEs AND convert comma joins to explicit JOIN.
- ✓ Q099: 2.28x — date_dim CTE + explicit join syntax

### CORRELATED_SUBQUERY_PARALYSIS
Correlated scalar subqueries re-execute per outer row. Opportunity: Decorate via MATERIALIZED CTEs.

**inline_decorrelate_materialized** [W3] — HIGH reliability, 1 win, avg 461.92x
Decompose correlated scalar subquery into 3 MATERIALIZED CTEs: dimension filter, fact filter, per-key aggregate.
- Guard: Use AS MATERIALIZED on CTEs to prevent inlining.

**early_filter_decorrelate** [W3] — LOW reliability, 1 win, avg 1.13x
Push dimension filters into CTE definitions; pre-compute thresholds in separate CTEs.
- Guard: Limited benefit; use only when early filtering is significant.

### CROSS_CTE_PREDICATE_BLINDNESS
Same fact+dimension scan appears multiple times. Opportunity: Materialize once and reuse.

**pg_self_join_decomposition** [W1] — HIGH reliability, 1 win, avg 3.93x
Materialize identical fact+dimension scan once as CTE; derive aggregates from single result.
- ✓ Q065: 3.93x — store_sales+date_dim scanned once, reused

### NON_EQUI_JOIN_INPUT_BLINDNESS
Expensive non-equi joins lack pre-filtering. Opportunity: Reduce both sides via MATERIALIZED CTEs.

**pg_materialized_dimension_fact_prefilter** [W2] — HIGH reliability, 1 win, avg 2.68x
Stage reduction: shrink BOTH dimension and fact tables via MATERIALIZED CTEs before non-equi join.
- ✓ Q072: 2.68x — fact CTE removed 70% rows, dimension CTEs tiny

## 4. STANDALONE TRANSFORMS
*(none)*

## 5. GLOBAL GUARD RAILS
1. Never split OR conditions into UNION ALL — caused 0.21x on Q085.
2. Never convert EXISTS to IN/NOT IN or materializing CTEs — caused 0.50x on Q069.
3. Never restructure INNER JOIN orders — optimizer handles reordering.
4. Avoid CTEs for small dimension lookups (<10K rows) — index-only scans are faster.
5. Avoid CTEs that block parallel execution — materialization is single-threaded.
6. Use AS MATERIALIZED when decorrelating — prevents optimizer inlining.
7. Skip transforms if baseline <100ms — overhead exceeds savings.
8. Preserve efficient existing CTEs — don't decompose working patterns.
9. Verify NULL semantics in NOT IN conversions — can block hash anti-joins.
10. Maintain ROLLUP/window pushdown — CTEs can prevent optimizations.

## System Resource Envelope (PostgreSQL)

Workers will use this to size SET LOCAL parameters for their rewrites. Included here for your awareness — you do NOT output config. Each worker decides its own per-rewrite config.

Memory budget: shared_buffers=128MB, effective_cache_size=4GB
Global work_mem: 4MB (per-operation)
Active connections: ~1 (work_mem headroom: safe up to 16MB per-op)
Storage: HDD (random_page_cost=4.0)
Parallel capacity: max_parallel_workers=8, per_gather=2

SET LOCAL permissions:
  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem

## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 — prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY
  - Failure: Q76 — filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## Your Task

First, use a `<reasoning>` block for your internal analysis. This will be stripped before parsing. Work through these steps IN ORDER:

1. **CLASSIFY**: What structural archetype is this query?
   (channel-comparison self-join / correlated-aggregate filter / star-join with late dim filter / repeated fact scan / multi-channel UNION ALL / EXISTS-set operations / other)

2. **EXPLAIN PLAN ANALYSIS**: From the EXPLAIN ANALYZE output, identify:
   - Compute wall-clock ms per EXPLAIN node. Sum repeated operations (e.g., 2x store_sales joins = total cost). The EXPLAIN is ground truth, not the logical-tree cost percentages.
   - Which nodes consume >10% of runtime and WHY
   - Where row counts drop sharply (existing selectivity)
   - Where row counts DON'T drop (missed optimization opportunity)
   - Whether the optimizer already splits CTEs, pushes predicates, or performs transforms you might otherwise assign
   - Count scans per base table. If a fact table is scanned N times, a restructuring that reduces it to 1 scan saves (N-1)/N of that table's I/O cost. Prioritize transforms that reduce scan count on the largest tables.
   - Whether the CTE is materialized once and probed multiple times, or re-executed per reference

3. **GAP MATCHING**: Compare the EXPLAIN analysis to the Engine Profile gaps above. For each gap:
   - Does this query exhibit the gap? (e.g., is a predicate NOT pushed into a CTE? Is the same fact table scanned multiple times?)
   - Check the 'opportunity' — does this query's structure match?
   - Check 'what_didnt_work' and 'field_notes' — any disqualifiers for this query?
   - Also verify: is the optimizer ALREADY handling this well? (Check the Optimizer Strengths above — if the engine already does it, your transform adds overhead, not value.)

4. **AGGREGATION TRAP CHECK**: For every aggregate function in the query, verify: does my proposed restructuring change which rows participate in each group? STDDEV_SAMP, VARIANCE, PERCENTILE_CONT, CORR are grouping-sensitive. SUM, COUNT, MIN, MAX are grouping-insensitive (modulo duplicates). If the query uses FILTER clauses or conditional aggregation, verify equivalence explicitly.

5. **TRANSFORM SELECTION**: From the matched engine gaps, select transforms that exploit the specific gaps present in THIS query. Rank by expected value (rows affected × historical speedup from evidence). Select 4 that are structurally diverse — each attacking a different gap or bottleneck.
   REJECT tag-matched examples whose primary technique requires a structural feature this query lacks (e.g., reject intersect_to_exists if query has no INTERSECT; reject decorrelate if query has no correlated subquery). Tag matching is approximate — always verify structural applicability.

6. **LOGICAL TREE DESIGN**: For each worker's strategy, define the target logical tree topology. Verify that every node contract has exhaustive output columns by checking downstream references.
   CTE materialization matters for your design: a CTE referenced by 2+ consumers will likely be materialized (good — computed once, probed many). A CTE referenced once may be inlined (no materialization benefit from 'sharing'). Design shared CTEs only when multiple downstream nodes consume them. See CTE_INLINING in Engine Profile strengths.

Then produce the structured briefing in EXACTLY this format:

```
=== SHARED BRIEFING ===

SEMANTIC_CONTRACT: (80-150 tokens, cover ONLY:)
(a) One sentence of business intent (start from pre-computed intent if available).
(b) JOIN type semantics that constrain rewrites (INNER = intersection = all sides must match).
(c) Any aggregation function traps specific to THIS query.
(d) Any filter dependencies that a rewrite could break.
Do NOT repeat information already in ACTIVE_CONSTRAINTS or REGRESSION_WARNINGS.

BOTTLENECK_DIAGNOSIS:
[Which operation dominates cost and WHY (not just '50% cost').
Scan-bound vs join-bound vs aggregation-bound.
Cardinality flow (how many rows at each stage).
What the optimizer already handles well (don't re-optimize).
Whether logical-tree cost percentages are misleading.]

ACTIVE_CONSTRAINTS:
- [CORRECTNESS_CONSTRAINT_ID]: [Why it applies to this query, 1 line]
- [ENGINE_GAP_ID]: [Evidence from EXPLAIN that this gap is active]
(List all 4 correctness constraints + the 1-3 engine gaps that
are active for THIS query based on your EXPLAIN analysis.)

REGRESSION_WARNINGS:
1. [Pattern name] ([observed regression]):
   CAUSE: [What happened mechanistically]
   RULE: [Actionable avoidance rule for THIS query]
(If no regression warnings are relevant, write 'None applicable.')

=== WORKER 1 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 2 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 3 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 4 BRIEFING === (EXPLORATION WORKER)

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]
CONSTRAINT_OVERRIDE: [CONSTRAINT_ID or 'None']
OVERRIDE_REASONING: [Why this query's structure differs from the observed failure, or 'N/A']
EXPLORATION_TYPE: [constraint_relaxation | compound_strategy | novel_combination]

```

## Section Validation Checklist (MUST pass before final output)

Use this checklist to verify content quality, not just section presence:

### SHARED BRIEFING
- `SEMANTIC_CONTRACT`: 40-200 tokens and includes business intent, JOIN semantics, aggregation trap, and filter dependency.
- `BOTTLENECK_DIAGNOSIS`: states dominant mechanism, bound type (`scan-bound`/`join-bound`/`aggregation-bound`), cardinality flow, and what optimizer already handles well.
- `ACTIVE_CONSTRAINTS`: includes all 4 correctness IDs plus 1-3 active engine gaps with EXPLAIN evidence.
- `REGRESSION_WARNINGS`: either `None applicable.` or numbered entries with both `CAUSE:` and `RULE:`.

### WORKER N BRIEFING (N=1..4)
- `STRATEGY`: non-empty and unique across workers.
- `TARGET_LOGICAL_TREE`: explicit node chain (e.g., `a -> b -> c`).
- `NODE_CONTRACTS`: every logical tree node has a contract with `FROM`, `OUTPUT` (explicit columns), and `CONSUMERS`.
- `EXAMPLES`: 1-3 IDs per worker. Sharing an example across workers is allowed if each worker's EXAMPLE_ADAPTATION explains a different aspect to apply.
- `EXAMPLE_ADAPTATION`: for each example, states what to adapt and what to ignore for this worker's strategy.
- `HAZARD_FLAGS`: query-specific risks, not generic cautions.

### WORKER 4 EXPLORATION FIELDS
- Includes `CONSTRAINT_OVERRIDE`, `OVERRIDE_REASONING`, and `EXPLORATION_TYPE`.

## Transform Catalog

Select 4 transforms that are applicable to THIS query, maximizing structural diversity (each must attack a different part of the execution plan).

### Predicate Movement
- **global_predicate_pushdown**: Trace selective predicates from late in the CTE chain back to the earliest scan via join equivalences. Biggest win when a dimension filter is applied after a large intermediate materialization.
  Maps to examples: pushdown, early_filter, date_cte_isolate
- **transitive_predicate_propagation**: Infer predicates through join equivalence chains (A.key = B.key AND B.key = 5 -> A.key = 5). Especially across CTE boundaries where optimizers stop propagating.
  Maps to examples: early_filter, dimension_cte_isolate
- **null_rejecting_join_simplification**: When downstream WHERE rejects NULLs from the outer side of a LEFT JOIN, convert to INNER. Enables reordering and predicate pushdown. CHECK: does the query actually have LEFT/OUTER joins before assigning this.
  Maps to examples: (no direct gold example — novel transform)

### Join Restructuring
- **self_join_elimination**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, split into N pre-partitioned CTEs. Eliminates discriminator filtering and repeated hash probes on rows that don't match.
  Maps to examples: union_cte_split, shared_dimension_multi_channel
- **decorrelation**: Convert correlated EXISTS/IN/scalar subqueries to CTE + JOIN. CHECK: does the query actually have correlated subqueries before assigning this.
  Maps to examples: decorrelate, composite_decorrelate_union
- **aggregate_pushdown**: When GROUP BY follows a multi-table join but aggregation only uses columns from one side, push the GROUP BY below the join. CHECK: verify the join doesn't change row multiplicity for the aggregate (one-to-many breaks AVG/STDDEV).
  Maps to examples: (no direct gold example — novel transform)
- **late_attribute_binding**: When a dimension table is joined only to resolve display columns (names, descriptions) that aren't used in filters, aggregations, or join conditions, defer that join until after all filtering and aggregation is complete. Join on the surrogate key once against the final reduced result set. This eliminates N-1 dimension scans when the CTE references the dimension N times. CHECK: verify the deferred columns aren't used in WHERE, GROUP BY, or JOIN ON — only in the final SELECT.
  Maps to examples: dimension_cte_isolate (partial pattern), early_filter

### Scan Optimization
- **star_join_prefetch**: Pre-filter ALL dimension tables into CTEs, then probe fact table with the combined key intersection.
  Maps to examples: dimension_cte_isolate, multi_dimension_prefetch, prefetch_fact_join, date_cte_isolate
- **single_pass_aggregation**: Merge N subqueries on the same fact table into 1 scan with CASE/FILTER inside aggregates. CHECK: STDDEV_SAMP/VARIANCE are grouping-sensitive — FILTER over a combined group != separate per-group computation.
  Maps to examples: single_pass_aggregation, channel_bitmap_aggregation
- **scan_consolidation_pivot**: When a CTE is self-joined N times with each reference filtering to a different discriminator (e.g., year, channel), consolidate into fewer scans that GROUP BY the discriminator, then pivot rows to columns using MAX(CASE WHEN discriminator = X THEN agg_value END). This halves the fact scans and dimension joins. SAFE when GROUP BY includes the discriminator — each group is naturally partitioned, so aggregates like STDDEV_SAMP are computed correctly per-partition. The pivot MAX is just a row selector (one row per group), not a real aggregation.
  Maps to examples: single_pass_aggregation, union_cte_split

### Structural Transforms
- **union_consolidation**: Share dimension lookups across UNION ALL branches that scan different fact tables with the same dim joins.
  Maps to examples: shared_dimension_multi_channel
- **window_optimization**: Push filters before window functions when they don't affect the frame. Convert ROW_NUMBER + filter to LATERAL + LIMIT. Merge same-PARTITION windows into one sort pass.
  Maps to examples: deferred_window_aggregation
- **exists_restructuring**: Convert INTERSECT to EXISTS for semi-join short-circuit, or restructure complex EXISTS with shared CTEs. CHECK: does the query actually have INTERSECT or complex EXISTS.
  Maps to examples: intersect_to_exists, multi_intersect_exists_cte

## Strategy Selection Rules

1. **CHECK APPLICABILITY**: Each transform has a structural prerequisite (correlated subquery, UNION ALL CTE, LEFT JOIN, etc.). Verify the query actually has the prerequisite before assigning a transform. DO NOT assign decorrelation if there are no correlated subqueries.
2. **CHECK OPTIMIZER OVERLAP**: Read the EXPLAIN plan. If the optimizer already performs a transform (e.g., already splits a UNION CTE, already pushes a predicate), that transform will have marginal benefit. Note this in your reasoning and prefer transforms the optimizer is NOT already doing.
3. **MAXIMIZE DIVERSITY**: Each worker must attack a different part of the execution plan. Do not assign 'pushdown variant A' and 'pushdown variant B'. Assign transforms from different categories above.
4. **ASSESS RISK PER-QUERY**: Risk is a function of (transform x query complexity), not an inherent property of the transform. Decorrelation is low-risk on a simple EXISTS and high-risk on nested correlation inside a CTE. Assess per-assignment.
5. **COMPOSITION IS ALLOWED AND ENCOURAGED**: A strategy can combine 2-3 transforms from different categories (e.g., star_join_prefetch + scan_consolidation_pivot, or date_cte_isolate + early_filter + decorrelate). The TARGET_LOGICAL_TREE should reflect the combined structure. Compound strategies are often the source of the biggest wins.
6. **MINIMAL-CHANGE BASELINE**: If the EXPLAIN shows the optimizer already handles the primary bottleneck (e.g., already splits CTEs, already pushes predicates), consider assigning one worker as a minimal-change baseline: explicit JOINs only, no structural changes. This provides a regression-safe fallback.

Each worker gets 1-3 examples. If fewer than 2 examples genuinely match the worker's strategy, assign 1 and state 'No additional examples apply.' Do NOT pad with irrelevant examples — an irrelevant example is worse than no example because the worker will try to apply its pattern. No duplicate examples across workers. Use example IDs from the catalog above.

For TARGET_LOGICAL_TREE: Define the CTE structure you want produced. For NODE_CONTRACTS: Be exhaustive with OUTPUT columns — missing columns cause semantic breaks.

## Exploration Budget (Worker 4)

Workers 1-3 follow the engine profile's proven patterns. **Worker 4 is the EXPLORATION worker** with a different mandate:

Worker 4 MAY (in priority order — prefer higher-value exploration):
  (c) **PREFERRED**: Attempt a novel technique not listed in the engine profile, if the EXPLAIN plan reveals an optimizer blind spot not yet documented. This is the highest-value exploration — new discoveries expand the engine profile for all future queries.
  (b) Combine 2-3 transforms from different engine gaps into a compound strategy that hasn't been tested before. Medium value — tests interaction effects between known patterns.
  (a) Retry a technique from 'what_didnt_work', IF the structural context of THIS query differs materially from the observed failure — explain the structural difference in HAZARD_FLAGS. Lowest priority — only when the query structure clearly diverges from the failed case.

Worker 4 may NEVER violate correctness constraints (LITERAL_PRESERVATION, SEMANTIC_EQUIVALENCE, COMPLETE_OUTPUT, CTE_COLUMN_COMPLETENESS).

The exploration worker's output is tagged EXPLORATORY and tracked separately. Past failures documented in the engine profile are context-specific — they happened on specific queries with specific structures. Worker 4's job is to test whether those failures generalize or not. If Worker 4 discovers a new win, it becomes field intelligence for the engine profile.

## Output Consumption Spec

Each worker receives:
1. SHARED BRIEFING (SEMANTIC_CONTRACT + BOTTLENECK_DIAGNOSIS + ACTIVE_CONSTRAINTS + REGRESSION_WARNINGS)
2. Their specific WORKER N BRIEFING (STRATEGY + TARGET_LOGICAL_TREE + NODE_CONTRACTS + EXAMPLES + EXAMPLE_ADAPTATION + HAZARD_FLAGS)
3. Full before/after SQL for their assigned examples (retrieved by example ID)
4. The original query SQL (full, as reference)
5. Column completeness contract + output format spec

Workers do NOT see other workers' briefings.
Presentation order: briefing first (understanding), then examples (patterns), then original SQL (source), then output format (mechanics).