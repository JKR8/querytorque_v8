## §1. ROLE & MISSION

You are a senior query optimization architect. Your job is to deeply analyze a SQL query and produce a structured briefing for 4 specialist workers who will each write a different optimized version.

You are the ONLY call that sees all the data: EXPLAIN plans, logical-tree costs, full constraint list, global knowledge, and the complete example catalog. The workers will only see what YOU put in their briefings. Your output quality directly determines their success.

## §2a. Original Query: query001_multi_i1 (postgres)

```sql
 1 | with customer_total_return as
 2 | (select sr_customer_sk as ctr_customer_sk
 3 | ,sr_store_sk as ctr_store_sk
 4 | ,sr_reason_sk as ctr_reason_sk
 5 | ,sum(SR_RETURN_AMT_INC_TAX) as ctr_total_return
 6 | from store_returns
 7 | ,date_dim
 8 | where sr_returned_date_sk = d_date_sk
 9 | and d_year =2002
10 | and sr_return_amt / sr_return_quantity between 108 and 167
11 | group by sr_customer_sk
12 | ,sr_store_sk, sr_reason_sk)
13 |  select  c_customer_id
14 | from customer_total_return ctr1
15 | ,store
16 | ,customer
17 | ,customer_demographics
18 | where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2
19 | from customer_total_return ctr2
20 | where ctr1.ctr_store_sk = ctr2.ctr_store_sk
21 | )
22 | and ctr1.ctr_reason_sk BETWEEN 43 AND 46
23 | and s_store_sk = ctr1.ctr_store_sk
24 | and s_state IN ('IL', 'KY', 'TX')
25 | and ctr1.ctr_customer_sk = c_customer_sk
26 | and c_current_cdemo_sk = cd_demo_sk
27 | and cd_marital_status IN ('M', 'M')
28 | and cd_education_status IN ('Advanced Degree', 'College')
29 | and cd_gender = 'F'
30 | and c_birth_month = 2
31 | and c_birth_year BETWEEN 1965 AND 1971
32 | order by c_customer_id
33 | limit 100;
```

## §2b. EXPLAIN ANALYZE Plan

```
Total execution time: 10482.3ms
Planning time: 2.2ms

-> Limit  (rows=0 loops=1 time=10482.3ms)
  -> Aggregate  (rows=96K loops=1 time=1038.6ms)
    -> Gather Merge  (rows=96K loops=1 time=984.8ms)
       Workers: 2/2 launched
      -> Aggregate  (rows=32K loops=3 time=539.3ms)
        -> Sort  (rows=32K loops=3 time=523.6ms)
           Sort Method: quicksort  Space: 1510kB (Memory)
          -> Nested Loop Inner  (rows=32K loops=3 time=507.5ms)
            -> Index Only Scan on date_dim  (rows=122 loops=3 time=0.8ms)
               Index Cond: (d_year = 2002)
            -> Index Scan on store_returns  (rows=263 loops=365 time=4.1ms)
               Filter: (((sr_return_amt / (sr_return_quantity)::numeric) >= '108'::numeric) AND ((sr_return_amt / (sr_re...
               Index Cond: (sr_returned_date_sk = date_dim.d_date_sk)
               Rows Removed by Filter: 2,612
  -> Sort  (rows=0 loops=1 time=10482.2ms)
     Sort Method: quicksort  Space: 25kB (Memory)
    -> Nested Loop Inner  (rows=0 loops=1 time=10482.2ms)
      -> Nested Loop Inner  (rows=0 loops=1 time=10482.2ms)
        -> Nested Loop Inner  (rows=30 loops=1 time=10481.3ms)
           Join Filter: (ctr1.ctr_store_sk = store.s_store_sk)
          -> CTE Scan ctr1 (CTE: customer_total_return)  (rows=582 loops=1 time=10461.0ms)
             Filter: ((ctr_reason_sk >= 43) AND (ctr_reason_sk <= 46) AND (ctr_total_return > (SubPlan 2)))
             Rows Removed by Filter: 95K
            -> Aggregate  (rows=1 loops=1845 time=5.1ms)
              -> CTE Scan ctr2 (CTE: customer_total_return)  (rows=2,450 loops=1845 time=4.9ms)
                 Filter: (ctr1.ctr_store_sk = ctr_store_sk)
                 Rows Removed by Filter: 93K
          -> Seq Scan on store  (rows=9 loops=582 time=0.0ms)
             Filter: (s_state = ANY ('{IL,KY,TX}'::bpchar[]))
             Rows Removed by Filter: 91
        -> Index Scan on customer  (rows=0 loops=30 time=0.0ms)
           Filter: ((c_birth_year >= 1965) AND (c_birth_year <= 1971) AND (c_birth_month = 2))
           Index Cond: (c_customer_sk = ctr1.ctr_customer_sk)
           Rows Removed by Filter: 1
      -> Index Scan on customer_demographics  (rows=0 loops=1 time=0.0ms)
         Filter: ((cd_marital_status = ANY ('{M,M}'::bpchar[])) AND (cd_education_status = ANY ('{"Advanced Degree...
         Index Cond: (cd_demo_sk = customer.c_current_cdemo_sk)
```

**NOTE:** EXPLAIN shows PHYSICAL execution — ground truth when it disagrees with the logical tree (optimizer may already split CTEs, push predicates, reorder joins).
Use EXPLAIN ANALYZE timings as ground truth, not logical-tree %.

### Plan-Space Scanner Intelligence

Baseline: 8478ms | CONFIDENCE: HIGH
CONFIG_CEILING: 1.35x (work_mem=256MB) — LOW
Config alone insufficient. SQL restructuring required.

JOINS: Stable. Join method changes have minimal impact.
MEMORY: MODERATE — work_mem_256mb gives 1.35x.
  -> Some spill benefit. Consider SET LOCAL work_mem = '256MB'.

Plan diversity: 10 distinct plans, 4 plan changers | HIGH

STRATEGY:
  JOINS=STABLE →
    Join methods are optimal. Focus on cardinality reduction, redundant scan elimination, or predicate pushdown.

CONFIG: SET LOCAL work_mem = '256MB'

postgres_optimizer_workflow:
  description: "Step-by-step logic for an LLM to apply PostgreSQL tuning flags based on DSB benchmark findings."

  step_1_apply_baseline_globals:
    description: "Apply to ALL queries. Based on Findings SF-001 (Cost Model) and SF-007 (Memory)."
    rationale: "Default random_page_cost is too high for SSDs; work_mem defaults are too low for star-schemas."
    commands:
      - "SET random_page_cost = 1.1;"
      - "SET work_mem = '256MB';"
      - "SET effective_cache_size = '4GB';"

  step_2_analyze_and_apply_conditionals:
    logic: "Check query structure and intent to apply specific overrides."
    branches:
      - case: "Low Latency / Point Lookup"
        condition: "Query filters on PKs or highly selective indexes; expected runtime < 100ms."
        finding_ref: "SF-006 (JIT overhead)"
        action: "Prepend: SET jit = off;"

      - case: "Heavy Aggregation / Star Scan"
        condition: "Query involves large fact table scans and GROUP BY."
        finding_ref: "SF-005, SF-010 (Conservative Parallelism)"
        action: "Force parallelism. Prepend: SET max_parallel_workers_per_gather = 4; SET min_parallel_table_scan_size = '8kB';"

      - case: "Complex Multi-Join"
        condition: "Query contains > 5 joins where the textual order implies a specific logical flow."
        finding_ref: "SF-004 (Join Order Trap)"
        action: "Respect author order. Prepend: SET join_collapse_limit = 1;"

  step_3_safety_constraints:
    description: "Critical anti-patterns to avoid. Based on Findings SF-002 and SF-009."
    rules:
      - "NEVER generate 'SET enable_nestloop = off;' (Risks 184x regression on lookups)."
      - "NEVER generate 'SET enable_hashjoin = off;' for large analytic queries (Structural bottleneck)."

  step_4_fallback_strategy:
    description: "If the query remains slow (1-10s range) despite baseline."
    finding_ref: "SF-003, SF-008"
    action: "Suggest trying: SET enable_mergejoin = off; (can force efficient hash/nested loops in rare 'trap' scenarios)."


## §2c. Query Structure (Logic Tree)

```
QUERY: (single statement)
├── [CTE] customer_total_return  [=]  Cost: 12%  Rows: ~95K
│   ├── SCAN (store_returns, date_dim (join))
│   ├── JOIN (sr_returned_date_sk = d_date_sk)
│   ├── FILTER (d_year = 2002)
│   ├── FILTER (sr_return_amt / sr_return_quantity BETWEEN 108 AND 167)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (ctr_customer_sk, ctr_store_sk, ctr_reason_sk, ctr_total_return)
└── [MAIN] main_query  [=]  Cost: 88%  Rows: ~4.5M
    ├── SCAN (customer_total_return AS ctr1 (join), store (join), customer (join), customer_demographics (join), customer_total_return AS ctr2 (correlated subquery))
    ├── JOIN (s_store_sk = ctr1.ctr_store_sk)
    ├── JOIN (ctr1.ctr_customer_sk = c_customer_sk)
    ├── JOIN (+1 more)
    ├── FILTER (ctr1.ctr_total_return > AVG(ctr_total_return) * 1.2 (per store_sk))
    ├── FILTER (ctr1.ctr_reason_sk BETWEEN 43 AND 46)
    ├── FILTER (+6 more)
    ├── AGG (GROUP BY)
    ├── SORT (c_customer_id ASC)
    └── OUTPUT (c_customer_id)
```

### Node Details

### 1. customer_total_return
**Role**: CTE (Definition Order: 0)
**Stats**: 12% Cost | ~95k rows
**Flags**: GROUP_BY
**Outputs**: [ctr_customer_sk, ctr_store_sk, ctr_reason_sk, ctr_total_return]
**Dependencies**: store_returns, date_dim (join)
**Joins**: sr_returned_date_sk = d_date_sk
**Filters**: d_year = 2002 | sr_return_amt / sr_return_quantity BETWEEN 108 AND 167
**Operators**: SEQ_SCAN[date_dim], SEQ_SCAN[store_returns]
**Key Logic (SQL)**:
```sql
SELECT
  sr_customer_sk AS ctr_customer_sk,
  sr_store_sk AS ctr_store_sk,
  sr_reason_sk AS ctr_reason_sk,
  SUM(SR_RETURN_AMT_INC_TAX) AS ctr_total_return
FROM store_returns, date_dim
WHERE
  sr_returned_date_sk = d_date_sk
  AND d_year = 2002
  AND sr_return_amt / sr_return_quantity BETWEEN 108 AND 167
GROUP BY
  sr_customer_sk,
  sr_store_sk,
  sr_reason_sk
```

### 2. main_query
**Role**: Root / Output (Definition Order: 1)
**Stats**: 88% Cost | ~4.5M rows processed → 100 rows output
**Flags**: GROUP_BY, CORRELATED_SUBQUERY(ctr1.ctr_store_sk = ctr2.ctr_store_sk), ORDER_BY, LIMIT(100)
**Outputs**: [c_customer_id] — ordered by c_customer_id ASC
**Dependencies**: customer_total_return AS ctr1 (join), store (join), customer (join), customer_demographics (join), customer_total_return AS ctr2 (correlated subquery)
**Joins**: s_store_sk = ctr1.ctr_store_sk | ctr1.ctr_customer_sk = c_customer_sk | c_current_cdemo_sk = cd_demo_sk
**Filters**: ctr1.ctr_total_return > AVG(ctr_total_return) * 1.2 (per store_sk) | ctr1.ctr_reason_sk BETWEEN 43 AND 46 | s_state IN ('IL', 'KY', 'TX') | cd_marital_status IN ('M', 'M') | cd_education_status IN ('Advanced Degree', 'College') | cd_gender = 'F' | c_birth_month = 2 | c_birth_year BETWEEN 1965 AND 1971
**Operators**: SEQ_SCAN[CTE Scan], SEQ_SCAN[CTE Scan], SEQ_SCAN[store], SEQ_SCAN[customer], SEQ_SCAN[customer_demographics]
**Key Logic (SQL)**:
```sql
SELECT
  c_customer_id
FROM customer_total_return AS ctr1, store, customer, customer_demographics
WHERE
  ctr1.ctr_total_return > (
    SELECT
      AVG(ctr_total_return) * 1.2
    FROM customer_total_return AS ctr2
    WHERE
      ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND ctr1.ctr_reason_sk BETWEEN 43 AND 46
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state IN ('IL', 'KY', 'TX')
  AND ctr1.ctr_customer_sk = c_customer_sk
  AND c_current_cdemo_sk = cd_demo_sk
  AND cd_marital_status IN ('M', 'M')
  AND cd_education_status IN ('Advanced Degree', 'College')
  AND cd_gender = 'F'
  AND c_birth_month = 2
...
```

### Edges
- customer_total_return → main_query
- customer_total_return → main_query


## §3a. Correctness Constraints (4 — NEVER violate)

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 — prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY
  - Failure: Q76 — filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## §3b. Aggregation Equivalence Rules

You MUST verify aggregation equivalence for any proposed restructuring:

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Returns NULL for 0-1 values. Changing group membership changes the result.
- `STDDEV_SAMP(x) FILTER (WHERE year=1999)` over a combined (1999,2000) group is NOT equivalent to `STDDEV_SAMP(x)` over only 1999 rows — FILTER still uses the combined group's membership for the stddev denominator.
- **AVG and STDDEV are NOT duplicate-safe**: if a join introduces row duplication, the aggregate result changes.
- When splitting a UNION ALL CTE with GROUP BY + aggregate, each split branch must preserve the exact GROUP BY columns and filter to the exact same row set as the original.
- **SAFE ALTERNATIVE**: If GROUP BY includes the discriminator column (e.g., d_year), each group is already partitioned. STDDEV_SAMP computed per-group is correct. You can then pivot using `MAX(CASE WHEN year = 1999 THEN year_total END) AS year_total_1999` because the GROUP BY guarantees exactly one row per (customer, year) — the MAX is just a row selector, not a real aggregation.

## §4. Exploit Algorithm: Evidence-Based Gap Intelligence

The following describes known optimizer gaps with detection rules, procedural exploit steps, and evidence. Use DETECT rules to match structural features of the query, then follow EXPLOIT_STEPS.

# PostgreSQL Rewrite Playbook
# 31 gold wins + 21 improved + 7 regressions | DSB SF10

## HOW TO USE THIS DOCUMENT

Work in phase order. Each phase changes the plan shape — re-evaluate later phases after each.

  Phase 1: Reduce scan volume (P1, P6, P7) — always first. Every optimization benefits from smaller input.
  Phase 2: Eliminate redundant work (P2, P3)
  Phase 3: Fix structural inefficiencies (P4, P5)

Before choosing any strategy, scan the explain plan for:
- Row count profile: monotonically decreasing = healthy. Flat then sharp drop = pushback opportunity.
- Join types: hash join = good. Nested loop on large table = decorrelation candidate.
- Repeated tables: same table N times = consolidation (P3).
- CTE materialization: large CTE + small post-filter = pushback. Use AS MATERIALIZED when needed.
- Bitmap OR scan: indexed OR already optimized — do NOT split to UNION.
- Parallel workers: active parallelism — avoid CTE fence that blocks parallel execution.
- Index-only scan on dimension: small dimension already efficient — CTE wrapper may hurt.
- EXISTS/NOT EXISTS: uses semi-join early termination — NEVER materialize.

## ENGINE STRENGTHS — do NOT rewrite these patterns

1. **BITMAP_OR_SCAN**: Multi-branch ORs on indexed columns handled via bitmap combination in one scan. Splitting ORs to UNION ALL is lethal (0.21x Q085 V1).
2. **EXISTS semi-join**: Uses early termination. Converting to materializing CTEs caused 0.50x Q069 V1, 0.75x Q069_i2. **Never materialize EXISTS.**
3. **INNER JOIN reordering**: Freely reorders INNER JOINs by selectivity estimates. Do NOT manually restructure INNER JOIN order.
4. **Index-only scan**: Reads only index when covering all requested columns. Small dimension lookups may not need CTEs.
5. **Parallel query execution**: Large scans and aggregations parallelized across workers. CTEs block parallelism (materialization is single-threaded).
6. **JIT compilation**: JIT-compiles complex expressions for long-running queries (>100ms).

## CORRECTNESS RULES

- Preserve exact row count — no filtering or duplication.
- Maintain NULL semantics in WHERE/ON conditions.
- Do not add/remove ORDER BY unless proven safe.
- Preserve LIMIT semantics — no result set expansion.
- NOT IN with NULLs blocks hash anti-joins — preserve EXISTS form.

## GLOBAL GUARDS (check always, before any rewrite)

1. OR conditions on indexed columns → never split to UNION ALL (0.21x Q085)
2. EXISTS/NOT EXISTS → never materialize into CTEs (0.50x Q069, 0.75x Q069_i2)
3. INNER JOIN order → never restructure (optimizer handles reordering)
4. Small dimensions (< 10K rows) → index-only scan may be faster than CTE
5. Baseline < 100ms → skip CTE-based rewrites (overhead exceeds savings)
6. CTEs block parallel execution — only use when benefit outweighs parallelism loss
7. Use AS MATERIALIZED when CTE must not be inlined (decorrelation, shared scans)
8. Preserve efficient existing CTEs — don't decompose working patterns
9. Verify NULL semantics in NOT IN conversions
10. ROLLUP/window in same query → CTE may prevent pushdown optimizations
11. Never inline a large UNION CTE — re-execution multiplied per reference (0.16x Q075)
12. Max 2 cascading fact-table CTE chains — deeper chains block parallelism
13. EXPLAIN cost gaps ≠ runtime gains for config tuning — 6 false positives caught (up to 84% EXPLAIN gap → 0% runtime). Always 3-race validate config changes.

---

## PATHOLOGIES

### P1: Comma join confusing cardinality estimation [Phase 1 — LOW RISK]

  Gap: COMMA_JOIN_WEAKNESS — PostgreSQL's planner uses cross-product estimation
  for comma-separated joins in the FROM clause. Without explicit JOIN syntax, the
  planner lacks the join-key hint that enables hash-join probing with filtered
  dimension tables. This manifests as poor row estimates on intermediate joins,
  leading to nested-loop plans on large fact tables.

  The fix has two parts: (1) convert comma joins to explicit INNER JOIN syntax,
  and (2) pre-filter selective dimensions into MATERIALIZED CTEs to create tiny
  hash probe tables. Both are required — the CTE alone can hurt, but CTE +
  explicit JOINs together enable optimal hash join planning.

  Signal: hash/nested-loop join with poor row estimates in EXPLAIN, large
  intermediate results. SQL shows FROM t1, t2, t3 WHERE t1.key = t2.key
  AND ... (comma joins, no explicit JOIN).

  Decision gates:
  - Structural: multiple tables in comma-separated FROM with equi-join predicates
  - Selectivity: dimension filters available (date range, state, category)
  - Fact table: 1-2 fact tables only (3+ → join order lock)
  - CTE count: max 3-4 dimension CTEs (avoid over-materialization)
  - Stop: if all JOINs already explicit → skip to P6/P7

  Transform selection (lightest sufficient):
  - Date filter + star schema → pg_date_cte_explicit_join (4 wins, 2.1x avg)
  - Multiple dimension filters → pg_dimension_prefetch_star (3 wins, 2.8x avg)
  - Complex multi-join → explicit_join_materialized (2 wins, 5.9x avg)

  Ordering: apply first — reduces fact table scan before other optimizations.
  Composition: combines well with P2 (decorrelation) and P6 (date consolidation).
  After applying: re-evaluate P4 (non-equi inputs now smaller).

  Wins: Q083 8.56x, Q025 3.10x, Q099 2.50x, Q023 1.83x
  Improved: Q080 1.42x
  Regressions: Q058_i1 0.88x (explicit join overhead on simple query)

### P2: Correlated subquery executing per outer row [Phase 2 — HIGHEST IMPACT]

  Gap: CORRELATED_SUBQUERY_PARALYSIS — PostgreSQL cannot decorrelate correlated
  aggregate subqueries into GROUP BY + hash join. It falls back to nested-loop
  re-execution, scanning the inner relation once per outer row. For N outer rows
  and M inner rows, cost is O(N × M) instead of O(N + M).

  This is the single most impactful pathology on PostgreSQL. It accounts for 9
  of 31 wins including the three largest speedups (8044x, 1465x, 439x). The
  extreme wins occur when the correlated subquery causes a timeout — the original
  query never finishes, but the decorrelated version completes in milliseconds.

  The fix: extract the correlated aggregate into a MATERIALIZED CTE with GROUP BY
  on the correlation key, then JOIN back. Use AS MATERIALIZED to prevent the
  optimizer from inlining the CTE back into a correlated form.

  Signal: nested loop in EXPLAIN, inner side re-executes aggregate per outer row.
  If EXPLAIN shows hash join on correlation key → already decorrelated → STOP.
  SQL signal: WHERE col > (SELECT AGG(...) FROM ... WHERE outer.key = inner.key)

  Decision gates:
  - Structural: correlated scalar subquery with aggregate (AVG, SUM, COUNT)
  - EXPLAIN: nested loop with inner re-execution (NOT hash join)
  - NOT EXISTS: NEVER decorrelate EXISTS/NOT EXISTS (destroys semi-join, 0.50x Q069)
  - Shared scan: if inner and outer scan same table → extract common scan to shared CTE
  - CTE keyword: ALWAYS use AS MATERIALIZED (prevents optimizer re-correlating)
  - Multi-fact: 1-2 fact tables safe, 3+ → STOP (0.51x Q054)

  Transform selection (lightest sufficient):
  - Simple avg comparison → inline_decorrelate_materialized (3 wins, avg 500x)
  - Multiple correlation keys → decorrelate (8 wins, avg 3.2x)
  - Inner = outer table → shared scan + decorrelate (2 wins, avg 7000x)

  Ordering: apply after P1 (smaller inputs make decorrelation cheaper).
  Composition: almost always combined with P1 (comma join conversion).
  After applying: re-evaluate P3 (decorrelated CTEs may now be reusable).

  Wins: Q092 8044x, Q032 1465x, Q081 439x, Q001 27.80x, Q083 8.56x,
        Q001_i1 7.99x, Q065 2.05x, Q065_i1 1.90x, Q030 1.86x
  Improved: Q058 1.49x, Q014_i2 1.12x
  Regressions: Q054 0.51x (multi-fact join lock), Q069_i2 0.75x (EXISTS materialized)

### P3: Same fact+dimension scan repeated across subquery boundaries [Phase 2 — ZERO REGRESSIONS]

  Gap: CROSS_CTE_PREDICATE_BLINDNESS — PostgreSQL cannot detect that N subqueries
  all scan the same fact table with identical joins and filters. Each subquery is
  an independent plan unit with no Common Subexpression Elimination across query
  boundaries. This includes self-join patterns where the same aggregation is
  computed at different granularities.

  Two fix strategies: (1) Materialize identical scan once as CTE, derive
  aggregates from single result. (2) Consolidate multiple channel scans
  (store/catalog/web) into single UNION ALL scan with CASE-based pivoting.

  Signal: identical scan subtrees appearing 2+ times in EXPLAIN with similar
  costs. SQL signal: same fact table joined to same dimensions in multiple
  subqueries, or self-join with different GROUP BY granularity.

  Decision gates:
  - Structural: 2+ subqueries scanning same fact table with identical filters
  - Aggregation: COUNT/SUM/AVG/MIN/MAX only (not STDDEV/PERCENTILE)
  - Self-join: if query joins CTE to itself → consolidate into single CTE
  - Channel pattern: 3 channel scans (store/catalog/web) → single_pass_aggregation

  Transform selection:
  - Multi-channel INTERSECT-like → single_pass_aggregation (1 win, 1.98x)
  - Year-over-year self-join → self_join_pivot (1 win, 1.79x)

  Ordering: apply after P1/P2 — reduced inputs make consolidation cheaper.
  After applying: P4 benefits from smaller materialized inputs.

  Wins: Q014 1.98x, Q031 1.79x
  Regressions: none observed

### P4: Non-equi join without prefiltering [Phase 3 — ZERO REGRESSIONS]

  Gap: NON_EQUI_JOIN_INPUT_BLINDNESS — PostgreSQL handles non-equi joins
  (BETWEEN, <, >) via nested-loop or hash join with recheck, but cannot push
  dimension filters past the non-equi join boundary. Both sides of the non-equi
  join receive full unfiltered input, making the join O(N × M) on large tables.

  Fix: shrink BOTH sides via MATERIALIZED CTEs before the non-equi join.
  Pre-filter dimensions (date, demographics, household) into small CTEs, then
  join fact table with pre-filtered dimensions to reduce cardinality before the
  non-equi join.

  Signal: expensive non-equi join (BETWEEN, <, >) in EXPLAIN with large inputs.
  SQL signal: JOIN ... ON a.col BETWEEN b.low AND b.high, neither side filtered.

  Decision gates:
  - Structural: non-equi join predicate (BETWEEN, range comparison)
  - Cardinality: both join inputs > 10K rows
  - Dimension filters: at least one side has selective dimension filter available
  - Fact side: reducible by pre-joining with filtered dimensions

  Transform selection:
  - Multiple dimension filters → pg_materialized_dimension_fact_prefilter (1 win, 12.07x)

  Ordering: apply after P1 (explicit join syntax) and P6 (date CTE).
  After applying: non-equi join now operates on pre-filtered inputs.

  Wins: Q072 12.07x
  Regressions: none observed

### P5: Set operation materializing full result sets [Phase 3 — CAUTION]

  Gap: SET_OPERATION_MATERIALIZATION — INTERSECT and EXCEPT are implemented via
  full materialization + sort/hash comparison. For EXISTS (positive set test)
  this destroys semi-join early termination. For NOT EXISTS (negative set test)
  the planner uses hash-anti-join which is efficient, but correlated set
  operations re-execute per outer row.

  Two opposite fixes depending on direction:
  (a) INTERSECT → EXISTS: replace set materialization with semi-join early
      termination. PostgreSQL's EXISTS uses index + early termination.
  (b) Correlated EXISTS/NOT EXISTS on large sets → MATERIALIZED CTE + LEFT JOIN
      + IS NULL: pre-compute distinct customer sets once, then hash join for
      set difference. Only when 3+ channel checks (store, web, catalog).

  Signal: INTERSECT/EXCEPT between large result sets in EXPLAIN.
  SQL signal: EXISTS subquery correlated to outer with fact+date scan inside.

  Decision gates:
  - INTERSECT with 10K+ rows → convert to EXISTS (P5a)
  - Correlated NOT EXISTS on 3+ channels → materialize channel sets (P5b)
  - Simple EXISTS (single channel) → KEEP EXISTS (semi-join is optimal)
  - NOT EXISTS already using hash anti-join in EXPLAIN → STOP
  - CAUTION: materializing simple EXISTS destroys semi-join (0.75x Q069_i2)

  Transform selection:
  - INTERSECT → intersect_to_exists (1 win, 1.78x)
  - Multi-channel EXISTS/NOT EXISTS → set_operation_materialization (1 win, 17.48x)

  Ordering: apply after P1 (explicit joins for channel CTEs).
  Composition: P5a (INTERSECT→EXISTS) is standalone; P5b combines with P1/P6.
  After applying: check P3 if set operations were the only repeated-scan source.

  Wins: Q069 17.48x, Q038 1.78x
  Regressions: Q069_i2 0.75x (over-materialized date CTE in EXISTS path)

### P6: Multiple date_dim aliases with overlapping filters [Phase 1 — HIGHEST RELIABILITY]

  Gap: DATE_DIM_REDUNDANCY — PostgreSQL cannot detect that N references to
  date_dim with overlapping year/month filters select the same rows. Each alias
  is an independent scan. On star schemas with 3 date_dim instances (sold,
  returned, shipped), the optimizer scans date_dim 3 times with similar predicates.

  Fix: consolidate overlapping date filters into a single CTE (all_dates) that
  selects the union of needed date_sk values, then join each fact table reference
  to the shared CTE with specific MOY conditions.

  Signal: 2+ date_dim aliases in FROM with similar year/month_seq/moy predicates.
  SQL signal: d1.d_year = 1999 AND d2.d_year = 1999 AND d3.d_year = 1999.

  Decision gates:
  - Structural: 2+ date_dim instances with overlapping date predicates
  - Selectivity: date filter selects < 1% of date_dim (always true for year+month)
  - Combine with: explicit JOIN conversion when comma joins present

  Transform selection:
  - 2-3 date aliases, same year → date_consolidation (1 win, 3.10x)
  - Single date filter, star schema → date_cte_isolate (3 wins + 7 improved)

  Ordering: apply first (Phase 1) — date CTE is the smallest, most reliable transform.
  Composition: always combine with P1 (explicit join syntax).
  After applying: fact table scans reduced, all downstream pathologies benefit.

  Wins: Q025 3.10x, Q025_i1 2.23x, Q010 2.00x
  Improved: Q102 1.26x, Q080_i2 1.22x, Q091 1.18x, Q050_i2 1.10x,
            Q018 1.07x, Q072_i1 1.07x, Q094_i2 1.07x
  Regressions: none observed

### P7: Multi-dimension prefetch for star-schema aggregation [Phase 1 — CAUTION]

  Gap: DIMENSION_FILTER_PUSHDOWN_FAILURE — when multiple selective dimension
  filters exist (item category, store state, customer demographics), the planner
  may not apply them early enough. Pre-filtering dimensions into small CTEs and
  joining them to the fact table reduces cardinality before expensive aggregation.

  Fix: create MATERIALIZED CTEs for each selective dimension, then join fact table
  to all filtered dimensions using explicit INNER JOIN syntax.

  Signal: large fact table scan followed by late dimension filter in EXPLAIN.
  SQL signal: star schema with 3+ dimension filters in WHERE clause.

  Decision gates:
  - Structural: star schema with 3+ selective dimension filters
  - Dimension selectivity: each dimension filter selects < 10% of dimension table
  - Fact table: single fact table, NOT self-join or multi-fact
  - Stop: if query has self-join pattern → use P3 instead (0.25x Q031_i1)
  - Stop: if query has multi-fact join → dimension prefetch locks join order (0.51x Q054)

  Transform selection:
  - 2-3 dimensions → pg_dimension_prefetch_star (2 wins, 2.5x avg)
  - Mixed dimensions + date → multi_dimension_prefetch (1 win, 2.50x)
  - Fact + date + non-equi → combine with P4 (pg_materialized_dimension_fact_prefilter)

  Ordering: apply with P1/P6 (all Phase 1 optimizations together).
  CAUTION: do NOT apply to self-join or multi-fact queries.

  Wins: Q099 2.50x, Q064 2.12x, Q023 1.83x
  Improved: Q094 1.25x, Q084 1.10x, Q040 1.09x
  Regressions: Q031_i1 0.25x (self-join), Q054 0.51x (multi-fact)

### NO MATCH

  Record: which pathologies checked, which gates failed.
  Nearest miss: closest pathology + why it didn't qualify.
  Features present: structural features for future pattern discovery.
  → Workers get: broad gold example set, analyst's manual reasoning.

---

## CONFIG TUNING PATTERNS

Config tuning is ADDITIVE to SQL rewrite — not a substitute. Apply after SQL rewrite.
Evidence: 52 queries benchmarked, 25 config wins, 3-race validated (PG 14.3, SF10).
CRITICAL: EXPLAIN ANALYZE cost gaps do NOT predict runtime gains. 6 false positives
caught where EXPLAIN showed 38-84% improvement but runtime showed 0% or regression.
Always 3-race validate config changes.

### C1: Merge join forcing suboptimal plan [HIGHEST IMPACT hint]

  Mechanism: /*+ Set(enable_mergejoin off) */
  Signal: EXPLAIN shows Merge Join with Sort node below it on large unsorted inputs.
  The optimizer chooses merge join for cost model reasons but the sort overhead
  exceeds hash join's hash-build cost.

  Decision gates:
  - Merge Join present in EXPLAIN with Sort node below it
  - Both inputs > 10K rows (small merge joins are fine)
  - Alternative: Hash Join would work (equi-join condition exists)
  - DANGER: Do NOT disable on queries already using merge join efficiently on pre-sorted data

  Wins: Q100_agg +82.5%, Q083 +68.2%, Q014 +66.9%, Q058 +60.2%,
        Q064 +17.1%, Q065 +8.6%
  6 wins, avg +50.6%

### C2: Cost model undervaluing index scans on SSD [HIGHEST RECOVERY]

  Mechanism: SET LOCAL random_page_cost = '1.1'; SET LOCAL effective_cache_size = '48GB'
  Signal: Seq Scan on fact tables in EXPLAIN when btree indexes exist on join/filter
  columns. The default random_page_cost=4.0 assumes spinning disk — on SSD the actual
  cost ratio is ~1.1. Combined with effective_cache_size, the optimizer tips to index scans.
  These two parameters have a nonlinear interaction — neither alone is sufficient.

  Decision gates:
  - Storage is SSD (not spinning disk)
  - Seq Scan on fact table in EXPLAIN despite btree index on join/filter columns
  - Buffer cache warm (shared_buffers + OS cache covers working set)

  Wins: Q100_spj +89.0%, Q102_spj +83.2%, Q027_agg +73.4% (with par4),
        Q075 +46.0%, Q100_agg +82.5% (with MJ_off), Q102_agg +52.5% (with par4)
  6 wins, avg +71.1%. Rescued 3 rewrite regressions (Q100_spj 0.61x→9.09x,
  Q102_spj 0.51x→5.95x, Q075 0.30x→1.85x).

### C3: Parallelism underutilized on large scans [MOST VERSATILE]

  Mechanism: SET LOCAL max_parallel_workers_per_gather = '4';
             SET LOCAL parallel_setup_cost = '100';
             SET LOCAL parallel_tuple_cost = '0.001'
  Signal: Large Seq Scan (>100K rows) without Gather/Parallel node above in EXPLAIN.
  Prefer cost reduction (setup=100, tuple=0.001) over max_workers forcing alone.

  Decision gates:
  - Seq Scan > 100K rows without parallel workers
  - Query execution > 500ms (CRITICAL: never on fast queries)
  - DANGER: Q039 got 7.34x REGRESSION when forced on 244ms query
  - DANGER: Q023 par4-alone caused -15.3% — must include work_mem=512MB
  - par4 alone insufficient for hash-heavy queries — combine with work_mem (C4)

  Wins (standalone): Q050_spj +28.2%, Q091_spj +17.4%, Q030 +12.5%,
                     Q084_spj +7.0%, Q023 +6.2% (with wm512)
  5 standalone wins, avg +14.3%. Also in 10+ combo wins.

### C4: Hash/sort spilling to disk [TARGETED]

  Mechanism: SET LOCAL work_mem = '256MB' or '512MB'
  Signal: Hash Batches > 1 or Sort Space Type = 'Disk' in EXPLAIN ANALYZE.
  Size by op count: ≤2 ops → 512MB, 3-5 → 256MB, 6+ → 128MB.
  work_mem is per-operation — count sort+hash nodes before sizing.

  Decision gates:
  - Hash Batches > 1 OR Sort Space = 'Disk' in EXPLAIN
  - Count sort+hash ops in EXPLAIN to size appropriately
  - Often needs par4 (C3) to realize full benefit

  Wins: Q010 +41.5% (wm512+par), Q069 +17.9% (wm256+par),
        Q091_agg +16.0% (wm256+par), Q087 +11.4% (wm256 alone)
  4 wins, avg +21.7%

### C5: Nested loop on large join inputs [HIGH IMPACT hint]

  Mechanism: /*+ Set(enable_nestloop off) */
  Signal: Nested Loop in EXPLAIN with >10K rows on both sides. NL is O(N×M)
  when both inputs are large — hash join is O(N+M) with equi-join condition.

  Decision gates:
  - Nested Loop in EXPLAIN with both inputs > 10K rows
  - Equi-join condition exists (hash join is viable alternative)
  - NOT correlated subquery (NL is correct there — use P2 decorrelation instead)
  - DANGER: Q075 NL_off caused -1454% regression — never on queries where NL is correct

  Wins: Q072_agg +81.3%, Q027_spj +57.5% (with par4),
        Q081 +42.5% (with par4)
  3 wins, avg +60.4%

### C6: Sort overhead on pre-ordered data [RARE]

  Mechanism: SET LOCAL enable_sort = 'off'
  Signal: Sort node in EXPLAIN on data that is already index-ordered or where
  hash-based aggregation would be cheaper. Forces hash-based execution paths.

  Decision gates:
  - Sort node in EXPLAIN with input from index scan (already ordered)
  - Or Sort node where hash aggregation is viable alternative
  - High variance observed (3.2-7.7% on Q059) — validate carefully

  Wins: Q083 +68.2% (with MJ_off), Q059 +4.7%
  2 wins, avg +36.5%

---

## SAFETY RANKING

| Rank | Pattern | Regr. | Worst | Action |
|------|---------|-------|-------|--------|
| 1 | P6: Date CTE isolation | 0 | — | Always fix (zero regressions) |
| 2 | P3: Repeated scans | 0 | — | Always fix (verify agg type) |
| 3 | P4: Non-equi prefilter | 0 | — | Always fix |
| 4 | C2: SSD cost model (rpc+cache) | 0 | — | Always apply on SSD (zero regressions) |
| 5 | C4: work_mem for spills | 0 | — | Size by op count (zero regressions) |
| 6 | C6: Sort disable | 0 | — | Rare, validate carefully |
| 7 | P1: Comma join + CTE | 1 | 0.88x | Fix when comma joins present |
| 8 | C1: Merge join disable | 0 | — | Only when Sort+MJ visible in EXPLAIN |
| 9 | P5: Set operation | 1 | 0.75x | Check direction (INTERSECT vs EXISTS) |
| 10 | C5: Nested loop disable | 1 | -1454% | ONLY when NL on large inputs, never on correlated |
| 11 | P2: Correlated subquery | 2 | 0.51x | Check EXPLAIN, never on EXISTS |
| 12 | C3: Forced parallelism | 1 | 7.34x regr | NEVER on queries < 500ms |
| 13 | P7: Multi-dim prefetch | 2 | 0.25x | Star schema only, not self-join |

## VERIFICATION CHECKLIST

Before finalizing any rewrite:
- [ ] AS MATERIALIZED used on all decorrelation CTEs (prevents re-correlation)
- [ ] EXISTS/NOT EXISTS still uses EXISTS (not materialized into CTE)
- [ ] OR conditions on indexed columns still intact (not split to UNION)
- [ ] Comma joins converted to explicit INNER JOIN
- [ ] Parallel execution not blocked by unnecessary CTE materialization
- [ ] No orphaned CTEs (every CTE referenced downstream)
- [ ] NULL semantics preserved in NOT IN conversions
- [ ] Row counts decrease monotonically through CTE chain
- [ ] Max 2 cascading fact-table CTE chains
- [ ] Rewrite doesn't match any REGRESSION REGISTRY pattern
- [ ] Config: query execution > 500ms before applying parallelism (C3)
- [ ] Config: EXPLAIN gap validated by 3-race (EXPLAIN ≠ runtime — 6 false positives caught)
- [ ] Config: work_mem sized by sort+hash op count, not query complexity
- [ ] Config: hint disable (MJ/NL/sort off) only when EXPLAIN shows the problematic operator

## PRUNING GUIDE

Skip pathologies the plan rules out:

| Plan shows | Skip |
|---|---|
| No comma joins (all explicit JOINs) | P1 (comma join fix) |
| No nested loops on large tables | P2 (decorrelation) |
| Each table appears once | P3 (repeated scans) |
| No non-equi joins (BETWEEN, <, >) | P4 (non-equi prefilter) |
| No INTERSECT/EXCEPT and no correlated multi-channel EXISTS | P5 (set operation) |
| Single date_dim reference | P6 (date consolidation) |
| No GROUP BY or only 1 dimension filter | P7 (multi-dim prefetch) |
| Baseline < 100ms | ALL CTE-based transforms |
| Bitmap OR scan present | OR→UNION rewrites |
| Parallel workers active + query fast | CTE-heavy transforms |

## REGRESSION REGISTRY

| Severity | Query | Transform | Result | Root cause |
|----------|-------|-----------|--------|------------|
| CATASTROPHIC | Q075_i1 | cte_inlining | 0.16x | Inlined large UNION CTE → 6 fact scans re-executed 2x each |
| SEVERE | Q101_i1 | multi_dim_prefetch | 0.15x | CTEs blocked date-predicate pushdown on 90-day interval join |
| SEVERE | Q031_i1 | dimension_prefetch | 0.25x | Applied star-schema pattern to 6-way self-join → parallelism destroyed |
| MAJOR | Q075_i2 | cte_materialization | 0.30x | Multi-scan CTE overhead similar to Q075_i1 |
| MAJOR | Q054_i1 | early_fact_filtering | 0.51x | Disabled nestloop too aggressively + DISTINCT forced hash spill |
| MAJOR | Q069_i2 | date_cte_prefetch | 0.75x | Over-materialized date CTE in EXISTS path → destroyed semi-join |
| MODERATE | Q058_i1 | explicit_join | 0.88x | Explicit join conversion overhead exceeded benefit on simple query |
| CATASTROPHIC | Q039 | forced_parallelism (C3) | 7.34x regr | Worker startup + coordination overhead on 244ms query. NEVER force par on < 500ms |
| CATASTROPHIC | Q075 | enable_nestloop_off (C5) | -1454% | NL was correct plan. Disabling forced catastrophic merge/hash on unsuitable query |
| MAJOR | Q064 | geqo_off | -254% | Exhaustive planner found "better" cost plan on 19 joins but cardinality errors made it catastrophic |
| MAJOR | Q023 | par4_without_wm | -15.3% | Parallelism without sufficient work_mem causes hash spill under parallel execution |


### System Resource Envelope (PostgreSQL)

Workers will use this to size SET LOCAL parameters for their rewrites. Included here for your awareness — you do NOT output config. Each worker decides its own per-rewrite config.

Memory budget: shared_buffers=128MB, effective_cache_size=4GB
Global work_mem: 4MB (per-operation)
Active connections: ~1 (work_mem headroom: safe up to 16MB per-op)
Storage: HDD (random_page_cost=4.0)
Parallel capacity: max_parallel_workers=8, per_gather=2

SET LOCAL permissions:
  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem

## §5a. Transform Catalog

Select 4 transforms that are applicable to THIS query, maximizing structural diversity (each must attack a different part of the execution plan).

### Predicate Movement
- **global_predicate_pushdown**: Trace selective predicates from late in the CTE chain back to the earliest scan via join equivalences. Biggest win when a dimension filter is applied after a large intermediate materialization.
  Maps to examples: pushdown, early_filter, date_cte_isolate
- **transitive_predicate_propagation**: Infer predicates through join equivalence chains (A.key = B.key AND B.key = 5 -> A.key = 5). Especially across CTE boundaries where optimizers stop propagating.
  Maps to examples: early_filter, dimension_cte_isolate
- **null_rejecting_join_simplification**: When downstream WHERE rejects NULLs from the outer side of a LEFT JOIN, convert to INNER. Enables reordering and predicate pushdown. CHECK: does the query actually have LEFT/OUTER joins before assigning this.
  Maps to examples: (no direct gold example — novel transform)

### Join Restructuring
- **self_join_elimination**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, split into N pre-partitioned CTEs. Eliminates discriminator filtering and repeated hash probes on rows that don't match.
  Maps to examples: union_cte_split, shared_dimension_multi_channel
- **decorrelation**: Convert correlated EXISTS/IN/scalar subqueries to CTE + JOIN. CHECK: does the query actually have correlated subqueries before assigning this.
  Maps to examples: decorrelate, composite_decorrelate_union
- **aggregate_pushdown**: When GROUP BY follows a multi-table join but aggregation only uses columns from one side, push the GROUP BY below the join. CHECK: verify the join doesn't change row multiplicity for the aggregate (one-to-many breaks AVG/STDDEV).
  Maps to examples: (no direct gold example — novel transform)
- **late_attribute_binding**: When a dimension table is joined only to resolve display columns (names, descriptions) that aren't used in filters, aggregations, or join conditions, defer that join until after all filtering and aggregation is complete. Join on the surrogate key once against the final reduced result set. This eliminates N-1 dimension scans when the CTE references the dimension N times. CHECK: verify the deferred columns aren't used in WHERE, GROUP BY, or JOIN ON — only in the final SELECT.
  Maps to examples: dimension_cte_isolate (partial pattern), early_filter

### Scan Optimization
- **star_join_prefetch**: Pre-filter ALL dimension tables into CTEs, then probe fact table with the combined key intersection.
  Maps to examples: dimension_cte_isolate, multi_dimension_prefetch, prefetch_fact_join, date_cte_isolate
- **single_pass_aggregation**: Merge N subqueries on the same fact table into 1 scan with CASE/FILTER inside aggregates. CHECK: STDDEV_SAMP/VARIANCE are grouping-sensitive — FILTER over a combined group != separate per-group computation.
  Maps to examples: single_pass_aggregation, channel_bitmap_aggregation
- **scan_consolidation_pivot**: When a CTE is self-joined N times with each reference filtering to a different discriminator (e.g., year, channel), consolidate into fewer scans that GROUP BY the discriminator, then pivot rows to columns using MAX(CASE WHEN discriminator = X THEN agg_value END). This halves the fact scans and dimension joins. SAFE when GROUP BY includes the discriminator — each group is naturally partitioned, so aggregates like STDDEV_SAMP are computed correctly per-partition. The pivot MAX is just a row selector (one row per group), not a real aggregation.
  Maps to examples: single_pass_aggregation, union_cte_split

### Structural Transforms
- **union_consolidation**: Share dimension lookups across UNION ALL branches that scan different fact tables with the same dim joins.
  Maps to examples: shared_dimension_multi_channel
- **window_optimization**: Push filters before window functions when they don't affect the frame. Convert ROW_NUMBER + filter to LATERAL + LIMIT. Merge same-PARTITION windows into one sort pass.
  Maps to examples: deferred_window_aggregation
- **exists_restructuring**: Convert INTERSECT to EXISTS for semi-join short-circuit, or restructure complex EXISTS with shared CTEs. CHECK: does the query actually have INTERSECT or complex EXISTS.
  Maps to examples: intersect_to_exists, multi_intersect_exists_cte

## §6. REASONING PROCESS

First, use a `<reasoning>` block for your internal analysis. This will be stripped before parsing. Work through these steps IN ORDER:

1. **CLASSIFY**: What structural archetype is this query?
   (channel-comparison self-join / correlated-aggregate filter / star-join with late dim filter / repeated fact scan / multi-channel UNION ALL / EXISTS-set operations / other)

2. **EXPLAIN PLAN ANALYSIS**: From the EXPLAIN ANALYZE output, identify:
   - Compute wall-clock ms per EXPLAIN node. Sum repeated operations (e.g., 2x store_sales joins = total cost). The EXPLAIN is ground truth, not the logical-tree cost percentages.
   - Which nodes consume >10% of runtime and WHY
   - Where row counts drop sharply (existing selectivity)
   - Where row counts DON'T drop (missed optimization opportunity)
   - Whether the optimizer already splits CTEs, pushes predicates, or performs transforms you might otherwise assign
   - Count scans per base table. If a fact table is scanned N times, a restructuring that reduces it to 1 scan saves (N-1)/N of that table's I/O cost. Prioritize transforms that reduce scan count on the largest tables.
   - Whether the CTE is materialized once and probed multiple times, or re-executed per reference

3. **GAP MATCHING**: Compare the EXPLAIN analysis to the Engine Profile gaps above. For each gap:
   - Does this query exhibit the gap? (e.g., is a predicate NOT pushed into a CTE? Is the same fact table scanned multiple times?)
   - Check the 'opportunity' — does this query's structure match?
   - Check 'what_didnt_work' and 'field_notes' — any disqualifiers for this query?
   - Also verify: is the optimizer ALREADY handling this well? (Check the Optimizer Strengths above — if the engine already does it, your transform adds overhead, not value.)

4. **AGGREGATION TRAP CHECK**: For every aggregate function in the query, verify: does my proposed restructuring change which rows participate in each group? STDDEV_SAMP, VARIANCE, PERCENTILE_CONT, CORR are grouping-sensitive. SUM, COUNT, MIN, MAX are grouping-insensitive (modulo duplicates). If the query uses FILTER clauses or conditional aggregation, verify equivalence explicitly.

5. **TRANSFORM SELECTION**: From the matched engine gaps, select transforms that exploit the specific gaps present in THIS query. Rank by expected value (rows affected × historical speedup from evidence). Select 4 that are structurally diverse — each attacking a different gap or bottleneck.
   REJECT tag-matched examples whose primary technique requires a structural feature this query lacks (e.g., reject intersect_to_exists if query has no INTERSECT; reject decorrelate if query has no correlated subquery). Tag matching is approximate — always verify structural applicability.

6. **LOGICAL TREE DESIGN**: For each worker's strategy, define the target logical tree topology. Verify that every node contract has exhaustive output columns by checking downstream references.
   CTE materialization matters for your design: a CTE referenced by 2+ consumers will likely be materialized (good — computed once, probed many). A CTE referenced once may be inlined (no materialization benefit from 'sharing'). Design shared CTEs only when multiple downstream nodes consume them. See CTE_INLINING in Engine Profile strengths.

### Strategy Selection Rules

1. **CHECK APPLICABILITY**: Each transform has a structural prerequisite (correlated subquery, UNION ALL CTE, LEFT JOIN, etc.). Verify the query actually has the prerequisite before assigning a transform. DO NOT assign decorrelation if there are no correlated subqueries.
2. **CHECK OPTIMIZER OVERLAP**: Read the EXPLAIN plan. If the optimizer already performs a transform (e.g., already splits a UNION CTE, already pushes a predicate), that transform will have marginal benefit. Note this in your reasoning and prefer transforms the optimizer is NOT already doing.
3. **MAXIMIZE DIVERSITY**: Each worker must attack a different part of the execution plan. Do not assign 'pushdown variant A' and 'pushdown variant B'. Assign transforms from different categories above.
4. **ASSESS RISK PER-QUERY**: Risk is a function of (transform x query complexity), not an inherent property of the transform. Decorrelation is low-risk on a simple EXISTS and high-risk on nested correlation inside a CTE. Assess per-assignment.
5. **COMPOSITION IS ALLOWED AND ENCOURAGED**: A strategy can combine 2-3 transforms from different categories (e.g., star_join_prefetch + scan_consolidation_pivot, or date_cte_isolate + early_filter + decorrelate). The TARGET_LOGICAL_TREE should reflect the combined structure. Compound strategies are often the source of the biggest wins.
6. **MINIMAL-CHANGE BASELINE**: If the EXPLAIN shows the optimizer already handles the primary bottleneck (e.g., already splits CTEs, already pushes predicates), consider assigning one worker as a minimal-change baseline: explicit JOINs only, no structural changes. This provides a regression-safe fallback.

Each worker gets 1-3 examples from the 'Maps to examples' notes in the Transform Catalog. The system auto-loads full before/after SQL for each assigned example. Do NOT pad with irrelevant examples — an irrelevant example is worse than no example.

For TARGET_LOGICAL_TREE: Define the CTE structure you want produced. For NODE_CONTRACTS: Be exhaustive with OUTPUT columns — missing columns cause semantic breaks.

## §7a. Output Format

Then produce the structured briefing in EXACTLY this format:

```
=== SHARED BRIEFING ===

SEMANTIC_CONTRACT: (80-150 tokens, cover ONLY:)
(a) One sentence of business intent (start from pre-computed intent if available).
(b) JOIN type semantics that constrain rewrites (INNER = intersection = all sides must match).
(c) Any aggregation function traps specific to THIS query.
(d) Any filter dependencies that a rewrite could break.
Do NOT repeat information already in ACTIVE_CONSTRAINTS or REGRESSION_WARNINGS.

BOTTLENECK_DIAGNOSIS:
[Which operation dominates cost and WHY (not just '50% cost').
Scan-bound vs join-bound vs aggregation-bound.
Cardinality flow (how many rows at each stage).
What the optimizer already handles well (don't re-optimize).
Whether logical-tree cost percentages are misleading.]

ACTIVE_CONSTRAINTS:
- [CORRECTNESS_CONSTRAINT_ID]: [Why it applies to this query, 1 line]
- [ENGINE_GAP_ID]: [Evidence from EXPLAIN that this gap is active]
(List all 4 correctness constraints + the 1-3 engine gaps that
are active for THIS query based on your EXPLAIN analysis.)

REGRESSION_WARNINGS:
1. [Pattern name] ([observed regression]):
   CAUSE: [What happened mechanistically]
   RULE: [Actionable avoidance rule for THIS query]
(If no regression warnings are relevant, write 'None applicable.')

NODE_CONTRACTS: Write all fields as SQL fragments, not natural language. Example: `WHERE: d_year IN (1999, 2000)` not `WHERE: filter to target years`. Workers use these as specifications to code against.

=== WORKER 1 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
  [For each: what to apply, what to IGNORE for this strategy.]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 2 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
  [For each: what to apply, what to IGNORE for this strategy.]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 3 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
  [For each: what to apply, what to IGNORE for this strategy.]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 4 BRIEFING === (EXPLORATION WORKER)

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
  [For each: what to apply, what to IGNORE for this strategy.]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]
CONSTRAINT_OVERRIDE: [CONSTRAINT_ID or 'None']
OVERRIDE_REASONING: [Why this query's structure differs from the observed failure, or 'N/A']
EXPLORATION_TYPE: [constraint_relaxation | compound_strategy | novel_combination]

```

## Section Validation Checklist (MUST pass before final output)

### SHARED BRIEFING
- `SEMANTIC_CONTRACT`: 40-200 tokens covering business intent, JOIN semantics, aggregation trap, filter dependency.
- `BOTTLENECK_DIAGNOSIS`: dominant mechanism, bound type (`scan-bound`/`join-bound`/`aggregation-bound`), what optimizer already handles.
- `ACTIVE_CONSTRAINTS`: all 4 correctness IDs + 1-3 engine gaps with EXPLAIN evidence.
- `REGRESSION_WARNINGS`: `None applicable.` or entries with `CAUSE:` and `RULE:`.

### WORKER N BRIEFING (N=1..4)
- `STRATEGY`: non-empty, unique across workers.
- `TARGET_LOGICAL_TREE`: explicit node chain. `NODE_CONTRACTS`: every logical tree node has a contract with FROM, OUTPUT, CONSUMERS.
- `EXAMPLES`: 1-3 IDs. `EXAMPLE_ADAPTATION`: what to adapt/ignore per example.
- `HAZARD_FLAGS`: query-specific risks, not generic cautions.

### WORKER 4 EXPLORATION FIELDS
- Includes `CONSTRAINT_OVERRIDE`, `OVERRIDE_REASONING`, and `EXPLORATION_TYPE`.

## §7c. Worker 4 Exploration Rules

Workers 1-3 follow the engine profile's proven patterns. **Worker 4 is the EXPLORATION worker** with a different mandate:

Worker 4 MAY (in priority order — prefer higher-value exploration):
  (c) **PREFERRED**: Attempt a novel technique not listed in the engine profile, if the EXPLAIN plan reveals an optimizer blind spot not yet documented. This is the highest-value exploration — new discoveries expand the engine profile for all future queries.
  (b) Combine 2-3 transforms from different engine gaps into a compound strategy that hasn't been tested before. Medium value — tests interaction effects between known patterns.
  (a) Retry a technique from 'what_didnt_work', IF the structural context of THIS query differs materially from the observed failure — explain the structural difference in HAZARD_FLAGS. Lowest priority — only when the query structure clearly diverges from the failed case.

Worker 4 may NEVER violate correctness constraints (LITERAL_PRESERVATION, SEMANTIC_EQUIVALENCE, COMPLETE_OUTPUT, CTE_COLUMN_COMPLETENESS).

The exploration worker's output is tagged EXPLORATORY and tracked separately. Past failures documented in the engine profile are context-specific — they happened on specific queries with specific structures. Worker 4's job is to test whether those failures generalize or not. If Worker 4 discovers a new win, it becomes field intelligence for the engine profile.

## §7d. Output Consumption Spec

Each worker receives: SHARED BRIEFING + their WORKER N BRIEFING + full before/after SQL for assigned examples + original SQL + output format.
Workers do NOT see other workers' briefings.