{
  "explain_plan_text": "Total execution time: 301.3ms\nPlanning time: 3.4ms\n\n-> Limit  (rows=0 loops=1 time=301.3ms)\n  -> Sort  (rows=0 loops=1 time=301.3ms)\n     Sort Method: quicksort  Space: 25kB (Memory)\n    -> Nested Loop Inner  (rows=0 loops=1 time=301.2ms)\n       Join Filter: (item.i_item_id = item_2.i_item_id)\n      -> Nested Loop Inner  (rows=0 loops=1 time=301.2ms)\n         Join Filter: (item.i_item_id = item_1.i_item_id)\n        -> Aggregate  (rows=2 loops=1 time=183.1ms)\n          -> Sort  (rows=2 loops=1 time=183.0ms)\n             Sort Method: quicksort  Space: 25kB (Memory)\n            -> Gather  (rows=2 loops=1 time=183.0ms)\n               Workers: 1/1 launched\n              -> Nested Loop Inner  (rows=1 loops=2 time=153.5ms)\n                -> Nested Loop Inner  (rows=50 loops=2 time=152.9ms)\n                  -> Hash Join Semi  (rows=60 loops=2 time=10.8ms)\n                     Hash Cond: (date_dim.d_date = date_dim_1.d_date)\n                    -> Seq Scan on date_dim  (rows=37K loops=2 time=3.2ms)\n                    -> Hash  (rows=120 loops=2 time=5.3ms)\n                      -> Nested Loop Inner  (rows=120 loops=2 time=5.3ms)\n                        -> Aggregate  (rows=4 loops=2 time=5.2ms)\n                          -> Index Only Scan on date_dim date_dim_2  (rows=4 loops=2 time=5.2ms)\n                             Filter: (d_date = ANY ('{2002-02-26,2002-05-03,2002-08-19,2002-11-18}'::date[]))\n                             Rows Removed by Filter: 73K\n                        -> Index Only Scan on date_dim date_dim_1  (rows=30 loops=8 time=0.0ms)\n                           Index Cond: (d_month_seq = date_dim_2.d_month_seq)\n                  -> Index Scan on store_returns  (rows=1 loops=120 time=2.4ms)\n                     Filter: ((sr_reason_sk = ANY ('{6,7,25,45,51}'::integer[])) AND ((sr_return_amt / (sr_return_quantity)::n...\n                     Index Cond: (sr_returned_date_sk = date_dim.d_date_sk)\n                     Rows Removed by Filter: 2,830\n                -> Index Scan on item  (rows=0 loops=101 time=0.0ms)\n                   Filter: ((i_category = ANY ('{Home,Men}'::bpchar[])) AND (i_manager_id >= 8) AND (i_manager_id <= 17))\n                   Index Cond: (i_item_sk = store_returns.sr_item_sk)\n                   Rows Removed by Filter: 1\n        -> Aggregate  (rows=1 loops=2 time=59.1ms)\n          -> Sort  (rows=2 loops=2 time=59.1ms)\n             Sort Method: quicksort  Space: 25kB (Memory)\n            -> Gather  (rows=2 loops=1 time=118.1ms)\n               Workers: 2/2 launched\n              -> Nested Loop Inner  (rows=1 loops=3 time=112.9ms)\n                -> Hash Join Semi  (rows=9 loops=3 time=112.7ms)\n                   Hash Cond: (date_dim_3.d_date = date_dim_4.d_date)\n                  -> Nested Loop Inner  (rows=176 loops=3 time=107.1ms)\n                    -> Seq Scan on catalog_returns  (rows=176 loops=3 time=106.2ms)\n                       Filter: ((cr_reason_sk = ANY ('{6,7,25,45,51}'::integer[])) AND ((cr_return_amount / (cr_return_quantity)...\n                       Rows Removed by Filter: 719K\n                    -> Memoize  (rows=1 loops=528 time=0.0ms)\n                      -> Index Scan on date_dim date_dim_3  (rows=1 loops=508 time=0.0ms)\n                         Index Cond: (d_date_sk = catalog_returns.cr_returned_date_sk)\n                  -> Hash  (rows=120 loops=3 time=5.4ms)\n                    -> Nested Loop Inner  (rows=120 loops=3 time=5.4ms)\n                      -> Aggregate  (rows=4 loops=3 time=5.4ms)\n                        -> Index Only Scan on date_dim date_dim_5  (rows=4 loops=3 time=5.4ms)\n                           Filter: (d_date = ANY ('{2002-02-26,2002-05-03,2002-08-19,2002-11-18}'::date[]))\n                           Rows Removed by Filter: 73K\n                      -> Index Only Scan on date_dim date_dim_4  (rows=30 loops=12 time=0.0ms)\n                         Index Cond: (d_month_seq = date_dim_5.d_month_seq)\n                -> Index Scan on item item_1  (rows=0 loops=28 time=0.0ms)\n                   Filter: ((i_category = ANY ('{Home,Men}'::bpchar[])) AND (i_manager_id >= 8) AND (i_manager_id <= 17))\n                   Index Cond: (i_item_sk = catalog_returns.cr_item_sk)\n                   Rows Removed by Filter: 1\n      -> Aggregate  (rows=0 loops=1 time=0.0ms)\n        -> Sort  (rows=0 loops=1 time=0.0ms)\n          -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)\n            -> Gather  (rows=0 loops=1 time=0.0ms)\n               Workers: 0/2 launched\n              -> Hash Join Semi  (rows=0 loops=1 time=0.0ms)\n                 Hash Cond: (date_dim_6.d_date = date_dim_7.d_date)\n                -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)\n                  -> Seq Scan on web_returns  (rows=0 loops=1 time=0.0ms)\n                     Filter: ((wr_reason_sk = ANY ('{6,7,25,45,51}'::integer[])) AND ((wr_return_amt / (wr_return_quantity)::n...\n                  -> Memoize  (rows=0 loops=1 time=0.0ms)\n                    -> Index Scan on date_dim date_dim_6  (rows=0 loops=1 time=0.0ms)\n                       Index Cond: (d_date_sk = web_returns.wr_returned_date_sk)\n                -> Hash  (rows=0 loops=1 time=0.0ms)\n                  -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)\n                    -> Aggregate  (rows=0 loops=1 time=0.0ms)\n                      -> Index Only Scan on date_dim date_dim_8  (rows=0 loops=1 time=0.0ms)\n                         Filter: (d_date = ANY ('{2002-02-26,2002-05-03,2002-08-19,2002-11-18}'::date[]))\n                    -> Index Only Scan on date_dim date_dim_7  (rows=0 loops=1 time=0.0ms)\n                       Index Cond: (d_month_seq = date_dim_8.d_month_seq)\n            -> Index Scan on item item_2  (rows=0 loops=1 time=0.0ms)\n               Filter: ((i_category = ANY ('{Home,Men}'::bpchar[])) AND (i_manager_id >= 8) AND (i_manager_id <= 17))\n               Index Cond: (i_item_sk = web_returns.wr_item_sk)",
  "plan_scanner_text": "Plan diversity: 40 distinct plans, 10 plan changers | HIGH\nBaseline joins: 11x Nested Loop, 3x Hash Join\n  JOIN_TYPE_TRAP (56 combos): no_nestloop: ['Nested Loop(Inner)'] \u2192 ['Hash Join(Inner)', 'Merge Join(Inner)']\n  JOIN_ORDER_TRAP (54 combos): table order unstable\n  SCAN_TYPE_TRAP (55 combos): scan methods fragile\n  MEMORY_SENSITIVITY: plan shape changes with more memory\nPlan changers: no_nestloop, no_hashjoin, no_seqscan, force_hash, force_merge, force_nestloop, no_parallel, max_parallel, no_reorder, ssd_costs",
  "global_knowledge": {
    "dataset": "postgresql_dsb",
    "last_updated": "2026-02-08T20:24:49.880702",
    "source_runs": [
      "swarm_batch_20260208_142643"
    ],
    "principles": [
      {
        "id": "or_to_union",
        "name": "Or To Union",
        "what": "Applied or_to_union achieving 73.35x speedup",
        "why": "Converting OR to UNION ALL lets optimizer choose independent index paths per branch",
        "when": "WHERE clause has OR conditions over different dimension keys (\u22643 branches)",
        "when_not": "Caused regression on query013_spj_spj, query085_agg, query085_spj_spj, query091_spj_spj, query101_agg (worst: 0.05x)",
        "verified_speedups": [
          73.35242111461433,
          66.32527638993845,
          53.10837425657921,
          3.067018811694743,
          1.7390934770856203,
          1.6973457487323271,
          1.170427997752986
        ],
        "avg_speedup": 28.637,
        "queries": [
          "query010_multi",
          "query013_agg",
          "query023_multi",
          "query054_multi",
          "query065_multi",
          "query072_spj_spj",
          "query080_multi"
        ],
        "transforms": [
          "date_cte_isolate",
          "dimension_cte_isolate",
          "or_to_union",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "decorrelate",
        "name": "Decorrelate",
        "what": "Applied decorrelate, date_cte_isolate, dimension_cte_isolate achieving 122.29x speedup",
        "why": "Correlated subqueries re-execute per outer row; converting to JOIN eliminates per-row overhead; Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering all dimension tables into CTEs avoids repeated full-table scans",
        "when": "Query has correlated subquery in WHERE or SELECT that references outer table",
        "when_not": "Caused regression on query030_multi, query038_multi, query054_multi, query058_multi, query072_spj_spj, query083_multi, query087_multi (worst: 0.00x)",
        "verified_speedups": [
          122.28628549405371,
          108.83433493033179,
          107.74576725495528,
          11.623200099186304,
          10.946612330947332,
          10.835117003229474,
          4.630291368154034,
          4.103093014813743,
          2.126719235185333,
          1.9521396747351196,
          1.9242588792425155,
          1.7574278136172428,
          1.7413608791729855,
          1.52735815246076,
          1.5244777783663208,
          1.3253961714080202
        ],
        "avg_speedup": 24.68,
        "queries": [
          "query039_multi",
          "query054_multi",
          "query059_multi",
          "query065_multi",
          "query081_multi",
          "query087_multi"
        ],
        "transforms": [
          "date_cte_isolate",
          "decorrelate",
          "dimension_cte_isolate",
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "prefetch_fact_join",
        "name": "Prefetch Fact Join",
        "what": "Applied prefetch_fact_join: Created separate pre-filtered CTEs for dimensions (date_dim, item) and fact subsets (store_sales with wholesale_cost filter), materialized distinct item_sk from frequent_ss_items, and converted IN subqueries to explicit joins to enable better plan optimization.",
        "why": "Pre-joining filtered dimensions with fact table before aggregation reduces join input",
        "when": "Query joins filtered dates/dims with large fact table; pre-join reduces probe size",
        "when_not": "Caused regression on query013_spj_spj, query025_agg, query031_multi, query050_spj_spj, query072_spj_spj, query083_multi, query085_agg, query087_multi, query101_agg (worst: 0.04x)",
        "verified_speedups": [
          64.55244269378329,
          3.3232686420331894,
          1.9096524241678212,
          1.1183033701998488
        ],
        "avg_speedup": 17.726,
        "queries": [
          "query023_multi",
          "query050_agg",
          "query080_multi"
        ],
        "transforms": [
          "prefetch_fact_join"
        ]
      },
      {
        "id": "date_cte_isolate",
        "name": "Date Cte Isolate",
        "what": "Applied date_cte_isolate, multi_dimension_prefetch achieving 11.23x speedup",
        "why": "Pre-filtering date dimension into CTE reduces hash join probe table from 73K to ~365 rows; Pre-filtering multiple dimension tables in parallel reduces join fan-out",
        "when": "Query joins date_dim on multiple conditions (year, month, etc.) with fact tables",
        "when_not": "Caused regression on many queries (worst: 0.00x); Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "verified_speedups": [
          11.232139433447843,
          9.62164454268518,
          5.230518436383954,
          4.427864781544116,
          4.332563879765709,
          4.1871005597949456,
          3.804719650483733,
          3.664518718324099,
          3.5183672412141727,
          3.280025955833233,
          2.020200575455194,
          1.8388338665963029,
          1.8219644169344114,
          1.820888647355756,
          1.5791616753126991,
          1.433334918594728,
          1.3943311758574517,
          1.3639086733889336,
          1.2724570762873264,
          1.2289723178206347,
          1.2090317549808018,
          1.1992900217595654,
          1.1873395682650838,
          1.180915193084758,
          1.1772966331778911,
          1.1760558717497558,
          1.1758970086549476,
          1.1715073534764526,
          1.1648213087268702,
          1.1404043526791572,
          1.1179640420855672,
          1.1119870666322185,
          1.1116793782034127,
          1.1084531937353757,
          1.1048729871954481
        ],
        "avg_speedup": 2.469,
        "queries": [
          "query010_multi",
          "query019_agg",
          "query019_spj_spj",
          "query025_spj_spj",
          "query027_agg",
          "query027_spj_spj",
          "query040_agg",
          "query040_spj_spj",
          "query050_agg",
          "query069_multi",
          "query072_agg",
          "query080_multi",
          "query099_agg",
          "query099_spj_spj",
          "query100_spj_spj",
          "query101_agg",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "transforms": [
          "date_cte_isolate",
          "dimension_cte_isolate",
          "multi_date_range_cte",
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      },
      {
        "id": "pushdown",
        "name": "Pushdown",
        "what": "Applied pushdown: Created separate CTEs for filtered stores and two date ranges, then computed the two periods' aggregations independently with explicit JOINs and early filter pushdown.",
        "why": "Pushing predicates closer to table scans reduces data volume in upper operators",
        "when": "WHERE predicates reference columns from tables deep in the join tree",
        "when_not": "Caused regression on query014_multi, query027_spj_spj, query031_multi, query058_multi, query083_multi, query085_spj_spj, query087_multi, query091_agg (worst: 0.19x)",
        "verified_speedups": [
          4.6799802539183615,
          2.877932543501555,
          1.9303353662366844,
          1.1778171040495835,
          1.1003399162251937
        ],
        "avg_speedup": 2.353,
        "queries": [
          "query019_agg",
          "query059_multi",
          "query072_agg",
          "query080_multi",
          "query094_multi"
        ],
        "transforms": [
          "pushdown"
        ]
      },
      {
        "id": "unknown",
        "name": "Unknown",
        "what": "Applied unknown: Isolated date_dim subquery into a materialized CTE and created pre-filtered dimension CTEs for item and customer before joining with fact tables. Converted implicit joins to explicit JOIN syntax.",
        "why": "",
        "when": "",
        "when_not": "",
        "verified_speedups": [
          2.524130607367415,
          1.8413157512267129,
          1.5782496133672843,
          1.3090493738395756,
          1.3023183424416436,
          1.2228752599892374
        ],
        "avg_speedup": 1.63,
        "queries": [
          "query018_spj_spj",
          "query019_agg",
          "query058_multi",
          "query084_agg",
          "query099_spj_spj",
          "query102_agg"
        ],
        "transforms": []
      },
      {
        "id": "dimension_cte_isolate",
        "name": "Dimension Cte Isolate",
        "what": "Applied dimension_cte_isolate achieving 1.91x speedup",
        "why": "Pre-filtering all dimension tables into CTEs avoids repeated full-table scans",
        "when": "Query joins 2+ dimension tables that could each be pre-filtered independently",
        "when_not": "Caused regression on many queries (worst: 0.00x); PostgreSQL's CTE optimization fence behavior can prevent predicate pushdown",
        "verified_speedups": [
          1.908835330896292,
          1.598830782308037,
          1.3077128182680382
        ],
        "avg_speedup": 1.605,
        "queries": [
          "query072_spj_spj",
          "query084_agg",
          "query084_spj_spj"
        ],
        "transforms": [
          "dimension_cte_isolate"
        ]
      },
      {
        "id": "multi_dimension_prefetch",
        "name": "Multi Dimension Prefetch",
        "what": "Applied multi_dimension_prefetch achieving 2.10x speedup",
        "why": "Pre-filtering multiple dimension tables in parallel reduces join fan-out",
        "when": "Query references multiple dimension tables (date + store, date + item, etc.)",
        "when_not": "Caused regression on many queries (worst: 0.05x); Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "verified_speedups": [
          2.102927380757268,
          1.9057258662371936,
          1.4104156816846927,
          1.405997493540603,
          1.148035192530136
        ],
        "avg_speedup": 1.595,
        "queries": [
          "query084_agg",
          "query084_spj_spj"
        ],
        "transforms": [
          "multi_dimension_prefetch",
          "prefetch_fact_join"
        ]
      }
    ],
    "anti_patterns": [
      {
        "id": "regression_date_cte_isolate",
        "name": "Regression: Date Cte Isolate",
        "mechanism": "PostgreSQL's CTE optimization fence behavior prevents pushdown of filters across CTE boundaries, forcing full scans of large fact tables.",
        "observed_regressions": [
          0.09,
          0.1,
          0.13,
          0.14,
          0.15,
          0.17,
          0.17,
          0.21,
          0.32,
          0.33,
          0.48,
          0.53,
          0.54,
          0.56,
          0.57,
          0.57,
          0.58,
          0.58,
          0.59,
          0.61,
          0.62,
          0.68,
          0.69,
          0.7,
          0.7,
          0.73,
          0.73,
          0.74,
          0.77,
          0.78,
          0.78,
          0.8,
          0.8,
          0.81,
          0.81,
          0.82,
          0.82,
          0.83,
          0.85,
          0.88,
          0.88,
          0.89,
          0.9,
          0.9,
          0.91,
          0.93,
          0.93
        ],
        "queries": [
          "query010_multi",
          "query013_agg",
          "query013_spj_spj",
          "query018_agg",
          "query018_spj_spj",
          "query019_spj_spj",
          "query025_agg",
          "query027_agg",
          "query027_spj_spj",
          "query031_multi",
          "query038_multi",
          "query040_spj_spj",
          "query050_spj_spj",
          "query064_multi",
          "query069_multi",
          "query072_agg",
          "query085_agg",
          "query085_spj_spj",
          "query087_multi",
          "query091_agg",
          "query091_spj_spj",
          "query094_multi",
          "query100_agg",
          "query101_agg",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "avoid_when": "Applying date_cte_isolate to queries similar to the above list"
      },
      {
        "id": "regression_decorrelate",
        "name": "Regression: Decorrelate",
        "mechanism": "Rewrite increased execution time \u2014 likely added overhead or prevented optimizer optimizations",
        "observed_regressions": [
          0.0,
          0.04,
          0.06,
          0.06,
          0.26,
          0.31,
          0.34,
          0.53,
          0.6,
          0.76,
          0.85
        ],
        "queries": [
          "query030_multi",
          "query038_multi",
          "query054_multi",
          "query058_multi",
          "query072_spj_spj",
          "query083_multi",
          "query087_multi"
        ],
        "avoid_when": "Applying decorrelate to queries similar to query030_multi, query038_multi, query054_multi, query058_multi, query072_spj_spj, query083_multi, query087_multi"
      },
      {
        "id": "regression_or_to_union",
        "name": "Regression: Or To Union",
        "mechanism": "Column resolution errors from CTEs that only select subset of columns, then reference missing columns in main query WHERE clause.",
        "observed_regressions": [
          0.05,
          0.2,
          0.45,
          0.45,
          0.49,
          0.68
        ],
        "queries": [
          "query013_spj_spj",
          "query085_agg",
          "query085_spj_spj",
          "query091_spj_spj",
          "query101_agg"
        ],
        "avoid_when": "Applying or_to_union to queries similar to query013_spj_spj, query085_agg, query085_spj_spj, query091_spj_spj, query101_agg"
      },
      {
        "id": "regression_prefetch_fact_join",
        "name": "Regression: Prefetch Fact Join",
        "mechanism": "CTEs materialized by default in PostgreSQL, creating temporary tables that eliminated index usage and increased I/O.",
        "observed_regressions": [
          0.47,
          0.58,
          0.68
        ],
        "queries": [
          "query025_agg",
          "query087_multi",
          "query101_agg"
        ],
        "avoid_when": "Applying prefetch_fact_join to queries similar to query025_agg, query087_multi, query101_agg"
      },
      {
        "id": "regression_pushdown",
        "name": "Regression: Pushdown",
        "mechanism": "PostgreSQL's optimizer treats CTEs as optimization fences, preventing pushdown of filters across CTE boundaries.",
        "observed_regressions": [
          0.19,
          0.27,
          0.3,
          0.53,
          0.61,
          0.62,
          0.64,
          0.65
        ],
        "queries": [
          "query014_multi",
          "query027_spj_spj",
          "query031_multi",
          "query058_multi",
          "query083_multi",
          "query085_spj_spj",
          "query087_multi",
          "query091_agg"
        ],
        "avoid_when": "Applying pushdown to queries similar to query014_multi, query027_spj_spj, query031_multi, query058_multi, query083_multi, query085_spj_spj, query087_multi, query091_agg"
      },
      {
        "id": "regression_single_pass_aggregation",
        "name": "Regression: Single Pass Aggregation",
        "mechanism": "Only addressed dimension table filtering, missing the core bottleneck: multi-way self-join pattern forces repeated scans of aggregated data.",
        "observed_regressions": [
          0.62,
          0.67
        ],
        "queries": [
          "query031_multi"
        ],
        "avoid_when": "Applying single_pass_aggregation to queries similar to query031_multi"
      },
      {
        "id": "regression_unknown_regression",
        "name": "Regression: Unknown Regression",
        "mechanism": "Triple INTERSECT in cross_items performs expensive set operations on large intermediate results before applying item filters.",
        "observed_regressions": [
          0.78
        ],
        "queries": [
          "query014_multi"
        ],
        "avoid_when": "Applying unknown_regression to queries similar to query014_multi"
      },
      {
        "id": "error_execution",
        "name": "Error Pattern: Execution",
        "mechanism": "DISTINCT is not implemented for window functions",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query094_multi"
        ],
        "avoid_when": "Watch for execution errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "error_timeout",
        "name": "Error Pattern: Timeout",
        "mechanism": "canceling statement due to statement timeout",
        "observed_regressions": [
          0.0,
          0.0
        ],
        "queries": [
          "query014_multi",
          "query085_spj_spj"
        ],
        "avoid_when": "Watch for timeout errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "error_unknown",
        "name": "Error Pattern: Unknown",
        "mechanism": "operator does not exist: integer = character \u2014 type mismatch from incorrect join column references",
        "observed_regressions": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query013_spj_spj",
          "query014_multi",
          "query019_agg",
          "query023_multi",
          "query025_spj_spj",
          "query030_multi",
          "query031_multi",
          "query054_multi",
          "query058_multi",
          "query064_multi",
          "query072_agg",
          "query075_multi",
          "query085_agg",
          "query085_spj_spj",
          "query091_agg",
          "query094_multi",
          "query101_spj_spj",
          "query102_agg",
          "query102_spj_spj"
        ],
        "avoid_when": "Watch for unknown errors when rewriting queries with complex joins/aliases"
      },
      {
        "id": "semantic_mismatch_date_cte_isolate",
        "name": "Semantic Mismatch: Date Cte Isolate",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query040_agg",
          "query075_multi",
          "query099_agg",
          "query100_agg"
        ],
        "avoid_when": "Applying date_cte_isolate to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_decorrelate",
        "name": "Semantic Mismatch: Decorrelate",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0,
          0.0
        ],
        "queries": [
          "query014_multi",
          "query059_multi",
          "query075_multi"
        ],
        "avoid_when": "Applying decorrelate to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_multi_dimension_prefetch",
        "name": "Semantic Mismatch: Multi Dimension Prefetch",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query084_agg"
        ],
        "avoid_when": "Applying multi_dimension_prefetch to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_or_to_union",
        "name": "Semantic Mismatch: Or To Union",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query100_agg"
        ],
        "avoid_when": "Applying or_to_union to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_prefetch_fact_join",
        "name": "Semantic Mismatch: Prefetch Fact Join",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0
        ],
        "queries": [
          "query075_multi"
        ],
        "avoid_when": "Applying prefetch_fact_join to queries where semantic equivalence is hard to verify"
      },
      {
        "id": "semantic_mismatch_union_cte_split",
        "name": "Semantic Mismatch: Union Cte Split",
        "mechanism": "Rewrite changed query semantics \u2014 different row counts or values returned",
        "observed_regressions": [
          0.0,
          0.0
        ],
        "queries": [
          "query075_multi"
        ],
        "avoid_when": "Applying union_cte_split to queries where semantic equivalence is hard to verify"
      }
    ]
  },
  "semantic_intents": null,
  "matched_examples": [
    {
      "id": "pg_explicit_join_materialized",
      "name": "Explicit Join + Materialized CTE Combo (PostgreSQL)",
      "description": "Convert comma joins to explicit INNER JOINs AND pre-filter date/item dimensions into MATERIALIZED CTEs. The combination enables PostgreSQL to use small materialized hash tables as probe inputs, dramatically improving join cardinality estimation on complex multi-table queries.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "8.56x",
      "principle": "Explicit Join + Materialized CTE: comma joins with 5+ tables produce poor cardinality estimates. Converting to explicit INNER JOIN + materializing selective dimensions into small CTEs gives the planner accurate row counts at each join step.",
      "transforms": [
        "explicit_join_materialized"
      ],
      "original_sql": "with sr_items as\n (select i_item_id item_id,\n        sum(sr_return_quantity) sr_item_qty\n from store_returns,\n      item,\n      date_dim\n where sr_item_sk = i_item_sk\n and   d_date    in\n\t(select d_date\n\tfrom date_dim\n\twhere d_month_seq in\n\t\t(select d_month_seq\n\t\tfrom date_dim\n\t  where d_date in ('2002-02-01','2002-04-11','2002-07-17','2002-10-09')))\n and   sr_returned_date_sk   = d_date_sk\n and i_category IN ('Jewelry', 'Music')\n and i_manager_id BETWEEN 16 and 25\n and sr_return_amt / sr_return_quantity between 184 and 213\n and sr_reason_sk in (26, 32, 40, 66, 73)\ngroup by i_item_id),\n cr_items as\n (select i_item_id item_id,\n        sum(cr_return_quantity) cr_item_qty\n from catalog_returns,\n      item,\n      date_dim\n where cr_item_sk = i_item_sk\n and   d_date    in\n\t(select d_date\n\tfrom date_dim\n  where d_month_seq in\n\t\t(select d_month_seq\n\t\tfrom date_dim\n\t  \t  where d_date in ('2002-02-01','2002-04-11','2002-07-17','2002-10-09')))\n and   cr_returned_date_sk   = d_date_sk\n and i_category IN ('Jewelry', 'Music')\n and i_manager_id BETWEEN 16 and 25\n and cr_return_amount / cr_return_quantity between 184 and 213\n and cr_reason_sk in (26, 32, 40, 66, 73)\n group by i_item_id),\n wr_items as\n (select i_item_id item_id,\n        sum(wr_return_quantity) wr_item_qty\n from web_returns,\n      item,\n      date_dim\n where wr_item_sk = i_item_sk\n and   d_date    in\n\t(select d_date\n\tfrom date_dim\n  where d_month_seq in\n\t\t(select d_month_seq\n\t\tfrom date_dim\n\t\t\t  where d_date in ('2002-02-01','2002-04-11','2002-07-17','2002-10-09')))\n and   wr_returned_date_sk   = d_date_sk\n and i_category IN ('Jewelry', 'Music')\n and i_manager_id BETWEEN 16 and 25\n and wr_return_amt / wr_return_quantity between 184 and 213\n and wr_reason_sk in (26, 32, 40, 66, 73)\n group by i_item_id)\n  select  sr_items.item_id\n       ,sr_item_qty\n       ,sr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 sr_dev\n       ,cr_item_qty\n       ,cr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 cr_dev\n       ,wr_item_qty\n       ,wr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 wr_dev\n       ,(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 average\n from sr_items\n     ,cr_items\n     ,wr_items\n where sr_items.item_id=cr_items.item_id\n   and sr_items.item_id=wr_items.item_id\n order by sr_items.item_id\n         ,sr_item_qty\n limit 100;",
      "optimized_sql": "WITH date_subquery_cte AS (\n  SELECT d_date\n  FROM date_dim\n  WHERE d_month_seq IN (\n    SELECT d_month_seq\n    FROM date_dim\n    WHERE d_date IN ('2002-02-01','2002-04-11','2002-07-17','2002-10-09')\n  )\n),\nsr_items AS (\n  SELECT i_item_id AS item_id, SUM(sr_return_quantity) AS sr_item_qty\n  FROM store_returns\n  INNER JOIN item ON sr_item_sk = i_item_sk\n  INNER JOIN date_dim ON sr_returned_date_sk = d_date_sk\n  WHERE d_date IN (SELECT d_date FROM date_subquery_cte)\n    AND i_category IN ('Jewelry', 'Music')\n    AND i_manager_id BETWEEN 16 AND 25\n    AND sr_reason_sk IN (26, 32, 40, 66, 73)\n    AND sr_return_amt / sr_return_quantity BETWEEN 184 AND 213\n  GROUP BY i_item_id\n),\ncr_items AS (\n  SELECT i_item_id AS item_id, SUM(cr_return_quantity) AS cr_item_qty\n  FROM catalog_returns\n  INNER JOIN item ON cr_item_sk = i_item_sk\n  INNER JOIN date_dim ON cr_returned_date_sk = d_date_sk\n  WHERE d_date IN (SELECT d_date FROM date_subquery_cte)\n    AND i_category IN ('Jewelry', 'Music')\n    AND i_manager_id BETWEEN 16 AND 25\n    AND cr_reason_sk IN (26, 32, 40, 66, 73)\n    AND cr_return_amount / cr_return_quantity BETWEEN 184 AND 213\n  GROUP BY i_item_id\n),\nwr_items AS (\n  SELECT i_item_id AS item_id, SUM(wr_return_quantity) AS wr_item_qty\n  FROM web_returns\n  INNER JOIN item ON wr_item_sk = i_item_sk\n  INNER JOIN date_dim ON wr_returned_date_sk = d_date_sk\n  WHERE d_date IN (SELECT d_date FROM date_subquery_cte)\n    AND i_category IN ('Jewelry', 'Music')\n    AND i_manager_id BETWEEN 16 AND 25\n    AND wr_reason_sk IN (26, 32, 40, 66, 73)\n    AND wr_return_amt / wr_return_quantity BETWEEN 184 AND 213\n  GROUP BY i_item_id\n)\nSELECT sr_items.item_id,\n       sr_item_qty,\n       sr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 AS sr_dev,\n       cr_item_qty,\n       cr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 AS cr_dev,\n       wr_item_qty,\n       wr_item_qty/(sr_item_qty+cr_item_qty+wr_item_qty)/3.0 * 100 AS wr_dev,\n       (sr_item_qty+cr_item_qty+wr_item_qty)/3.0 AS average\nFROM sr_items\nJOIN cr_items ON sr_items.item_id = cr_items.item_id\nJOIN wr_items ON sr_items.item_id = wr_items.item_id\nORDER BY sr_items.item_id, sr_item_qty\nLIMIT 100;",
      "example": {
        "opportunity": "EXPLICIT_JOIN_MATERIALIZED",
        "input_slice": "Complex star-schema query with comma-separated joins across 5+ tables including item, date_dim, store, and fact tables. WHERE clause has item category, date range, and store state filters applied late.",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "explicit_join_materialized",
              "nodes": {
                "filtered_date": "SELECT d_date_sk, d_date, d_week_seq FROM date_dim WHERE d_year = ...",
                "filtered_item": "SELECT i_item_sk, i_item_id, i_item_desc FROM item WHERE i_category IN (...)",
                "main_query": "SELECT ... FROM fact_table JOIN filtered_date ON ... JOIN filtered_item ON ... JOIN store ON ... WHERE ..."
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation"
              ],
              "expected_speedup": "8.5x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Explicit Join + Materialized CTE \u2014 when 5+ tables use comma joins with dimension filters scattered in WHERE, the planner's cardinality estimation breaks down. Fix: (1) extract selective dimensions into MATERIALIZED CTEs (date: 365 rows from 73K, item: hundreds from 300K), (2) convert all comma joins to explicit INNER JOIN. The planner now sees tiny hash tables and chooses optimal join order. This pattern turned a prior 0.49x regression (V1) into an 8.56x win (V2).",
        "pattern_detection": "Look for: 5+ table comma joins with item/date/store dimension filters in WHERE clause. The more tables in comma-separated FROM, the worse the cardinality estimation. Convert to explicit JOINs + materialized dimension CTEs."
      }
    },
    {
      "id": "pg_dimension_prefetch_star",
      "name": "Dimension Pre-filter with Explicit JOINs (PostgreSQL)",
      "description": "On multi-channel UNION queries with comma-separated implicit joins, pre-filter dimension tables (date, item, promotion) into CTEs and convert to explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates and join ordering from explicit JOINs with pre-materialized small dimension results.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "3.32x",
      "principle": "Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.",
      "transforms": [
        "date_cte_isolate",
        "early_filter"
      ],
      "original_sql": "with ssr as\n (select  s_store_id as store_id,\n          sum(ss_ext_sales_price) as sales,\n          sum(coalesce(sr_return_amt, 0)) as returns,\n          sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit\n  from store_sales left outer join store_returns on\n         (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),\n     date_dim,\n     store,\n     item,\n     promotion\n where ss_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n       and ss_store_sk = s_store_sk\n       and ss_item_sk = i_item_sk\n       and i_current_price > 50\n       and ss_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and ss_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\n group by s_store_id)\n ,\n csr as\n (select  cp_catalog_page_id as catalog_page_id,\n          sum(cs_ext_sales_price) as sales,\n          sum(coalesce(cr_return_amount, 0)) as returns,\n          sum(cs_net_profit - coalesce(cr_net_loss, 0)) as profit\n  from catalog_sales left outer join catalog_returns on\n         (cs_item_sk = cr_item_sk and cs_order_number = cr_order_number),\n     date_dim,\n     catalog_page,\n     item,\n     promotion\n where cs_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n        and cs_catalog_page_sk = cp_catalog_page_sk\n       and cs_item_sk = i_item_sk\n       and i_current_price > 50\n       and cs_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and cs_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\ngroup by cp_catalog_page_id)\n ,\n wsr as\n (select  web_site_id,\n          sum(ws_ext_sales_price) as sales,\n          sum(coalesce(wr_return_amt, 0)) as returns,\n          sum(ws_net_profit - coalesce(wr_net_loss, 0)) as profit\n  from web_sales left outer join web_returns on\n         (ws_item_sk = wr_item_sk and ws_order_number = wr_order_number),\n     date_dim,\n     web_site,\n     item,\n     promotion\n where ws_sold_date_sk = d_date_sk\n       and d_date between cast('1998-08-23' as date)\n                  and cast('1998-08-23' as date) + interval '30 day'\n        and ws_web_site_sk = web_site_sk\n       and ws_item_sk = i_item_sk\n       and i_current_price > 50\n       and ws_promo_sk = p_promo_sk\n       and p_channel_email = 'Y'\n       and p_channel_tv = 'Y'\n       and p_channel_radio = 'N'\n       and p_channel_press = 'N'\n       and p_channel_event = 'Y'\n       and ws_wholesale_cost BETWEEN 63 AND 78\n       and i_category IN ('Jewelry', 'Music')\ngroup by web_site_id)\n  select  channel\n        , id\n        , sum(sales) as sales\n        , sum(returns) as returns\n        , sum(profit) as profit\n from\n (select 'store channel' as channel\n        , 'store' || store_id as id\n        , sales\n        , returns\n        , profit\n from   ssr\n union all\n select 'catalog channel' as channel\n        , 'catalog_page' || catalog_page_id as id\n        , sales\n        , returns\n        , profit\n from  csr\n union all\n select 'web channel' as channel\n        , 'web_site' || web_site_id as id\n        , sales\n        , returns\n        , profit\n from   wsr\n ) x\n group by rollup (channel, id)\n order by channel\n         ,id\n limit 100;",
      "optimized_sql": "WITH filtered_date AS (SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1998-08-23' AS DATE) AND CAST('1998-08-23' AS DATE) + INTERVAL '30 DAY'), filtered_item AS (SELECT i_item_sk FROM item WHERE i_current_price > 50 AND i_category IN ('Jewelry', 'Music')), filtered_promotion AS (SELECT p_promo_sk FROM promotion WHERE p_channel_email = 'Y' AND p_channel_tv = 'Y' AND p_channel_radio = 'N' AND p_channel_press = 'N' AND p_channel_event = 'Y'), ssr AS (SELECT s_store_id AS store_id, SUM(ss_ext_sales_price) AS sales, SUM(COALESCE(sr_return_amt, 0)) AS returns, SUM(ss_net_profit - COALESCE(sr_net_loss, 0)) AS profit FROM store_sales LEFT OUTER JOIN store_returns ON (ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number) INNER JOIN filtered_date ON ss_sold_date_sk = filtered_date.d_date_sk INNER JOIN store ON ss_store_sk = s_store_sk INNER JOIN filtered_item ON ss_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ss_promo_sk = filtered_promotion.p_promo_sk WHERE ss_wholesale_cost BETWEEN 63 AND 78 GROUP BY s_store_id), csr AS (SELECT cp_catalog_page_id AS catalog_page_id, SUM(cs_ext_sales_price) AS sales, SUM(COALESCE(cr_return_amount, 0)) AS returns, SUM(cs_net_profit - COALESCE(cr_net_loss, 0)) AS profit FROM catalog_sales LEFT OUTER JOIN catalog_returns ON (cs_item_sk = cr_item_sk AND cs_order_number = cr_order_number) INNER JOIN filtered_date ON cs_sold_date_sk = filtered_date.d_date_sk INNER JOIN catalog_page ON cs_catalog_page_sk = cp_catalog_page_sk INNER JOIN filtered_item ON cs_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON cs_promo_sk = filtered_promotion.p_promo_sk WHERE cs_wholesale_cost BETWEEN 63 AND 78 GROUP BY cp_catalog_page_id), wsr AS (SELECT web_site_id, SUM(ws_ext_sales_price) AS sales, SUM(COALESCE(wr_return_amt, 0)) AS returns, SUM(ws_net_profit - COALESCE(wr_net_loss, 0)) AS profit FROM web_sales LEFT OUTER JOIN web_returns ON (ws_item_sk = wr_item_sk AND ws_order_number = wr_order_number) INNER JOIN filtered_date ON ws_sold_date_sk = filtered_date.d_date_sk INNER JOIN web_site ON ws_web_site_sk = web_site_sk INNER JOIN filtered_item ON ws_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ws_promo_sk = filtered_promotion.p_promo_sk WHERE ws_wholesale_cost BETWEEN 63 AND 78 GROUP BY web_site_id) SELECT channel, id, SUM(sales) AS sales, SUM(returns) AS returns, SUM(profit) AS profit FROM (SELECT 'store channel' AS channel, 'store' || store_id AS id, sales, returns, profit FROM ssr UNION ALL SELECT 'catalog channel' AS channel, 'catalog_page' || catalog_page_id AS id, sales, returns, profit FROM csr UNION ALL SELECT 'web channel' AS channel, 'web_site' || web_site_id AS id, sales, returns, profit FROM wsr) AS x GROUP BY ROLLUP (channel, id) ORDER BY channel, id LIMIT 100",
      "example": {
        "opportunity": "DIMENSION_PREFETCH_STAR",
        "input_slice": "with ssr as\n (select s_store_id as store_id,\n         sum(ss_ext_sales_price) as sales,\n         sum(coalesce(sr_return_amt, 0)) as returns,\n         sum(ss_net_profit - coalesce(sr_net_loss, 0)) as profit\n  from store_sales left outer join store_returns on\n       (ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number),\n     date_dim, store, item, promotion\n where ss_sold_date_sk = d_date_sk\n   and d_date between '1998-08-23' and '1998-08-23'::date + interval '30 day'\n   and ss_store_sk = s_store_sk\n   and ss_item_sk = i_item_sk\n   and i_current_price > 50\n   and ss_promo_sk = p_promo_sk\n   and p_channel_email = 'Y' and p_channel_tv = 'Y'\n   and p_channel_radio = 'N' and p_channel_press = 'N' and p_channel_event = 'Y'\n   and ss_wholesale_cost BETWEEN 63 AND 78\n   and i_category IN ('Jewelry', 'Music')\n group by s_store_id)\n-- ... csr and wsr CTEs similar ...\nselect channel, id, sum(sales), sum(returns), sum(profit)\nfrom (...) x group by rollup(channel, id) order by channel, id limit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "date_cte_isolate + early_filter",
              "nodes": {
                "filtered_date": "SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1998-08-23' AS DATE) AND CAST('1998-08-23' AS DATE) + INTERVAL '30 DAY'",
                "filtered_item": "SELECT i_item_sk FROM item WHERE i_current_price > 50 AND i_category IN ('Jewelry', 'Music')",
                "filtered_promotion": "SELECT p_promo_sk FROM promotion WHERE p_channel_email = 'Y' AND p_channel_tv = 'Y' AND p_channel_radio = 'N' AND p_channel_press = 'N' AND p_channel_event = 'Y'",
                "ssr": "SELECT s_store_id AS store_id, SUM(ss_ext_sales_price) AS sales, SUM(COALESCE(sr_return_amt, 0)) AS returns, SUM(ss_net_profit - COALESCE(sr_net_loss, 0)) AS profit FROM store_sales LEFT OUTER JOIN store_returns ON (ss_item_sk = sr_item_sk AND ss_ticket_number = sr_ticket_number) INNER JOIN filtered_date ON ss_sold_date_sk = filtered_date.d_date_sk INNER JOIN store ON ss_store_sk = s_store_sk INNER JOIN filtered_item ON ss_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ss_promo_sk = filtered_promotion.p_promo_sk WHERE ss_wholesale_cost BETWEEN 63 AND 78 GROUP BY s_store_id",
                "csr": "SELECT cp_catalog_page_id AS catalog_page_id, SUM(cs_ext_sales_price) AS sales, SUM(COALESCE(cr_return_amount, 0)) AS returns, SUM(cs_net_profit - COALESCE(cr_net_loss, 0)) AS profit FROM catalog_sales LEFT OUTER JOIN catalog_returns ON (cs_item_sk = cr_item_sk AND cs_order_number = cr_order_number) INNER JOIN filtered_date ON cs_sold_date_sk = filtered_date.d_date_sk INNER JOIN catalog_page ON cs_catalog_page_sk = cp_catalog_page_sk INNER JOIN filtered_item ON cs_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON cs_promo_sk = filtered_promotion.p_promo_sk WHERE cs_wholesale_cost BETWEEN 63 AND 78 GROUP BY cp_catalog_page_id",
                "wsr": "SELECT web_site_id, SUM(ws_ext_sales_price) AS sales, SUM(COALESCE(wr_return_amt, 0)) AS returns, SUM(ws_net_profit - COALESCE(wr_net_loss, 0)) AS profit FROM web_sales LEFT OUTER JOIN web_returns ON (ws_item_sk = wr_item_sk AND ws_order_number = wr_order_number) INNER JOIN filtered_date ON ws_sold_date_sk = filtered_date.d_date_sk INNER JOIN web_site ON ws_web_site_sk = web_site_sk INNER JOIN filtered_item ON ws_item_sk = filtered_item.i_item_sk INNER JOIN filtered_promotion ON ws_promo_sk = filtered_promotion.p_promo_sk WHERE ws_wholesale_cost BETWEEN 63 AND 78 GROUP BY web_site_id",
                "main_query": "SELECT channel, id, SUM(sales) AS sales, SUM(returns) AS returns, SUM(profit) AS profit FROM (SELECT 'store channel' AS channel, 'store' || store_id AS id, sales, returns, profit FROM ssr UNION ALL SELECT 'catalog channel' AS channel, 'catalog_page' || catalog_page_id AS id, sales, returns, profit FROM csr UNION ALL SELECT 'web channel' AS channel, 'web_site' || web_site_id AS id, sales, returns, profit FROM wsr) AS x GROUP BY ROLLUP (channel, id) ORDER BY channel, id LIMIT 100"
              },
              "node_contracts": {
                "filtered_date": [
                  "d_date_sk"
                ],
                "filtered_item": [
                  "i_item_sk"
                ],
                "filtered_promotion": [
                  "p_promo_sk"
                ],
                "ssr": [
                  "store_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "csr": [
                  "catalog_page_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "wsr": [
                  "web_site_id",
                  "sales",
                  "returns",
                  "profit"
                ],
                "main_query": [
                  "channel",
                  "id",
                  "sales",
                  "returns",
                  "profit"
                ]
              },
              "data_flow": "filtered_date, filtered_item, filtered_promotion -> ssr, csr, wsr -> main_query",
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ROLLUP"
              ],
              "expected_speedup": "3.3x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Multi-Dimension Prefetch (PG) \u2014 pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax for PostgreSQL optimizer join-order freedom. Even partial transformation helps when one branch dominates runtime. Here: date (30/73K), item (2 categories), promotion (5 filters) all become CTEs; comma joins converted to INNER JOIN.",
        "pattern_detection": "Look for multi-channel queries (store/catalog/web) using comma-separated implicit joins with shared dimension filters (date range, item category, promotion flags). These queries benefit from extracting shared dimension filters into CTEs and converting to explicit JOINs."
      }
    },
    {
      "id": "pg_single_pass_aggregation",
      "name": "Single-Pass Channel Aggregation (PostgreSQL)",
      "description": "Consolidate multiple fact table scans (store_sales, catalog_sales, web_sales) into a single UNION ALL CTE, then use shared item/dimension filters to reduce redundant I/O. The consolidated CTE scans each fact table once instead of N times across N INTERSECT branches.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "1.98x",
      "principle": "Single-Pass Channel Aggregation: when a query scans store_sales, catalog_sales, and web_sales with identical item/date filters for INTERSECT or cross-channel comparison, consolidate all 3 scans into a single UNION ALL CTE with a channel discriminator column. Downstream queries filter by channel tag instead of re-scanning.",
      "transforms": [
        "single_pass_aggregation"
      ],
      "original_sql": "with  cross_items as\n (select i_item_sk ss_item_sk\n from item,\n (select iss.i_brand_id brand_id\n     ,iss.i_class_id class_id\n     ,iss.i_category_id category_id\n from store_sales\n     ,item iss\n     ,date_dim d1\n where ss_item_sk = iss.i_item_sk\n   and ss_sold_date_sk = d1.d_date_sk\n   and d1.d_year between 1999 AND 1999 + 2\n   and i_category IN ('Electronics', 'Jewelry', 'Men')\n   and i_manager_id BETWEEN 91 and 100\n   and ss_wholesale_cost BETWEEN 35 AND 55\nintersect\n select ics.i_brand_id\n     ,ics.i_class_id\n     ,ics.i_category_id\n from catalog_sales\n     ,item ics\n     ,date_dim d2\n where cs_item_sk = ics.i_item_sk\n   and cs_sold_date_sk = d2.d_date_sk\n   and d2.d_year between 1999 AND 1999 + 2\n   and i_category IN ('Electronics', 'Jewelry', 'Men')\n   and i_manager_id BETWEEN 91 and 100\n   and cs_wholesale_cost BETWEEN 35 AND 55\nintersect\n select iws.i_brand_id\n     ,iws.i_class_id\n     ,iws.i_category_id\n from web_sales\n     ,item iws\n     ,date_dim d3\n where ws_item_sk = iws.i_item_sk\n   and ws_sold_date_sk = d3.d_date_sk\n   and ws_wholesale_cost BETWEEN 35 AND 55\n   and d3.d_year between 1999 AND 1999 + 2) x\n where i_brand_id = brand_id\n      and i_class_id = class_id\n      and i_category_id = category_id\n      and i_category IN ('Electronics', 'Jewelry', 'Men')\n      and i_manager_id BETWEEN 91 and 100\n),\n avg_sales as\n(select avg(quantity*list_price) average_sales\n  from (select ss_quantity quantity\n             ,ss_list_price list_price\n       from store_sales\n           ,date_dim\n       where ss_sold_date_sk = d_date_sk\n         and d_year between 1999 and 1999 + 2\n         and ss_wholesale_cost BETWEEN 35 AND 55\n       union all\n       select cs_quantity quantity\n             ,cs_list_price list_price\n       from catalog_sales\n           ,date_dim\n       where cs_sold_date_sk = d_date_sk\n         and d_year between 1999 and 1999 + 2\n         and cs_wholesale_cost BETWEEN 35 AND 55\n       union all\n       select ws_quantity quantity\n             ,ws_list_price list_price\n       from web_sales\n           ,date_dim\n       where ws_sold_date_sk = d_date_sk\n        and ws_wholesale_cost BETWEEN 35 AND 55\n         and d_year between 1999 and 1999 + 2) x)\n  select  this_year.channel ty_channel\n                           ,this_year.i_brand_id ty_brand\n                           ,this_year.i_class_id ty_class\n                           ,this_year.i_category_id ty_category\n                           ,this_year.sales ty_sales\n                           ,this_year.number_sales ty_number_sales\n                           ,last_year.channel ly_channel\n                           ,last_year.i_brand_id ly_brand\n                           ,last_year.i_class_id ly_class\n                           ,last_year.i_category_id ly_category\n                           ,last_year.sales ly_sales\n                           ,last_year.number_sales ly_number_sales\n from\n (select 'store' channel, i_brand_id,i_class_id,i_category_id\n        ,sum(ss_quantity*ss_list_price) sales, count(*) number_sales\n from store_sales\n     ,item\n     ,date_dim\n where ss_item_sk in (select ss_item_sk from cross_items)\n   and ss_item_sk = i_item_sk\n   and ss_sold_date_sk = d_date_sk\n   and d_week_seq = (select d_week_seq\n                     from date_dim\n                     where d_year = 1999 + 1\n                       and d_moy = 12\n                       and d_dom = 20)\n   and i_category IN ('Electronics', 'Jewelry', 'Men')\n   and i_manager_id BETWEEN 91 and 100\n   and ss_wholesale_cost BETWEEN 35 AND 55\n group by i_brand_id,i_class_id,i_category_id\n having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)) this_year,\n (select 'store' channel, i_brand_id,i_class_id\n        ,i_category_id, sum(ss_quantity*ss_list_price) sales, count(*) number_sales\n from store_sales\n     ,item\n     ,date_dim\n where ss_item_sk in (select ss_item_sk from cross_items)\n   and ss_item_sk = i_item_sk\n   and ss_sold_date_sk = d_date_sk\n   and d_week_seq = (select d_week_seq\n                     from date_dim\n                     where d_year = 1999\n                       and d_moy = 12\n                       and d_dom = 20)\n   and i_category IN ('Electronics', 'Jewelry', 'Men')\n   and ss_wholesale_cost BETWEEN 35 AND 55\n   and i_manager_id BETWEEN 91 and 100\ngroup by i_brand_id,i_class_id,i_category_id\n having sum(ss_quantity*ss_list_price) > (select average_sales from avg_sales)) last_year\n where this_year.i_brand_id= last_year.i_brand_id\n   and this_year.i_class_id = last_year.i_class_id\n   and this_year.i_category_id = last_year.i_category_id\n order by this_year.channel, this_year.i_brand_id, this_year.i_class_id, this_year.i_category_id\n limit 100;",
      "optimized_sql": "WITH consolidated_sales AS (\n  SELECT 'store' AS channel, ss_item_sk AS item_sk, i_brand_id, i_class_id,\n         i_category_id, ss_quantity AS quantity, ss_list_price AS list_price,\n         d_date_sk, d_week_seq\n  FROM store_sales\n  JOIN item ON ss_item_sk = i_item_sk\n  JOIN date_dim ON ss_sold_date_sk = d_date_sk\n  WHERE d_year BETWEEN 1999 AND 2001\n    AND i_category IN ('Electronics', 'Jewelry', 'Men')\n    AND i_manager_id BETWEEN 91 AND 100\n    AND ss_wholesale_cost BETWEEN 35 AND 55\n  UNION ALL\n  SELECT 'catalog', cs_item_sk, i_brand_id, i_class_id, i_category_id,\n         cs_quantity, cs_list_price, d_date_sk, d_week_seq\n  FROM catalog_sales\n  JOIN item ON cs_item_sk = i_item_sk\n  JOIN date_dim ON cs_sold_date_sk = d_date_sk\n  WHERE d_year BETWEEN 1999 AND 2001\n    AND i_category IN ('Electronics', 'Jewelry', 'Men')\n    AND i_manager_id BETWEEN 91 AND 100\n    AND cs_wholesale_cost BETWEEN 35 AND 55\n  UNION ALL\n  SELECT 'web', ws_item_sk, i_brand_id, i_class_id, i_category_id,\n         ws_quantity, ws_list_price, d_date_sk, d_week_seq\n  FROM web_sales\n  JOIN item ON ws_item_sk = i_item_sk\n  JOIN date_dim ON ws_sold_date_sk = d_date_sk\n  WHERE d_year BETWEEN 1999 AND 2001\n    AND i_category IN ('Electronics', 'Jewelry', 'Men')\n    AND i_manager_id BETWEEN 91 AND 100\n    AND ws_wholesale_cost BETWEEN 35 AND 55\n),\ncross_items AS (\n  SELECT i_item_sk AS ss_item_sk\n  FROM item\n  WHERE i_category IN ('Electronics', 'Jewelry', 'Men')\n    AND i_manager_id BETWEEN 91 AND 100\n    AND (i_brand_id, i_class_id, i_category_id) IN (\n      SELECT i_brand_id, i_class_id, i_category_id\n      FROM consolidated_sales WHERE channel = 'store'\n      INTERSECT\n      SELECT i_brand_id, i_class_id, i_category_id\n      FROM consolidated_sales WHERE channel = 'catalog'\n      INTERSECT\n      SELECT i_brand_id, i_class_id, i_category_id\n      FROM consolidated_sales WHERE channel = 'web'\n    )\n),\navg_sales AS (\n  SELECT AVG(quantity * list_price) AS average_sales\n  FROM consolidated_sales\n)\nSELECT 'store' AS channel,\n       ty.i_brand_id, ty.i_class_id, ty.i_category_id,\n       ty.sales AS this_year_sales, ty.number_sales AS this_year_count,\n       ly.sales AS last_year_sales, ly.number_sales AS last_year_count\nFROM (\n  SELECT i_brand_id, i_class_id, i_category_id,\n         SUM(quantity * list_price) AS sales, COUNT(*) AS number_sales\n  FROM consolidated_sales cs\n  JOIN cross_items ci ON cs.item_sk = ci.ss_item_sk\n  WHERE cs.d_week_seq = (SELECT d_week_seq FROM date_dim\n                         WHERE d_year = 2000 AND d_moy = 12 AND d_dom = 20)\n  GROUP BY i_brand_id, i_class_id, i_category_id\n  HAVING SUM(quantity * list_price) > (SELECT average_sales FROM avg_sales)\n) ty\nJOIN (\n  SELECT i_brand_id, i_class_id, i_category_id,\n         SUM(quantity * list_price) AS sales, COUNT(*) AS number_sales\n  FROM consolidated_sales cs\n  JOIN cross_items ci ON cs.item_sk = ci.ss_item_sk\n  WHERE cs.d_week_seq = (SELECT d_week_seq FROM date_dim\n                         WHERE d_year = 1999 AND d_moy = 12 AND d_dom = 20)\n  GROUP BY i_brand_id, i_class_id, i_category_id\n  HAVING SUM(quantity * list_price) > (SELECT average_sales FROM avg_sales)\n) ly ON ty.i_brand_id = ly.i_brand_id\n   AND ty.i_class_id = ly.i_class_id\n   AND ty.i_category_id = ly.i_category_id\nORDER BY channel, ty.i_brand_id, ty.i_class_id, ty.i_category_id\nLIMIT 100;",
      "example": {
        "opportunity": "SINGLE_PASS_AGGREGATION",
        "input_slice": "3 separate INTERSECT branches scanning store_sales + item + date_dim, catalog_sales + item + date_dim, web_sales + item + date_dim with identical item/date/cost filters",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "single_pass_aggregation",
              "nodes": {
                "consolidated_sales": "UNION ALL of all 3 channels with channel discriminator column, shared item/date/cost filters",
                "cross_items": "INTERSECT on consolidated_sales filtered by channel tag",
                "avg_sales": "AVG computed from consolidated_sales (single pass)"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation"
              ],
              "expected_speedup": "2.0x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Single-Pass Channel Aggregation \u2014 when store_sales, catalog_sales, and web_sales are scanned with identical item/date filters for INTERSECT or cross-channel comparison, consolidate into one UNION ALL CTE with a 'channel' discriminator. Downstream INTERSECT operates on the same CTE filtered by channel tag, avoiding 3x redundant dimension joins.",
        "pattern_detection": "Look for: (1) 3 INTERSECT branches or 3 UNION ALL subqueries, (2) each scanning a different fact table (store/catalog/web), (3) identical item + date + cost filters across all 3. The consolidation reduces 9 dimension joins to 3."
      }
    },
    {
      "id": "pg_self_join_pivot",
      "name": "Self-Join Elimination via Pivot (PostgreSQL)",
      "description": "When a query self-joins the same fact aggregation 6 times (e.g., ss1, ss2, ss3, ws1, ws2, ws3 for 3 quarters \u00d7 2 channels), materialize the fact+dimension scan once and use CASE/FILTER aggregation to pivot quarters in a single pass. Eliminates 5 redundant fact table scans.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "1.79x",
      "principle": "Self-Join Elimination via Pivot: when a query computes the same aggregation across N time periods by self-joining N copies of the same CTE, materialize the base scan once and pivot time periods using CASE WHEN or FILTER (WHERE quarter = X) aggregation.",
      "transforms": [
        "single_pass_aggregation"
      ],
      "original_sql": "with ss as\n (select ca_county,d_qoy, d_year,sum(ss_ext_sales_price) as store_sales\n from store_sales,date_dim,customer_address, item\n where ss_sold_date_sk = d_date_sk\n  and ss_addr_sk=ca_address_sk\n  and ss_item_sk = i_item_sk\n  and i_color IN ('blanched', 'rosy')\n  and i_manager_id BETWEEN 16 and 35\n  and ss_list_price between 286 and 300\n  and ca_state in ('TX','VA')\n group by ca_county,d_qoy, d_year),\n ws as\n (select ca_county,d_qoy, d_year,sum(ws_ext_sales_price) as web_sales\n from web_sales,date_dim,customer_address, item\n where ws_sold_date_sk = d_date_sk\n  and ws_bill_addr_sk=ca_address_sk\n  and ws_item_sk = i_item_sk\n  and i_color IN ('blanched', 'rosy')\n  and i_manager_id BETWEEN 16 and 35\n  and ws_list_price between 286 and 300\n  and ca_state in ('TX','VA')\ngroup by ca_county,d_qoy, d_year)\n select\n        ss1.ca_county\n       ,ss1.d_year\n       ,ws2.web_sales/ws1.web_sales web_q1_q2_increase\n       ,ss2.store_sales/ss1.store_sales store_q1_q2_increase\n       ,ws3.web_sales/ws2.web_sales web_q2_q3_increase\n       ,ss3.store_sales/ss2.store_sales store_q2_q3_increase\n from\n        ss ss1\n       ,ss ss2\n       ,ss ss3\n       ,ws ws1\n       ,ws ws2\n       ,ws ws3\n where\n    ss1.d_qoy = 1\n    and ss1.d_year = 1998\n    and ss1.ca_county = ss2.ca_county\n    and ss2.d_qoy = 2\n    and ss2.d_year = 1998\n and ss2.ca_county = ss3.ca_county\n    and ss3.d_qoy = 3\n    and ss3.d_year = 1998\n    and ss1.ca_county = ws1.ca_county\n    and ws1.d_qoy = 1\n    and ws1.d_year = 1998\n    and ws1.ca_county = ws2.ca_county\n    and ws2.d_qoy = 2\n    and ws2.d_year = 1998\n    and ws1.ca_county = ws3.ca_county\n    and ws3.d_qoy = 3\n    and ws3.d_year =1998\n    and case when ws1.web_sales > 0 then ws2.web_sales/ws1.web_sales else null end\n       > case when ss1.store_sales > 0 then ss2.store_sales/ss1.store_sales else null end\n    and case when ws2.web_sales > 0 then ws3.web_sales/ws2.web_sales else null end\n       > case when ss2.store_sales > 0 then ss3.store_sales/ss2.store_sales else null end\n order by web_q1_q2_increase;",
      "optimized_sql": "WITH ss_all_quarters AS (\n  SELECT ca_county, d_qoy, d_year,\n         SUM(ss_ext_sales_price) AS store_sales\n  FROM store_sales, date_dim, customer_address, item\n  WHERE ss_sold_date_sk = d_date_sk\n    AND ss_addr_sk = ca_address_sk\n    AND ss_item_sk = i_item_sk\n    AND i_color IN ('blanched', 'rosy')\n    AND i_manager_id BETWEEN 16 AND 35\n    AND ss_list_price BETWEEN 286 AND 300\n    AND ca_state IN ('TX', 'VA')\n    AND d_year = 1998 AND d_qoy IN (1, 2, 3)\n  GROUP BY ca_county, d_qoy, d_year\n),\nss_pivot AS (\n  SELECT ca_county, d_year,\n         MAX(CASE WHEN d_qoy = 1 THEN store_sales END) AS store_sales_q1,\n         MAX(CASE WHEN d_qoy = 2 THEN store_sales END) AS store_sales_q2,\n         MAX(CASE WHEN d_qoy = 3 THEN store_sales END) AS store_sales_q3\n  FROM ss_all_quarters\n  GROUP BY ca_county, d_year\n),\nws_all_quarters AS (\n  SELECT ca_county, d_qoy, d_year,\n         SUM(ws_ext_sales_price) AS web_sales\n  FROM web_sales, date_dim, customer_address, item\n  WHERE ws_sold_date_sk = d_date_sk\n    AND ws_bill_addr_sk = ca_address_sk\n    AND ws_item_sk = i_item_sk\n    AND i_color IN ('blanched', 'rosy')\n    AND i_manager_id BETWEEN 16 AND 35\n    AND ws_list_price BETWEEN 286 AND 300\n    AND ca_state IN ('TX', 'VA')\n    AND d_year = 1998 AND d_qoy IN (1, 2, 3)\n  GROUP BY ca_county, d_qoy, d_year\n),\nws_pivot AS (\n  SELECT ca_county, d_year,\n         MAX(CASE WHEN d_qoy = 1 THEN web_sales END) AS web_sales_q1,\n         MAX(CASE WHEN d_qoy = 2 THEN web_sales END) AS web_sales_q2,\n         MAX(CASE WHEN d_qoy = 3 THEN web_sales END) AS web_sales_q3\n  FROM ws_all_quarters\n  GROUP BY ca_county, d_year\n)\nSELECT ss_pivot.ca_county, ss_pivot.d_year,\n       CASE WHEN web_sales_q1 > 0 THEN web_sales_q2/web_sales_q1 ELSE NULL END AS web_q1_q2_increase,\n       CASE WHEN store_sales_q1 > 0 THEN store_sales_q2/store_sales_q1 ELSE NULL END AS store_q1_q2_increase,\n       CASE WHEN web_sales_q2 > 0 THEN web_sales_q3/web_sales_q2 ELSE NULL END AS web_q2_q3_increase,\n       CASE WHEN store_sales_q2 > 0 THEN store_sales_q3/store_sales_q2 ELSE NULL END AS store_q2_q3_increase\nFROM ss_pivot\nJOIN ws_pivot ON ss_pivot.ca_county = ws_pivot.ca_county\nWHERE CASE WHEN web_sales_q1 > 0 THEN web_sales_q2/web_sales_q1 ELSE NULL END\n      > CASE WHEN store_sales_q1 > 0 THEN store_sales_q2/store_sales_q1 ELSE NULL END\n  AND CASE WHEN web_sales_q2 > 0 THEN web_sales_q3/web_sales_q2 ELSE NULL END\n      > CASE WHEN store_sales_q2 > 0 THEN store_sales_q3/store_sales_q2 ELSE NULL END\nORDER BY web_q1_q2_increase;",
      "example": {
        "opportunity": "SELF_JOIN_PIVOT",
        "input_slice": "select ss1.column_name, ss1.value, ss2.value, ss3.value, ws1.value, ws2.value, ws3.value\nfrom (\n  select ..., sum(sales) as value from store_sales, date_dim, store\n  where d_qoy = 1 and d_year = 2000 ... group by ...\n) ss1,\n(\n  select ..., sum(sales) as value from store_sales, date_dim, store\n  where d_qoy = 2 and d_year = 2000 ... group by ...\n) ss2,\n... -- 6 self-join aliases total",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "single_pass_aggregation",
              "nodes": {
                "base_scan": "SELECT store_sk, d_qoy, SUM(sales) AS value FROM store_sales JOIN date_dim ON ... JOIN store ON ... WHERE d_year = 2000 AND d_qoy IN (1,2,3) GROUP BY store_sk, d_qoy",
                "pivoted": "SELECT store_sk, SUM(CASE WHEN d_qoy = 1 THEN value END) AS q1, SUM(CASE WHEN d_qoy = 2 THEN value END) AS q2, SUM(CASE WHEN d_qoy = 3 THEN value END) AS q3 FROM base_scan GROUP BY store_sk"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation"
              ],
              "expected_speedup": "1.8x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Self-Join Elimination via Pivot \u2014 when N self-join aliases scan the same fact table with different discriminator values (quarter, year, channel), scan once with all discriminators and pivot via CASE WHEN aggregation. CAUTION: do NOT apply dimension prefetch to self-join patterns (0.25x Q031_i1 regression). The bottleneck is the N-way self-join, not the dimension filtering.",
        "pattern_detection": "Look for: 4+ self-join aliases of the same fact aggregation subquery, distinguished by a single discriminator column (d_qoy, d_year, channel). Each alias computes the same SUM/AVG/COUNT but for a different time period or channel."
      }
    },
    {
      "id": "pg_self_join_decomposition",
      "name": "Self-Join Decomposition (PostgreSQL)",
      "description": "Eliminate duplicate fact table scans in self-join patterns by computing the aggregation ONCE in a CTE and deriving both per-item and per-store averages from the same materialized result. PostgreSQL materializes CTEs by default, making this extremely effective.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "3.93x",
      "principle": "Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.",
      "transforms": [
        "materialize_cte"
      ],
      "original_sql": "select \n\ts_store_name,\n\ti_item_desc,\n\tsc.revenue,\n\ti_current_price,\n\ti_wholesale_cost,\n\ti_brand\n from store, item,\n     (select ss_store_sk, avg(revenue) as ave\n\tfrom\n\t    (select  ss_store_sk, ss_item_sk,\n\t\t     sum(ss_sales_price) as revenue\n\t\tfrom store_sales, date_dim\n\t\twhere ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11\n   and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01\n\t\tgroup by ss_store_sk, ss_item_sk) sa\n\tgroup by ss_store_sk) sb,\n     (select  ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n\tfrom store_sales, date_dim\n\twhere ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1213+11\n  and ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01\n\tgroup by ss_store_sk, ss_item_sk) sc\n where sb.ss_store_sk = sc.ss_store_sk and\n       sc.revenue <= 0.1 * sb.ave and\n       s_store_sk = sc.ss_store_sk and\n       i_item_sk = sc.ss_item_sk\n       and i_manager_id BETWEEN 32 and 36\n       and s_state in ('TN','TX','VA')\n order by s_store_name, i_item_desc\nlimit 100;",
      "optimized_sql": "WITH date_filter AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1213 + 11), store_sales_revenue AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON store_sales.ss_sold_date_sk = date_filter.d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 38 * 0.01 AND 48 * 0.01 GROUP BY ss_store_sk, ss_item_sk), store_avg_revenue AS (SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk), filtered_store AS (SELECT s_store_sk, s_store_name, s_state FROM store WHERE s_state IN ('TN', 'TX', 'VA')), filtered_item AS (SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand, i_manager_id FROM item WHERE i_manager_id BETWEEN 32 AND 36) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100",
      "example": {
        "opportunity": "SELF_JOIN_DECOMPOSITION",
        "input_slice": "select s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand\nfrom store, item,\n  (select ss_store_sk, avg(revenue) as ave\n   from (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n         from store_sales, date_dim\n         where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224\n           and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48\n         group by ss_store_sk, ss_item_sk) sa\n   group by ss_store_sk) sb,\n  (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue\n   from store_sales, date_dim\n   where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224\n     and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48\n   group by ss_store_sk, ss_item_sk) sc\nwhere sb.ss_store_sk = sc.ss_store_sk\n  and sc.revenue <= 0.1 * sb.ave\n  and s_store_sk = sc.ss_store_sk\n  and i_item_sk = sc.ss_item_sk\n  and i_manager_id BETWEEN 32 AND 36\n  and s_state in ('TN','TX','VA')\norder by s_store_name, i_item_desc\nlimit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "materialize_cte",
              "nodes": {
                "date_filter": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224",
                "store_sales_revenue": "SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk",
                "store_avg_revenue": "SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk",
                "filtered_store": "SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')",
                "filtered_item": "SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36",
                "main_query": "SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering"
              ],
              "expected_speedup": "3.9x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Shared Materialization (PG) \u2014 when the same fact+dimension scan appears multiple times, materialize it once as a CTE and reference it from each consumer. PostgreSQL CTE materialization guarantees single execution. Here: store_sales+date_dim scanned twice with identical predicates becomes one materialized CTE, reused for both per-item revenue and per-store averages. Combined with dimension pre-filtering to reduce I/O.",
        "pattern_detection": "Look for queries where the same fact table + date_dim join appears in two or more subqueries with identical WHERE predicates. The subqueries typically compute per-group aggregates at different granularities (e.g., per-item vs per-store)."
      }
    },
    {
      "id": "early_filter_decorrelate",
      "name": "Early Filter + Decorrelate",
      "database": "postgres",
      "verified_speedup": "27.80x (V2 DSB SF10, was 1.13x in V1)",
      "principle": "Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.",
      "benchmark": {
        "dataset": "dsb_sf10",
        "query": "query001",
        "original_time_s": 13.43,
        "optimized_time_s": 11.85
      },
      "ast_flags": {
        "kb_patterns_detected": [
          {
            "id": "CORRELATED_TO_CTE",
            "name": "Correlated Subquery to Pre-computed CTE",
            "trigger": "WHERE col > (SELECT AVG/SUM/COUNT FROM ... WHERE correlated)"
          },
          {
            "id": "DATE_CTE_ISOLATION",
            "name": "Date CTE Isolation",
            "trigger": "date_dim joined with d_year/d_qoy/d_month filter, fact table present"
          }
        ],
        "structural_patterns": [
          "CTE with correlated subquery reference (ctr1 -> ctr2)",
          "Dimension filter (s_state) applied AFTER CTE in main query",
          "AVG aggregate in correlated subquery with GROUP BY correlation key"
        ]
      },
      "original_sql": "WITH customer_total_return AS (\n  SELECT sr_customer_sk AS ctr_customer_sk,\n         sr_store_sk AS ctr_store_sk,\n         sr_reason_sk AS ctr_reason_sk,\n         SUM(SR_REFUNDED_CASH) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year = 2001\n    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer, customer_demographics\nWHERE ctr1.ctr_total_return > (\n    SELECT AVG(ctr_total_return) * 1.2\n    FROM customer_total_return ctr2\n    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk\n  )\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state IN ('MI', 'NC', 'WI')\n  AND ctr1.ctr_customer_sk = c_customer_sk\n  AND c_current_cdemo_sk = cd_demo_sk\n  AND cd_marital_status IN ('W', 'W')\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100",
      "optimized_sql": "WITH customer_total_return AS (\n    SELECT sr_customer_sk AS ctr_customer_sk,\n           sr_store_sk AS ctr_store_sk,\n           sr_reason_sk AS ctr_reason_sk,\n           SUM(SR_REFUNDED_CASH) AS ctr_total_return\n    FROM store_returns\n    JOIN date_dim ON sr_returned_date_sk = d_date_sk\n    JOIN store ON sr_store_sk = s_store_sk\n    WHERE d_year = 2001\n      AND s_state IN ('MI', 'NC', 'WI')\n      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n),\nstore_thresholds AS (\n    SELECT ctr_store_sk,\n           AVG(ctr_total_return) * 1.2 AS avg_limit\n    FROM customer_total_return\n    GROUP BY ctr_store_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1\nJOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk\nJOIN customer ON ctr1.ctr_customer_sk = c_customer_sk\nJOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk\nJOIN store s ON ctr1.ctr_store_sk = s.s_store_sk\nWHERE ctr1.ctr_total_return > st.avg_limit\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s.s_state IN ('MI', 'NC', 'WI')\n  AND cd_marital_status = 'W'\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100",
      "pg_blind_spots": [
        "Cannot push filters into materialized CTEs",
        "Correlated subqueries may execute row-by-row on large CTEs"
      ],
      "input": {
        "description": "CTE aggregates fact table, main query has correlated subquery for AVG threshold, dimension filter applied late",
        "sql": "WITH customer_total_return AS (\n  SELECT sr_customer_sk AS ctr_customer_sk,\n         sr_store_sk AS ctr_store_sk,\n         sr_reason_sk AS ctr_reason_sk,\n         SUM(SR_REFUNDED_CASH) AS ctr_total_return\n  FROM store_returns, date_dim\n  WHERE sr_returned_date_sk = d_date_sk\n    AND d_year = 2001\n    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1, store, customer, customer_demographics\nWHERE ctr1.ctr_total_return > (\n    SELECT AVG(ctr_total_return) * 1.2\n    FROM customer_total_return ctr2\n    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk\n  )\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s_store_sk = ctr1.ctr_store_sk\n  AND s_state IN ('MI', 'NC', 'WI')\n  AND ctr1.ctr_customer_sk = c_customer_sk\n  AND c_current_cdemo_sk = cd_demo_sk\n  AND cd_marital_status IN ('W', 'W')\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100"
      },
      "output": {
        "description": "Push dimension filter INTO CTE, decorrelate AVG into separate CTE, JOIN on threshold",
        "sql": "WITH customer_total_return AS (\n    SELECT sr_customer_sk AS ctr_customer_sk,\n           sr_store_sk AS ctr_store_sk,\n           sr_reason_sk AS ctr_reason_sk,\n           SUM(SR_REFUNDED_CASH) AS ctr_total_return\n    FROM store_returns\n    JOIN date_dim ON sr_returned_date_sk = d_date_sk\n    JOIN store ON sr_store_sk = s_store_sk\n    WHERE d_year = 2001\n      AND s_state IN ('MI', 'NC', 'WI')\n      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295\n    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk\n),\nstore_thresholds AS (\n    SELECT ctr_store_sk,\n           AVG(ctr_total_return) * 1.2 AS avg_limit\n    FROM customer_total_return\n    GROUP BY ctr_store_sk\n)\nSELECT c_customer_id\nFROM customer_total_return ctr1\nJOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk\nJOIN customer ON ctr1.ctr_customer_sk = c_customer_sk\nJOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk\nJOIN store s ON ctr1.ctr_store_sk = s.s_store_sk\nWHERE ctr1.ctr_total_return > st.avg_limit\n  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31\n  AND s.s_state IN ('MI', 'NC', 'WI')\n  AND cd_marital_status = 'W'\n  AND cd_education_status IN ('4 yr Degree', 'College')\n  AND cd_gender = 'M'\n  AND c_birth_month = 5\n  AND c_birth_year BETWEEN 1950 AND 1956\nORDER BY c_customer_id\nLIMIT 100"
      },
      "transforms_applied": [
        "early_filter: Push s_state filter INTO CTE before aggregation",
        "decorrelate: Convert correlated AVG subquery to separate CTE with GROUP BY",
        "join_rewrite: Replace correlated lookup with JOIN on pre-computed threshold"
      ],
      "key_insight": "Principle: Early Selection + Decorrelation \u2014 push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN. Here: dimension filters pushed into CTEs, AVG threshold pre-computed and JOINed."
    },
    {
      "id": "pg_materialized_dimension_fact_prefilter",
      "name": "MATERIALIZED Dimension + Fact Pre-filter (PostgreSQL)",
      "description": "Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then join with explicit JOIN syntax. On queries with expensive non-equi joins (inventory quantity < sales quantity, week_seq correlation), reducing both dimension AND fact table sizes before the join dramatically cuts the search space. The MATERIALIZED keyword on PG12+ forces early execution of each CTE.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "12.07x (V2 DSB SF10, was 2.68x in V1)",
      "principle": "Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.",
      "transforms": [
        "early_filter",
        "date_cte_isolate"
      ],
      "original_sql": "select  i_item_desc\n      ,w_warehouse_name\n      ,d1.d_week_seq\n      ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo\n      ,sum(case when p_promo_sk is not null then 1 else 0 end) promo\n      ,count(*) total_cnt\nfrom catalog_sales\njoin inventory on (cs_item_sk = inv_item_sk)\njoin warehouse on (w_warehouse_sk=inv_warehouse_sk)\njoin item on (i_item_sk = cs_item_sk)\njoin customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)\njoin household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)\njoin date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)\njoin date_dim d2 on (inv_date_sk = d2.d_date_sk)\njoin date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)\nleft outer join promotion on (cs_promo_sk=p_promo_sk)\nleft outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)\nwhere d1.d_week_seq = d2.d_week_seq\n  and inv_quantity_on_hand < cs_quantity\n  and d3.d_date > d1.d_date + interval '3 day'\n  and hd_buy_potential = '501-1000'\n  and d1.d_year = 1998\n  and cd_marital_status = 'M'\n  and cd_dep_count between 9 and 11\n  and i_category IN ('Home', 'Men', 'Music')\n  and cs_wholesale_cost BETWEEN 34 AND 54\ngroup by i_item_desc,w_warehouse_name,d1.d_week_seq\norder by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq\nlimit 100;",
      "optimized_sql": "WITH filtered_date AS MATERIALIZED (\n  SELECT d_date_sk, d_date, d_week_seq\n  FROM date_dim\n  WHERE d_year = 1998\n),\nfiltered_item AS MATERIALIZED (\n  SELECT i_item_sk, i_item_desc\n  FROM item\n  WHERE i_category IN ('Home', 'Men', 'Music')\n),\nfiltered_cd AS MATERIALIZED (\n  SELECT cd_demo_sk\n  FROM customer_demographics\n  WHERE cd_marital_status = 'M'\n    AND cd_dep_count BETWEEN 9 AND 11\n),\nfiltered_hd AS MATERIALIZED (\n  SELECT hd_demo_sk\n  FROM household_demographics\n  WHERE hd_buy_potential = '501-1000'\n),\ncs_filtered AS MATERIALIZED (\n  SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk,\n         cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost,\n         cs_order_number\n  FROM catalog_sales\n  WHERE cs_wholesale_cost BETWEEN 34 AND 54\n)\nSELECT i.i_item_desc,\n       w.w_warehouse_name,\n       d1.d_week_seq,\n       SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo,\n       SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo,\n       COUNT(*) AS total_cnt\nFROM cs_filtered cs\nJOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk\nJOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk\nJOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk\nJOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk\nJOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk\nJOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk\nJOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk\nJOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk\nLEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk\nLEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk \n  AND cr.cr_order_number = cs.cs_order_number\nWHERE d1.d_week_seq = d2.d_week_seq\n  AND inv.inv_quantity_on_hand < cs.cs_quantity\n  AND d3.d_date > d1.d_date + INTERVAL '3 day'\nGROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq\nORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq\nLIMIT 100;",
      "example": {
        "opportunity": "MATERIALIZED_DIMENSION_FACT_PREFILTER",
        "input_slice": "select i_item_desc, w_warehouse_name, d1.d_week_seq,\n  sum(case when p_promo_sk is null then 1 else 0 end) no_promo,\n  sum(case when p_promo_sk is not null then 1 else 0 end) promo,\n  count(*) total_cnt\nfrom catalog_sales\njoin inventory on (cs_item_sk = inv_item_sk)\njoin warehouse on (w_warehouse_sk=inv_warehouse_sk)\njoin item on (i_item_sk = cs_item_sk)\njoin customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)\njoin household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)\njoin date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)\njoin date_dim d2 on (inv_date_sk = d2.d_date_sk)\njoin date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)\nleft outer join promotion on (cs_promo_sk=p_promo_sk)\nleft outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)\nwhere d1.d_week_seq = d2.d_week_seq\n  and inv_quantity_on_hand < cs_quantity\n  and d3.d_date > d1.d_date + interval '3 day'\n  and hd_buy_potential = '501-1000'\n  and d1.d_year = 1998\n  and cd_marital_status = 'M'\n  and cd_dep_count between 9 and 11\n  and i_category IN ('Home', 'Men', 'Music')\n  and cs_wholesale_cost BETWEEN 34 AND 54\ngroup by i_item_desc,w_warehouse_name,d1.d_week_seq\norder by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq\nlimit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "early_filter",
              "nodes": {
                "filtered_date": "SELECT d_date_sk, d_date, d_week_seq FROM date_dim WHERE d_year = 1998",
                "filtered_item": "SELECT i_item_sk, i_item_desc FROM item WHERE i_category IN ('Home', 'Men', 'Music')",
                "filtered_cd": "SELECT cd_demo_sk FROM customer_demographics WHERE cd_marital_status = 'M' AND cd_dep_count BETWEEN 9 AND 11",
                "filtered_hd": "SELECT hd_demo_sk FROM household_demographics WHERE hd_buy_potential = '501-1000'",
                "cs_filtered": "SELECT cs_item_sk, cs_bill_cdemo_sk, cs_bill_hdemo_sk, cs_sold_date_sk, cs_ship_date_sk, cs_promo_sk, cs_quantity, cs_wholesale_cost, cs_order_number FROM catalog_sales WHERE cs_wholesale_cost BETWEEN 34 AND 54",
                "main_query": "SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq, SUM(CASE WHEN p.p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo, SUM(CASE WHEN p.p_promo_sk IS NOT NULL THEN 1 ELSE 0 END) AS promo, COUNT(*) AS total_cnt FROM cs_filtered cs JOIN inventory inv ON cs.cs_item_sk = inv.inv_item_sk JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk JOIN filtered_item i ON i.i_item_sk = cs.cs_item_sk JOIN filtered_cd cd ON cs.cs_bill_cdemo_sk = cd.cd_demo_sk JOIN filtered_hd hd ON cs.cs_bill_hdemo_sk = hd.hd_demo_sk JOIN filtered_date d1 ON cs.cs_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON inv.inv_date_sk = d2.d_date_sk JOIN date_dim d3 ON cs.cs_ship_date_sk = d3.d_date_sk LEFT OUTER JOIN promotion p ON cs.cs_promo_sk = p.p_promo_sk LEFT OUTER JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk AND cr.cr_order_number = cs.cs_order_number WHERE d1.d_week_seq = d2.d_week_seq AND inv.inv_quantity_on_hand < cs.cs_quantity AND d3.d_date > d1.d_date + INTERVAL '3 day' GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering",
                "same join semantics"
              ],
              "expected_speedup": "2.7x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Staged Reduction for Non-Equi Joins \u2014 when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join to shrink the search space. MATERIALIZED on PG12+ forces early execution. Here: fact table CTE removes ~70% of catalog_sales rows, dimension CTEs reduce date (365/73K), item (3 categories), and demographics to tiny sets \u2014 all before the expensive inventory non-equi join.",
        "pattern_detection": "Look for multi-table star-schema queries with: (1) expensive non-equi joins (quantity comparisons, date arithmetic), (2) a range filter on the fact table that removes >50% of rows, (3) multiple dimension filters. The combination of MATERIALIZED dimension CTEs + fact table pre-filter works when the non-equi joins dominate cost."
      }
    },
    {
      "id": "pg_date_consolidation",
      "name": "Date Dimension Consolidation (PostgreSQL)",
      "description": "When a query references date_dim 3+ times (d1 for sold, d2 for returned, d3 for shipped) with overlapping year/month filters, consolidate all date filters into a single all_dates CTE. Each fact table join references this shared CTE with specific MOY conditions, eliminating redundant date_dim scans.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "3.10x",
      "principle": "Date Dimension Consolidation: when 2+ date_dim instances have overlapping predicates (same year, overlapping months), extract one CTE with the union of all needed date keys. Each downstream join adds its specific MOY condition. Reduces N date_dim scans to 1.",
      "transforms": [
        "date_cte_isolate"
      ],
      "original_sql": "select \n i_item_id, i_item_desc, s_store_id, s_store_name,\n sum(ss_net_profit) as store_sales_profit,\n sum(sr_net_loss) as store_returns_loss,\n sum(cs_net_profit) as catalog_sales_profit\nfrom\n store_sales, store_returns, catalog_sales,\n date_dim d1, date_dim d2, date_dim d3,\n store, item\nwhere\n d1.d_moy = 5 and d1.d_year = 1999\n and d1.d_date_sk = ss_sold_date_sk\n and i_item_sk = ss_item_sk\n and s_store_sk = ss_store_sk\n and ss_customer_sk = sr_customer_sk\n and ss_item_sk = sr_item_sk\n and ss_ticket_number = sr_ticket_number\n and sr_returned_date_sk = d2.d_date_sk\n and d2.d_moy between 5 and 7\n and d2.d_year = 1999\n and sr_customer_sk = cs_bill_customer_sk\n and sr_item_sk = cs_item_sk\n and cs_sold_date_sk = d3.d_date_sk\n and d3.d_moy between 5 and 7\n and d3.d_year = 1999\ngroup by i_item_id, i_item_desc, s_store_id, s_store_name\norder by i_item_id, i_item_desc, s_store_id, s_store_name\nlimit 100;",
      "optimized_sql": "WITH all_dates AS (\n  SELECT d_date_sk, d_moy\n  FROM date_dim\n  WHERE d_year = 1999 AND ((d_moy = 5) OR (d_moy BETWEEN 5 AND 7))\n)\nSELECT i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name,\n       SUM(ss.ss_net_profit) AS store_sales_profit,\n       SUM(sr.sr_net_loss) AS store_returns_loss,\n       SUM(cs.cs_net_profit) AS catalog_sales_profit\nFROM store_sales ss\nINNER JOIN all_dates d1 ON ss.ss_sold_date_sk = d1.d_date_sk AND d1.d_moy = 5\nINNER JOIN store s ON s.s_store_sk = ss.ss_store_sk\nINNER JOIN item i ON i.i_item_sk = ss.ss_item_sk\nINNER JOIN store_returns sr\n  ON ss.ss_customer_sk = sr.sr_customer_sk\n  AND ss.ss_item_sk = sr.sr_item_sk\n  AND ss.ss_ticket_number = sr.sr_ticket_number\nINNER JOIN all_dates d2 ON sr.sr_returned_date_sk = d2.d_date_sk AND d2.d_moy BETWEEN 5 AND 7\nINNER JOIN catalog_sales cs\n  ON sr.sr_customer_sk = cs.cs_bill_customer_sk\n  AND sr.sr_item_sk = cs.cs_item_sk\nINNER JOIN all_dates d3 ON cs.cs_sold_date_sk = d3.d_date_sk AND d3.d_moy BETWEEN 5 AND 7\nGROUP BY i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name\nORDER BY i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name\nLIMIT 100",
      "example": {
        "opportunity": "DATE_CONSOLIDATION",
        "input_slice": "from store_sales, store_returns, catalog_sales,\n     date_dim d1, date_dim d2, date_dim d3, store, item\nwhere d1.d_moy = 5 and d1.d_year = 1999 and d1.d_date_sk = ss_sold_date_sk\n  and d2.d_moy between 5 and 7 and d2.d_year = 1999 and sr_returned_date_sk = d2.d_date_sk\n  and d3.d_moy between 5 and 7 and d3.d_year = 1999 and cs_sold_date_sk = d3.d_date_sk",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "date_cte_isolate",
              "nodes": {
                "all_dates": "SELECT d_date_sk, d_moy FROM date_dim WHERE d_year = 1999 AND (d_moy = 5 OR d_moy BETWEEN 5 AND 7)",
                "main_query": "... JOIN all_dates d1 ON ss.ss_sold_date_sk = d1.d_date_sk AND d1.d_moy = 5 JOIN all_dates d2 ON sr.sr_returned_date_sk = d2.d_date_sk AND d2.d_moy BETWEEN 5 AND 7 JOIN all_dates d3 ON cs.cs_sold_date_sk = d3.d_date_sk AND d3.d_moy BETWEEN 5 AND 7"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering"
              ],
              "expected_speedup": "3.1x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Date Dimension Consolidation \u2014 when 2-3 date_dim instances share the same year but different month ranges, create a single all_dates CTE with the union of all needed date keys. Each downstream JOIN applies its specific MOY condition on the shared CTE. This eliminates redundant date_dim scans and gives the planner a single tiny hash table to probe. Also converts comma joins to explicit INNER JOIN syntax.",
        "pattern_detection": "Look for: 2+ date_dim aliases in FROM with same d_year but different d_moy/d_month_seq predicates. The year overlap means the same date_dim rows are being scanned multiple times. Common in return/shipment queries (sold_date, returned_date, shipped_date)."
      }
    },
    {
      "id": "inline_decorrelate_materialized",
      "name": "Inline Correlated Subquery to Materialized CTEs",
      "database": "postgres",
      "verified_speedup": "1465x (V2 DSB SF10, timeout rescue)",
      "principle": "Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs \u2014 (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data \u2014 then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.",
      "benchmark": {
        "dataset": "dsb_sf5",
        "query": "query032",
        "original_time_s": "timeout (>300s)",
        "optimized_time_s": 0.66
      },
      "ast_flags": {
        "kb_patterns_detected": [
          {
            "id": "INLINE_CORRELATED_SCALAR",
            "name": "Inline Correlated Scalar Subquery",
            "trigger": "WHERE col > (SELECT agg_func(col) FROM fact_table WHERE fact.key = outer.key)"
          },
          {
            "id": "FLAT_FROM_NO_CTE",
            "name": "Flat FROM clause without CTEs",
            "trigger": "FROM fact_table, dim1, dim2 WHERE ... AND col > (SELECT ...)"
          }
        ],
        "structural_patterns": [
          "No CTEs in original \u2014 flat FROM with implicit joins",
          "Correlated scalar subquery in WHERE with aggregate (avg/sum/count)",
          "Subquery re-scans fact table with date range filter",
          "Outer query joins fact \u00d7 dimension \u00d7 date_dim"
        ]
      },
      "original_sql": "select  sum(cs_ext_discount_amt)  as \"excess discount amount\"\nfrom\n   catalog_sales\n   ,item\n   ,date_dim\nwhere\n(i_manufact_id in (1, 78, 97, 516, 521)\nor i_manager_id BETWEEN 25 and 54)\nand i_item_sk = cs_item_sk\nand d_date between '1999-03-07' and\n        cast('1999-03-07' as date) + interval '90 day'\nand d_date_sk = cs_sold_date_sk\nand cs_ext_discount_amt\n     > (\n         select\n            1.3 * avg(cs_ext_discount_amt)\n         from\n            catalog_sales\n           ,date_dim\n         where\n              cs_item_sk = i_item_sk\n          and d_date between '1999-03-07' and\n                             cast('1999-03-07' as date) + interval '90 day'\n          and d_date_sk = cs_sold_date_sk\n          and cs_list_price between 16 and 45\n          and cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n      )\norder by sum(cs_ext_discount_amt)\nlimit 100;",
      "optimized_sql": "WITH filtered_items AS MATERIALIZED (\n    SELECT i_item_sk\n    FROM item\n    WHERE i_manufact_id IN (1, 78, 97, 516, 521)\n       OR i_manager_id BETWEEN 25 AND 54\n),\ndate_filtered_sales AS MATERIALIZED (\n    SELECT cs.cs_item_sk, cs.cs_ext_discount_amt,\n           cs.cs_list_price, cs.cs_sales_price\n    FROM catalog_sales cs\n    JOIN date_dim d ON d.d_date_sk = cs.cs_sold_date_sk\n    WHERE d.d_date BETWEEN '1999-03-07' AND cast('1999-03-07' as date) + interval '90 day'\n),\nitem_avg_discount AS MATERIALIZED (\n    SELECT dfs.cs_item_sk,\n           1.3 * avg(dfs.cs_ext_discount_amt) AS threshold\n    FROM date_filtered_sales dfs\n    JOIN filtered_items fi ON fi.i_item_sk = dfs.cs_item_sk\n    WHERE dfs.cs_list_price BETWEEN 16 AND 45\n      AND dfs.cs_sales_price / dfs.cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n    GROUP BY dfs.cs_item_sk\n)\nSELECT sum(dfs.cs_ext_discount_amt) AS \"excess discount amount\"\nFROM date_filtered_sales dfs\nJOIN item_avg_discount iad ON iad.cs_item_sk = dfs.cs_item_sk\nWHERE dfs.cs_ext_discount_amt > iad.threshold\nORDER BY 1\nLIMIT 100;",
      "pg_blind_spots": [
        "PG re-executes correlated scalar subquery per outer row \u2014 O(N*M) scans",
        "PG underestimates CTE cardinality (estimated 7 rows, actual 5,021) causing nested loop instead of hash join",
        "Without MATERIALIZED keyword, PG may inline CTE back into correlated form"
      ],
      "input": {
        "description": "Flat FROM (fact \u00d7 dim \u00d7 date) with inline correlated scalar subquery computing per-item discount threshold. Subquery re-scans the same fact table with date filter.",
        "sql": "select  sum(cs_ext_discount_amt)  as \"excess discount amount\"\nfrom\n   catalog_sales\n   ,item\n   ,date_dim\nwhere\n(i_manufact_id in (1, 78, 97, 516, 521)\nor i_manager_id BETWEEN 25 and 54)\nand i_item_sk = cs_item_sk\nand d_date between '1999-03-07' and\n        cast('1999-03-07' as date) + interval '90 day'\nand d_date_sk = cs_sold_date_sk\nand cs_ext_discount_amt\n     > (\n         select\n            1.3 * avg(cs_ext_discount_amt)\n         from\n            catalog_sales\n           ,date_dim\n         where\n              cs_item_sk = i_item_sk\n          and d_date between '1999-03-07' and\n                             cast('1999-03-07' as date) + interval '90 day'\n          and d_date_sk = cs_sold_date_sk\n          and cs_list_price between 16 and 45\n          and cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n      )\norder by sum(cs_ext_discount_amt)\nlimit 100;"
      },
      "output": {
        "description": "3 MATERIALIZED CTEs: (1) filtered dimension keys, (2) date-filtered fact rows, (3) per-item threshold via GROUP BY. Final query JOINs threshold CTE \u2014 hash join replaces per-row subquery.",
        "sql": "WITH filtered_items AS MATERIALIZED (\n    SELECT i_item_sk\n    FROM item\n    WHERE i_manufact_id IN (1, 78, 97, 516, 521)\n       OR i_manager_id BETWEEN 25 AND 54\n),\ndate_filtered_sales AS MATERIALIZED (\n    SELECT cs.cs_item_sk, cs.cs_ext_discount_amt,\n           cs.cs_list_price, cs.cs_sales_price\n    FROM catalog_sales cs\n    JOIN date_dim d ON d.d_date_sk = cs.cs_sold_date_sk\n    WHERE d.d_date BETWEEN '1999-03-07' AND cast('1999-03-07' as date) + interval '90 day'\n),\nitem_avg_discount AS MATERIALIZED (\n    SELECT dfs.cs_item_sk,\n           1.3 * avg(dfs.cs_ext_discount_amt) AS threshold\n    FROM date_filtered_sales dfs\n    JOIN filtered_items fi ON fi.i_item_sk = dfs.cs_item_sk\n    WHERE dfs.cs_list_price BETWEEN 16 AND 45\n      AND dfs.cs_sales_price / dfs.cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01\n    GROUP BY dfs.cs_item_sk\n)\nSELECT sum(dfs.cs_ext_discount_amt) AS \"excess discount amount\"\nFROM date_filtered_sales dfs\nJOIN item_avg_discount iad ON iad.cs_item_sk = dfs.cs_item_sk\nWHERE dfs.cs_ext_discount_amt > iad.threshold\nORDER BY 1\nLIMIT 100;"
      },
      "transforms_applied": [
        "decorrelate: Convert inline correlated scalar subquery (WHERE col > SELECT avg(...) WHERE key = outer.key) to MATERIALIZED CTE with GROUP BY + JOIN",
        "early_filter: Pre-filter dimension table (item) into separate MATERIALIZED CTE",
        "date_cte_isolate: Pre-filter fact table by date range into MATERIALIZED CTE, reuse for both threshold computation and final query"
      ],
      "key_insight": "Principle: Inline Decorrelation \u2014 when WHERE has a correlated scalar subquery that re-scans the fact table per outer row, decompose into 3 MATERIALIZED CTEs: (1) dimension filter, (2) date-filtered fact rows, (3) per-key aggregate threshold. The final query JOINs the threshold CTE, replacing O(N*M) correlated scans with a single hash join. CRITICAL: use AS MATERIALIZED on PostgreSQL to prevent the optimizer from inlining CTEs back into the original correlated form."
    },
    {
      "id": "pg_shared_scan_decorrelate",
      "name": "Shared Scan Decorrelation (PostgreSQL)",
      "description": "When a correlated subquery re-scans the same fact table as the outer query, extract the common scan into a shared CTE, then derive both the threshold computation and the outer rows from a single materialized result. Eliminates O(N*M) re-execution of the fact+date join.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "8043.91x (timeout rescue)",
      "principle": "Shared Scan Decorrelation: when inner and outer queries scan the same fact table with the same date/cost filters, extract the common scan into a single CTE. Then compute per-item thresholds via GROUP BY in a second CTE, and JOIN back to filter. Converts O(N*M) correlated execution to O(N+M) hash join.",
      "transforms": [
        "decorrelate"
      ],
      "original_sql": "select \n   sum(ws_ext_discount_amt)  as \"Excess Discount Amount\"\nfrom\n    web_sales\n   ,item\n   ,date_dim\nwhere\n(i_manufact_id BETWEEN 341 and 540\nor i_category IN ('Home', 'Men', 'Music'))\nand i_item_sk = ws_item_sk\nand d_date between '1998-03-13' and\n        cast('1998-03-13' as date) + interval '90 day'\nand d_date_sk = ws_sold_date_sk\nand ws_wholesale_cost BETWEEN 26 AND 46\nand ws_ext_discount_amt\n     > (\n         SELECT\n            1.3 * avg(ws_ext_discount_amt)\n         FROM\n            web_sales\n           ,date_dim\n         WHERE\n              ws_item_sk = i_item_sk\n          and d_date between '1998-03-13' and\n                             cast('1998-03-13' as date) + interval '90 day'\n          and d_date_sk = ws_sold_date_sk\n          and ws_wholesale_cost BETWEEN 26 AND 46\n          and ws_sales_price / ws_list_price BETWEEN 34 * 0.01 AND 49 * 0.01\n      )\norder by sum(ws_ext_discount_amt)\nlimit 100;",
      "optimized_sql": "WITH common_scan AS (\n  SELECT ws_item_sk, ws_ext_discount_amt, ws_sales_price, ws_list_price\n  FROM web_sales\n  INNER JOIN date_dim ON d_date_sk = ws_sold_date_sk\n  WHERE d_date BETWEEN '1998-03-13' AND CAST('1998-03-13' AS DATE) + INTERVAL '90 DAY'\n    AND ws_wholesale_cost BETWEEN 26 AND 46\n),\nthreshold_computation AS (\n  SELECT ws_item_sk, 1.3 * AVG(ws_ext_discount_amt) AS threshold\n  FROM common_scan\n  WHERE ws_sales_price / ws_list_price BETWEEN 34 * 0.01 AND 49 * 0.01\n  GROUP BY ws_item_sk\n),\nouter_rows AS (\n  SELECT cs.ws_item_sk, cs.ws_ext_discount_amt\n  FROM common_scan cs\n  INNER JOIN item ON i_item_sk = cs.ws_item_sk\n  WHERE i_manufact_id BETWEEN 341 AND 540\n     OR i_category IN ('Home', 'Men', 'Music')\n),\njoin_filter AS (\n  SELECT o.ws_ext_discount_amt\n  FROM outer_rows o\n  INNER JOIN threshold_computation t ON o.ws_item_sk = t.ws_item_sk\n  WHERE o.ws_ext_discount_amt > t.threshold\n)\nSELECT SUM(ws_ext_discount_amt) AS \"Excess Discount Amount\"\nFROM join_filter\nORDER BY SUM(ws_ext_discount_amt)\nLIMIT 100",
      "example": {
        "opportunity": "SHARED_SCAN_DECORRELATE",
        "input_slice": "select sum(ws_ext_discount_amt) as \"Excess Discount Amount\"\nfrom web_sales, item, date_dim\nwhere (i_manufact_id BETWEEN 341 and 540 or i_category IN (...))\n  and i_item_sk = ws_item_sk\n  and d_date between '...' and '...' + interval '90 day'\n  and d_date_sk = ws_sold_date_sk\n  and ws_wholesale_cost BETWEEN 26 AND 46\n  and ws_ext_discount_amt > (\n    SELECT 1.3 * avg(ws_ext_discount_amt)\n    FROM web_sales, date_dim\n    WHERE ws_item_sk = i_item_sk  -- correlated!\n      and d_date between ... and d_date_sk = ws_sold_date_sk\n      and ws_wholesale_cost BETWEEN 26 AND 46\n      and ws_sales_price / ws_list_price BETWEEN ...)",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "decorrelate",
              "nodes": {
                "common_scan": "SELECT ws_item_sk, ws_ext_discount_amt, ws_sales_price, ws_list_price FROM web_sales JOIN date_dim ON d_date_sk = ws_sold_date_sk WHERE d_date BETWEEN ... AND ws_wholesale_cost BETWEEN ...",
                "threshold_computation": "SELECT ws_item_sk, 1.3 * AVG(ws_ext_discount_amt) AS threshold FROM common_scan WHERE ws_sales_price / ws_list_price BETWEEN ... GROUP BY ws_item_sk",
                "outer_rows": "SELECT cs.ws_item_sk, cs.ws_ext_discount_amt FROM common_scan cs JOIN item ON i_item_sk = cs.ws_item_sk WHERE i_manufact_id BETWEEN ... OR i_category IN (...)",
                "join_filter": "SELECT o.ws_ext_discount_amt FROM outer_rows o JOIN threshold_computation t ON o.ws_item_sk = t.ws_item_sk WHERE o.ws_ext_discount_amt > t.threshold",
                "main_query": "SELECT SUM(ws_ext_discount_amt) AS \"Excess Discount Amount\" FROM join_filter ORDER BY 1 LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation"
              ],
              "expected_speedup": "8000x+ (timeout rescue)",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Shared Scan Decorrelation \u2014 when the correlated subquery re-scans the same fact table with the same date/cost filters as the outer query, extract the common scan once into a CTE, then derive (1) per-item thresholds via GROUP BY and (2) outer rows via item join from the same materialized result. The correlated execution that caused timeout becomes a single hash join.",
        "pattern_detection": "Look for: (1) correlated scalar subquery with aggregate (AVG/SUM), (2) inner query scans the SAME fact table as outer query, (3) identical date range and cost filters in both inner and outer. The shared-scan pattern applies when inner = outer table with overlapping filters."
      }
    },
    {
      "id": "pg_state_avg_decorrelate",
      "name": "State-Average Decorrelation (PostgreSQL)",
      "description": "Decorrelate a correlated subquery that computes per-state averages by extracting the state aggregation into a separate CTE, then JOINing back on the state key. Converts per-row subquery re-execution into a single GROUP BY + hash join.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "438.93x (timeout rescue)",
      "principle": "State-Average Decorrelation: when a CTE self-references with a correlated AVG (e.g., ctr1.state = ctr2.state), extract the per-state average into a separate CTE with GROUP BY state, then JOIN. Converts O(states \u00d7 rows_per_state) to O(total_rows).",
      "transforms": [
        "decorrelate"
      ],
      "original_sql": "with customer_total_return as\n (select cr_returning_customer_sk as ctr_customer_sk\n        ,ca_state as ctr_state, \n \tsum(cr_return_amt_inc_tax) as ctr_total_return\n from catalog_returns\n     ,date_dim\n     ,customer_address\n where cr_returned_date_sk = d_date_sk \n   and d_year =1998\n   and cr_returning_addr_sk = ca_address_sk \n group by cr_returning_customer_sk\n         ,ca_state )\n  select  c_customer_id,c_salutation,c_first_name,c_last_name,ca_street_number,ca_street_name\n                   ,ca_street_type,ca_suite_number,ca_city,ca_county,ca_state,ca_zip,ca_country,ca_gmt_offset\n                  ,ca_location_type,ctr_total_return\n from customer_total_return ctr1\n     ,customer_address\n     ,customer\n where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2\n \t\t\t  from customer_total_return ctr2 \n                  \t  where ctr1.ctr_state = ctr2.ctr_state)\n       and ca_address_sk = c_current_addr_sk\n       and ca_state = 'VA'\n       and ctr1.ctr_customer_sk = c_customer_sk\n order by c_customer_id,c_salutation,c_first_name,c_last_name,ca_street_number,ca_street_name\n                   ,ca_street_type,ca_suite_number,ca_city,ca_county,ca_state,ca_zip,ca_country,ca_gmt_offset\n                  ,ca_location_type,ctr_total_return\n limit 100;",
      "optimized_sql": "WITH filtered_date AS (\n  SELECT d_date_sk FROM date_dim WHERE d_year = 1998\n),\nreturns_cte AS (\n  SELECT cr_returning_customer_sk AS ctr_customer_sk,\n         ca_state AS ctr_state,\n         SUM(cr_return_amt_inc_tax) AS ctr_total_return\n  FROM catalog_returns\n  JOIN filtered_date ON cr_returned_date_sk = d_date_sk\n  JOIN customer_address ON cr_returning_addr_sk = ca_address_sk\n  GROUP BY cr_returning_customer_sk, ca_state\n),\nstate_avg_cte AS (\n  SELECT ctr_state, AVG(ctr_total_return) AS state_avg\n  FROM returns_cte\n  GROUP BY ctr_state\n)\nSELECT c_customer_id, c_salutation, c_first_name, c_last_name,\n       ca_street_number, ca_street_name, ca_street_type, ca_suite_number,\n       ca_city, ca_county, ca_state, ca_zip, ca_country, ca_gmt_offset,\n       ca_location_type, ctr1.ctr_total_return\nFROM returns_cte ctr1\nJOIN state_avg_cte ON ctr1.ctr_state = state_avg_cte.ctr_state\nJOIN customer ON ctr1.ctr_customer_sk = c_customer_sk\nJOIN customer_address ON ca_address_sk = c_current_addr_sk\nWHERE ctr1.ctr_total_return > state_avg_cte.state_avg * 1.2\n  AND ca_state = 'VA'\nORDER BY c_customer_id, c_salutation, c_first_name, c_last_name,\n         ca_street_number, ca_street_name, ca_street_type, ca_suite_number,\n         ca_city, ca_county, ca_state, ca_zip, ca_country, ca_gmt_offset,\n         ca_location_type, ctr_total_return\nLIMIT 100",
      "example": {
        "opportunity": "STATE_AVG_DECORRELATE",
        "input_slice": "with customer_total_return as (...)\nselect c_customer_id, ...\nfrom customer_total_return ctr1, customer_address, customer\nwhere ctr1.ctr_total_return > (\n    select avg(ctr_total_return)*1.2\n    from customer_total_return ctr2\n    where ctr1.ctr_state = ctr2.ctr_state)  -- correlated on state\n  and ca_state = 'VA'\n  and ctr1.ctr_customer_sk = c_customer_sk",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "decorrelate",
              "nodes": {
                "filtered_date": "SELECT d_date_sk FROM date_dim WHERE d_year = 1998",
                "returns_cte": "SELECT cr_returning_customer_sk AS ctr_customer_sk, ca_state AS ctr_state, SUM(cr_return_amt_inc_tax) AS ctr_total_return FROM catalog_returns JOIN filtered_date ON ... JOIN customer_address ON ... GROUP BY ...",
                "state_avg_cte": "SELECT ctr_state, AVG(ctr_total_return) AS state_avg FROM returns_cte GROUP BY ctr_state",
                "main_query": "SELECT ... FROM returns_cte ctr1 JOIN state_avg_cte ON ctr1.ctr_state = state_avg_cte.ctr_state JOIN customer ON ... WHERE ctr1.ctr_total_return > state_avg_cte.state_avg * 1.2 AND ca_state = 'VA'"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering"
              ],
              "expected_speedup": "400x+ (timeout rescue)",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: State-Average Decorrelation \u2014 the correlated subquery computes AVG per state for every outer row. By extracting this into a state_avg_cte with GROUP BY ctr_state (only ~50 states), the O(N * rows_per_state) nested loop becomes a tiny hash join on ~50 rows. Also converts comma joins to explicit JOINs and isolates date filter into CTE.",
        "pattern_detection": "Look for: CTE that self-references with a correlated AVG where the correlation key is a low-cardinality dimension (state, category, year). The AVG can be pre-computed once with GROUP BY and JOINed back."
      }
    },
    {
      "id": "pg_set_operation_materialization",
      "name": "Multi-Channel Set Operation Materialization (PostgreSQL)",
      "description": "When a query uses EXISTS + NOT EXISTS across 3 channels (store, web, catalog) with correlated subqueries, pre-materialize each channel's distinct customer set into a MATERIALIZED CTE, then use INNER JOIN + LEFT JOIN + IS NULL for set operations. Eliminates per-row correlated subquery execution across 3 fact tables.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "17.48x",
      "principle": "Channel Set Materialization: when EXISTS/NOT EXISTS checks 3 channels (store_sales, web_sales, catalog_sales) with identical date+price filters, pre-compute DISTINCT customer sets per channel as MATERIALIZED CTEs. Replace EXISTS with INNER JOIN, NOT EXISTS with LEFT JOIN + IS NULL. The MATERIALIZED keyword forces early computation.",
      "transforms": [
        "materialize_cte"
      ],
      "original_sql": "select \n  cd_gender,\n  cd_marital_status,\n  cd_education_status,\n  count(*) cnt1,\n  cd_purchase_estimate,\n  count(*) cnt2,\n  cd_credit_rating,\n  count(*) cnt3\n from\n  customer c,customer_address ca,customer_demographics\n where\n  c.c_current_addr_sk = ca.ca_address_sk and\n  ca_state in ('IA','MO','TX') and\n  cd_demo_sk = c.c_current_cdemo_sk\n  and cd_marital_status in ('S', 'S', 'S')\n  and cd_education_status in ('Primary', 'Secondary') and\n  exists (select *\n          from store_sales,date_dim\n          where c.c_customer_sk = ss_customer_sk and\n                ss_sold_date_sk = d_date_sk and\n                d_year = 2002 and\n                d_moy between 3 and 3+2\n                and ss_list_price between 100 and 189\n          ) and\n   (not exists (select *\n            from web_sales,date_dim\n            where c.c_customer_sk = ws_bill_customer_sk and\n                  ws_sold_date_sk = d_date_sk and\n                  d_year = 2002 and\n                  d_moy between 3 and 3+2\n                  and ws_list_price between 100 and 189\n            ) and\n    not exists (select *\n            from catalog_sales,date_dim\n            where c.c_customer_sk = cs_ship_customer_sk and\n                  cs_sold_date_sk = d_date_sk and\n                  d_year = 2002 and\n                  d_moy between 3 and 3+2\n                  and cs_list_price between 100 and 189)\n            )\n group by cd_gender, cd_marital_status, cd_education_status,\n          cd_purchase_estimate, cd_credit_rating\n order by cd_gender, cd_marital_status, cd_education_status,\n          cd_purchase_estimate, cd_credit_rating\n limit 100;",
      "optimized_sql": "WITH filtered_date AS (\n  SELECT d_date_sk FROM date_dim\n  WHERE d_year = 2002 AND d_moy BETWEEN 3 AND 5\n),\nstore_customers AS MATERIALIZED (\n  SELECT DISTINCT ss_customer_sk\n  FROM store_sales\n  JOIN filtered_date ON ss_sold_date_sk = d_date_sk\n  WHERE ss_list_price BETWEEN 100 AND 189\n),\nweb_customers AS MATERIALIZED (\n  SELECT DISTINCT ws_bill_customer_sk\n  FROM web_sales\n  JOIN filtered_date ON ws_sold_date_sk = d_date_sk\n  WHERE ws_list_price BETWEEN 100 AND 189\n),\ncatalog_customers AS MATERIALIZED (\n  SELECT DISTINCT cs_ship_customer_sk\n  FROM catalog_sales\n  JOIN filtered_date ON cs_sold_date_sk = d_date_sk\n  WHERE cs_list_price BETWEEN 100 AND 189\n),\ncustomer_base AS (\n  SELECT c.c_customer_sk, cd.cd_gender, cd.cd_marital_status,\n         cd.cd_education_status, cd.cd_purchase_estimate, cd.cd_credit_rating\n  FROM customer c\n  JOIN customer_address ca ON c.c_current_addr_sk = ca.ca_address_sk\n  JOIN customer_demographics cd ON cd.cd_demo_sk = c.c_current_cdemo_sk\n  WHERE ca.ca_state IN ('IA','MO','TX')\n    AND cd.cd_marital_status IN ('S')\n    AND cd.cd_education_status IN ('Primary','Secondary')\n),\nset_joins AS (\n  SELECT cb.cd_gender, cb.cd_marital_status, cb.cd_education_status,\n         cb.cd_purchase_estimate, cb.cd_credit_rating\n  FROM customer_base cb\n  INNER JOIN store_customers sc ON cb.c_customer_sk = sc.ss_customer_sk\n  LEFT JOIN web_customers wc ON cb.c_customer_sk = wc.ws_bill_customer_sk\n  LEFT JOIN catalog_customers cc ON cb.c_customer_sk = cc.cs_ship_customer_sk\n  WHERE wc.ws_bill_customer_sk IS NULL\n    AND cc.cs_ship_customer_sk IS NULL\n)\nSELECT cd_gender, cd_marital_status, cd_education_status,\n       COUNT(*) AS cnt1, cd_purchase_estimate, COUNT(*) AS cnt2,\n       cd_credit_rating, COUNT(*) AS cnt3\nFROM set_joins\nGROUP BY cd_gender, cd_marital_status, cd_education_status,\n         cd_purchase_estimate, cd_credit_rating\nORDER BY cd_gender, cd_marital_status, cd_education_status,\n         cd_purchase_estimate, cd_credit_rating\nLIMIT 100",
      "example": {
        "opportunity": "SET_OPERATION_MATERIALIZATION",
        "input_slice": "select cd_gender, cd_marital_status, cd_education_status, count(*) ...\nfrom customer c, customer_address ca, customer_demographics\nwhere ...\n  and exists (select * from store_sales, date_dim where c.c_customer_sk = ss_customer_sk ...)\n  and not exists (select * from web_sales, date_dim where c.c_customer_sk = ws_bill_customer_sk ...)\n  and not exists (select * from catalog_sales, date_dim where c.c_customer_sk = cs_ship_customer_sk ...)",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "materialize_cte",
              "nodes": {
                "filtered_date": "SELECT d_date_sk FROM date_dim WHERE d_year = 2002 AND d_moy BETWEEN 3 AND 5",
                "store_customers": "AS MATERIALIZED: SELECT DISTINCT ss_customer_sk FROM store_sales JOIN filtered_date ...",
                "web_customers": "AS MATERIALIZED: SELECT DISTINCT ws_bill_customer_sk FROM web_sales JOIN filtered_date ...",
                "catalog_customers": "AS MATERIALIZED: SELECT DISTINCT cs_ship_customer_sk FROM catalog_sales JOIN filtered_date ...",
                "customer_base": "SELECT c.c_customer_sk, cd.* FROM customer c JOIN customer_address ca ... JOIN customer_demographics cd ...",
                "set_joins": "SELECT ... FROM customer_base cb INNER JOIN store_customers sc ON ... LEFT JOIN web_customers wc ON ... LEFT JOIN catalog_customers cc ON ... WHERE wc.ws_bill_customer_sk IS NULL AND cc.cs_ship_customer_sk IS NULL"
              },
              "invariants_kept": [
                "same result rows",
                "same aggregation",
                "same ordering"
              ],
              "expected_speedup": "17x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Channel Set Materialization \u2014 when 3 correlated EXISTS/NOT EXISTS check store, web, and catalog channels with identical date+price filters, pre-compute DISTINCT customer_sk per channel once as MATERIALIZED CTEs. EXISTS becomes INNER JOIN, NOT EXISTS becomes LEFT JOIN + IS NULL. Critical: shared date CTE avoids 3 redundant date_dim scans. CAUTION: do NOT apply to simple single-channel EXISTS \u2014 semi-join early termination is already optimal there.",
        "pattern_detection": "Look for: (1) EXISTS + NOT EXISTS pattern with 3 fact tables (store_sales, web_sales, catalog_sales), (2) identical date range and price range filters in each subquery, (3) correlation on customer_sk. All 3 must be present for this pattern."
      }
    },
    {
      "id": "pg_intersect_to_exists",
      "name": "INTERSECT to EXISTS Semi-Join (PostgreSQL)",
      "description": "Convert INTERSECT set operations to EXISTS semi-joins. PostgreSQL implements INTERSECT via full materialization + sort comparison, while EXISTS uses semi-join with early termination. For large result sets, EXISTS is significantly faster.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "1.78x",
      "principle": "INTERSECT to EXISTS: INTERSECT materializes both sides fully, sorts, and compares. EXISTS uses semi-join with index + early termination per row. When both sides produce 10K+ rows, EXISTS is cheaper because it stops at first match.",
      "transforms": [
        "intersect_to_exists"
      ],
      "original_sql": "select count(*) from (\n    select distinct c_last_name, c_first_name, d_date\n    from store_sales, date_dim, customer\n    where ss_sold_date_sk = d_date_sk\n      and ss_customer_sk = c_customer_sk\n      and d_month_seq between 1189 and 1200\n      and c_birth_month in (4, 9, 10, 12)\n      and ss_list_price between 25 and 84\n      and ss_wholesale_cost BETWEEN 34 AND 54\n  intersect\n    select distinct c_last_name, c_first_name, d_date\n    from catalog_sales, date_dim, customer\n    where cs_sold_date_sk = d_date_sk\n      and cs_bill_customer_sk = c_customer_sk\n      and d_month_seq between 1189 and 1200\n      and c_birth_month in (4, 9, 10, 12)\n      and cs_list_price between 25 and 84\n      and cs_wholesale_cost BETWEEN 34 AND 54\n  intersect\n    select distinct c_last_name, c_first_name, d_date\n    from web_sales, date_dim, customer\n    where ws_sold_date_sk = d_date_sk\n      and ws_bill_customer_sk = c_customer_sk\n      and d_month_seq between 1189 and 1200\n      and c_birth_month in (4, 9, 10, 12)\n      and ws_list_price between 25 and 84\n      and ws_wholesale_cost BETWEEN 34 AND 54\n) hot_cust\nlimit 100;",
      "optimized_sql": "WITH store_sales_base AS (\n  SELECT DISTINCT c.c_last_name, c.c_first_name, d.d_date\n  FROM store_sales ss\n  INNER JOIN date_dim d ON ss.ss_sold_date_sk = d.d_date_sk\n  INNER JOIN customer c ON ss.ss_customer_sk = c.c_customer_sk\n  WHERE d.d_month_seq BETWEEN 1189 AND 1200\n    AND c.c_birth_month IN (4, 9, 10, 12)\n    AND ss.ss_list_price BETWEEN 25 AND 84\n    AND ss.ss_wholesale_cost BETWEEN 34 AND 54\n)\nSELECT COUNT(*)\nFROM store_sales_base ssb\nWHERE EXISTS (\n  SELECT 1 FROM catalog_sales cs\n  INNER JOIN date_dim d ON cs.cs_sold_date_sk = d.d_date_sk\n  INNER JOIN customer c ON cs.cs_bill_customer_sk = c.c_customer_sk\n  WHERE d.d_month_seq BETWEEN 1189 AND 1200\n    AND c.c_birth_month IN (4, 9, 10, 12)\n    AND cs.cs_list_price BETWEEN 25 AND 84\n    AND cs.cs_wholesale_cost BETWEEN 34 AND 54\n    AND c.c_last_name = ssb.c_last_name\n    AND c.c_first_name = ssb.c_first_name\n    AND d.d_date = ssb.d_date\n) AND EXISTS (\n  SELECT 1 FROM web_sales ws\n  INNER JOIN date_dim d ON ws.ws_sold_date_sk = d.d_date_sk\n  INNER JOIN customer c ON ws.ws_bill_customer_sk = c.c_customer_sk\n  WHERE d.d_month_seq BETWEEN 1189 AND 1200\n    AND c.c_birth_month IN (4, 9, 10, 12)\n    AND ws.ws_list_price BETWEEN 25 AND 84\n    AND ws.ws_wholesale_cost BETWEEN 34 AND 54\n    AND c.c_last_name = ssb.c_last_name\n    AND c.c_first_name = ssb.c_first_name\n    AND d.d_date = ssb.d_date\n)\nLIMIT 100",
      "example": {
        "opportunity": "INTERSECT_TO_EXISTS",
        "input_slice": "select count(*) from (\n  select distinct ... from store_sales, date_dim, customer where ...\n  intersect\n  select distinct ... from catalog_sales, date_dim, customer where ...\n  intersect\n  select distinct ... from web_sales, date_dim, customer where ...\n) hot_cust limit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "intersect_to_exists",
              "nodes": {
                "store_sales_base": "SELECT DISTINCT c_last_name, c_first_name, d_date FROM store_sales JOIN date_dim JOIN customer WHERE ...",
                "main_query": "SELECT COUNT(*) FROM store_sales_base ssb WHERE EXISTS (catalog match) AND EXISTS (web match)"
              },
              "invariants_kept": [
                "same result count",
                "same set semantics"
              ],
              "expected_speedup": "1.8x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: INTERSECT to EXISTS \u2014 INTERSECT materializes both sides fully before comparing. EXISTS uses semi-join with early termination: it stops scanning as soon as the first matching row is found. For the 3-way channel intersection pattern (store \u2229 catalog \u2229 web), materialize the first channel as a base CTE, then use EXISTS to probe the other two. Also converts comma joins to explicit INNER JOIN.",
        "pattern_detection": "Look for: INTERSECT between 2+ SELECT DISTINCT queries over different fact tables with identical customer/date/filter structure. Each INTERSECT branch scans a different fact table but returns the same column set."
      }
    },
    {
      "id": "pg_date_cte_explicit_join",
      "name": "Date CTE with Explicit JOIN Conversion (PostgreSQL)",
      "description": "Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.",
      "engine": "postgresql",
      "benchmark": "DSB SF10",
      "verified_speedup": "2.28x",
      "principle": "Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.",
      "transforms": [
        "date_cte_isolate"
      ],
      "original_sql": "select \n   substring(w_warehouse_name,1,20)\n  ,sm_type\n  ,cc_name\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30 ) then 1 else 0 end)  as \"30 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 30) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 60) then 1 else 0 end )  as \"31-60 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 60) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 90) then 1 else 0 end)  as \"61-90 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 90) and\n                 (cs_ship_date_sk - cs_sold_date_sk <= 120) then 1 else 0 end)  as \"91-120 days\"\n  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk  > 120) then 1 else 0 end)  as \">120 days\"\nfrom\n   catalog_sales\n  ,warehouse\n  ,ship_mode\n  ,call_center\n  ,date_dim\nwhere\nd_month_seq between 1193 and 1193 + 23\nand cs_ship_date_sk   = d_date_sk\nand cs_warehouse_sk   = w_warehouse_sk\nand cs_ship_mode_sk   = sm_ship_mode_sk\nand cs_call_center_sk = cc_call_center_sk\nand cs_list_price between 271 and 300\nand sm_type = 'REGULAR'\nand cc_class = 'small'\nand w_gmt_offset = -5\ngroup by\n   substring(w_warehouse_name,1,20)\n  ,sm_type\n  ,cc_name\norder by substring(w_warehouse_name,1,20)\n        ,sm_type\n        ,cc_name\nlimit 100;",
      "optimized_sql": "WITH filtered_dates AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1193 + 23)\nSELECT SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name, SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS \"30 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 30) AND (cs_ship_date_sk - cs_sold_date_sk <= 60) THEN 1 ELSE 0 END) AS \"31-60 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 60) AND (cs_ship_date_sk - cs_sold_date_sk <= 90) THEN 1 ELSE 0 END) AS \"61-90 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 90) AND (cs_ship_date_sk - cs_sold_date_sk <= 120) THEN 1 ELSE 0 END) AS \"91-120 days\", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 120) THEN 1 ELSE 0 END) AS \">120 days\" FROM catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk WHERE cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5 GROUP BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name ORDER BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name LIMIT 100",
      "example": {
        "opportunity": "DATE_CTE_EXPLICIT_JOIN",
        "input_slice": "select substring(w_warehouse_name,1,20), sm_type, cc_name,\n  sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30) then 1 else 0 end) as \"30 days\",\n  sum(case when ... > 30 and ... <= 60 then 1 else 0 end) as \"31-60 days\",\n  sum(case when ... > 60 and ... <= 90 then 1 else 0 end) as \"61-90 days\",\n  sum(case when ... > 90 and ... <= 120 then 1 else 0 end) as \"91-120 days\",\n  sum(case when ... > 120 then 1 else 0 end) as \">120 days\"\nfrom catalog_sales, warehouse, ship_mode, call_center, date_dim\nwhere d_month_seq between 1193 and 1216\n  and cs_ship_date_sk = d_date_sk\n  and cs_warehouse_sk = w_warehouse_sk\n  and cs_ship_mode_sk = sm_ship_mode_sk\n  and cs_call_center_sk = cc_call_center_sk\n  and cs_list_price between 271 and 300\n  and sm_type = 'REGULAR' and cc_class = 'small' and w_gmt_offset = -5\ngroup by substring(w_warehouse_name,1,20), sm_type, cc_name\norder by 1, 2, 3 limit 100",
        "output": {
          "rewrite_sets": [
            {
              "id": "rs_01",
              "transform": "date_cte_isolate",
              "nodes": {
                "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1216",
                "main_query": "SELECT SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name, SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS \"30 days\", ... FROM catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk WHERE cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5 GROUP BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name ORDER BY 1, 2, 3 LIMIT 100"
              },
              "invariants_kept": [
                "same result rows",
                "same CASE bucketing",
                "same aggregation"
              ],
              "expected_speedup": "2.3x",
              "risk": "low"
            }
          ]
        },
        "key_insight": "Principle: Dimension Isolation + Explicit Joins \u2014 materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma joins to explicit JOIN syntax. On PostgreSQL, both are required: the CTE reduces probe size, while explicit JOINs give the optimizer join-order freedom. Here: date_dim (730 from 73K rows) becomes a CTE hash table that catalog_sales probes; comma joins converted to explicit INNER JOIN.",
        "pattern_detection": "Look for star-schema queries with comma-separated joins including date_dim with a month_seq/year/date range filter. The query should NOT contain EXISTS/NOT EXISTS or INTERSECT/EXCEPT (those patterns are harmed by CTE isolation on PostgreSQL)."
      }
    }
  ],
  "all_available_examples": [
    {
      "id": "pg_date_consolidation",
      "speedup": "3.10x",
      "description": "When a query references date_dim 3+ times (d1 for sold, d2 for returned, d3 for "
    },
    {
      "id": "pg_date_cte_explicit_join",
      "speedup": "2.28x",
      "description": "Isolate a selective date_dim filter into a CTE AND convert all comma-separated j"
    },
    {
      "id": "pg_dimension_prefetch_star",
      "speedup": "3.32x",
      "description": "On multi-channel UNION queries with comma-separated implicit joins, pre-filter d"
    },
    {
      "id": "early_filter_decorrelate",
      "speedup": "27.80x (V2 DSB SF10, was 1.13x in V1)",
      "description": ""
    },
    {
      "id": "pg_explicit_join_materialized",
      "speedup": "8.56x",
      "description": "Convert comma joins to explicit INNER JOINs AND pre-filter date/item dimensions "
    },
    {
      "id": "inline_decorrelate_materialized",
      "speedup": "1465x (V2 DSB SF10, timeout rescue)",
      "description": ""
    },
    {
      "id": "pg_intersect_to_exists",
      "speedup": "1.78x",
      "description": "Convert INTERSECT set operations to EXISTS semi-joins. PostgreSQL implements INT"
    },
    {
      "id": "pg_materialized_dimension_fact_prefilter",
      "speedup": "12.07x (V2 DSB SF10, was 2.68x in V1)",
      "description": "Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then "
    },
    {
      "id": "pg_self_join_decomposition",
      "speedup": "3.93x",
      "description": "Eliminate duplicate fact table scans in self-join patterns by computing the aggr"
    },
    {
      "id": "pg_self_join_pivot",
      "speedup": "1.79x",
      "description": "When a query self-joins the same fact aggregation 6 times (e.g., ss1, ss2, ss3, "
    },
    {
      "id": "pg_set_operation_materialization",
      "speedup": "17.48x",
      "description": "When a query uses EXISTS + NOT EXISTS across 3 channels (store, web, catalog) wi"
    },
    {
      "id": "pg_shared_scan_decorrelate",
      "speedup": "8043.91x (timeout rescue)",
      "description": "When a correlated subquery re-scans the same fact table as the outer query, extr"
    },
    {
      "id": "pg_single_pass_aggregation",
      "speedup": "1.98x",
      "description": "Consolidate multiple fact table scans (store_sales, catalog_sales, web_sales) in"
    },
    {
      "id": "pg_state_avg_decorrelate",
      "speedup": "438.93x (timeout rescue)",
      "description": "Decorrelate a correlated subquery that computes per-state averages by extracting"
    }
  ],
  "engine_profile": {
    "engine": "postgresql",
    "version_tested": "14.3+",
    "profile_type": "engine_profile",
    "briefing_note": "This is field intelligence gathered from 53 DSB queries at SF5-SF10. PostgreSQL is a fundamentally different optimizer than DuckDB \u2014 it has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here. Use this to guide your analysis but apply your own judgment \u2014 every query is different. Add to this knowledge if you observe something new.",
    "strengths": [
      {
        "id": "BITMAP_OR_SCAN",
        "summary": "Multi-branch OR conditions on indexed columns are handled via BitmapOr \u2014 a single fact table scan with bitmap combination. Extremely efficient.",
        "field_note": "NEVER split OR conditions into UNION ALL branches on PostgreSQL. BitmapOr is categorically faster. We saw 0.21x and 0.26x observed \u2014 each UNION branch forced a full 7-table join + fact scan that BitmapOr avoids. The only conceivable case for OR-to-UNION on PG is when branches reference completely different tables, and even then it's risky."
      },
      {
        "id": "SEMI_JOIN_EXISTS",
        "summary": "EXISTS/NOT EXISTS uses semi-join with early termination. Stops scanning after the first match per outer row.",
        "field_note": "NEVER convert EXISTS to IN/NOT IN or to materialized CTEs with SELECT DISTINCT. The semi-join stops after first match \u2014 materializing forces a full DISTINCT scan of million-row fact tables. We saw 0.50x (3 DISTINCT CTEs vs 3 semi-joins) and 0.86x (UNION ALL CTE without dedup vs OR'd EXISTS short-circuits) observed. Also: NOT IN has NULL-handling semantics that can block hash anti-join optimization."
      },
      {
        "id": "INNER_JOIN_REORDERING",
        "summary": "PostgreSQL freely reorders INNER JOINs based on estimated selectivity. The cost model works well for explicit JOIN...ON syntax.",
        "field_note": "Don't restructure INNER JOIN orders \u2014 the optimizer handles this well. Focus on queries where JOIN type (LEFT) prevents reordering, or where comma-joins confuse the cost model (see COMMA_JOIN_WEAKNESS gap)."
      },
      {
        "id": "INDEX_ONLY_SCAN",
        "summary": "When an index covers all requested columns, PostgreSQL reads only the index without touching the heap.",
        "field_note": "Dimension table lookups are already fast via index-only scans. Pre-filtering small dimensions (<10K rows) into CTEs adds materialization overhead with minimal benefit."
      },
      {
        "id": "PARALLEL_QUERY_EXECUTION",
        "summary": "PostgreSQL parallelizes large scans and aggregations across worker processes with partial aggregation finalization.",
        "field_note": "Large fact table scans are already parallelized. Restructuring into CTEs may reduce parallelism opportunities because CTE materialization is single-threaded."
      },
      {
        "id": "JIT_COMPILATION",
        "summary": "PostgreSQL JIT-compiles complex expressions and tuple deforming for long-running queries.",
        "field_note": "Complex WHERE expressions have low per-row overhead due to JIT. Simplifying expressions for performance is usually unnecessary."
      }
    ],
    "gaps": [
      {
        "id": "COMMA_JOIN_WEAKNESS",
        "priority": "HIGH",
        "what": "Implicit comma-separated FROM tables (FROM t1, t2, t3 WHERE t1.id = t2.id) are treated as cross products initially. The cost model is significantly weaker on comma-joins than on explicit JOIN...ON syntax.",
        "why": "The planner's join search space is less constrained with comma-joins. Explicit JOINs provide structural hints that help the optimizer find better plans faster, especially for 5+ table joins.",
        "opportunity": "Convert comma-joins to explicit JOIN...ON syntax. This alone can unlock 2-3x improvements. Best when combined with date_cte_isolate.",
        "what_worked": [
          "3.32x \u2014 comma-joins to explicit JOINs + date CTE on multi-channel UNION query",
          "2.28x \u2014 same pattern on star schema",
          "1.14x \u2014 JOIN conversion alone"
        ],
        "what_didnt_work": [],
        "field_notes": [
          "Look for FROM t1, t2, t3 WHERE ... syntax. 5+ comma-separated tables is the sweet spot.",
          "EXPLAIN will show unexpected join orders or high-cost nested loops when comma-joins confuse the cost model.",
          "The win usually comes from explicit JOINs + CTE together, not CTE alone. date_cte_isolate without JOIN conversion is often neutral or harmful.",
          "This is our most reliable PG optimization \u2014 convert the implicit syntax and the optimizer rewards you.",
          "Validate at target scale \u2014 SF5 wins don't predict SF10 on PG (one query went from 9.62x to 0.97x)."
        ]
      },
      {
        "id": "CORRELATED_SUBQUERY_PARALYSIS",
        "priority": "HIGH",
        "what": "Cannot automatically decorrelate complex correlated subqueries. Correlated scalar subqueries with aggregates are executed as nested-loop with repeated evaluation.",
        "why": "Same limitation as DuckDB \u2014 correlation requires recognizing GROUP BY + JOIN equivalence. PostgreSQL does basic decorrelation for simple IN/EXISTS but fails on complex aggregate correlations.",
        "opportunity": "Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.",
        "what_worked": [
          "4428x \u2014 timeout recovery. Unbounded correlated subquery converted to explicit JOIN.",
          "391x \u2014 same pattern, timeout to sub-second."
        ],
        "what_didnt_work": [],
        "field_notes": [
          "Look for WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key) patterns.",
          "EXPLAIN will show SubPlan or nested-loop with repeated subquery execution if the optimizer failed to decorrelate.",
          "These are often the queries that time out \u2014 if a DSB query runs >10s, check for correlated scalar subqueries first.",
          "Simple IN/EXISTS correlation is already handled by PG's semi-join optimization \u2014 only complex aggregate correlations need manual decorrelation.",
          "CRITICAL: when decorrelating, preserve ALL filters from the original subquery in the new CTE.",
          "Validate at target scale \u2014 decorrelation wins are usually robust across scales, but verify on SF10."
        ]
      },
      {
        "id": "NON_EQUI_JOIN_INPUT_BLINDNESS",
        "priority": "HIGH",
        "what": "Cannot pre-filter fact tables before non-equi join operations (date arithmetic, range comparisons, quantity < quantity). Non-equi joins fall back to nested-loop, which is O(N*M).",
        "why": "Hash joins require equi-conditions. Non-equi joins fall back to nested-loop, which processes all input rows. The optimizer cannot recognize that reducing N or M via pre-filtering would dramatically reduce cost.",
        "opportunity": "Reduce fact table input size via filtered CTE before the non-equi join.",
        "what_worked": [
          "2.68x \u2014 pre-filtered catalog_sales by wholesale_cost range before non-equi quantity comparison with inventory. Reduced nested-loop input by ~70%."
        ],
        "what_didnt_work": [
          "0.79x \u2014 pre-filtered with UNION/OR superset (loose filter). CTE fence blocked dimension predicate pushdown."
        ],
        "field_notes": [
          "Look for non-equi join conditions: >, <, BETWEEN, date arithmetic, quantity comparisons.",
          "EXPLAIN will show nested-loop join with high row estimates on both sides.",
          "A simple range filter on the fact table (e.g., wholesale_cost BETWEEN 34 AND 54) works well. A union/OR superset filter does NOT \u2014 it materializes too many rows.",
          "Only pre-filter when one side of the non-equi join is a large fact table. Small dimension tables (<10K rows) don't benefit.",
          "The CTE fence cost is negligible vs the non-equi join savings when the filter is tight.",
          "Validate at target scale \u2014 non-equi join cost grows super-linearly, so wins tend to hold or improve at larger scales."
        ]
      },
      {
        "id": "CTE_MATERIALIZATION_FENCE",
        "priority": "MEDIUM",
        "what": "PostgreSQL materializes CTEs by default (multi-referenced) or by choice (AS MATERIALIZED). This creates a hard optimization fence \u2014 no predicate pushdown from outer query into CTE. This makes CTE-based strategies a double-edged sword on PG.",
        "why": "CTE is computed and stored in memory/temp before the outer query executes. Any WHERE clause filters in the outer query cannot be pushed back into the CTE definition. Single-reference CTEs may be inlined in PG 12+, but multi-referenced CTEs are always materialized.",
        "opportunity": "Use materialization STRATEGICALLY: materialize when the CTE is expensive and reused multiple times. Avoid CTEs that fence off predicate pushdown for single-use cases.",
        "what_worked": [
          "1.95x \u2014 strategic materialization prevented redundant fact table scan multiplication"
        ],
        "what_didnt_work": [
          "0.74x \u2014 CTE fence blocked predicate pushdown that worked in original",
          "0.77x \u2014 date_cte_isolate added fence that blocked INTERSECT optimization",
          "0.65x \u2014 duplicated 18-table CTE body to push filters inside. NEVER do this \u2014 computing an 18-table join twice is always worse than computing once and filtering."
        ],
        "field_notes": [
          "NEVER duplicate a CTE body to push a filter inside when the CTE contains 5+ table joins. Filter the materialized result with WHERE, don't recompute.",
          "Do NOT use the AS MATERIALIZED keyword on CTEs. Write plain CTEs: 'name AS (SELECT ...)'. PG auto-materializes when beneficial. Forcing materialization on small dimension CTEs (<1000 rows) adds temp-table I/O overhead (0.69x observed).",
          "CTE fence + EXISTS = disaster. If the query uses EXISTS/NOT EXISTS, a CTE that fences off the semi-join optimization is actively harmful.",
          "CTE fence + INTERSECT/EXCEPT = harmful. Set operations handle their inputs efficiently inline. A CTE fence per branch adds overhead.",
          "A CTE result referenced 2+ times is materialized once, probed many \u2014 this IS the valid use case for CTEs on PG.",
          "When a date_cte_isolate CTE is applied to UNION ALL branches, apply to ALL branches or NONE. Partial application creates asymmetric plans."
        ]
      },
      {
        "id": "CROSS_CTE_PREDICATE_BLINDNESS",
        "priority": "MEDIUM",
        "what": "Same gap as DuckDB but WORSE on PostgreSQL because CTE materialization fence makes it more impactful. Predicates in the outer WHERE cannot propagate into materialized CTEs.",
        "why": "Even single-reference CTEs may be materialized (version-dependent). The optimizer does not trace data lineage through CTE boundaries.",
        "opportunity": "Same as DuckDB: pre-filter into CTE definition. But be more cautious \u2014 only when the CTE is clearly suboptimal.",
        "what_worked": [
          "3.32x \u2014 date filter + comma-join conversion (the combo is key)",
          "2.28x \u2014 date CTE with explicit JOIN"
        ],
        "what_didnt_work": [
          "0.97x \u2014 won at SF5 (9.62x) but neutral at SF10. Cost model unreliability across scale.",
          "0.55x \u2014 over-decomposed an already-efficient query"
        ],
        "field_notes": [
          "Convert comma-joins to explicit JOINs simultaneously \u2014 the CTE alone often isn't enough on PG.",
          "EXPLAIN will show sequential scan on dimension table without index condition \u2014 that's the signal.",
          "Don't use this on queries with INTERSECT, EXCEPT, or set operations \u2014 the CTE fence blocks set operation optimization.",
          "If the query already returns quickly (<100ms), the CTE materialization overhead can negate any savings.",
          "Validate at target scale \u2014 SF5 wins don't reliably predict SF10 on PostgreSQL."
        ]
      }
    ],
    "set_local_config_intel": {
      "briefing_note": "Field intelligence from 52 DSB queries at SF10 (PG 14.3). 25 config wins, 3-race validated. Config tuning is ADDITIVE to SQL rewrite \u2014 not a substitute. 6 strategy categories: enable_mergejoin_off (6 wins, avg +50.6%), random_page_cost+cache (6 wins, avg +71.1%), par4 (5 wins, avg +14.3%), enable_nestloop_off (3 wins, avg +60.4%), work_mem+par4 (3 wins, avg +25.1%), enable_sort_off (2 wins, avg +36.5%). CRITICAL WARNING: EXPLAIN ANALYZE cost gaps do NOT predict runtime gains \u2014 6 false positives caught where EXPLAIN showed 38-84% improvement but 3-race showed 0% or regression.",
      "rules": [
        {
          "id": "MERGE_JOIN_DISABLE",
          "trigger": "EXPLAIN shows Merge Join with Sort node below it on inputs > 10K rows",
          "config": "/*+ Set(enable_mergejoin off) */ or SET LOCAL enable_mergejoin = 'off'",
          "evidence": "6 wins: +82.5%, +68.2%, +66.9%, +60.2%, +17.1%, +8.6%. Highest impact single hint. Tight race variance (e.g., 68.1-68.4%).",
          "risk": "LOW when Sort+MJ visible in EXPLAIN. Do NOT disable on pre-sorted data or index-ordered inputs."
        },
        {
          "id": "SSD_COST_MODEL_FIX",
          "trigger": "Seq Scan on fact table despite btree index on join/filter columns",
          "config": "SET LOCAL random_page_cost = '1.1'; SET LOCAL effective_cache_size = '48GB'",
          "evidence": "6 wins: +89.0%, +83.2%, +73.4%, +82.5%, +52.5%, +46.0%. Rescued 3 rewrite regressions. CRITICAL: nonlinear interaction \u2014 neither param alone sufficient.",
          "risk": "LOW on SSD storage. Zero regressions observed across 52 queries."
        },
        {
          "id": "PARALLEL_COST_REDUCTION",
          "trigger": "Large Seq Scan (>100K rows) without Gather/Parallel node, query > 500ms",
          "config": "SET LOCAL max_parallel_workers_per_gather = '4'; SET LOCAL parallel_setup_cost = '100'; SET LOCAL parallel_tuple_cost = '0.001'",
          "evidence": "5 standalone wins: +28.2%, +17.4%, +12.5%, +7.0%, +6.2%. Also in 10+ combo wins. Prefer cost reduction over max_workers forcing alone.",
          "risk": "MEDIUM. CRITICAL: 7.34x REGRESSION observed on 244ms query. NEVER on queries < 500ms. par4-alone caused -15.3% on one query \u2014 must include work_mem."
        },
        {
          "id": "SORT_SPILL_WORK_MEM",
          "trigger": "EXPLAIN shows Hash Batches > 1 or Sort Space Type = 'Disk'",
          "config": "work_mem sized by sort/hash op count: \u22642 ops \u2192 512MB, 3-5 \u2192 256MB, 6+ \u2192 128MB",
          "evidence": "4 wins: +41.5% (wm512+par), +17.9% (wm256+par), +16.0% (wm256+par), +11.4% (wm256 alone). Often needs par4 to realize full benefit.",
          "risk": "LOW. work_mem is per-operation \u2014 count sort+hash ops before sizing."
        },
        {
          "id": "NESTED_LOOP_DISABLE",
          "trigger": "Nested Loop in EXPLAIN with both inputs > 10K rows and equi-join condition exists",
          "config": "/*+ Set(enable_nestloop off) */ or SET LOCAL enable_nestloop = 'off'",
          "evidence": "3 wins: +81.3%, +57.5% (with par4), +42.5% (with par4). One query was completely config-resistant before this hint.",
          "risk": "HIGH. NL_off caused -1454% regression on one query. NEVER on correlated subqueries (NL is correct there \u2014 use SQL decorrelation P2 instead). NEVER when NL is the correct plan shape."
        },
        {
          "id": "SORT_DISABLE",
          "trigger": "Sort node on index-ordered data or where hash aggregation is viable",
          "config": "SET LOCAL enable_sort = 'off'",
          "evidence": "2 wins: +68.2% (with MJ_off), +4.7%. One had high variance (3.2-7.7%).",
          "risk": "MEDIUM. Forces hash-based execution. Validate carefully \u2014 high variance observed."
        },
        {
          "id": "EXPLAIN_FALSE_POSITIVE_WARNING",
          "trigger": "ALWAYS \u2014 applies to all config tuning decisions",
          "config": "3-race validate ALL config changes",
          "evidence": "6 false positives caught: geqo_off (EXPLAIN +38% \u2192 runtime -254%), ALL 12 configs (EXPLAIN +84% \u2192 runtime 0%), one query (EXPLAIN +81% \u2192 runtime -1.3%), one query (EXPLAIN +74% \u2192 runtime -2.4%), 10 hints (EXPLAIN +15% \u2192 runtime 0%), par4 (EXPLAIN +25% \u2192 race -15.3%).",
          "risk": "CRITICAL. EXPLAIN ANALYZE cost gaps measure plan cost, not runtime. Buffer cache, I/O patterns, and parallelism effects are invisible to EXPLAIN."
        }
      ],
      "combo_patterns": [
        "rpc+cache (C2) + MJ_off (C1): +82.5% \u2014 index scan tips + sort elimination",
        "NL_off (C5) + par4 (C3): +57.5%, +42.5% \u2014 hint alone insufficient, parallelism adds 12-35%",
        "wm512 (C4) + par4 (C3): +41.5%, +6.2% \u2014 CRITICAL: par4 alone regresses without work_mem",
        "sort_off (C6) + MJ_off (C1): +68.2% \u2014 eliminates both sort and merge overhead",
        "rpc+cache (C2) + par4 (C3): +73.4%, +52.5% \u2014 tips to index + parallelizes remaining scans"
      ],
      "key_findings": [
        "Config tuning is ADDITIVE to SQL rewrite \u2014 not a substitute. Best results combine good rewrite + targeted config.",
        "EXPLAIN ANALYZE cost gaps do NOT predict runtime gains. 6 false positives caught (38-84% EXPLAIN gap \u2192 0% or regression). Always 3-race validate.",
        "enable_mergejoin_off is the single highest-impact hint (6 wins, avg +50.6%). Look for Sort+Merge Join in EXPLAIN.",
        "random_page_cost=1.1 + effective_cache_size=48GB have a nonlinear interaction \u2014 neither alone sufficient. Together: 6 wins, avg +71.1%.",
        "Forced parallelism (max_parallel_workers_per_gather=4) is DANGEROUS on fast queries (<500ms). 7.34x REGRESSION observed.",
        "work_mem + parallelism must be applied together \u2014 par4 alone on hash-heavy queries causes spill regression (-15.3% observed).",
        "Combo patterns (hint + config) beat either alone. Example: hint alone +23%, hint+par4 +57.5%."
      ]
    },
    "scale_sensitivity_warning": "PostgreSQL optimizations validated at SF5 do NOT reliably predict SF10 behavior. 7 queries that won at SF5 regressed at SF10 (one went from 9.62x to 0.97x). Always validate at target scale. Cost estimates are overconfident on sample data."
  },
  "constraints": [
    {
      "id": "COMPLETE_OUTPUT",
      "severity": "CRITICAL",
      "description": "The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns.",
      "constraint_rules": [
        {
          "rule": "ALL_COLUMNS_PRESENT",
          "description": "Every column in the original SELECT list must appear in the rewritten SELECT list."
        },
        {
          "rule": "NO_COLUMN_RENAME",
          "description": "Column aliases must be preserved exactly. If the original says 'AS total_sales', the rewrite must use the same alias."
        },
        {
          "rule": "PRESERVE_COLUMN_ORDER",
          "description": "Columns must appear in the same order as the original SELECT list."
        }
      ],
      "prompt_instruction": "The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original."
    },
    {
      "id": "CTE_COLUMN_COMPLETENESS",
      "severity": "CRITICAL",
      "description": "When creating or modifying a CTE, its SELECT list MUST include ALL columns that downstream nodes reference. Check the Node Contracts and Downstream Usage sections before writing any CTE.",
      "failure_rate": "Caused 54% of all execution errors (7 of 13 failures)",
      "observed_failures": [
        {
          "query": "Q21",
          "error": "prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q76",
          "error": "filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q24",
          "error": "filtered_base CTE omits s_state, i_current_price, i_manager_id, i_units, i_size needed by downstream CTEs",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q64",
          "error": "filtered_store_sales CTE omits ss_sold_date_sk needed for JOIN in cross_sales CTE",
          "type": "MISSING_COLUMN_IN_CTE"
        },
        {
          "query": "Q60",
          "error": "ss/ws/cs CTEs reference item.i_item_sk and item.i_category in WHERE but item table not joined in CTE",
          "type": "MISSING_TABLE_IN_CTE"
        },
        {
          "query": "Q13",
          "error": "filtered_store_sales CTE references hd_demo_sk, cd_demo_sk from tables not joined in the CTE",
          "type": "MISSING_TABLE_IN_CTE"
        },
        {
          "query": "Q2",
          "error": "Ambiguous d_date_sk and d_week_seq columns between CTE and re-joined date_dim",
          "type": "AMBIGUOUS_COLUMN_REF"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CHECK_DOWNSTREAM_REFS",
          "description": "Before writing a CTE, check the Downstream Usage section. Every column listed in downstream_refs for that node MUST appear in the CTE's SELECT list."
        },
        {
          "rule": "CHECK_JOIN_COLUMNS",
          "description": "If a downstream node JOINs on a column from this CTE (e.g., ON cte.d_date_sk = ...), that column MUST be in the CTE's SELECT."
        },
        {
          "rule": "CHECK_TABLE_PRESENCE",
          "description": "If a CTE's WHERE clause references columns from a table, that table MUST be in the CTE's FROM/JOIN clause."
        }
      ],
      "prompt_instruction": "CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error."
    },
    {
      "id": "LITERAL_PRESERVATION",
      "severity": "CRITICAL",
      "description": "All literal values (strings, numbers, dates) from the original query MUST be preserved EXACTLY in the rewrite",
      "failure_rate": "100% of Q2-Q16 failures were caused by hallucinated literals",
      "observed_failures": [
        {
          "query": "Q2",
          "original": "d_year = 2001, d_year = 2002",
          "hallucinated": "d_year = 1998, d_year = 1999",
          "type": "YEAR_HALLUCINATION"
        },
        {
          "query": "Q7",
          "original": "cd_gender = 'M', cd_marital_status = 'S', d_year = 2000",
          "hallucinated": "cd_gender = 'F', cd_marital_status = 'W', d_year = 2001",
          "type": "MULTIPLE_LITERAL_HALLUCINATION"
        },
        {
          "query": "Q10",
          "original": "d_year = 2002, ca_county IN ('Rush County', 'Toole County', 'Jefferson County', 'Dona Ana County', 'La Porte County')",
          "hallucinated": "d_year = 2001, ca_county IN ('Storey County', 'Marquette County', 'Warren County', 'Cochran County', 'Kandiyohi County')",
          "type": "YEAR_AND_STRING_HALLUCINATION"
        },
        {
          "query": "Q13",
          "original": "cd_marital_status = 'M', cd_education_status = 'Advanced Degree'",
          "hallucinated": "cd_marital_status = 'D', cd_education_status = 'Unknown'",
          "type": "STRING_HALLUCINATION"
        },
        {
          "query": "Q16",
          "original": "ca_state = 'GA', cc_county = 'Williamson County', d_date BETWEEN '2002-02-01' AND '2002-04-02'",
          "hallucinated": "ca_state = 'WV', cc_county IN ('Ziebach County', ...), d_date BETWEEN '2002-4-01' AND ...",
          "type": "STATE_COUNTY_DATE_HALLUCINATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "EXACT_STRING_MATCH",
          "description": "String literals in WHERE clauses must be copied character-for-character",
          "examples": [
            "'M' not 'F'",
            "'GA' not 'WV'",
            "'Rush County' not 'Storey County'"
          ]
        },
        {
          "rule": "EXACT_NUMBER_MATCH",
          "description": "Numeric literals (years, amounts, counts) must be copied exactly",
          "examples": [
            "2000 not 2001",
            "2002 not 2001",
            "100.00 not 150.00"
          ]
        },
        {
          "rule": "EXACT_DATE_MATCH",
          "description": "Date literals must be copied exactly, including format",
          "examples": [
            "'2002-02-01' not '2002-4-01'"
          ]
        },
        {
          "rule": "EXACT_LIST_MATCH",
          "description": "IN lists must contain the exact same values in the same order",
          "examples": [
            "IN ('TX', 'OH', 'TX') not IN ('SD', 'KS', 'MI')"
          ]
        }
      ],
      "prompt_instruction": "CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED."
    },
    {
      "id": "PG_OR_TO_UNION_BLOCK",
      "severity": "CRITICAL",
      "engine": "postgresql",
      "description": "NEVER apply or_to_union on PostgreSQL when OR branches share the same fact table joins. PostgreSQL handles OR via BitmapOr index scans in a single pass. UNION ALL forces N separate fact table scans.",
      "failure_rate": "2/2 queries regressed severely (0.21x-0.26x)",
      "observed_failures": [
        {
          "query": "DSB Q085_agg",
          "speedup": "0.21x",
          "original": "7-table join with 2 nested OR blocks (demographics + geography)",
          "rewrite": "3 UNION ALL branches, each repeating the full 7-table join",
          "problem": "3x scans of web_sales + web_returns fact tables. Original did 1 scan with BitmapOr."
        },
        {
          "query": "DSB Q091_spj_spj",
          "speedup": "0.26x",
          "original": "7-table join with 2-branch OR on demographics",
          "rewrite": "7 CTEs + 2 branch data CTEs + UNION ALL",
          "problem": "Over-decomposed into 7+ CTEs. Doubled catalog_returns scan. CTE materialization overhead."
        }
      ],
      "constraint_rules": [
        {
          "rule": "BLOCK_ON_POSTGRESQL",
          "description": "Do not use or_to_union transform on PostgreSQL databases",
          "rationale": "PostgreSQL BitmapOr handles OR predicates in a single scan. UNION ALL always causes multi-scan regression."
        },
        {
          "rule": "SHARED_FACT_TABLE_CHECK",
          "description": "If all OR branches scan the same fact table with same joins, never split to UNION",
          "rationale": "The only valid case for OR\u2192UNION is when branches reference different tables or indexes."
        }
      ],
      "when_or_to_union_helps_on_pg": [
        "Almost never on PostgreSQL",
        "Only when OR branches reference completely different tables"
      ],
      "when_or_to_union_hurts_on_pg": [
        "Always when branches share the same fact table (web_sales, store_sales, catalog_sales)",
        "Always when OR is on filter predicates (demographics, geography)",
        "Always when combined with CTE over-decomposition"
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER use OR\u2192UNION on PostgreSQL. PostgreSQL handles OR conditions efficiently via BitmapOr index scans in a single pass. Converting OR to UNION ALL forces multiple scans of fact tables and causes catastrophic regressions (0.21x-0.26x observed on DSB benchmark). Keep the original OR structure."
    },
    {
      "id": "SEMANTIC_EQUIVALENCE",
      "severity": "CRITICAL",
      "description": "The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive.",
      "constraint_rules": [
        {
          "rule": "SAME_ROWS",
          "description": "The rewritten query must produce the same set of rows as the original. No rows may be added or removed."
        },
        {
          "rule": "SAME_COLUMNS",
          "description": "The rewritten query must return the same columns in the same order with the same names and data types."
        },
        {
          "rule": "SAME_ORDERING",
          "description": "If the original query has an ORDER BY clause, the rewritten query must preserve the same ordering."
        }
      ],
      "prompt_instruction": "The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set \u2014 even by one row, one column, or a different sort order \u2014 is WRONG and will be REJECTED."
    },
    {
      "id": "KEEP_EXISTS_AS_EXISTS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Prefer preserving EXISTS/NOT EXISTS subqueries. Converting to IN/NOT IN risks NULL-handling changes; converting to JOINs risks duplicate rows.",
      "observed_failures": [
        {
          "problem": "Converting NOT EXISTS to NOT IN changes behavior when the subquery column contains NULLs. NOT IN with NULLs returns no rows.",
          "type": "NULL_SEMANTIC_CHANGE"
        },
        {
          "problem": "Converting EXISTS to JOIN can produce duplicate rows when the subquery matches multiple rows per outer row.",
          "type": "DUPLICATE_ROW_INTRODUCTION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "AVOID_EXISTS_TO_IN",
          "description": "Avoid converting EXISTS/NOT EXISTS to IN/NOT IN unless the subquery column is provably NOT NULL (has a NOT NULL constraint or is a primary key)."
        },
        {
          "rule": "EXISTS_TO_JOIN_NEEDS_DISTINCT",
          "description": "Converting EXISTS to JOIN requires SELECT DISTINCT or GROUP BY to prevent row duplication when the subquery matches multiple rows per outer row."
        }
      ],
      "override_conditions": [
        "The subquery join column has a NOT NULL constraint or is a primary key (safe for IN conversion)",
        "The subquery returns at most 1 row per outer row (1:1 relationship, safe for JOIN)",
        "EXISTS is converted to JOIN + DISTINCT/GROUP BY to explicitly handle duplicates"
      ],
      "prompt_instruction": "DEFAULT: Preserve EXISTS/NOT EXISTS as-is. NOT EXISTS\u2192NOT IN breaks with NULLs; EXISTS\u2192JOIN can duplicate rows. HOWEVER: if the join column is NOT NULL (PK or explicit constraint), EXISTS\u2192IN is safe. If the subquery is 1:1 with the outer query, EXISTS\u2192JOIN is safe. The exploration worker MAY convert EXISTS with written proof of NULL safety or 1:1 cardinality."
    },
    {
      "id": "NO_CROSS_JOIN_DIMENSIONS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid CROSS JOINing dimension tables into a single CTE. The Cartesian product can explode row counts and prevent index use on fact tables.",
      "failure_rate": "Caused 0.0076x regression on Q080 (132x slower) when 3 dimensions were cross-joined",
      "observed_failures": [
        {
          "query": "Q080_multi",
          "regression": "0.0076x (57ms -> 7500ms)",
          "broken_rewrite": "filtered_dims AS (SELECT d_date_sk, i_item_sk, p_promo_sk FROM date_dim CROSS JOIN item CROSS JOIN promotion WHERE ...)",
          "problem": "CROSS JOIN created 120K-row CTE (30 \u00d7 200 \u00d7 20), then 3-key join prevented index use on fact tables.",
          "type": "CROSS_JOIN_DIMENSION_EXPLOSION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_SEPARATE_DIMENSION_CTES",
          "description": "Each dimension table should generally be its own CTE with its own filter. Combining via CROSS JOIN risks Cartesian explosion."
        }
      ],
      "override_conditions": [
        "Only 2 dimensions are joined (not 3+) AND the product is <1000 rows",
        "The dimensions share a foreign key (not a true Cartesian \u2014 it's a filtered JOIN)",
        "The combined CTE replaces N separate semi-joins with 1 multi-key join on the fact table"
      ],
      "prompt_instruction": "DEFAULT: Keep each dimension as a SEPARATE CTE (filtered_date, filtered_item, etc.). Cross-joining 3 dimensions caused 0.0076x on Q080 (30\u00d7200\u00d720 = 120K rows). HOWEVER: joining exactly 2 small dimensions (<1000 row product) via a foreign key (not Cartesian) may be acceptable if it reduces total join count on the fact table. The exploration worker MAY attempt a 2-dimension join with size estimate. Never cross-join 3+ dimensions."
    },
    {
      "id": "NO_MATERIALIZE_EXISTS",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid converting EXISTS/NOT EXISTS subqueries into materialized CTEs with full table scans. EXISTS uses semi-join short-circuiting which is typically more efficient.",
      "failure_rate": "Caused 0.14x and 0.54x regressions (7x and 2x slowdowns)",
      "observed_failures": [
        {
          "query": "Q16",
          "regression": "0.14x (18ms -> 126ms)",
          "original": "EXISTS (SELECT * FROM catalog_sales cs2 WHERE cs1.cs_order_number = cs2.cs_order_number AND cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)",
          "broken_rewrite": "WITH multi_warehouse_orders AS (SELECT DISTINCT cs_order_number FROM catalog_sales GROUP BY cs_order_number HAVING MIN(cs_warehouse_sk) <> MAX(cs_warehouse_sk))",
          "type": "EXISTS_TO_FULL_SCAN_CTE"
        },
        {
          "query": "Q95",
          "regression": "0.54x (390ms -> 728ms)",
          "original": "EXISTS(SELECT 1 FROM ws_wh WHERE ws_wh.ws_order_number = ws1.ws_order_number)",
          "broken_rewrite": "WITH multi_warehouse_orders AS (SELECT DISTINCT ws_order_number FROM ws_wh)",
          "type": "EXISTS_TO_MATERIALIZED_DISTINCT"
        }
      ],
      "observed_successes": [
        {
          "query": "Q14",
          "speedup": "1.83x",
          "context": "intersect_to_exists: INTERSECT converted to EXISTS for semi-join short-circuit. Shows EXISTS restructuring CAN help when applied in the right direction."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_EXISTS_SEMI_JOIN",
          "description": "EXISTS and NOT EXISTS use semi-join optimization that short-circuits after finding the first match. Converting to materialized CTEs usually forces a full scan."
        },
        {
          "rule": "AVOID_FULL_TABLE_DISTINCT_CTE",
          "description": "Avoid creating CTEs like SELECT DISTINCT key FROM large_table to replace EXISTS. The CTE scans the entire table; EXISTS can stop after one match."
        }
      ],
      "override_conditions": [
        "The EXISTS subquery is correlated and executed many times (optimizer fails to decorrelate it)",
        "The CTE would be small (<10K rows) and probed multiple times, amortizing materialization cost",
        "The EXISTS is inside a UNION ALL branch where each branch re-executes the same correlated subquery"
      ],
      "prompt_instruction": "DEFAULT: Keep EXISTS/NOT EXISTS as-is \u2014 semi-join short-circuiting is usually faster than materialization. Converting to CTEs caused 0.14x on Q16 and 0.54x on Q95. HOWEVER: if the correlated EXISTS is executed many times and the optimizer fails to decorrelate it, materializing into a small CTE (<10K rows) probed via JOIN may help. The exploration worker MAY attempt this with reasoning about correlation frequency and CTE size."
    },
    {
      "id": "NO_MATERIALIZED_KEYWORD_PG",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "On PostgreSQL, do not add the AS MATERIALIZED keyword to CTEs unless a gold example explicitly uses it. PG 12+ already materializes CTEs referenced more than once. Adding AS MATERIALIZED on small CTEs prevents the optimizer from inlining them, adding temp-table I/O overhead.",
      "failure_rate": "Caused 0.69x regression on Q080",
      "observed_failures": [
        {
          "query": "Q080_multi",
          "regression": "0.69x (57ms -> 83ms)",
          "broken_rewrite": "filtered_date AS MATERIALIZED (SELECT d_date_sk FROM date_dim WHERE ...)",
          "problem": "MATERIALIZED keyword forced temp-table spill for tiny CTEs (30 rows), adding overhead that exceeded filtering benefit.",
          "type": "UNNECESSARY_MATERIALIZATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_EXPLICIT_MATERIALIZED",
          "description": "Do not add the AS MATERIALIZED keyword. Write plain CTEs: 'name AS (SELECT ...)'. PostgreSQL will materialize them automatically when beneficial."
        }
      ],
      "prompt_instruction": "Do NOT use the AS MATERIALIZED keyword on CTEs. Write plain CTEs: 'name AS (SELECT ...)'. PostgreSQL automatically materializes CTEs when beneficial. Forcing materialization on small dimension CTEs (< 1000 rows) adds temp-table I/O overhead that causes regressions (0.69x observed). The proven gold examples use plain CTEs without MATERIALIZED."
    },
    {
      "id": "NO_UNFILTERED_DIMENSION_CTE",
      "severity": "HIGH",
      "description": "Never create a 'filtered' dimension CTE that has no WHERE clause. A CTE that selects all rows from a dimension table is pure materialization overhead with zero filtering benefit.",
      "failure_rate": "Caused 0.85x regression on Q67",
      "observed_failures": [
        {
          "query": "Q67",
          "regression": "0.85x (4509ms -> 5291ms)",
          "broken_rewrite": "filtered_stores AS (SELECT s_store_sk, s_store_id FROM store), filtered_items AS (SELECT i_item_sk, i_category, i_class, i_brand, i_product_name FROM item)",
          "problem": "Both CTEs select ALL rows - no WHERE clause, no filtering. Pure overhead.",
          "type": "UNFILTERED_DIMENSION_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CTE_MUST_FILTER",
          "description": "Every dimension CTE you create MUST have a WHERE clause that reduces the row count. If a dimension table has no filter to apply, do NOT extract it into a CTE."
        },
        {
          "rule": "COLUMN_PROJECTION_IS_NOT_FILTERING",
          "description": "Selecting a subset of columns (SELECT a, b FROM table) is NOT filtering. The CTE still materializes all rows. Only a WHERE clause reduces rows."
        }
      ],
      "prompt_instruction": "Every CTE you create must include a WHERE clause that actually reduces row count. Selecting fewer columns is not filtering \u2014 the CTE still materializes every row. If a dimension table has no predicate to push down, leave it as a direct join in the main query instead of wrapping it in a CTE."
    },
    {
      "id": "OR_TO_UNION_GUARD",
      "severity": "HIGH",
      "overridable": true,
      "description": "Guard rails for or_to_union: branches should have different access paths (not same column) and be limited to 3 or fewer.",
      "observed_failures": [
        {
          "query": "Q90",
          "regression": "0.59x (16ms -> 27ms)",
          "original": "WHERE t.t_hour BETWEEN 10 AND 11 OR t.t_hour BETWEEN 16 AND 17",
          "broken_rewrite": "UNION ALL of two separate web_sales scans (one for AM hours, one for PM hours)",
          "problem": "Doubles the fact table scan. The OR on t_hour is trivial for the optimizer - it just checks two ranges on one column.",
          "type": "UNION_SAME_COLUMN_OR"
        },
        {
          "query": "Q13",
          "regression": "0.23x",
          "problem": "9 UNION branches from nested OR expansion (3 conditions x 3 values) caused 9x fact table scans.",
          "type": "UNION_BRANCH_EXPLOSION"
        },
        {
          "query": "Q48",
          "regression": "0.41x",
          "problem": "9 UNION branches from nested OR expansion caused severe regression from multiplied fact table scans.",
          "type": "UNION_BRANCH_EXPLOSION"
        }
      ],
      "observed_successes": [
        {
          "query": "Q88",
          "speedup": "6.28x",
          "context": "8 time-bucket subqueries on store_sales, each filtering distinct hour ranges via different WHERE clauses. Branches access genuinely different row subsets."
        },
        {
          "query": "Q10",
          "speedup": "1.49x",
          "context": "OR across different dimension table lookups creating distinct access paths."
        },
        {
          "query": "Q45",
          "speedup": "1.35x",
          "context": "OR conditions reference different tables/subqueries."
        }
      ],
      "constraint_rules": [
        {
          "rule": "OR_TO_UNION_REQUIRES_DIFFERENT_PATHS",
          "description": "or_to_union is most beneficial when OR conditions create fundamentally different access paths (e.g., across different tables or between a correlated subquery and a direct filter). Same-column ORs on trivial ranges are usually handled efficiently by the optimizer as a single scan."
        },
        {
          "rule": "OR_TO_UNION_PREFER_3_OR_FEWER",
          "description": "Prefer 3 or fewer UNION ALL branches. Nested ORs that expand into 9+ combinations are almost always harmful. 4-5 branches may be acceptable if each accesses genuinely different row subsets."
        }
      ],
      "override_conditions": [
        "Branches access genuinely different row subsets (different WHERE predicates, not just same-column ranges)",
        "Total branch count stays at 4-5 or fewer (not Cartesian expansion of nested ORs)",
        "EXPLAIN shows the fact table is already scanned N times in baseline, so splitting does not increase scan count",
        "Each branch filters to <20% of fact table rows (high selectivity per branch)"
      ],
      "prompt_instruction": "DEFAULT: Prefer 3 or fewer UNION ALL branches with different access paths per branch. Same-column ORs on simple ranges are usually handled efficiently by the optimizer. Nested ORs that expand into 4+ branches (e.g., 3 x 3 = 9 combinations) caused 0.23x-0.41x regressions. HOWEVER: or_to_union achieved 6.28x on Q88 where branches had genuinely different row subsets. The exploration worker MAY try 4-5 branches if each branch has distinct access paths and high selectivity. Provide reasoning."
    },
    {
      "id": "OR_TO_UNION_SELF_JOIN",
      "severity": "HIGH",
      "overridable": true,
      "description": "Avoid or_to_union on queries with self-joins. Splitting OR conditions on self-joined tables can create multiple independent scans that cannot share the self-join optimization.",
      "observed_failures": [
        {
          "query": "Q23",
          "regression": "0.51x",
          "problem": "Self-join on store_sales was split into separate UNION branches, each requiring its own full self-join, doubling execution time.",
          "type": "SELF_JOIN_SPLIT"
        }
      ],
      "constraint_rules": [
        {
          "rule": "AVOID_OR_TO_UNION_ON_SELF_JOINS",
          "description": "If a query contains a self-join (same table aliased twice), or_to_union is risky because the self-join must typically remain in a single query block to share the scan."
        }
      ],
      "override_conditions": [
        "The OR conditions are on a column NOT involved in the self-join predicate",
        "The self-join aliases have independent WHERE filters that make each branch selective",
        "EXPLAIN shows the self-join is already executed multiple times in baseline"
      ],
      "prompt_instruction": "DEFAULT: Avoid or_to_union when the query contains a self-join (same table with different aliases). Splitting forces each branch to independently perform the self-join (observed 0.51x on Q23). HOWEVER: if the OR conditions target a column not involved in the self-join predicate, or if each alias already has independent selective filters, splitting may still help. The exploration worker MAY attempt this with written reasoning about why the structural context differs from Q23."
    },
    {
      "id": "PG_CTE_DUPLICATION_BLOCK",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "NEVER duplicate a CTE body to push a filter inside when the CTE contains 5+ table joins. If a CTE is referenced N times in the original, the optimized version must reference it at most N times. Duplicating expensive CTEs forces the full join to execute multiple times.",
      "failure_rate": "1/1 query regressed severely (0.65x)",
      "observed_failures": [
        {
          "query": "DSB Q064_multi",
          "speedup": "0.65x",
          "original": "cross_sales CTE (18-table join) referenced twice with year filter in WHERE",
          "rewrite": "Duplicated cross_sales into cross_sales_1998 and cross_sales_1999, each with year pushed inside",
          "problem": "18-table join executed TWICE instead of once. Original computed it once and self-joined with year filter."
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_CTE_BODY_DUPLICATION",
          "description": "If a CTE has 5+ table joins, never duplicate its body to push a filter inside",
          "rationale": "The CTE computation cost dominates. Computing it twice always exceeds the filter benefit."
        },
        {
          "rule": "REUSE_MATERIALIZED_CTE",
          "description": "PostgreSQL materializes CTEs. Reuse the materialized result with WHERE filters instead of duplicating.",
          "rationale": "CTE materialization means the result is computed once and stored. Filter on the stored result, don't recompute."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER duplicate a CTE body to push a single-column filter inside. If the original has one CTE referenced multiple times, keep it as one CTE and filter in the WHERE clause. PostgreSQL materializes CTEs, so computing an expensive multi-table join twice is always worse than computing once and filtering. Observed 0.65x regression on 18-table CTE duplication."
    },
    {
      "id": "PG_EXISTS_TO_IN_BLOCK",
      "severity": "HIGH",
      "engine": "postgresql",
      "description": "NEVER convert EXISTS/NOT EXISTS correlated subqueries into IN/NOT IN with materialized CTEs on PostgreSQL. PostgreSQL's semi-join optimization for EXISTS uses early termination (stops after first match). Materializing into CTEs forces full DISTINCT aggregation of fact tables.",
      "failure_rate": "2/2 queries regressed (0.50x-0.86x)",
      "observed_failures": [
        {
          "query": "DSB Q069_multi",
          "speedup": "0.50x",
          "original": "EXISTS/NOT EXISTS subqueries against store_sales, web_sales, catalog_sales",
          "rewrite": "3 CTEs with DISTINCT customer_sk, then IN/NOT IN checks",
          "problem": "DISTINCT on multi-million-row fact tables is expensive. EXISTS semi-join stops after first match per row."
        },
        {
          "query": "DSB Q010_multi",
          "speedup": "0.86x",
          "original": "EXISTS against store_sales, OR'd EXISTS against web_sales/catalog_sales",
          "rewrite": "store_customers CTE + web_or_catalog_customers UNION ALL CTE",
          "problem": "UNION ALL without deduplication creates massive CTE. Original OR'd EXISTS short-circuits independently."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PRESERVE_EXISTS_SEMIJOIN",
          "description": "Keep EXISTS/NOT EXISTS as correlated subqueries on PostgreSQL",
          "rationale": "PostgreSQL converts EXISTS to efficient semi-join with early termination. CTE materialization loses this."
        },
        {
          "rule": "NO_DISTINCT_FACT_CTE",
          "description": "Never materialize SELECT DISTINCT customer_sk FROM fact_table into a CTE",
          "rationale": "DISTINCT on fact tables (millions of rows) is always expensive. Semi-join avoids the full scan."
        },
        {
          "rule": "NO_NOT_IN_REPLACEMENT",
          "description": "Never replace NOT EXISTS with NOT IN on PostgreSQL",
          "rationale": "NOT IN has NULL-handling semantics that can block hash anti-join optimization."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: NEVER convert EXISTS/NOT EXISTS to IN/NOT IN with materialized CTEs. PostgreSQL uses efficient semi-join with early termination for EXISTS. Materializing DISTINCT keys from fact tables forces full scans and loses the early-termination benefit. Observed 0.50x-0.86x regressions."
    },
    {
      "id": "REMOVE_REPLACED_CTES",
      "severity": "HIGH",
      "description": "When creating replacement CTEs, always remove the original CTEs from the WITH clause. Leaving dead/unused CTEs causes unnecessary materialization overhead.",
      "failure_rate": "Contributed to 0.49x and 0.68x regressions",
      "observed_failures": [
        {
          "query": "Q31",
          "regression": "0.49x (99ms -> 201ms)",
          "problem": "Created new store_sales_agg and web_sales_agg CTEs but left the original ss and ws CTEs in the WITH clause. Both old and new CTEs coexist, wasting materialization.",
          "type": "DEAD_CTE_OVERHEAD"
        },
        {
          "query": "Q74",
          "regression": "0.68x (493ms -> 724ms)",
          "problem": "Created 4 new year-specific CTEs but left the original year_total, year_total_store, year_total_web CTEs. Total of 8 CTEs instead of 4.",
          "type": "DEAD_CTE_OVERHEAD"
        }
      ],
      "constraint_rules": [
        {
          "rule": "REPLACE_NOT_APPEND",
          "description": "When your rewrite replaces a CTE with a new version, the original CTE node must be removed or overwritten. Do not define both the old and new CTE."
        }
      ],
      "prompt_instruction": "When creating replacement CTEs, overwrite the original by using the same node_id in your rewrite_sets, or ensure the original is removed from the WITH clause. Every CTE in the final query should be actively used \u2014 dead CTEs still get materialized and waste resources (caused 0.49x on Q31, 0.68x on Q74)."
    },
    {
      "id": "UNION_CTE_SPLIT_MUST_REPLACE",
      "severity": "HIGH",
      "description": "When splitting a UNION into separate CTEs, the original UNION must be eliminated. Creating CTEs that duplicate the UNION branches while keeping the original UNION doubles the work.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "UNION branches were extracted into CTEs but the original UNION ALL remained in the main query, causing each branch to be computed twice.",
          "type": "DUPLICATE_UNION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CTE_SPLIT_REPLACES_UNION",
          "description": "When applying union_cte_split, the final query must reference the CTEs instead of the original UNION. The total number of UNION ALL operations should not increase."
        }
      ],
      "prompt_instruction": "When applying union_cte_split (splitting UNION into CTEs), the original UNION must be eliminated from the main query. The main query should reference the split CTEs, not duplicate the UNION branches. If the rewritten query has more UNION ALL operations than the original, the rewrite is incorrect."
    },
    {
      "id": "DECORRELATE_MUST_FILTER_FIRST",
      "severity": "MEDIUM",
      "description": "When decorrelating a subquery into a JOIN, the replacement JOIN must include a selective filter. A decorrelation that produces an unfiltered cross-product is worse than the original correlated subquery.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "Correlated subquery was converted to JOIN without carrying over the original WHERE filters, producing a much larger intermediate result than the correlated version.",
          "type": "UNFILTERED_DECORRELATION"
        }
      ],
      "constraint_rules": [
        {
          "rule": "DECORRELATE_PRESERVES_FILTERS",
          "description": "When converting a correlated subquery to a JOIN + GROUP BY CTE, all WHERE conditions from the original subquery must be preserved in the CTE or JOIN condition. The replacement must not produce more rows than the original correlated subquery."
        }
      ],
      "prompt_instruction": "When decorrelating a correlated subquery into a JOIN, ensure all original WHERE filters are preserved in the replacement CTE or JOIN condition. A decorrelation without selective filters creates a cross-product that is larger than the original per-row correlated execution. The replacement CTE must filter to at most the same cardinality as the original subquery."
    },
    {
      "id": "DIMENSION_CTE_SAME_COLUMN_OR",
      "severity": "MEDIUM",
      "description": "Do not extract dimension CTE filters when the WHERE clause has OR conditions on the same column. Same-column ORs are efficiently handled by the optimizer in a single scan; CTE extraction adds overhead without benefit.",
      "observed_failures": [
        {
          "query": "Q37",
          "regression": "0.89x",
          "problem": "OR conditions on item.i_current_price ranges were extracted into separate CTEs, adding CTE materialization overhead without improving selectivity.",
          "type": "SAME_COLUMN_OR_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "KEEP_SAME_COLUMN_OR_INLINE",
          "description": "When OR conditions filter the same column (e.g., i_current_price BETWEEN X AND Y OR i_current_price BETWEEN A AND B), keep them inline in WHERE. Only extract dimension CTEs when filters span different columns or tables."
        }
      ],
      "prompt_instruction": "Do not create dimension CTEs to isolate OR conditions that filter the same column. The optimizer handles same-column ORs efficiently in a single scan. Only apply dimension_cte_isolate when filters span different columns or different dimension tables."
    },
    {
      "id": "EARLY_FILTER_CTE_BEFORE_CHAIN",
      "severity": "MEDIUM",
      "description": "Early filter CTEs must be referenced by the main query chain. An orphaned CTE that pre-filters data but is never joined back into the main query wastes materialization effort.",
      "observed_failures": [
        {
          "query": "multiple",
          "problem": "Early filter CTEs were created but not referenced in subsequent JOINs, resulting in wasted CTE materialization plus the original unfiltered joins remaining.",
          "type": "ORPHANED_FILTER_CTE"
        }
      ],
      "constraint_rules": [
        {
          "rule": "FILTER_CTE_MUST_BE_REFERENCED",
          "description": "Every early_filter CTE must be referenced by at least one downstream CTE or the main query. If a filter CTE is created, the original unfiltered table reference must be replaced with the CTE reference."
        }
      ],
      "prompt_instruction": "When creating an early_filter CTE, ensure it is actually referenced in the main query chain. The original unfiltered table reference must be replaced with the CTE reference. Do not create CTEs that filter a table if the main query still joins the original unfiltered table \u2014 this adds overhead without benefit."
    },
    {
      "id": "EXPLICIT_JOINS",
      "severity": "MEDIUM",
      "description": "Convert comma-separated implicit joins to explicit JOIN ... ON syntax. This gives the optimizer better join-order freedom.",
      "constraint_rules": [
        {
          "rule": "PREFER_EXPLICIT_JOIN",
          "description": "When the original query uses comma-separated tables with WHERE conditions for joining, convert to explicit JOIN ... ON syntax."
        }
      ],
      "prompt_instruction": "Convert comma-separated implicit joins to explicit JOIN ... ON syntax. This gives the optimizer better join-order freedom."
    },
    {
      "id": "MIN_BASELINE_THRESHOLD",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Be conservative with CTE-based transforms on queries with very short baseline runtimes. CTE materialization overhead can dominate when the query is already fast.",
      "failure_rate": "Caused 0.14x-0.59x regressions on queries under 50ms",
      "observed_failures": [
        {
          "query": "Q25",
          "regression": "0.50x (31ms -> 62ms)",
          "baseline_ms": 31,
          "transform": "prefetch_fact_join with 6 CTEs",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        },
        {
          "query": "Q90",
          "regression": "0.59x (16ms -> 27ms)",
          "baseline_ms": 16,
          "transform": "multi_dimension_prefetch with 4 CTEs + UNION ALL",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        },
        {
          "query": "Q16",
          "regression": "0.14x (18ms -> 126ms)",
          "baseline_ms": 18,
          "transform": "materialize_cte with 2 full-scan CTEs",
          "type": "CTE_OVERHEAD_ON_FAST_QUERY"
        }
      ],
      "constraint_rules": [
        {
          "rule": "CHECK_BASELINE_RUNTIME",
          "description": "If the Execution Plan shows estimated or actual runtime under 50ms, prefer minimal rewrites. DuckDB already optimizes simple star-join patterns efficiently."
        }
      ],
      "override_conditions": [
        "The transform reduces scan count (e.g., 3 scans \u2192 1 scan) even on a fast query",
        "The query is a component of a larger pipeline where cumulative savings matter",
        "The transform simplifies the query structure without adding CTEs (e.g., pushdown, decorrelate)"
      ],
      "prompt_instruction": "DEFAULT: If baseline is under 100ms, prefer minimal rewrites. CTE materialization overhead (hash tables, intermediate storage) can exceed filtering benefit on fast queries. HOWEVER: transforms that reduce scan count without adding CTEs (pushdown, decorrelate) may still help. The exploration worker MAY attempt structural changes on fast queries if the transform is scan-reducing, not CTE-adding."
    },
    {
      "id": "PG_DATE_CTE_CAUTION",
      "severity": "MEDIUM",
      "engine": "postgresql",
      "description": "date_cte_isolate on PostgreSQL is a double-edged sword. It helps on star-schema queries with explicit JOINs (3.32x, 2.28x wins) but HURTS on queries with EXISTS/NOT EXISTS, INTERSECT/EXCEPT, or when the CTE fence blocks predicate pushdown. Only use when converting comma-joins to explicit JOINs simultaneously.",
      "failure_rate": "7/29 queries regressed when date_cte_isolate was applied (24% regression rate)",
      "observed_wins": [
        {
          "query": "DSB Q080_multi",
          "speedup": "3.32x",
          "pattern": "Multi-channel UNION with comma joins \u2192 explicit JOINs + date CTE"
        },
        {
          "query": "DSB Q099_agg",
          "speedup": "2.28x",
          "pattern": "Star schema with comma joins \u2192 explicit JOINs + date CTE"
        },
        {
          "query": "DSB Q040_agg",
          "speedup": "1.22x",
          "pattern": "Star schema + date range CTE"
        },
        {
          "query": "DSB Q072_spj_spj",
          "speedup": "1.14x",
          "pattern": "3 date_dim instances consolidated into 1 CTE"
        }
      ],
      "observed_failures": [
        {
          "query": "DSB Q069_multi",
          "speedup": "0.50x",
          "pattern": "EXISTS subqueries \u2192 CTE + IN (killed semi-join)"
        },
        {
          "query": "DSB Q031_multi",
          "speedup": "0.55x",
          "pattern": "Already had efficient CTEs, CTE fence blocked pushdown"
        },
        {
          "query": "DSB Q087_multi",
          "speedup": "0.59x",
          "pattern": "EXCEPT branches, broad customer CTE"
        },
        {
          "query": "DSB Q038_multi",
          "speedup": "0.85x",
          "pattern": "INTERSECT branches, CTE overhead"
        },
        {
          "query": "DSB Q010_multi",
          "speedup": "0.86x",
          "pattern": "EXISTS \u2192 CTE + IN conversion"
        },
        {
          "query": "DSB Q014_multi",
          "speedup": "0.87x",
          "pattern": "Inconsistent application, scalar subquery overhead"
        },
        {
          "query": "DSB Q058_multi",
          "speedup": "0.89x",
          "pattern": "Inconsistent application across branches"
        }
      ],
      "constraint_rules": [
        {
          "rule": "REQUIRE_EXPLICIT_JOIN_CONVERSION",
          "description": "Only use date_cte_isolate when ALSO converting comma-separated joins to explicit JOINs",
          "rationale": "The win comes from explicit JOINs + CTE together, not CTE alone."
        },
        {
          "rule": "BLOCK_WITH_EXISTS",
          "description": "Never use date_cte_isolate on queries containing EXISTS/NOT EXISTS subqueries",
          "rationale": "CTE materialization kills semi-join early termination."
        },
        {
          "rule": "BLOCK_WITH_SET_OPS",
          "description": "Never use date_cte_isolate on queries with INTERSECT/EXCEPT",
          "rationale": "Each set operation branch handles date_dim efficiently inline. CTE adds overhead."
        },
        {
          "rule": "CONSISTENT_APPLICATION",
          "description": "If applying to multi-branch queries (UNION ALL), apply to ALL branches or NONE",
          "rationale": "Partial application creates asymmetric plans that confuse the optimizer."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: date_cte_isolate is ONLY beneficial when combined with converting comma-joins to explicit JOINs on star-schema queries. DO NOT use it on queries with EXISTS/NOT EXISTS (kills semi-join), INTERSECT/EXCEPT (adds CTE overhead per branch), or when the query already has efficient CTEs. If applying to UNION ALL branches, apply to ALL or NONE."
    },
    {
      "id": "PG_LOOSE_PREFILTER_BLOCK",
      "severity": "MEDIUM",
      "engine": "postgresql",
      "description": "Do not pre-filter a fact table into a CTE with a UNION/OR superset filter when the actual WHERE clause has correlated conditions. However, a fact table CTE with a simple range filter IS beneficial when the query has expensive non-equi joins (quantity comparisons, date arithmetic) that dominate cost.",
      "failure_rate": "1/2 queries: 1 regression (0.79x), 1 win (2.68x)",
      "observed_failures": [
        {
          "query": "DSB Q013_agg",
          "speedup": "0.79x",
          "original": "store_sales joined with 5 dims, correlated OR predicates on price/profit ranges",
          "rewrite": "filtered_ss CTE with union of all price/profit ranges (loose filter), then join dims",
          "problem": "Loose CTE filter is a superset of correlated OR conditions. CTE fence blocks dimension predicate pushdown."
        }
      ],
      "observed_wins": [
        {
          "query": "DSB Q072_agg",
          "speedup": "2.68x",
          "original": "catalog_sales joined with inventory (non-equi: inv_quantity < cs_quantity), 3x date_dim, item, cd, hd",
          "rewrite": "MATERIALIZED CTEs for all dims + fact table (cs_wholesale_cost BETWEEN 34 AND 54)",
          "why_it_worked": "Non-equi joins (quantity comparison, week_seq correlation) dominate cost. Reducing fact table by ~70% before these joins shrinks the search space. The CTE fence cost is negligible vs the non-equi join savings."
        }
      ],
      "constraint_rules": [
        {
          "rule": "NO_FACT_CTE_WITH_CORRELATED_OR",
          "description": "Do not pre-filter fact tables into CTEs when the WHERE clause has correlated OR conditions",
          "rationale": "OR conditions create complex predicate interactions that the optimizer handles better inline. A union of all OR branches is a superset that materializes too many rows."
        },
        {
          "rule": "FACT_CTE_OK_WITH_NON_EQUI_JOINS",
          "description": "A fact table CTE with a simple range filter IS beneficial when the query has expensive non-equi joins (e.g., quantity < quantity, date arithmetic comparisons)",
          "rationale": "Non-equi joins cannot use hash/merge join efficiently. Reducing input size before these joins has outsized impact. The CTE fence cost is negligible compared to non-equi join savings."
        }
      ],
      "prompt_instruction": "POSTGRESQL RULE: Do NOT pre-filter fact tables into CTEs when the WHERE has correlated OR conditions (the union/superset filter materializes too many rows, 0.79x regression). BUT DO pre-filter fact tables when the query has expensive non-equi joins (quantity comparisons, date arithmetic) - reducing the fact table before these joins is highly effective (2.68x win on Q072)."
    },
    {
      "id": "PREFETCH_MULTI_FACT_CHAIN",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Prefer limiting cascading fact-table CTEs to 2. Each additional CTE materializes a large intermediate result.",
      "observed_failures": [
        {
          "query": "Q4",
          "regression": "0.78x",
          "problem": "3 cascading fact-table CTEs (store_sales -> catalog_sales -> web_sales) created excessive intermediate materialization.",
          "type": "FACT_CHAIN_OVERHEAD"
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_2_OR_FEWER_FACT_CTES",
          "description": "When pre-joining fact tables with filtered dimensions in CTEs, 2 cascading fact CTEs is safe. A third adds risk of excessive materialization."
        }
      ],
      "override_conditions": [
        "Each fact CTE has highly selective filters (<5% of rows survive), keeping intermediate sizes small",
        "The 3rd CTE reads from a dimension-filtered result, not a raw fact table",
        "The query already has 3+ separate fact table scans in baseline \u2014 chaining cannot be worse"
      ],
      "prompt_instruction": "DEFAULT: Limit to 2 cascading fact-table CTEs. A 3rd CTE caused 0.78x on Q4 from excessive materialization. HOWEVER: if each CTE applies highly selective filters (<5% row survival), the intermediate results stay small. The exploration worker MAY try a 3-CTE chain if filters are selective and baseline already has 3+ separate scans."
    },
    {
      "id": "SINGLE_PASS_AGGREGATION_LIMIT",
      "severity": "MEDIUM",
      "overridable": true,
      "description": "Prefer limiting single-pass aggregation to 8 CASE branches. Beyond 8, CASE evaluation overhead may reduce benefit.",
      "observed_failures": [
        {
          "query": "Q88",
          "note": "8 CASE branches (time slices) was the maximum tested that still showed improvement (6.28x). More branches are untested, not proven harmful.",
          "type": "CASE_BRANCH_LIMIT"
        }
      ],
      "observed_successes": [
        {
          "query": "Q88",
          "speedup": "6.28x",
          "context": "8 CASE branches consolidating 8 separate time-bucket subqueries into a single scan."
        },
        {
          "query": "Q9",
          "speedup": "4.47x",
          "context": "5 CASE branches consolidating repeated store_sales scans."
        }
      ],
      "constraint_rules": [
        {
          "rule": "PREFER_8_OR_FEWER_CASE_BRANCHES",
          "description": "When consolidating repeated scans into CASE WHEN aggregates, 8 or fewer branches is well-tested. More branches are untested territory."
        }
      ],
      "override_conditions": [
        "The original query has 9-12 repeated scans on the same fact table (high consolidation value)",
        "Each CASE branch is a simple equality check (low per-row overhead)",
        "The fact table is large (>1M rows) so scan reduction dominates CASE evaluation cost"
      ],
      "prompt_instruction": "DEFAULT: Use at most 8 CASE branches for single_pass_aggregation (tested up to 8 at 6.28x on Q88). HOWEVER: 9-12 branches with simple equality checks on large fact tables may still net positive. The exploration worker MAY try 9-12 branches if the scan reduction value is high. Beyond 12 branches is not recommended."
    }
  ],
  "regression_warnings": [],
  "strategy_leaderboard": null,
  "query_archetype": null,
  "resource_envelope": "Memory budget: shared_buffers=128MB, effective_cache_size=4GB\nGlobal work_mem: 4MB (per-operation)\nActive connections: ~1 (work_mem headroom: safe up to 16MB per-op)\nStorage: HDD (random_page_cost=4.0)\nParallel capacity: max_parallel_workers=8, per_gather=2\n\nSET LOCAL permissions:\n  user-level (always available): effective_cache_size, enable_hashjoin, enable_mergejoin, enable_nestloop, enable_seqscan, from_collapse_limit, geqo_threshold, hash_mem_multiplier, jit, jit_above_cost, join_collapse_limit, max_parallel_workers_per_gather, parallel_setup_cost, parallel_tuple_cost, random_page_cost, work_mem",
  "exploit_algorithm_text": "# PostgreSQL Rewrite Playbook\n# DSB SF10 field intelligence\n\n## HOW TO USE THIS DOCUMENT\n\nWork in phase order. Each phase changes the plan shape \u2014 re-evaluate later phases after each.\n\n  Phase 1: Reduce scan volume (P1, P6, P7) \u2014 always first. Every optimization benefits from smaller input.\n  Phase 2: Eliminate redundant work (P2, P3)\n  Phase 3: Fix structural inefficiencies (P4, P5)\n\nBefore choosing any strategy, scan the explain plan for:\n- Row count profile: monotonically decreasing = healthy. Flat then sharp drop = pushback opportunity.\n- Join types: hash join = good. Nested loop on large table = decorrelation candidate.\n- Repeated tables: same table N times = consolidation (P3).\n- CTE materialization: large CTE + small post-filter = pushback. Use AS MATERIALIZED when needed.\n- Bitmap OR scan: indexed OR already optimized \u2014 do NOT split to UNION.\n- Parallel workers: active parallelism \u2014 avoid CTE fence that blocks parallel execution.\n- Index-only scan on dimension: small dimension already efficient \u2014 CTE wrapper may hurt.\n- EXISTS/NOT EXISTS: uses semi-join early termination \u2014 NEVER materialize.\n\n## ENGINE STRENGTHS \u2014 do NOT rewrite these patterns\n\n1. **BITMAP_OR_SCAN**: Multi-branch ORs on indexed columns handled via bitmap combination in one scan. Splitting ORs to UNION ALL is lethal (0.21x observed).\n2. **EXISTS semi-join**: Uses early termination. Converting to materializing CTEs caused 0.50x, 0.75x \u2014 semi-join destroyed. **Never materialize EXISTS.**\n3. **INNER JOIN reordering**: Freely reorders INNER JOINs by selectivity estimates. Do NOT manually restructure INNER JOIN order.\n4. **Index-only scan**: Reads only index when covering all requested columns. Small dimension lookups may not need CTEs.\n5. **Parallel query execution**: Large scans and aggregations parallelized across workers. CTEs block parallelism (materialization is single-threaded).\n6. **JIT compilation**: JIT-compiles complex expressions for long-running queries (>100ms).\n\n## CORRECTNESS RULES\n\n- Preserve exact row count \u2014 no filtering or duplication.\n- Maintain NULL semantics in WHERE/ON conditions.\n- Do not add/remove ORDER BY unless proven safe.\n- Preserve LIMIT semantics \u2014 no result set expansion.\n- NOT IN with NULLs blocks hash anti-joins \u2014 preserve EXISTS form.\n\n## GLOBAL GUARDS (check always, before any rewrite)\n\n1. OR conditions on indexed columns \u2192 never split to UNION ALL (0.21x observed)\n2. EXISTS/NOT EXISTS \u2192 never materialize into CTEs (0.50x, 0.75x \u2014 semi-join destroyed)\n3. INNER JOIN order \u2192 never restructure (optimizer handles reordering)\n4. Small dimensions (< 10K rows) \u2192 index-only scan may be faster than CTE\n5. Baseline < 100ms \u2192 skip CTE-based rewrites (overhead exceeds savings)\n6. CTEs block parallel execution \u2014 only use when benefit outweighs parallelism loss\n7. Use AS MATERIALIZED when CTE must not be inlined (decorrelation, shared scans)\n8. Preserve efficient existing CTEs \u2014 don't decompose working patterns\n9. Verify NULL semantics in NOT IN conversions\n10. ROLLUP/window in same query \u2192 CTE may prevent pushdown optimizations\n11. Never inline a large UNION CTE \u2014 re-execution multiplied per reference (0.16x \u2014 6 fact scans re-executed)\n12. Max 2 cascading fact-table CTE chains \u2014 deeper chains block parallelism\n13. EXPLAIN cost gaps \u2260 runtime gains for config tuning \u2014 6 false positives caught (up to 84% EXPLAIN gap \u2192 0% runtime). Always 3-race validate config changes.\n\n---\n\n## PATHOLOGIES\n\n### P1: Comma join confusing cardinality estimation [Phase 1 \u2014 LOW RISK]\n\n  Gap: COMMA_JOIN_WEAKNESS \u2014 PostgreSQL's planner uses cross-product estimation\n  for comma-separated joins in the FROM clause. Without explicit JOIN syntax, the\n  planner lacks the join-key hint that enables hash-join probing with filtered\n  dimension tables. This manifests as poor row estimates on intermediate joins,\n  leading to nested-loop plans on large fact tables.\n\n  The fix has two parts: (1) convert comma joins to explicit INNER JOIN syntax,\n  and (2) pre-filter selective dimensions into MATERIALIZED CTEs to create tiny\n  hash probe tables. Both are required \u2014 the CTE alone can hurt, but CTE +\n  explicit JOINs together enable optimal hash join planning.\n\n  Signal: hash/nested-loop join with poor row estimates in EXPLAIN, large\n  intermediate results. SQL shows FROM t1, t2, t3 WHERE t1.key = t2.key\n  AND ... (comma joins, no explicit JOIN).\n\n  Decision gates:\n  - Structural: multiple tables in comma-separated FROM with equi-join predicates\n  - Selectivity: dimension filters available (date range, state, category)\n  - Fact table: 1-2 fact tables only (3+ \u2192 join order lock)\n  - CTE count: max 3-4 dimension CTEs (avoid over-materialization)\n  - Stop: if all JOINs already explicit \u2192 skip to P6/P7\n\n  Transform selection (lightest sufficient):\n  - Date filter + star schema \u2192 pg_date_cte_explicit_join (4 wins, 2.1x avg)\n  - Multiple dimension filters \u2192 pg_dimension_prefetch_star (3 wins, 2.8x avg)\n  - Complex multi-join \u2192 explicit_join_materialized (2 wins, 5.9x avg)\n\n  Ordering: apply first \u2014 reduces fact table scan before other optimizations.\n  Composition: combines well with P2 (decorrelation) and P6 (date consolidation).\n  After applying: re-evaluate P4 (non-equi inputs now smaller).\n\n  Wins: 4 validated (1.8x\u20138.6x, avg 4.0x)\n  Improved: 1 (1.4x)\n  Regressions: 0.88x (explicit join overhead on simple query)\n\n### P2: Correlated subquery executing per outer row [Phase 2 \u2014 HIGHEST IMPACT]\n\n  Gap: CORRELATED_SUBQUERY_PARALYSIS \u2014 PostgreSQL cannot decorrelate correlated\n  aggregate subqueries into GROUP BY + hash join. It falls back to nested-loop\n  re-execution, scanning the inner relation once per outer row. For N outer rows\n  and M inner rows, cost is O(N \u00d7 M) instead of O(N + M).\n\n  This is the single most impactful pathology on PostgreSQL. It accounts for 9\n  of 31 wins including the three largest speedups (8044x, 1465x, 439x). The\n  extreme wins occur when the correlated subquery causes a timeout \u2014 the original\n  query never finishes, but the decorrelated version completes in milliseconds.\n\n  The fix: extract the correlated aggregate into a MATERIALIZED CTE with GROUP BY\n  on the correlation key, then JOIN back. Use AS MATERIALIZED to prevent the\n  optimizer from inlining the CTE back into a correlated form.\n\n  Signal: nested loop in EXPLAIN, inner side re-executes aggregate per outer row.\n  If EXPLAIN shows hash join on correlation key \u2192 already decorrelated \u2192 STOP.\n  SQL signal: WHERE col > (SELECT AGG(...) FROM ... WHERE outer.key = inner.key)\n\n  Decision gates:\n  - Structural: correlated scalar subquery with aggregate (AVG, SUM, COUNT)\n  - EXPLAIN: nested loop with inner re-execution (NOT hash join)\n  - NOT EXISTS: NEVER decorrelate EXISTS/NOT EXISTS (destroys semi-join, 0.50x observed)\n  - Shared scan: if inner and outer scan same table \u2192 extract common scan to shared CTE\n  - CTE keyword: ALWAYS use AS MATERIALIZED (prevents optimizer re-correlating)\n  - Multi-fact: 1-2 fact tables safe, 3+ \u2192 STOP (0.51x on multi-fact query)\n\n  Transform selection (lightest sufficient):\n  - Simple avg comparison \u2192 inline_decorrelate_materialized (3 wins, avg 500x)\n  - Multiple correlation keys \u2192 decorrelate (8 wins, avg 3.2x)\n  - Inner = outer table \u2192 shared scan + decorrelate (2 wins, avg 7000x)\n\n  Ordering: apply after P1 (smaller inputs make decorrelation cheaper).\n  Composition: almost always combined with P1 (comma join conversion).\n  After applying: re-evaluate P3 (decorrelated CTEs may now be reusable).\n\n  Wins: 9 validated (1.9x\u20138044x, avg 1100x), 3 timeout recoveries (439x\u20138044x)\n  Improved: 2 (1.1x\u20131.5x)\n  Regressions: 0.51x (multi-fact join lock), 0.75x (EXISTS materialized)\n\n### P3: Same fact+dimension scan repeated across subquery boundaries [Phase 2 \u2014 ZERO REGRESSIONS]\n\n  Gap: CROSS_CTE_PREDICATE_BLINDNESS \u2014 PostgreSQL cannot detect that N subqueries\n  all scan the same fact table with identical joins and filters. Each subquery is\n  an independent plan unit with no Common Subexpression Elimination across query\n  boundaries. This includes self-join patterns where the same aggregation is\n  computed at different granularities.\n\n  Two fix strategies: (1) Materialize identical scan once as CTE, derive\n  aggregates from single result. (2) Consolidate multiple channel scans\n  (store/catalog/web) into single UNION ALL scan with CASE-based pivoting.\n\n  Signal: identical scan subtrees appearing 2+ times in EXPLAIN with similar\n  costs. SQL signal: same fact table joined to same dimensions in multiple\n  subqueries, or self-join with different GROUP BY granularity.\n\n  Decision gates:\n  - Structural: 2+ subqueries scanning same fact table with identical filters\n  - Aggregation: COUNT/SUM/AVG/MIN/MAX only (not STDDEV/PERCENTILE)\n  - Self-join: if query joins CTE to itself \u2192 consolidate into single CTE\n  - Channel pattern: 3 channel scans (store/catalog/web) \u2192 single_pass_aggregation\n\n  Transform selection:\n  - Multi-channel INTERSECT-like \u2192 single_pass_aggregation (1 win, 1.98x)\n  - Year-over-year self-join \u2192 self_join_pivot (1 win, 1.79x)\n\n  Ordering: apply after P1/P2 \u2014 reduced inputs make consolidation cheaper.\n  After applying: P4 benefits from smaller materialized inputs.\n\n  Wins: 2 validated (1.8x\u20132.0x, avg 1.9x)\n  Regressions: none observed\n\n### P4: Non-equi join without prefiltering [Phase 3 \u2014 ZERO REGRESSIONS]\n\n  Gap: NON_EQUI_JOIN_INPUT_BLINDNESS \u2014 PostgreSQL handles non-equi joins\n  (BETWEEN, <, >) via nested-loop or hash join with recheck, but cannot push\n  dimension filters past the non-equi join boundary. Both sides of the non-equi\n  join receive full unfiltered input, making the join O(N \u00d7 M) on large tables.\n\n  Fix: shrink BOTH sides via MATERIALIZED CTEs before the non-equi join.\n  Pre-filter dimensions (date, demographics, household) into small CTEs, then\n  join fact table with pre-filtered dimensions to reduce cardinality before the\n  non-equi join.\n\n  Signal: expensive non-equi join (BETWEEN, <, >) in EXPLAIN with large inputs.\n  SQL signal: JOIN ... ON a.col BETWEEN b.low AND b.high, neither side filtered.\n\n  Decision gates:\n  - Structural: non-equi join predicate (BETWEEN, range comparison)\n  - Cardinality: both join inputs > 10K rows\n  - Dimension filters: at least one side has selective dimension filter available\n  - Fact side: reducible by pre-joining with filtered dimensions\n\n  Transform selection:\n  - Multiple dimension filters \u2192 pg_materialized_dimension_fact_prefilter (1 win, 12.07x)\n\n  Ordering: apply after P1 (explicit join syntax) and P6 (date CTE).\n  After applying: non-equi join now operates on pre-filtered inputs.\n\n  Wins: 1 validated (12.1x)\n  Regressions: none observed\n\n### P5: Set operation materializing full result sets [Phase 3 \u2014 CAUTION]\n\n  Gap: SET_OPERATION_MATERIALIZATION \u2014 INTERSECT and EXCEPT are implemented via\n  full materialization + sort/hash comparison. For EXISTS (positive set test)\n  this destroys semi-join early termination. For NOT EXISTS (negative set test)\n  the planner uses hash-anti-join which is efficient, but correlated set\n  operations re-execute per outer row.\n\n  Two opposite fixes depending on direction:\n  (a) INTERSECT \u2192 EXISTS: replace set materialization with semi-join early\n      termination. PostgreSQL's EXISTS uses index + early termination.\n  (b) Correlated EXISTS/NOT EXISTS on large sets \u2192 MATERIALIZED CTE + LEFT JOIN\n      + IS NULL: pre-compute distinct customer sets once, then hash join for\n      set difference. Only when 3+ channel checks (store, web, catalog).\n\n  Signal: INTERSECT/EXCEPT between large result sets in EXPLAIN.\n  SQL signal: EXISTS subquery correlated to outer with fact+date scan inside.\n\n  Decision gates:\n  - INTERSECT with 10K+ rows \u2192 convert to EXISTS (P5a)\n  - Correlated NOT EXISTS on 3+ channels \u2192 materialize channel sets (P5b)\n  - Simple EXISTS (single channel) \u2192 KEEP EXISTS (semi-join is optimal)\n  - NOT EXISTS already using hash anti-join in EXPLAIN \u2192 STOP\n  - CAUTION: materializing simple EXISTS destroys semi-join (0.75x observed)\n\n  Transform selection:\n  - INTERSECT \u2192 intersect_to_exists (1 win, 1.78x)\n  - Multi-channel EXISTS/NOT EXISTS \u2192 set_operation_materialization (1 win, 17.48x)\n\n  Ordering: apply after P1 (explicit joins for channel CTEs).\n  Composition: P5a (INTERSECT\u2192EXISTS) is standalone; P5b combines with P1/P6.\n  After applying: check P3 if set operations were the only repeated-scan source.\n\n  Wins: 2 validated (1.8x\u201317.5x, avg 9.6x)\n  Regressions: 0.75x (over-materialized date CTE in EXISTS path)\n\n### P6: Multiple date_dim aliases with overlapping filters [Phase 1 \u2014 HIGHEST RELIABILITY]\n\n  Gap: DATE_DIM_REDUNDANCY \u2014 PostgreSQL cannot detect that N references to\n  date_dim with overlapping year/month filters select the same rows. Each alias\n  is an independent scan. On star schemas with 3 date_dim instances (sold,\n  returned, shipped), the optimizer scans date_dim 3 times with similar predicates.\n\n  Fix: consolidate overlapping date filters into a single CTE (all_dates) that\n  selects the union of needed date_sk values, then join each fact table reference\n  to the shared CTE with specific MOY conditions.\n\n  Signal: 2+ date_dim aliases in FROM with similar year/month_seq/moy predicates.\n  SQL signal: d1.d_year = 1999 AND d2.d_year = 1999 AND d3.d_year = 1999.\n\n  Decision gates:\n  - Structural: 2+ date_dim instances with overlapping date predicates\n  - Selectivity: date filter selects < 1% of date_dim (always true for year+month)\n  - Combine with: explicit JOIN conversion when comma joins present\n\n  Transform selection:\n  - 2-3 date aliases, same year \u2192 date_consolidation (1 win, 3.10x)\n  - Single date filter, star schema \u2192 date_cte_isolate (3 wins + 7 improved)\n\n  Ordering: apply first (Phase 1) \u2014 date CTE is the smallest, most reliable transform.\n  Composition: always combine with P1 (explicit join syntax).\n  After applying: fact table scans reduced, all downstream pathologies benefit.\n\n  Wins: 3 validated (2.0x\u20133.1x, avg 2.4x)\n  Improved: 7 (1.07x\u20131.26x)\n  Regressions: none observed\n\n### P7: Multi-dimension prefetch for star-schema aggregation [Phase 1 \u2014 CAUTION]\n\n  Gap: DIMENSION_FILTER_PUSHDOWN_FAILURE \u2014 when multiple selective dimension\n  filters exist (item category, store state, customer demographics), the planner\n  may not apply them early enough. Pre-filtering dimensions into small CTEs and\n  joining them to the fact table reduces cardinality before expensive aggregation.\n\n  Fix: create MATERIALIZED CTEs for each selective dimension, then join fact table\n  to all filtered dimensions using explicit INNER JOIN syntax.\n\n  Signal: large fact table scan followed by late dimension filter in EXPLAIN.\n  SQL signal: star schema with 3+ dimension filters in WHERE clause.\n\n  Decision gates:\n  - Structural: star schema with 3+ selective dimension filters\n  - Dimension selectivity: each dimension filter selects < 10% of dimension table\n  - Fact table: single fact table, NOT self-join or multi-fact\n  - Stop: if query has self-join pattern \u2192 use P3 instead (0.25x observed)\n  - Stop: if query has multi-fact join \u2192 dimension prefetch locks join order (0.51x observed)\n\n  Transform selection:\n  - 2-3 dimensions \u2192 pg_dimension_prefetch_star (2 wins, 2.5x avg)\n  - Mixed dimensions + date \u2192 multi_dimension_prefetch (1 win, 2.50x)\n  - Fact + date + non-equi \u2192 combine with P4 (pg_materialized_dimension_fact_prefilter)\n\n  Ordering: apply with P1/P6 (all Phase 1 optimizations together).\n  CAUTION: do NOT apply to self-join or multi-fact queries.\n\n  Wins: 3 validated (1.8x\u20132.5x, avg 2.2x)\n  Improved: 3 (1.09x\u20131.25x)\n  Regressions: 0.25x (self-join), 0.51x (multi-fact)\n\n### NO MATCH \u2014 First-Principles Reasoning\n\n  If no pathology matches, do NOT stop.\n\n  1. **Check \u00a72b-i Q-Error routing first.** Direction+locus still points to\n     where the planner is wrong \u2014 use as starting hypothesis.\n  2. Identify the largest cost node. What dominates? Can it be restructured?\n  3. Count scans per base table. Repeated scans \u2192 consolidation opportunity.\n  4. Trace row counts. Where do they stay flat or increase?\n  5. Check transform catalog (\u00a75a) as a menu.\n\n  Record: which pathologies checked, which gates failed, nearest miss,\n  structural features present.\n\n---\n\n## CONFIG TUNING PATTERNS\n\nConfig tuning is ADDITIVE to SQL rewrite \u2014 not a substitute. Apply after SQL rewrite.\nEvidence: 52 queries benchmarked, 25 config wins, 3-race validated (PG 14.3, SF10).\nCRITICAL: EXPLAIN ANALYZE cost gaps do NOT predict runtime gains. 6 false positives\ncaught where EXPLAIN showed 38-84% improvement but runtime showed 0% or regression.\nAlways 3-race validate config changes.\n\n### C1: Merge join forcing suboptimal plan [HIGHEST IMPACT hint]\n\n  Mechanism: /*+ Set(enable_mergejoin off) */\n  Signal: EXPLAIN shows Merge Join with Sort node below it on large unsorted inputs.\n  The optimizer chooses merge join for cost model reasons but the sort overhead\n  exceeds hash join's hash-build cost.\n\n  Decision gates:\n  - Merge Join present in EXPLAIN with Sort node below it\n  - Both inputs > 10K rows (small merge joins are fine)\n  - Alternative: Hash Join would work (equi-join condition exists)\n  - DANGER: Do NOT disable on queries already using merge join efficiently on pre-sorted data\n\n  Wins: 6 validated (+8.6%\u2013+82.5%, avg +50.6%)\n\n### C2: Cost model undervaluing index scans on SSD [HIGHEST RECOVERY]\n\n  Mechanism: SET LOCAL random_page_cost = '1.1'; SET LOCAL effective_cache_size = '48GB'\n  Signal: Seq Scan on fact tables in EXPLAIN when btree indexes exist on join/filter\n  columns. The default random_page_cost=4.0 assumes spinning disk \u2014 on SSD the actual\n  cost ratio is ~1.1. Combined with effective_cache_size, the optimizer tips to index scans.\n  These two parameters have a nonlinear interaction \u2014 neither alone is sufficient.\n\n  Decision gates:\n  - Storage is SSD (not spinning disk)\n  - Seq Scan on fact table in EXPLAIN despite btree index on join/filter columns\n  - Buffer cache warm (shared_buffers + OS cache covers working set)\n\n  Wins: 6 validated (+46.0%\u2013+89.0%, avg +71.1%). Rescued 3 rewrite regressions\n  (0.61x\u21929.09x, 0.51x\u21925.95x, 0.30x\u21921.85x).\n\n### C3: Parallelism underutilized on large scans [MOST VERSATILE]\n\n  Mechanism: SET LOCAL max_parallel_workers_per_gather = '4';\n             SET LOCAL parallel_setup_cost = '100';\n             SET LOCAL parallel_tuple_cost = '0.001'\n  Signal: Large Seq Scan (>100K rows) without Gather/Parallel node above in EXPLAIN.\n  Prefer cost reduction (setup=100, tuple=0.001) over max_workers forcing alone.\n\n  Decision gates:\n  - Seq Scan > 100K rows without parallel workers\n  - Query execution > 500ms (CRITICAL: never on fast queries)\n  - DANGER: 7.34x REGRESSION observed when forced on 244ms query\n  - DANGER: par4-alone caused -15.3% \u2014 must include work_mem=512MB\n  - par4 alone insufficient for hash-heavy queries \u2014 combine with work_mem (C4)\n\n  Wins (standalone): 5 validated (+6.2%\u2013+28.2%, avg +14.3%). Also in 10+ combo wins.\n\n### C4: Hash/sort spilling to disk [TARGETED]\n\n  Mechanism: SET LOCAL work_mem = '256MB' or '512MB'\n  Signal: Hash Batches > 1 or Sort Space Type = 'Disk' in EXPLAIN ANALYZE.\n  Size by op count: \u22642 ops \u2192 512MB, 3-5 \u2192 256MB, 6+ \u2192 128MB.\n  work_mem is per-operation \u2014 count sort+hash nodes before sizing.\n\n  Decision gates:\n  - Hash Batches > 1 OR Sort Space = 'Disk' in EXPLAIN\n  - Count sort+hash ops in EXPLAIN to size appropriately\n  - Often needs par4 (C3) to realize full benefit\n\n  Wins: 4 validated (+11.4%\u2013+41.5%, avg +21.7%)\n\n### C5: Nested loop on large join inputs [HIGH IMPACT hint]\n\n  Mechanism: /*+ Set(enable_nestloop off) */\n  Signal: Nested Loop in EXPLAIN with >10K rows on both sides. NL is O(N\u00d7M)\n  when both inputs are large \u2014 hash join is O(N+M) with equi-join condition.\n\n  Decision gates:\n  - Nested Loop in EXPLAIN with both inputs > 10K rows\n  - Equi-join condition exists (hash join is viable alternative)\n  - NOT correlated subquery (NL is correct there \u2014 use P2 decorrelation instead)\n  - DANGER: NL_off caused -1454% regression \u2014 never on queries where NL is correct\n\n  Wins: 3 validated (+42.5%\u2013+81.3%, avg +60.4%)\n\n### C6: Sort overhead on pre-ordered data [RARE]\n\n  Mechanism: SET LOCAL enable_sort = 'off'\n  Signal: Sort node in EXPLAIN on data that is already index-ordered or where\n  hash-based aggregation would be cheaper. Forces hash-based execution paths.\n\n  Decision gates:\n  - Sort node in EXPLAIN with input from index scan (already ordered)\n  - Or Sort node where hash aggregation is viable alternative\n  - High variance observed (3.2-7.7%) \u2014 validate carefully\n\n  Wins: 2 validated (+4.7%\u2013+68.2%, avg +36.5%)\n\n---\n\n## SAFETY RANKING\n\n| Rank | Pattern | Regr. | Worst | Action |\n|------|---------|-------|-------|--------|\n| 1 | P6: Date CTE isolation | 0 | \u2014 | Always fix (zero regressions) |\n| 2 | P3: Repeated scans | 0 | \u2014 | Always fix (verify agg type) |\n| 3 | P4: Non-equi prefilter | 0 | \u2014 | Always fix |\n| 4 | C2: SSD cost model (rpc+cache) | 0 | \u2014 | Always apply on SSD (zero regressions) |\n| 5 | C4: work_mem for spills | 0 | \u2014 | Size by op count (zero regressions) |\n| 6 | C6: Sort disable | 0 | \u2014 | Rare, validate carefully |\n| 7 | P1: Comma join + CTE | 1 | 0.88x | Fix when comma joins present |\n| 8 | C1: Merge join disable | 0 | \u2014 | Only when Sort+MJ visible in EXPLAIN |\n| 9 | P5: Set operation | 1 | 0.75x | Check direction (INTERSECT vs EXISTS) |\n| 10 | C5: Nested loop disable | 1 | -1454% | ONLY when NL on large inputs, never on correlated |\n| 11 | P2: Correlated subquery | 2 | 0.51x | Check EXPLAIN, never on EXISTS |\n| 12 | C3: Forced parallelism | 1 | 7.34x regr | NEVER on queries < 500ms |\n| 13 | P7: Multi-dim prefetch | 2 | 0.25x | Star schema only, not self-join |\n\n## VERIFICATION CHECKLIST\n\nBefore finalizing any rewrite:\n- [ ] AS MATERIALIZED used on all decorrelation CTEs (prevents re-correlation)\n- [ ] EXISTS/NOT EXISTS still uses EXISTS (not materialized into CTE)\n- [ ] OR conditions on indexed columns still intact (not split to UNION)\n- [ ] Comma joins converted to explicit INNER JOIN\n- [ ] Parallel execution not blocked by unnecessary CTE materialization\n- [ ] No orphaned CTEs (every CTE referenced downstream)\n- [ ] NULL semantics preserved in NOT IN conversions\n- [ ] Row counts decrease monotonically through CTE chain\n- [ ] Max 2 cascading fact-table CTE chains\n- [ ] Rewrite doesn't match any REGRESSION REGISTRY pattern\n- [ ] Config: query execution > 500ms before applying parallelism (C3)\n- [ ] Config: EXPLAIN gap validated by 3-race (EXPLAIN \u2260 runtime \u2014 6 false positives caught)\n- [ ] Config: work_mem sized by sort+hash op count, not query complexity\n- [ ] Config: hint disable (MJ/NL/sort off) only when EXPLAIN shows the problematic operator\n\n## PRUNING GUIDE\n\nSkip pathologies the plan rules out:\n\n| Plan shows | Skip |\n|---|---|\n| No comma joins (all explicit JOINs) | P1 (comma join fix) |\n| No nested loops on large tables | P2 (decorrelation) |\n| Each table appears once | P3 (repeated scans) |\n| No non-equi joins (BETWEEN, <, >) | P4 (non-equi prefilter) |\n| No INTERSECT/EXCEPT and no correlated multi-channel EXISTS | P5 (set operation) |\n| Single date_dim reference | P6 (date consolidation) |\n| No GROUP BY or only 1 dimension filter | P7 (multi-dim prefetch) |\n| Baseline < 100ms | ALL CTE-based transforms |\n| Bitmap OR scan present | OR\u2192UNION rewrites |\n| Parallel workers active + query fast | CTE-heavy transforms |\n\n## REGRESSION REGISTRY\n\n| Severity | Transform | Result | Root cause |\n|----------|-----------|--------|------------|\n| CATASTROPHIC | cte_inlining | 0.16x | Inlined large UNION CTE \u2192 6 fact scans re-executed 2x each |\n| SEVERE | multi_dim_prefetch | 0.15x | CTEs blocked date-predicate pushdown on 90-day interval join |\n| SEVERE | dimension_prefetch | 0.25x | Applied star-schema pattern to 6-way self-join \u2192 parallelism destroyed |\n| MAJOR | cte_materialization | 0.30x | Multi-scan CTE overhead similar to above cte_inlining pattern |\n| MAJOR | early_fact_filtering | 0.51x | Disabled nestloop too aggressively + DISTINCT forced hash spill |\n| MAJOR | date_cte_prefetch | 0.75x | Over-materialized date CTE in EXISTS path \u2192 destroyed semi-join |\n| MODERATE | explicit_join | 0.88x | Explicit join conversion overhead exceeded benefit on simple query |\n| CATASTROPHIC | forced_parallelism (C3) | 7.34x regr | Worker startup + coordination overhead on 244ms query. NEVER force par on < 500ms |\n| CATASTROPHIC | enable_nestloop_off (C5) | -1454% | NL was correct plan. Disabling forced catastrophic merge/hash on unsuitable query |\n| MAJOR | geqo_off | -254% | Exhaustive planner found \"better\" cost plan on 19 joins but cardinality errors made it catastrophic |\n| MAJOR | par4_without_wm | -15.3% | Parallelism without sufficient work_mem causes hash spill under parallel execution |\n",
  "detected_transforms": "[TransformMatch(id='early_filter_decorrelate', overlap_ratio=0.8333333333333334, matched_features=['AGG_SUM', 'BETWEEN', 'CTE', 'DATE_DIM', 'GROUP_BY'], missing_features=['AGG_AVG'], total_required=6, gap='CORRELATED_SUBQUERY_PARALYSIS', engines=['postgresql'], contraindications=[]), TransformMatch(id='pg_date_cte_explicit_join', overlap_ratio=0.8, matched_features=['AGG_SUM', 'BETWEEN', 'DATE_DIM', 'GROUP_BY'], missing_features=['CASE_EXPR'], total_required=5, gap='COMMA_JOIN_WEAKNESS', engines=['postgresql'], contraindications=[]), TransformMatch(id='pg_self_join_decomposition', overlap_ratio=0.8, matched_features=['AGG_SUM', 'BETWEEN', 'DATE_DIM', 'GROUP_BY'], missing_features=['AGG_AVG'], total_required=5, gap='CROSS_CTE_PREDICATE_BLINDNESS', engines=['postgresql'], contraindications=[]), TransformMatch(id='inline_decorrelate_materialized', overlap_ratio=0.75, matched_features=['AGG_SUM', 'BETWEEN', 'DATE_DIM'], missing_features=['AGG_AVG'], total_required=4, gap='CORRELATED_SUBQUERY_PARALYSIS', engines=['postgresql'], contraindications=[]), TransformMatch(id='pg_dimension_prefetch_star', overlap_ratio=0.625, matched_features=['AGG_SUM', 'BETWEEN', 'CTE', 'DATE_DIM', 'GROUP_BY'], missing_features=['LEFT_JOIN', 'ROLLUP', 'UNION'], total_required=8, gap='COMMA_JOIN_WEAKNESS', engines=['postgresql'], contraindications=[])]",
  "qerror_analysis": "QErrorAnalysis(signals=[QErrorSignal(node_type='Seq Scan', estimated=296, actual=0, q_error=296.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Nested Loop', estimated=282, actual=0, q_error=282.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Hash', estimated=122, actual=0, q_error=122.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Nested Loop', estimated=122, actual=0, q_error=122.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Index Only Scan', estimated=30, actual=0, q_error=30.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Nested Loop', estimated=4, actual=50, q_error=12.5, direction='UNDER_EST', timing_ms=279.596), QErrorSignal(node_type='Hash Join', estimated=1, actual=9, q_error=9.0, direction='UNDER_EST', timing_ms=0.4620000000000317), QErrorSignal(node_type='Aggregate', estimated=4, actual=0, q_error=4.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Index Only Scan', estimated=4, actual=0, q_error=4.0, direction='ZERO_EST', timing_ms=0.0), QErrorSignal(node_type='Nested Loop', estimated=446, actual=176, q_error=2.534090909090909, direction='OVER_EST', timing_ms=2.756999999999991), QErrorSignal(node_type='Seq Scan', estimated=446, actual=176, q_error=2.534090909090909, direction='OVER_EST', timing_ms=318.654), QErrorSignal(node_type='Aggregate', estimated=1, actual=2, q_error=2.0, direction='UNDER_EST', timing_ms=0.009000000000014552), QErrorSignal(node_type='Sort', estimated=1, actual=2, q_error=2.0, direction='UNDER_EST', timing_ms=0.015999999999991132), QErrorSignal(node_type='Gather', estimated=1, actual=2, q_error=2.0, direction='UNDER_EST', timing_ms=29.537000000000006), QErrorSignal(node_type='Index Scan', estimated=2, actual=1, q_error=2.0, direction='OVER_EST', timing_ms=284.15999999999997), QErrorSignal(node_type='Sort', estimated=1, actual=2, q_error=2.0, direction='UNDER_EST', timing_ms=0), QErrorSignal(node_type='Gather', estimated=1, actual=2, q_error=2.0, direction='UNDER_EST', timing_ms=5.231999999999999)], max_q_error=296.0, severity='MAJOR_HALLUCINATION', direction='ZERO_EST', locus='SCAN', magnitude='3_ORDER', structural_flags=['EST_ONE_NONLEAF', 'REPEATED_TABLE'], pathology_candidates=['P2', 'P0', 'P6', 'P1'])"
}