### Step 1 — EXPLAIN Plan Analysis

**t1 (NEUTRAL) vs Original:**
- **Improved operator**:  
  `Nested Loop  (rows=168134, time=1739.038) → Nested Loop  (rows=56045, time=350.942)`  
  **Structural change**: Early filtering via `filtered_dates` CTE reduced `date_dim` rows from 365 to 122, cutting nested loop iterations by 67%.  
- **Most expensive remaining operator**:  
  `Hash Join  (rows=6265, time=358.827)`  
  This now dominates runtime due to unfiltered joins with `household_demographics` (1,440 rows).

**t3 (REGRESSION) vs Original**:  
- **Regressed operator**:  
  `Index Only Scan on store_sales  (rows=461, time=4.714 → rows=0, time=46.932)`  
  **Why**: EXISTS subqueries and materialization forced nested loops over `household_demographics`/`customer_demographics` per row, exploding latency.

**Classification**:  
- **t1** is COMPLEMENTARY (reduced fact-dimension join)  
- No other winners to combine (FAIL/REGRESSION are redundant with t1’s approach)

---

### Step 2 — Target Design

**Baseline**: t1 SQL (early date filter)  
**Primary bottleneck**: `Hash Join` (6265 rows, 359 ms) due to full scans of:  
1. `household_demographics` (1,440 rows)  
2. `customer_demographics` (no pre-filter)  

**Target 1 (Combination: A+E)**  
```json
{
  "family": "A+E",
  "transform": "prefilter_dimensions",
  "target_id": "t1",
  "relevance_score": 0.95,
  "hypothesis": "Filter household_demographics (hd_dep_count IN (1,3)) and customer_demographics (cd_marital_status/education IN valid values) early to shrink Hash Join input from 1,440 to <100 rows.",
  "target_ir": "S0",
  "recommended_examples": ["multi_dimension_prefetch", "pg_date_cte_explicit_join"]
}
```

**Target 2 (Refinement: A)**  
```json
{
  "family": "A",
  "transform": "push_cd_filters",
  "target_id": "t1",
  "relevance_score": 0.85,
  "hypothesis": "Push customer_demographics filters (cd_marital_status/education) into its join to avoid materializing invalid rows for the Hash Join (current input: 1,440 rows).",
  "target_ir": "S0",
  "recommended_examples": ["pg_date_cte_explicit_join"]
}
```

**Target 3 (Rescue: D)**  
```json
{
  "family": "D",
  "transform": "union_demographics_branches",
  "target_id": "t1",
  "relevance_score": 0.75,
  "hypothesis": "Convert OR conditions on demographics to UNION ALL branches while preserving t1’s date filter. Avoids single complex join by isolating branch-specific filters.",
  "target_ir": "S0",
  "recommended_examples": ["pg_intersect_to_exists", "pg_union_all_rewrite"]
}
```

**Target 4 (Novel: F)**  
```json
{
  "family": "F",
  "transform": "reorder_joins",
  "target_id": "t1",
  "relevance_score": 0.70,
  "hypothesis": "Reorder joins to sequence small prefiltered dimensions (store, household_demographics) before large fact tables. Targets Hash Join row explosion (6,265 rows).",
  "target_ir": "S0",
  "recommended_examples": ["pg_explicit_join_materialized"]
}
```

---

### Rationale
1. **A+E (Primary)**: Directly attacks the Hash Join bottleneck by reducing dimension table sizes (1,440 → <100 rows) via prefiltering. Combines t1’s date filtering with new dimension optimizations.  
2. **A (Secondary)**: Simpler version of Target 1 if CTE materialization overheats.  
3. **D (Rescue)**: Salvages UNION ALL from failed t3 but applies ONLY to demographics OR branches (not address) to avoid complexity.  
4. **F (Novel)**: Join reordering may help if prefiltering alone can’t reduce Hash Join rows sufficiently.  

**Why not C/B?**  
- No aggregations pre-join (C irrelevant).  
- No correlated subqueries (B irrelevant).