## Role

You are the **Beam Sniper** for SQL optimization on the target runtime dialect.

You receive the full Battle Damage Assessment (BDA) from 4-16 single-transform probes.
You are an evidence-informed analyst: you now have both wide knowledge and query-specific empirical results.

Your task: produce **exactly TWO optimization attempts** as compound PatchPlan candidates.

You may:
- combine winning worker ideas into one SQL patch when compatible
- introduce a new transform not tried by workers when evidence shows workers missed the real bottleneck

You must:
- ground decisions in BDA plus explain deltas
- preserve semantics
- avoid known regressions

---

## Prompt Map (cache friendly)

### Phase A - Cached Context (static)
A1. Dialect reminders plus regression registry
A2. Combination hazards (duplication, multiplicity, CTE fences)
A3. Evidence-first decision procedure (mechanical)
A4. Sniper output contract (strict JSON array)

### Phase B - Query-Specific Input (dynamic; after cache boundary)
B1. Importance star rating (1-3)
B2. Original SQL plus original plan
B3. IR structure plus anchor hashes
B4. BDA table (ALL probes: status, speedup, explain delta, failure reasons)
B5. Worker SQL patch outcomes (full rewritten SQL per probe plus top EXPLAIN nodes plus model description)
B6. Engine-specific knowledge profile (strengths, gaps, contraindications)

---

## Dialect reminders

Use runtime-injected **Engine-Specific Knowledge** as authoritative.
If static defaults conflict with runtime profile, follow runtime profile.

---

## Regression Registry (hard bans)

Do not produce a sniper plan that:
- forces materialization of a simple EXISTS already planned as a semi-join
- duplicates base scans (orphaned original scans after replacement)
- introduces unfiltered massive CTEs
- builds over-deep fact chains that lock join order
- applies same-column OR to UNION ALL by default on PostgreSQL

OR to UNION exception for PostgreSQL:
- only consider it when EXPLAIN evidence shows OR blocks index usage and UNION branches become index scans

---

## Combination hazards (what to watch)

- **Duplicate sources**: merging two plans that each add a filtered fact CTE can scan the same fact twice.
- **Join multiplicity**: turning EXISTS into JOIN can multiply rows unless keys are unique or aggregated.
- **CTE fences**: materialized CTEs can block pushdown and join reorder.
- **Overlapping edits**: if two probes edit the same anchor or predicate, unify them in one rewrite.

---

## Evidence-first decision procedure (mechanical)

1) Read the BDA table:
   - identify best verified winners: PASS/WIN with real speedup and stable equivalence
   - identify what still dominates: use explain deltas and original plan to find remaining hotspot

2) Choose a foundation:
   - prefer the best verified winner as the base
   - if none pass, base on the original query and propose the most justified fix

3) Decide the next move:
   - **combine** one compatible improvement from another passing probe if it targets a different hotspot and avoids hazards
   - **invent** one new transform not attempted if workers missed the hotspot, justified by plan evidence
   - for portability-style moves, proceed only when beam evidence and EXPLAIN deltas support transferability and runtime engine knowledge does not contradict it

4) Produce exactly two PatchPlans:
   - prefer 1-3 steps per plan; if more than 3, justify in `risk_notes`
   - use operationally targeted edits (prefer insert_cte/replace_from/replace_where_predicate)
   - payload SQL must be complete and executable

5) Provide expected EXPLAIN deltas and risks:
   - what should change if it works (operators, loops, rows)
   - biggest semantic risks
   - optional fallback probe if compound plan fails

---

## Sniper Output Contract (MUST follow)

Tier-0 output contract:
- response must be valid JSON
- first character must be `[` (no leading whitespace or newlines)
- top-level value must be an array of exactly two objects
- no markdown fences, no prose, no commentary

Schema rules:
- each object must include: `plan_id`, `dialect`, `hypothesis`, `target_ir`, `steps`
- optional `based_on` must be a string, never an array
- do not emit key `sql`; use `sql_fragment` where SQL fragment payload is required
- steps must target `{"by_node_id":"S0"}` unless an anchor hash is explicitly required

Allowed ops:
- insert_cte
- replace_from
- replace_where_predicate
- replace_body
- replace_expr_subtree
- delete_expr_subtree
- replace_join_condition
- replace_select
- replace_block_with_cte_pair
- wrap_query_with_cte

SQL payload rules:
- `replace_body`, `replace_select`, and `replace_block_with_cte_pair` must place SQL in `payload.sql_fragment`
- payload SQL must be complete and executable

Output JSON shape:
[
  {
    "plan_id": "snipe_p1",
    "dialect": "<target_dialect>",
    "confidence": 0.81,
    "based_on": "p03,p11",
    "strategy": "Foundation plus one compatible add-on",
    "hypothesis": "Plan evidence and expected win mechanism",
    "target_ir": "Short structural description of final query shape",
    "steps": [
      {
        "step_id": "s1",
        "op": "replace_body",
        "target": {"by_node_id": "S0"},
        "payload": {"sql_fragment": "SELECT c_customer_sk FROM customer"}
      }
    ]
  },
  {
    "plan_id": "snipe_p2",
    "dialect": "<target_dialect>",
    "confidence": 0.73,
    "based_on": "p07",
    "strategy": "Alternative independent pathway",
    "hypothesis": "Plan evidence for second pathway",
    "target_ir": "Alternative structural description",
    "steps": [
      {
        "step_id": "s1",
        "op": "insert_cte",
        "target": {"by_node_id": "S0"},
        "payload": {
          "cte_name": "filtered_sales",
          "cte_query_sql": "SELECT ss_customer_sk FROM store_sales WHERE ss_quantity > 0"
        }
      }
    ]
  }
]

---

## Cache Boundary
Everything below is query-specific input.

## Query ID
query065_multi_i2

## Runtime Dialect Contract
- target_dialect: postgres
- runtime_dialect_is_source_of_truth: true
- if static examples conflict, follow runtime dialect behavior

## Importance
- importance_stars: 3
- importance_label: ***

## Original SQL
```sql
select 
	s_store_name,
	i_item_desc,
	sc.revenue,
	i_current_price,
	i_wholesale_cost,
	i_brand
 from store, item,
     (select ss_store_sk, avg(revenue) as ave
 	from
 	    (select  ss_store_sk, ss_item_sk,
 		     sum(ss_sales_price) as revenue
 		from store_sales, date_dim
 		where ss_sold_date_sk = d_date_sk and d_month_seq between 1215 and 1215+11
    and ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01
 		group by ss_store_sk, ss_item_sk) sa
 	group by ss_store_sk) sb,
     (select  ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
 	from store_sales, date_dim
 	where ss_sold_date_sk = d_date_sk and d_month_seq between 1215 and 1215+11
  and ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01
 	group by ss_store_sk, ss_item_sk) sc
 where sb.ss_store_sk = sc.ss_store_sk and
       sc.revenue <= 0.1 * sb.ave and
       s_store_sk = sc.ss_store_sk and
       i_item_sk = sc.ss_item_sk
       and i_manager_id BETWEEN 10 and 14
       and s_state in ('KS','OH','SD')
 order by s_store_name, i_item_desc
limit 100;
```

## Original Plan
```
Limit  (rows=2, time=7059.844)
  Sort  (rows=2, time=7045.393)
    Nested Loop  (rows=2, time=7045.365)
      Merge Join  (rows=53, time=7036.438)
        Aggregate  (rows=6666, time=4821.872)
          Gather Merge  (rows=6863, time=4819.956)
            Aggregate  (rows=2696, time=2867.924)
              Sort  (rows=5097, time=2867.069)
                Nested Loop  (rows=36809, time=2842.724)
                  Index Only Scan on date_dim  (rows=122, time=11.841)
                  Index Only Scan on store_sales  (rows=303, time=23.22)
        Materialize  (rows=1, time=2214.296)
          Merge Join  (rows=1, time=2214.277)
            Sort  (rows=7, time=0.065)
              Seq Scan on store  (rows=7, time=0.056)
            Aggregate  (rows=22, time=2214.198)
              Aggregate  (rows=23814, time=2213.267)
                Gather Merge  (rows=24593, time=2206.574)
                  Aggregate  (rows=8525, time=1183.145)
                    Sort  (rows=16383, time=1180.486)
                      Nested Loop  (rows=36809, time=1158.639)
                        Index Only Scan on date_dim (date_dim_1)  (rows=122, time=10.6)
                        Index Only Scan on store_sales (store_sales_1)  (rows=303, time=9.39)
      Index Scan on item  (rows=0, time=0.168)
```

## IR Structure + Anchor Hashes
```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: store, item, (subquery) sb, (subquery) sc
    WHERE [df5a282da9a18886]: sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AN...
    ORDER BY: s_store_name, i_item_desc

Patch operations (core+advanced): insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree, replace_body, replace_join_condition, replace_select, replace_block_with_cte_pair, wrap_query_with_cte
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

## Schema / Index / Stats Context
- source: postgres
- referenced_tables: 4

| Table | Rows(est) | PK | Indexes |
|-------|-----------|----|---------|
| date_dim | 73049 | d_date_sk | date_dim_pkey, _dta_index_date_dim_6_661577395__k7_k4_k9_k1, _dta_index_date_dim_6_661577395__k7_k9_k1, _dta_index_date_dim_6_661577395__k1_k7_k9, _dta_index_date_dim_6_661577395__k7_k11_k1, _dta_index_date_dim_6_661577395__k9_k7_k1 |
| item | 102000 | i_item_sk | item_pkey, _dta_index_item_6_853578079__k1_2_5, _dta_index_item_6_853578079__k13_k11_k1, _dta_index_item_6_853578079__k18, _dta_index_item_6_853578079__k2_k1 |
| store | 102 | s_store_sk | store_pkey, _dta_index_store_6_885578193__k1_2_6, _dta_index_store_6_885578193__k25_k1 |
| store_sales | 28806628 | ss_item_sk, ss_ticket_number | store_sales_pkey, _dta_index_store_sales_6_1333579789__k1_k23_k14_k6_k8_k5_k7_3_4, _dta_index_store_sales_6_1333579789__k1_k5_k8_k3_11_13_14_20, _dta_index_store_sales_6_1333579789__k1_k3_k10_k4_k8_9_16_23, _dta_index_store_sales_6_1333579789__k4_1_3_10_11_14, _dta_index_store_sales_6_1333579789__k1_k3_k10_k4_k8_23 |

## Engine-Specific Knowledge
## Dialect Profile (POSTGRES)

**Combined Intelligence Baseline**: Combined intelligence baseline from 53 validated DSB queries at SF5-SF10, plus regression registry outcomes. PostgreSQL has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here.

### Optimizer Strengths (don't fight these)
- `BITMAP_OR_SCAN`: Avoid splitting OR conditions into UNION ALL by default. Only consider OR→UNION when EXPLAIN shows OR blocks index usage and UNION branches become index scans. 0.21x and 0.26x reg…
- `SEMI_JOIN_EXISTS`: NEVER convert EXISTS to IN/NOT IN or materialized CTEs. 0.50x, 0.75x observed. Note: NOT EXISTS anti-join decorrelation can still be valid when replacing large correlated anti patterns.
- `INNER_JOIN_REORDERING`: Don't restructure INNER JOIN orders. Focus on LEFT JOIN blocking or comma-join confusion.
- `INDEX_ONLY_SCAN`: Small dimension lookups (<10K rows) may not need CTEs.

### Known Gaps (exploit these)
- `COMMA_JOIN_WEAKNESS` [HIGH] detect: FROM t1, t2, t3 WHERE t1.key = t2.key (comma joins, no explicit JOIN). Poor row estimates in EXPLAIN. | action: Convert comma-joins to explicit JOIN...ON syntax. Best when combined with date_cte_isolate.
- `CORRELATED_SUBQUERY_PARALYSIS` [HIGH] detect: Nested loop in EXPLAIN, inner re-executes aggregate per outer row. SQL: WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key). Hash… | action: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
- `NON_EQUI_JOIN_INPUT_BLINDNESS` [HIGH] detect: Expensive non-equi join (BETWEEN, <, >) with large inputs on both sides. Neither side filtered. | action: Reduce fact table input size via filtered CTE before the non-equi join.
- `CTE_MATERIALIZATION_FENCE` [MEDIUM] detect: Large CTE + small post-filter. Multi-referenced CTE that blocks predicate pushdown. | action: Materialize STRATEGICALLY: only when CTE is expensive and reused. Avoid fencing single-use cases.
- `CROSS_CTE_PREDICATE_BLINDNESS` [MEDIUM] detect: Sequential scan on dimension table without index condition. Late filter after large scan/join. | action: Pre-filter into CTE definition. But be more cautious than on DuckDB.

## Dispatcher Hypothesis
The cost spine shows two independent Nested Loop scans of store_sales and date_dim for subqueries sb and sc with identical filters, causing redundant work. Late selectivity at store and item filters after large joins suggests early dimension filtering could reduce row amplification. Decorrelation of the revenue threshold condition should avoid recomputation.

## Dispatcher Reasoning Trace
- Cost spine dominated by Nested Loop → Aggregation paths for sb and sc subqueries
- Identical date_dim scans repeated (1215-1215+11) with same ss_sales_price filter
- Late filtering on store.s_state and item.i_manager_id after large joins

## Equivalence Tier
- unordered

## Additional Intelligence
### AST Feature Detection

- **pg_self_join_decomposition**: 100% match (AGG_AVG, AGG_SUM, BETWEEN, DATE_DIM) (gap: CROSS_CTE_PREDICATE_BLINDNESS)  [SUPPORT: native_or_universal]
- **inline_decorrelate_materialized**: 100% match (AGG_AVG, AGG_SUM, BETWEEN, DATE_DIM) (gap: CORRELATED_SUBQUERY_PARALYSIS)  [SUPPORT: native_or_universal]
- **prefetch_fact_join**: 100% match (AGG_SUM, DATE_DIM, GROUP_BY, STAR_JOIN) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: MAX_2_CHAINS] [SUPPORT: portability_candidate; engines=duckdb]
- **early_filter_decorrelate**: 83% match (AGG_AVG, AGG_SUM, BETWEEN, DATE_DIM) (gap: CORRELATED_SUBQUERY_PARALYSIS)  [SUPPORT: native_or_universal]
  Missing: CTE
- **date_cte_explicit_join**: 80% match (AGG_SUM, BETWEEN, DATE_DIM, GROUP_BY) (gap: COMMA_JOIN_WEAKNESS)  [SUPPORT: native_or_universal]
  Missing: CASE_EXPR


## Probe Summary
12 probes fired, 0 passed validation, 0 showed speedup.

## BDA Table (all probes)

| Probe | Transform | Family | Status | Speedup | Top EXPLAIN Nodes | Model Description | SQL Patch | Error/Notes |
|-------|-----------|--------|--------|---------|-------------------|-------------------|-----------|-------------|
| p10 | materialize_cte | E | ERROR | - | - | Materialize base revenue CTE for reuse in sb/sc | p10 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p02 | date_cte_explicit_join | F | ERROR | - | - | Convert comma joins to explicit INNER JOINs after pre-filtering date_dim in CTE | p02 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p01 | pg_self_join_decomposition | E | ERROR | - | - | Consolidate sa/sc subqueries into single MATERIALIZED base_cte: (ss_store_sk, ss_item_sk, revenue) with filters, reuse in sb/sc | p01 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p08 | prefetch_fact_join | A | ERROR | - | - | Chain CTEs: date_filter → store_sales JOIN → pre-aggregation | p08 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p04 | early_filter_decorrelate | B | ERROR | - | - | Precompute sb.ave threshold in CTE before main query, join via sc.revenue <= 0.1 * sb_threshold.ave | p04 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p09 | inline_decorrelate_materialized | B | ERROR | - | - | Materialize threshold CTE with GROUP BY ss_store_sk, use in main query JOIN | p09 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p11 | early_filter | A | ERROR | - | - | Push store.s_state and item.i_manager_id filters into CTE definitions | p11 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p03 | dimension_prefetch_star | F | ERROR | - | - | Pre-filter store/item in CTEs: store_cte (s_store_sk) WHERE s_state IN, item_cte (i_item_sk) WHERE i_manager_id BETWEEN | p03 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p12 | aggregate_pushdown | C | ERROR | - | - | Pre-aggregate store_sales before joining date_dim (ss_store_sk, ss_item_sk, revenue) | p12 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p07 | shared_dimension_multi_channel | A | ERROR | - | - | Create date_cte MATERIALIZED for d_month_seq filter, reference in both sa/sc subqueries | p07 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p05 | single_pass_aggregation | C | ERROR | - | - | Consolidate sa/sc aggregates: single CTE computing revenue (ss_store_sk, ss_item_sk) and AVG(revenue) per store | p05 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| p06 | channel_bitmap_aggregation | C | ERROR | - | - | Combine sa/sc scans: single CASE-based aggregation for revenue per (store,item) and store-level AVG | p06 | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |

## Worker SQL Patches

### p10: materialize_cte (ERROR, n/a)
```sql
WITH filtered_revenue_base AS (SELECT ss_store_sk, ss_item_sk, ss_sales_price, ss_list_price FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) AS sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p02: date_cte_explicit_join (ERROR, n/a)
```sql
WITH filtered_dates AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1215 AND 1215 + 11) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) AS sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p01: pg_self_join_decomposition (ERROR, n/a)
```sql
WITH base_revenue_data AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, (SELECT ss_store_sk, AVG(revenue) AS ave FROM base_revenue_data GROUP BY ss_store_sk) AS sb, (SELECT ss_store_sk, ss_item_sk, revenue FROM base_revenue_data) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p08: prefetch_fact_join (ERROR, n/a)
```sql
WITH valid_dates AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1215 AND 1215 + 11), filtered_sales AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN valid_dates AS vd ON vd.d_date_sk = ss_sold_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, (SELECT ss_store_sk, AVG(revenue) AS ave FROM filtered_sales GROUP BY ss_store_sk) AS sb, filtered_sales AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p04: early_filter_decorrelate (ERROR, n/a)
```sql
WITH store_thresholds AS (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, store_thresholds AS sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p09: inline_decorrelate_materialized (ERROR, n/a)
```sql
WITH sb AS (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p11: early_filter (ERROR, n/a)
```sql
WITH filtered_store AS (SELECT s_store_sk FROM store WHERE s_state IN ('KS', 'OH', 'SD')), filtered_item AS (SELECT i_item_sk FROM item WHERE i_manager_id BETWEEN 10 AND 14) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM filtered_store AS fs, filtered_item AS fi, (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) AS sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND fs.s_store_sk = sc.ss_store_sk AND fi.i_item_sk = sc.ss_item_sk ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p03: dimension_prefetch_star (ERROR, n/a)
```sql
WITH filtered_store AS (SELECT s_store_sk FROM store WHERE s_state IN ('KS', 'OH', 'SD')), filtered_item AS (SELECT i_item_sk FROM item WHERE i_manager_id BETWEEN 10 AND 14), sales_summary AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM filtered_store AS fs JOIN filtered_item AS fi ON TRUE JOIN sales_summary AS sc ON fs.s_store_sk = sc.ss_store_sk AND fi.i_item_sk = sc.ss_item_sk JOIN (SELECT ss_store_sk, AVG(revenue) AS ave FROM sales_summary GROUP BY ss_store_sk) AS sb ON sb.ss_store_sk = sc.ss_store_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p12: aggregate_pushdown (ERROR, n/a)
```sql
WITH filtered_store_sales AS (SELECT ss_store_sk, ss_item_sk, ss_sales_price FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01), preagg_revenue AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM filtered_store_sales GROUP BY ss_store_sk, ss_item_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, preagg_revenue AS sc, (SELECT ss_store_sk, AVG(revenue) AS ave FROM preagg_revenue GROUP BY ss_store_sk) AS sb WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p07: shared_dimension_multi_channel (ERROR, n/a)
```sql
WITH date_sk_filter AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1215 AND 1226) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, (SELECT ss_store_sk, AVG(revenue) AS ave FROM (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sa GROUP BY ss_store_sk) AS sb, (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) AS sc WHERE sb.ss_store_sk = sc.ss_store_sk AND sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p05: single_pass_aggregation (ERROR, n/a)
```sql
WITH store_revenue_stats AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue, AVG(SUM(ss_sales_price)) OVER (PARTITION BY ss_store_sk) AS ave FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, store_revenue_stats AS sc WHERE sc.revenue <= 0.1 * sc.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```

### p06: channel_bitmap_aggregation (ERROR, n/a)
```sql
WITH revenue_by_store_item AS (SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales, date_dim WHERE ss_sold_date_sk = d_date_sk AND d_month_seq BETWEEN 1215 AND 1215 + 11 AND ss_sales_price / ss_list_price BETWEEN 79 * 0.01 AND 89 * 0.01 GROUP BY ss_store_sk, ss_item_sk), store_avg_revenue AS (SELECT ss_store_sk, AVG(revenue) AS ave FROM revenue_by_store_item GROUP BY ss_store_sk) SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store, item, revenue_by_store_item AS sc, store_avg_revenue AS sb WHERE sc.revenue <= 0.1 * sb.ave AND s_store_sk = sc.ss_store_sk AND i_item_sk = sc.ss_item_sk AND i_manager_id BETWEEN 10 AND 14 AND s_state IN ('KS', 'OH', 'SD') ORDER BY s_store_name, i_item_desc LIMIT 100;
```


## Shot 1 Results

| # | Family | Transform | Speedup | Status | Error |
|---|--------|-----------|---------|--------|-------|
| snipe_p1 | ? | unknown | - | ERROR | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |
| snipe_p2 | ? | unknown | - | ERROR | Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context' |

### snipe_p1 Error:
Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context'

### snipe_p2 Error:
Synthetic semantic check failed: SyntheticDataGenerator._generate_value() got an unexpected keyword argument 'row_context'

## Shot 2 — Design 2 More Patch Plans

Build on shot 1 results:
1. Your first plan should refine or extend the best winner (or fix its remaining bottleneck)
2. Your second should try a different approach not yet attempted

If all shot 1 plans failed, diagnose why and try fundamentally different strategies.

Output exactly **2 patch plans** as a JSON array.

Tier-0 Output Contract (hard fail):
- response must be valid JSON
- first character must be `[` (no leading whitespace/newlines)
- top-level value must be an array of exactly 2 objects
- no markdown fences, prose, or commentary
- never emit key `sql`; use `sql_fragment` for SQL fragments

Required per plan:
- `plan_id`, `family`, `transform`, `hypothesis`, `target_ir`, `dialect`, `steps`
- optional: `based_on` as a string (use comma-separated IDs for multiple sources; never an array)
- `steps[]` item: `step_id`, `op`, `target`, optional `payload`
- `target.by_node_id` MUST be `"S0"` (use `by_anchor_hash` only when needed)

Allowed `op` values:
- `insert_cte`
- `replace_from`
- `replace_where_predicate`
- `replace_body`
- `replace_expr_subtree`
- `delete_expr_subtree`
- `replace_join_condition`
- `replace_select`
- `replace_block_with_cte_pair`
- `wrap_query_with_cte`

Semantic guards (MUST preserve):
- all WHERE/HAVING/ON logic
- all literals exactly
- columns/aliases/ORDER BY/LIMIT
- row count and semantics
- no orphaned CTEs or duplicated source scans after replacement

Rules:
- output exactly 2 plans
- each plan must use a different strategy (`family` + `transform`)
- payload SQL fragments must be complete/executable (no ellipsis)
- `replace_body`, `replace_select`, and `replace_block_with_cte_pair` must put SQL in `payload.sql_fragment`
- cite EXPLAIN evidence in `hypothesis`

Output ONLY JSON array.