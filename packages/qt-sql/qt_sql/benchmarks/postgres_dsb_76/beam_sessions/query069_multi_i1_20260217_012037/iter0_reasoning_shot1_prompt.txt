## Role

You are a SQL optimization specialist for PostgreSQL. Your task is to analyze a query's execution plan, identify the primary bottleneck, and propose **exactly 2 independent patch plans** that target different optimization strategies.

Each patch plan must:
- Be atomic (steps applied sequentially: s1 → s2 → s3 → ...)
- Transform the original query using patch operations
- Preserve semantic equivalence (same rows, columns, ordering)
- Follow the patterns shown in reference examples below

## Prompt Map

### Phase A — Cached Context
A1. Dialect Profile
A2. Optimization Families (with decision gates)
A3. EXPLAIN Analysis Procedure
A4. Regression Registry
A5. Aggregation Equivalence Rules
A6. Task Contract

### Phase B — Query-Specific Input (after cache boundary)
B1. Query SQL
B2. Execution Plan
B3. IR Structure
B4. Detected Patterns (if available)

## Dialect Profile (POSTGRES)

**Combined Intelligence Baseline**: Combined intelligence baseline from 53 validated DSB queries at SF5-SF10, plus regression registry outcomes. PostgreSQL has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here.

### Optimizer Strengths (don't fight these)
- `BITMAP_OR_SCAN`: NEVER split OR conditions into UNION ALL. 0.21x and 0.26x observed.
- `SEMI_JOIN_EXISTS`: NEVER convert EXISTS to IN/NOT IN or materialized CTEs. 0.50x, 0.75x observed. Note: NOT EXISTS anti-join decorrelation can still be valid when replacing large correlated anti patterns.
- `INNER_JOIN_REORDERING`: Don't restructure INNER JOIN orders. Focus on LEFT JOIN blocking or comma-join confusion.
- `INDEX_ONLY_SCAN`: Small dimension lookups (<10K rows) may not need CTEs.

### Known Gaps (exploit these)
- `COMMA_JOIN_WEAKNESS` [HIGH] detect: FROM t1, t2, t3 WHERE t1.key = t2.key (comma joins, no explicit JOIN). Poor row estimates in EXPLAIN. | action: Convert comma-joins to explicit JOIN...ON syntax. Best when combined with date_cte_isolate.
- `CORRELATED_SUBQUERY_PARALYSIS` [HIGH] detect: Nested loop in EXPLAIN, inner re-executes aggregate per outer row. SQL: WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key). Hash… | action: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
- `NON_EQUI_JOIN_INPUT_BLINDNESS` [HIGH] detect: Expensive non-equi join (BETWEEN, <, >) with large inputs on both sides. Neither side filtered. | action: Reduce fact table input size via filtered CTE before the non-equi join.
- `CTE_MATERIALIZATION_FENCE` [MEDIUM] detect: Large CTE + small post-filter. Multi-referenced CTE that blocks predicate pushdown. | action: Materialize STRATEGICALLY: only when CTE is expensive and reused. Avoid fencing single-use cases.
- `CROSS_CTE_PREDICATE_BLINDNESS` [MEDIUM] detect: Sequential scan on dimension table without index condition. Late filter after large scan/join. | action: Pre-filter into CTE definition. But be more cautious than on DuckDB.

## Optimization Families

6/6 families have validated gold examples on this dialect. Treat these as priors, not hard rules.

Prioritize by: EXPLAIN bottleneck, transform precondition fit, and dialect gap match.


### Family A: Early Filtering (Predicate Pushback)
**Description**: Push small filters into CTEs early, reduce row count before expensive operations
**Speedup Range**: 1.3–4.0x (~35% of all wins)
**Use When**:
  1. Late WHERE filters on dimension tables
  2. Cascading CTEs with filters applied downstream
  3. Expensive joins after filters could be pushed earlier
**Decision Gates (STOP when):**
  1. Filter ratio is weak and baseline runtime is already low
  2. Target CTE already contains the relevant selective predicate
  3. Three or more fact tables in a deep CTE chain (join-order lock risk)

**Gold Example**: `pg_date_cte_explicit_join` (2.28x)
**Canonical transforms**: `date_cte_isolate`
**Targeted gaps**: `CROSS_CTE_PREDICATE_BLINDNESS`
**Pattern**: Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe t…
**Patch shape**: insert_cte → replace_from → replace_where_predicate



### Family B: Decorrelation (Sets Over Loops)
**Description**: Convert correlated subqueries to standalone CTEs with GROUP BY, eliminate per-row re-execution
**Speedup Range**: 2.4–2.9x (~15% of all wins)
**Use When**:
  1. Correlated subqueries in WHERE clause
  2. Scalar aggregates computed per outer row
  3. DELIM_SCAN in execution plan (indicates correlation)
**Decision Gates (STOP when):**
  1. EXPLAIN already shows a hash semi join on the same correlation key
  2. Simple EXISTS path already optimized by semi-join
  3. Outer side is already tiny after early filtering

**Gold Example**: `pg_shared_scan_decorrelate` (8043.91x (timeout rescue))
**Canonical transforms**: `decorrelate`
**Targeted gaps**: `CORRELATED_SUBQUERY_PARALYSIS`
**Pattern**: Shared Scan Decorrelation: when inner and outer queries scan the same fact table with the same date/cost filters, extract the common scan into a single CTE. Then compute per-item thresholds via GROUP BY in a second CTE, and JOIN back to filter. Converts O(N*M…
**Patch shape**: insert_cte → insert_cte → insert_cte → insert_cte → replace_from → delete_expr_subtree



### Family C: Aggregation Pushdown (Minimize Rows Touched)
**Description**: Aggregate before expensive joins when GROUP BY keys ⊇ join keys, reduce intermediate sizes
**Speedup Range**: 1.3–15.3x (~5% of all wins (high variance))
**Use When**:
  1. GROUP BY happens after large joins
  2. GROUP BY keys are subset of join keys
  3. Intermediate result size >> final result size
**Decision Gates (STOP when):**
  1. GROUP BY keys are not compatible with join keys (semantic risk)
  2. Aggregation includes grouping-sensitive metrics (e.g., STDDEV/VARIANCE)
  3. Rewrite introduces join duplication before AVG/STDDEV-style aggregates

**Gold Example**: `pg_materialized_dimension_fact_prefilter` (12.07x (V2 DSB SF10, was 2.68x in V1))
**Canonical transforms**: `early_filter`, `date_cte_isolate`
**Targeted gaps**: `CROSS_CTE_PREDICATE_BLINDNESS`
**Pattern**: Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.
**Patch shape**: insert_cte → insert_cte → insert_cte → insert_cte → insert_cte → replace_from



### Family D: Set Operation Optimization (Sets Over Loops)
**Description**: Replace INTERSECT/UNION-based patterns with EXISTS/NOT EXISTS, avoid full materialization
**Speedup Range**: 1.7–2.7x (~8% of all wins)
**Use When**:
  1. INTERSECT patterns between large sets
  2. UNION ALL with duplicate elimination
  3. Set operations materializing full intermediate results
**Decision Gates (STOP when):**
  1. Both set-operation sides are already small
  2. Result needs columns from both sides (semi-join rewrite invalid)

**Gold Example**: `pg_intersect_to_exists` (1.78x)
**Canonical transforms**: `intersect_to_exists`
**Pattern**: INTERSECT to EXISTS: INTERSECT materializes both sides fully, sorts, and compares. EXISTS uses semi-join with index + early termination per row. When both sides produce 10K+ rows, EXISTS is cheaper because it stops at first match.
**Patch shape**: insert_cte → replace_from → replace_where_predicate



### Family E: Materialization / Prefetch (Don't Repeat Work)
**Description**: Extract repeated scans or pre-compute intermediate results for reuse across multiple consumers
**Speedup Range**: 1.3–6.2x (~18% of all wins)
**Use When**:
  1. Repeated scans of same table with different filters
  2. Dimension filters applied independently multiple times
  3. CTE referenced multiple times with implicit re-evaluation
**Decision Gates (STOP when):**
  1. CTE is single-use and not expensive
  2. New CTE would be unfiltered (materialize-everything pattern)
  3. Original source scan would remain alongside replacement (orphan risk)

**Gold Example**: `multi_dimension_prefetch` (2.71x)
**Canonical transforms**: `multi_dimension_prefetch`
**Targeted gaps**: `CROSS_CTE_PREDICATE_BLINDNESS`
**Pattern**: Multi-Dimension Prefetch: when multiple dimension tables have selective filters, pre-filter ALL of them into CTEs before the fact table join. Combined selectivity compounds — each dimension CTE reduces the fact scan further.
**Patch shape**: insert_cte → insert_cte → insert_cte → replace_from → delete_expr_subtree



### Family F: Join Transform (Right Shape First)
**Description**: Restructure join topology: convert comma joins to explicit INNER JOIN, optimize join order, eliminate self-joins via single-pass aggregation
**Speedup Range**: 1.8–8.6x (~19% of all wins)
**Use When**:
  1. Comma-separated joins (implicit cross joins) in FROM clause
  2. Self-joins scanning same table multiple times
  3. Dimension-fact join order suboptimal for predicate pushdown
**Decision Gates (STOP when):**
  1. Tiny join graph where optimizer is already accurate
  2. EXPLAIN shows good cardinality estimates and stable join shape

**Gold Example**: `pg_explicit_join_materialized` (8.56x)
**Canonical transforms**: `date_cte_explicit_join`
**Targeted gaps**: `COMMA_JOIN_WEAKNESS`
**Pattern**: Explicit Join + Materialized CTE: comma joins with 5+ tables produce poor cardinality estimates. Converting to explicit INNER JOIN + materializing selective dimensions into small CTEs gives the planner accurate row counts at each join step.
**Patch shape**: insert_cte → insert_cte+replace_from → insert_cte+replace_from → insert_cte+replace_from → replace_from → delete_expr_subtree



## EXPLAIN Analysis Procedure

1. **Identify cost spine**: isolate operator chain driving most runtime.
2. **Classify spine nodes**:
- SEQ_SCAN: row count + filter selectivity
- NESTED_LOOP/ANTI: inner re-execution risk
- AGGREGATE: input/output compression ratio
- MATERIALIZE: loops × rows amplification
3. **Trace row flow**: find where rows stay flat then collapse late.
4. **Count repeated scans**: same table scanned N times with similar joins/filters.
5. **State bottleneck hypothesis**: optimizer does X; transform Y should help because Z.

## Regression Registry

Hard failures to gate against:
- Materialized simple EXISTS path -> severe regressions (semi-join lost)
- Same-column OR split to UNION ALL -> regressions on bitmap-or capable plans
- Orphaned original CTE/table after replacement -> double materialization
- Unfiltered new CTE -> materialize-everything anti-pattern
- Over-deep fact-table CTE chains -> join-order lock / parallelism loss

## Aggregation Equivalence Rules

- GROUP BY keys must remain compatible with join keys after rewrite.
- AVG/STDDEV/VARIANCE are duplication-sensitive.
- FILTER() semantics are group-membership sensitive.
- When pivoting with CASE/FILTER, preserve discriminator semantics exactly.

## Reasoning Process

1. Run EXPLAIN analysis procedure.
2. Route bottleneck to candidate families.
3. Apply decision gates and regression checks.
4. Design 2 plans from different families targeting different bottlenecks.

Output exactly **2 patch plans** as a JSON array.

Required per plan:
- `plan_id`, `family`, `transform`, `hypothesis`, `target_ir`, `dialect`, `steps`
- optional: `based_on` (probe IDs used as foundation/combination)
- `steps[]` item: `step_id`, `op`, `target`, optional `payload`
- `target.by_node_id` MUST be `"S0"` (use `by_anchor_hash` only when needed)

Allowed `op` values:
- `insert_cte`
- `replace_from`
- `replace_where_predicate`
- `replace_body`
- `replace_expr_subtree`
- `delete_expr_subtree`

Semantic guards (MUST preserve):
- all WHERE/HAVING/ON logic
- all literals exactly
- columns/aliases/ORDER BY/LIMIT
- row count and semantics
- no orphaned CTEs or duplicated source scans after replacement

Rules:
- output exactly 2 plans
- each plan must use a different strategy (`family` + `transform`)
- payload SQL fragments must be complete/executable (no ellipsis)
- cite EXPLAIN evidence in `hypothesis`

Output ONLY JSON array (no markdown, no prose).

---

## Cache Boundary
Everything below is query-specific input.

## Query to Analyze

**Dialect**: POSTGRES

```sql
select 
  cd_gender,
  cd_marital_status,
  cd_education_status,
  count(*) cnt1,
  cd_purchase_estimate,
  count(*) cnt2,
  cd_credit_rating,
  count(*) cnt3
 from
  customer c,customer_address ca,customer_demographics
 where
  c.c_current_addr_sk = ca.ca_address_sk and
  ca_state in ('CO','NC','TX') and
  cd_demo_sk = c.c_current_cdemo_sk
  and cd_marital_status in ('S', 'M', 'U')
  and cd_education_status in ('Primary', 'College') and
  exists (select *
          from store_sales,date_dim
          where c.c_customer_sk = ss_customer_sk and
                ss_sold_date_sk = d_date_sk and
                d_year = 2002 and
                d_moy between 10 and 10+2
                and ss_list_price between 80 and 169
          ) and
   (not exists (select *
            from web_sales,date_dim
            where c.c_customer_sk = ws_bill_customer_sk and
                  ws_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and ws_list_price between 80 and 169
            ) and
    not exists (select *
            from catalog_sales,date_dim
            where c.c_customer_sk = cs_ship_customer_sk and
                  cs_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and cs_list_price between 80 and 169)
            )
 group by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 order by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 limit 100;
```

### Execution Plan

```
(EXPLAIN unavailable)
```

### IR Structure (for patch targeting)

```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: customer c, customer_address ca, customer_demographics
    WHERE [dae945277e160f9b]: c.c_current_addr_sk = ca.ca_address_sk AND ca_state IN ('CO', 'NC', 'TX') AND cd_demo_sk = c.c_cu...
    GROUP BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating
    ORDER BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**Note**: Use `by_node_id` (e.g., "S0") and `by_anchor_hash` (16-char hex) from map above to target patch operations.

### Detected Patterns

### AST Feature Detection

- **date_cte_explicit_join**: 60% match (BETWEEN, DATE_DIM, GROUP_BY) (gap: COMMA_JOIN_WEAKNESS)
  Missing: AGG_SUM, CASE_EXPR
- **pg_self_join_decomposition**: 60% match (BETWEEN, DATE_DIM, GROUP_BY) (gap: CROSS_CTE_PREDICATE_BLINDNESS)
  Missing: AGG_AVG, AGG_SUM
- **materialized_dimension_fact_prefilter**: 57% match (AGG_COUNT, BETWEEN, DATE_DIM, GROUP_BY) (gap: NON_EQUI_JOIN_INPUT_BLINDNESS)
  Missing: AGG_SUM, CASE_EXPR, LEFT_JOIN
- **early_filter_decorrelate**: 50% match (BETWEEN, DATE_DIM, GROUP_BY) (gap: CORRELATED_SUBQUERY_PARALYSIS)
  Missing: AGG_AVG, AGG_SUM, CTE
- **inline_decorrelate_materialized**: 50% match (BETWEEN, DATE_DIM) (gap: CORRELATED_SUBQUERY_PARALYSIS)
  Missing: AGG_AVG, AGG_SUM


**Instruction**: Prioritize detected patterns above. If a high-confidence
pathology is detected, your primary target SHOULD address it.