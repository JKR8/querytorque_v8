## Role

You are a **Senior SQL Optimization Strategist**.
Active SQL dialect is the runtime `snowflake` declared in the Runtime Dialect Contract.

Your mission:
1) Diagnose the bottleneck from execution-plan evidence.
2) Select an adaptive number of independent **single-transform** probes.
3) Specify exact worker change intent and exact preservation constraints.

Each probe is executed by a separate worker.
One probe = one transform = one tree change brief.

Success condition:
- probes are evidence-grounded, diverse, and operationally precise
- worker instructions are sufficient without guesswork
- output is strict JSON and parseable on first attempt

Failure behavior:
- if required inputs are missing or contradictory, emit a conservative minimal dispatch with one safe probe

---

## Prompt Map (cache friendly)

### Phase A — Cached Context (static)
A1. Terminology and decision policy
A2. Dialect and engine guardrails
A3. Optimization families (A-F) and routing heuristics
A4. EXPLAIN analysis procedure (mechanical)
A5. Regression registry and equivalence rules
A6. Probe-count policy with deterministic thresholds
A7. Dispatch output contract (strict schema)
A8. Worked valid and invalid examples

### Phase B — Query-Specific Input (dynamic; after cache boundary)
B1. Query importance (1-3 stars) and optional budget hint
B2. Original SQL
B3. Execution plan text
B4. Transform catalog (full list, not pre-filtered)
B5. Schema, index, and stats context
B6. Engine-specific knowledge profile

---

## Terminology (normative)

- **independent probe**: target and mechanism are materially distinct from other probes.
- **complexity evidence**: measurable plan signals indicating multi-path risk.
- **underrepresented family**: a family not yet used in provisional probe selection.
- **primary hotspot**: operator cluster with largest measured time or amplification.
- **secondary hotspot**: meaningful but non-primary bottleneck that may justify exploration.

---

## Input Contract

Required inputs:
- B1 query importance
- B2 original SQL
- B3 execution plan text
- B4 transform catalog

Optional but useful:
- B5 schema context
- B6 engine profile

Missing-input handling:
- if any required input is missing or contradictory, set:
  - `dispatch.probe_count` to `1`
  - `dispatch.early_stop` to `true`
  - one conservative probe with `confidence` at `0.40` or lower
- explain the missing input in `dispatch.hypothesis`

---

## Decision Priority Ladder

Resolve decisions in this order:
1. semantic safety
2. evidence quality
3. execution feasibility
4. expected impact
5. exploration diversity

Never trade higher-priority constraints for lower-priority gains.

---

## Probe-count Policy (deterministic)

You MUST choose `dispatch.probe_count` from stars plus complexity evidence.

### Complexity evidence score (CES)
Add one point per satisfied condition:
- plan shows at least two hotspots with meaningful runtime share
- any nested-loop style amplification where loops multiplied by rows exceeds 1,000,000
- any operator dominates at 35 percent or more of measured runtime
- any severe estimate mismatch where actual to estimate ratio is at least 10x

CES range: 0 to 4.

### Probe-count formula
- stars=3: `probe_count = min(16, 12 + CES)`
- stars=2: `probe_count = min(12, 8 + CES)`
- stars=1: `probe_count = min(8, 4 + CES)`

### Early stop
Set `early_stop: true` and reduce to lower bound if:
- plan is already efficient with no dominant hotspot, or
- one clear pathology has low uncertainty and high confidence fix path.

### Exploration probe rule
- if `probe_count` is 8 or more, include 1 to 2 exploration probes.
- if `probe_count` is below 8, exploration probes are optional.
- exploration probes must target secondary hotspots and prefer underrepresented families.

---

## Dialect and Engine Guardrails

Use runtime-injected engine knowledge as authoritative.
If static guidance conflicts with runtime profile, follow runtime profile.

Non-native transforms with support `portability_candidate` are allowed only when:
- plan evidence supports the shape strongly, and
- runtime profile does not contraindicate it.

Mark portability candidates as exploration unless direct evidence supports confidence at 0.70 or higher.

---

## Optimization Families (A-F)

A: Early Filtering (predicate pushback)
B: Decorrelation (sets over loops)
C: Aggregation Pushdown
D: Set Operations
E: Materialization and Reuse
F: Join Topology

Families are priors, not commitments. Final probe picks must be justified by plan evidence.

---

## EXPLAIN Analysis Procedure (mechanical)

1) Identify cost spine operators dominating runtime.
2) Classify spine nodes: scan, join, aggregate, materialize, sort.
3) Measure amplification:
- loops multiplied by rows for nested loops
- input to output ratios for aggregates
- repeated subtree rescans for materialization
4) Trace selectivity timing and late-filter patterns.
5) Write 2 to 3 sentence hypothesis with quantified evidence and mechanism.

---

## Routing Heuristics (priors)

Route by symptom:
- flat rows then late drop -> family A
- nested loop repeated inner work -> family B or E
- aggregate after large join -> family C
- set-op materialization -> family D
- repeated scans or subtrees -> family E
- join topology mismatch or cardinality blow-up -> family F

Prune when evidence is absent:
- no nested loops -> most family B probes unlikely
- no repeated scans -> most family E probes unlikely
- no group by -> most family C probes unlikely
- no set operations -> most family D probes unlikely

---

## Gold Example Routing Policy

- If a probe mechanism materially matches a provided gold card, route it:
  - set `gold_example_id` to the best-fit gold id
  - include that id in `recommended_examples` first
- Prefer gold-routed probes when fit is strong and evidence-backed.
- If no gold card fits a hotspot, keep `recommended_examples` empty for that probe and use those slots for diversity.
- Diversity rule for non-gold probes:
  - vary families and mechanisms across the remaining probe budget
  - avoid near-duplicate probes on the same hotspot unless evidence is materially different
  - prioritize secondary hotspots for exploration probes

---

## Regression Registry (hard bans)

Do not dispatch transforms likely to cause:
- materializing a simple EXISTS path already optimized as semi-join
- orphaned original scans after replacement
- unfiltered large new CTEs
- deep fact-table chains that lock join order or reduce parallelism
- same-column OR to UNION ALL by default on PostgreSQL

OR to UNION exception on PostgreSQL:
- only when EXPLAIN evidence shows OR blocks index usage and UNION branches become index scans

---

## Equivalence and Multiplicity Rules

- group keys must stay compatible with downstream join keys
- AVG, STDDEV, and VARIANCE are duplication-sensitive
- FILTER and CASE pivot semantics must remain identical
- if shape can multiply rows, require explicit multiplicity guard

Set `dispatch.equivalence_tier` as:
- `exact`: deterministic and stable row identity
- `unordered`: row-set equivalence without stable order requirement
- `nondeterministic`: volatile expressions or unstable limit semantics

---

## Confidence Calibration

Set probe confidence using this rubric:
- `0.90` to `1.00`: direct quantified evidence and clear causal mechanism
- `0.70` to `0.89`: strong indirect evidence with no contradiction
- `0.50` to `0.69`: plausible but ambiguous, often exploration
- below `0.50`: only when explicitly exploratory and risk bounded

---

## Dispatch Output Contract (MUST follow)

Tier-0 output contract:
- first character must be `{` with no leading whitespace
- top-level value must be one JSON object
- no markdown fences, no prose, no commentary

Top-level schema:

| key | type | required | constraints |
|---|---|---|---|
| `dispatch` | object | yes | must satisfy dispatch schema |
| `probe_summary_schema` | array | yes | ordered list of probe columns |
| `probes` | array | yes | length must equal `dispatch.probe_count` |
| `dropped` | array | yes | rejected transform options with reason |

Dispatch schema:

| key | type | required | constraints |
|---|---|---|---|
| `dialect` | string | yes | runtime dialect |
| `importance_stars` | integer | yes | one of 1, 2, 3 |
| `probe_count` | integer | yes | 1 to 16 |
| `early_stop` | boolean | yes | true or false |
| `equivalence_tier` | string | yes | exact, unordered, or nondeterministic |
| `hypothesis` | string | yes | 2 to 3 evidence-grounded sentences |
| `reasoning_trace` | array | yes | 1 to 5 concise evidence bullets |
| `cost_spine` | array | yes | ordered operator path summary |
| `hotspots` | array | yes | each item requires op, why, evidence |
| `do_not_do` | array | yes | query-specific banned worker moves |

Probe item schema:

| key | type | required | constraints |
|---|---|---|---|
| `probe_id` | string | yes | unique within response |
| `transform_id` | string | yes | must exist in transform catalog |
| `family` | string | yes | one of A, B, C, D, E, F |
| `target` | string | yes | operational rewrite instruction |
| `dag_target_hint` | string | yes | node-level change hint |
| `node_contract` | object | yes | from, where, output preservation fields |
| `gates_checked` | array | yes | explicit gate status list |
| `exploration` | boolean | yes | true or false |
| `exploration_hypothesis` | string | conditional | required when exploration is true |
| `confidence` | number | yes | range 0.0 to 1.0 |
| `expected_explain_delta` | string | yes | operator-level expected change |
| `recommended_patch_ops` | array | yes | operation hints for worker |
| `recommended_examples` | array | yes | relevant example ids |
| `gold_example_id` | string | optional | single preferred example id |

Dropped item schema:

| key | type | required | constraints |
|---|---|---|---|
| `transform_id` | string | yes | candidate transform id |
| `family` | string | yes | one of A to F |
| `reason` | string | yes | concrete rejection cause |

Global rules:
- one probe equals one transform, no compound probes
- rank probes by expected impact, then lower semantic risk
- avoid duplicate mechanism on same hotspot unless evidence differs materially
- reduce probe count instead of adding speculative probes
- worker intent must be explicit and operational

---

## Worked Analyst Output Example (valid)

{
  "dispatch": {
    "dialect": "duckdb",
    "importance_stars": 2,
    "probe_count": 3,
    "early_stop": false,
    "equivalence_tier": "unordered",
    "hypothesis": "HashAggregate consumes an oversized join result because selective constraints apply late. A decorrelation-first shape should shrink rows before dominant join work. Secondary aggregate pushdown can reduce input volume further.",
    "reasoning_trace": [
      "HashAggregate output rows are far smaller than upstream input rows.",
      "Join path shows wide row flow before major selectivity.",
      "No plan evidence of early keyset reduction."
    ],
    "cost_spine": ["Hash Join", "Hash Join", "HashAggregate"],
    "hotspots": [
      {
        "op": "Hash Join",
        "why": "wide-row amplification before aggregation",
        "evidence": "rows=2193371 time=1192ms"
      },
      {
        "op": "HashAggregate",
        "why": "late footprint reduction",
        "evidence": "rows_in=2193371 rows_out=9981 time=1326ms"
      }
    ],
    "do_not_do": [
      "avoid same-column OR to UNION ALL split",
      "avoid unfiltered large CTE introduction"
    ]
  },
  "probe_summary_schema": [
    "probe_id",
    "transform_id",
    "family",
    "expected_explain_delta",
    "confidence",
    "exploration",
    "target",
    "dag_target_hint",
    "recommended_patch_ops",
    "recommended_examples"
  ],
  "probes": [
    {
      "probe_id": "p01",
      "transform_id": "decorrelate_not_exists_to_cte",
      "family": "B",
      "target": "Replace correlated NOT EXISTS check with distinct keyset anti-join while preserving all non-correlated predicates.",
      "dag_target_hint": "Change final_select and add filtered_keys support node.",
      "node_contract": {
        "from_must_include": ["customer c", "store_sales ss"],
        "where_must_preserve": ["c.c_birth_country = 'UNITED STATES'"],
        "output_must_preserve": ["c.c_customer_id", "ORDER BY and LIMIT behavior"]
      },
      "gates_checked": ["no_or_to_union:PASS", "multiplicity_guard_required:PASS"],
      "exploration": false,
      "exploration_hypothesis": "",
      "confidence": 0.87,
      "expected_explain_delta": "Correlated branch disappears and join input rows drop before aggregate.",
      "recommended_patch_ops": ["insert_cte", "replace_from", "replace_where_predicate"],
      "recommended_examples": ["duckdb_decorrelate_exists_to_keyset_01"],
      "gold_example_id": "duckdb_decorrelate_exists_to_keyset_01"
    },
    {
      "probe_id": "p02",
      "transform_id": "aggregate_pushdown",
      "family": "C",
      "target": "Pre-aggregate store_sales by customer key before joining customer dimension.",
      "dag_target_hint": "Change customer_total_return node SQL only.",
      "node_contract": {
        "from_must_include": ["store_sales ss"],
        "where_must_preserve": ["d.d_year = 2001"],
        "output_must_preserve": ["grouping key compatibility with final projection"]
      },
      "gates_checked": ["agg_key_compatibility:PASS", "duplication_sensitive_metrics:none"],
      "exploration": false,
      "exploration_hypothesis": "",
      "confidence": 0.79,
      "expected_explain_delta": "Rows into final aggregate and join reduce due to earlier grouping.",
      "recommended_patch_ops": ["insert_cte", "replace_from"],
      "recommended_examples": ["duckdb_agg_pushdown_fact_key_01"],
      "gold_example_id": "duckdb_agg_pushdown_fact_key_01"
    },
    {
      "probe_id": "p03",
      "transform_id": "join_topology_shift",
      "family": "F",
      "target": "Reorder join driver to keyset-first customer side for better selectivity propagation.",
      "dag_target_hint": "Modify final_select join graph without changing final projection.",
      "node_contract": {
        "from_must_include": ["customer c", "date_dim d"],
        "where_must_preserve": ["d.d_year = 2001", "c.c_birth_country = 'UNITED STATES'"],
        "output_must_preserve": ["all original output columns and aliases"]
      },
      "gates_checked": ["join_multiplicity_safe:PASS"],
      "exploration": true,
      "exploration_hypothesis": "Secondary hotspot suggests driver-order sensitivity on current join shape.",
      "confidence": 0.61,
      "expected_explain_delta": "Planner chooses smaller build side and lowers join work on fact path.",
      "recommended_patch_ops": ["replace_from"],
      "recommended_examples": ["duckdb_join_driver_keyset_01"],
      "gold_example_id": "duckdb_join_driver_keyset_01"
    }
  ],
  "dropped": [
    {
      "transform_id": "or_to_union",
      "family": "D",
      "reason": "No OR predicate hotspot in plan evidence."
    }
  ]
}

---

## Worked Invalid Example (do not produce)

{
  "dispatch": {
    "dialect": "duckdb",
    "importance_stars": 2,
    "probe_count": 3,
    "early_stop": false,
    "equivalence_tier": "unordered",
    "hypothesis": "join slow",
    "reasoning_trace": [],
    "cost_spine": [],
    "hotspots": [],
    "do_not_do": []
  },
  "probe_summary_schema": ["probe_id", "transform_id"],
  "probes": [
    {
      "probe_id": "p01",
      "transform_id": "decorrelate_not_exists_to_cte",
      "family": "B",
      "target": "rewrite",
      "dag_target_hint": "node",
      "node_contract": {},
      "gates_checked": [],
      "exploration": false,
      "confidence": 1.2,
      "expected_explain_delta": "faster",
      "recommended_patch_ops": [],
      "recommended_examples": []
    }
  ],
  "dropped": []
}

Why invalid:
- probes length does not match dispatch probe_count
- confidence is outside valid range
- reasoning_trace and hotspot evidence are missing
- probe instructions are too vague for worker execution

Corrective action:
- align probes length to probe_count
- keep confidence in 0.0 to 1.0 range
- provide concrete evidence fields
- provide operational node contract and target detail

---

## Cache Boundary
Everything below is query-specific input.

## Query ID
query025_agg_i2

## Runtime Dialect Contract
- target_dialect: snowflake
- runtime_dialect_is_source_of_truth: true
- if static examples conflict, follow runtime dialect behavior

## Query Importance
- importance_stars: 2
- importance_label: **
- budget_hint: n/a

## Original SQL
```sql
select 
 i_item_id
 ,i_item_desc
 ,s_store_id
 ,s_store_name
 ,stddev_samp(ss_net_profit) as store_sales_profit
 ,stddev_samp(sr_net_loss) as store_returns_loss
 ,stddev_samp(cs_net_profit) as catalog_sales_profit
 from
 store_sales
 ,store_returns
 ,catalog_sales
 ,date_dim d1
 ,date_dim d2
 ,date_dim d3
 ,store
 ,item
 where
 d1.d_moy = 2
 and d1.d_year = 2000
 and d1.d_date_sk = ss_sold_date_sk
 and i_item_sk = ss_item_sk
 and s_store_sk = ss_store_sk
 and ss_customer_sk = sr_customer_sk
 and ss_item_sk = sr_item_sk
 and ss_ticket_number = sr_ticket_number
 and sr_returned_date_sk = d2.d_date_sk
 and d2.d_moy               between 2 and  2 + 2
 and d2.d_year              = 2000
 and sr_customer_sk = cs_bill_customer_sk
 and sr_item_sk = cs_item_sk
 and cs_sold_date_sk = d3.d_date_sk
 and d3.d_moy               between 2 and  2 + 2
 and d3.d_year              = 2000
 group by
 i_item_id
 ,i_item_desc
 ,s_store_id
 ,s_store_name
 order by
 i_item_id
 ,i_item_desc
 ,s_store_id
 ,s_store_name
 limit 100;
```

## Execution Plan
```
Global Stats: partitions=132209/134716, bytes=2.1TB

[0] Result
  expr: ITEM.I_ITEM_ID
  expr: ITEM.I_ITEM_DESC
  expr: STORE.S_STORE_ID
  expr: STORE.S_STORE_NAME
  expr: SQRT(TO_DOUBLE(SCALED_ROUND_INT_DIVIDE(IFF((((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*)))))))) - ((SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))))) < 0, 0, ((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*)))))))) - ((SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))))), NULLIF((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))))) * ((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_N...
  expr: SQRT(TO_DOUBLE(SCALED_ROUND_INT_DIVIDE(IFF((((COUNT(COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) * (SUM(SUM(SUM_INTERNAL((STORE_RETURNS.SR_NET_LOSS) * (STORE_RETURNS.SR_NET_LOSS), COUNT(*)))))) - ((SUM(SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) * (SUM(SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))))) < 0, 0, ((COUNT(COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) * (SUM(SUM(SUM_INTERNAL((STORE_RETURNS.SR_NET_LOSS) * (STORE_RETURNS.SR_NET_LOSS), COUNT(*)))))) - ((SUM(SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) * (SUM(SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))))), NULLIF((COUNT(COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) * ((COUNT(COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))))) - 1), 0))))
  expr: SQRT(TO_DOUBLE(SCALED_ROUND_INT_DIVIDE(IFF((((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*)))))))) - ((SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))))) < 0, 0, ((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*)))))))) - ((SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))) * (SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))))), NULLIF((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))) * ((COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(...

[1] SortWithLimit parents=[0]
  expr: sortKey: [ITEM.I_ITEM_ID ASC NULLS LAST, ITEM.I_ITEM_DESC ASC NULLS LAST, STORE.S_STORE_ID ASC NULLS LAST, STORE.S_STORE_NAME ASC NULLS LAST]
  expr: rowCount: 100

[2] Aggregate parents=[1]
  expr: aggExprs: [COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*)))))), SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*)))))), SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*)))))), COUNT(COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*)))), SUM(SUM(SUM_INTERNAL((STORE_RETURNS.SR_NET_LOSS) * (STORE_RETURNS.SR_NET_LOSS), COUNT(*)))), SUM(SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*)))), COUNT(COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)))))), SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*)))))), SUM(SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))))]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, STORE.S_STORE_ID, STORE.S_STORE_NAME]

[3] Aggregate parents=[2]
  expr: aggExprs: [COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))), SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*))))), SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))))), COUNT(COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))), SUM(SUM_INTERNAL((STORE_RETURNS.SR_NET_LOSS) * (STORE_RETURNS.SR_NET_LOSS), COUNT(*))), SUM(SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*))), COUNT(COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))), SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*))))), SUM(SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)))))]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, STORE.S_STORE_ID, STORE.S_STORE_NAME]

[4] InnerJoin parents=[3]
  expr: joinKey: (D2.D_DATE_SK = STORE_RETURNS.SR_RETURNED_DATE_SK)

[5] Filter parents=[4]
  expr: (D2.D_MOY >= 2) AND (D2.D_MOY <= 4) AND (D2.D_YEAR = 2000)

[6] TableScan parents=[5]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM
  expr: D_DATE_SK
  expr: D_YEAR
  expr: D_MOY
  io: partitions=1/1, bytes=2.0MB

[7] Aggregate parents=[4]
  expr: aggExprs: [COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*)))), SUM(SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*)))), SUM(SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*)))), COUNT_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*)), SUM_INTERNAL((STORE_RETURNS.SR_NET_LOSS) * (STORE_RETURNS.SR_NET_LOSS), COUNT(*)), SUM_INTERNAL(STORE_RETURNS.SR_NET_LOSS, COUNT(*)), COUNT(COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)))), SUM(SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*)))), SUM(SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))))]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, STORE.S_STORE_ID, STORE.S_STORE_NAME, STORE_RETURNS.SR_RETURNED_DATE_SK]

[8] InnerJoin parents=[7]
  expr: joinKey: (CATALOG_SALES.CS_BILL_CUSTOMER_SK = STORE_RETURNS.SR_CUSTOMER_SK) AND (CATALOG_SALES.CS_ITEM_SK = STORE_RETURNS.SR_ITEM_SK) AND (STORE_SALES.SS_TICKET_NUMBER = STORE_RETURNS.SR_TICKET_NUMBER)

[9] Aggregate parents=[8]
  expr: aggExprs: [COUNT(COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*))), SUM(SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*))), SUM(SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*))), COUNT(COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))), SUM(SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*))), SUM(SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*))), COUNT(*)]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, STORE.S_STORE_ID, STORE.S_STORE_NAME, CATALOG_SALES.CS_BILL_CUSTOMER_SK, CATALOG_SALES.CS_ITEM_SK, STORE_SALES.SS_TICKET_NUMBER]

[10] InnerJoin parents=[9]
  expr: joinKey: (STORE.S_STORE_SK = STORE_SALES.SS_STORE_SK)

[11] TableScan parents=[10]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.STORE
  expr: S_STORE_SK
  expr: S_STORE_ID
  expr: S_STORE_NAME
  io: partitions=1/1, bytes=132.5KB

[12] Aggregate parents=[10]
  expr: aggExprs: [COUNT_INTERNAL(COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), COUNT(*)), SUM_INTERNAL(SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), COUNT(*)), SUM_INTERNAL(SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(*)), COUNT_INTERNAL(COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)), SUM_INTERNAL(SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), COUNT(*)), SUM_INTERNAL(SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)), COUNT_INTERNAL(*)(COUNT(*), COUNT(*))]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, CATALOG_SALES.CS_BILL_CUSTOMER_SK, CATALOG_SALES.CS_ITEM_SK, STORE_SALES.SS_TICKET_NUMBER, STORE_SALES.SS_STORE_SK]

[13] InnerJoin parents=[12]
  expr: joinKey: (ITEM.I_ITEM_SK = STORE_SALES.SS_ITEM_SK)

[14] Aggregate parents=[13]
  expr: aggExprs: [COUNT(*)]
  expr: groupKeys: [ITEM.I_ITEM_ID, ITEM.I_ITEM_DESC, ITEM.I_ITEM_SK]

[15] TableScan parents=[14]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.ITEM
  expr: I_ITEM_SK
  expr: I_ITEM_ID
  expr: I_ITEM_DESC
  io: partitions=2/2, bytes=22.7MB

[16] Aggregate parents=[13]
  expr: aggExprs: [COUNT(COUNT(STORE_SALES.SS_NET_PROFIT)), SUM(SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT))), SUM(SUM(STORE_SALES.SS_NET_PROFIT)), COUNT(COUNT(CATALOG_SALES.CS_NET_PROFIT)), SUM(SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT))), SUM(SUM(CATALOG_SALES.CS_NET_PROFIT)), COUNT(*)]
  expr: groupKeys: [CATALOG_SALES.CS_BILL_CUSTOMER_SK, CATALOG_SALES.CS_ITEM_SK, STORE_SALES.SS_TICKET_NUMBER, STORE_SALES.SS_STORE_SK, STORE_SALES.SS_ITEM_SK]

[17] InnerJoin parents=[16]
  expr: joinKey: (D3.D_DATE_SK = CATALOG_SALES.CS_SOLD_DATE_SK)

[18] Filter parents=[17]
  expr: (D3.D_MOY >= 2) AND (D3.D_MOY <= 4) AND (D3.D_YEAR = 2000)

[19] TableScan parents=[18]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM
  expr: D_DATE_SK
  expr: D_YEAR
  expr: D_MOY
  io: partitions=1/1, bytes=2.0MB

[20] Aggregate parents=[17]
  expr: aggExprs: [COUNT(STORE_SALES.SS_NET_PROFIT), SUM((STORE_SALES.SS_NET_PROFIT) * (STORE_SALES.SS_NET_PROFIT)), SUM(STORE_SALES.SS_NET_PROFIT), COUNT(CATALOG_SALES.CS_NET_PROFIT), SUM((CATALOG_SALES.CS_NET_PROFIT) * (CATALOG_SALES.CS_NET_PROFIT)), SUM(CATALOG_SALES.CS_NET_PROFIT), COUNT(*)]
  expr: groupKeys: [CATALOG_SALES.CS_BILL_CUSTOMER_SK, CATALOG_SALES.CS_ITEM_SK, STORE_SALES.SS_TICKET_NUMBER, STORE_SALES.SS_STORE_SK, STORE_SALES.SS_ITEM_SK, CATALOG_SALES.CS_SOLD_DATE_SK]

[21] InnerJoin parents=[20]
  expr: joinKey: (STORE_SALES.SS_CUSTOMER_SK = CATALOG_SALES.CS_BILL_CUSTOMER_SK) AND (STORE_SALES.SS_ITEM_SK = CATALOG_SALES.CS_ITEM_SK)

[22] InnerJoin parents=[21]
  expr: joinKey: (D1.D_DATE_SK = STORE_SALES.SS_SOLD_DATE_SK)

[23] Filter parents=[22]
  expr: (D1.D_MOY = 2) AND (D1.D_YEAR = 2000)

[24] TableScan parents=[23]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM
  expr: D_DATE_SK
  expr: D_YEAR
  expr: D_MOY
  io: partitions=1/1, bytes=2.0MB

[25] Filter parents=[22]
  expr: (STORE_SALES.SS_STORE_SK IS NOT NULL) AND (STORE_SALES.SS_SOLD_DATE_SK IS NOT NULL) AND (STORE_SALES.SS_CUSTOMER_SK IS NOT NULL)

[26] JoinFilter parents=[25]
  expr: joinKey: (STORE.S_STORE_SK = STORE_SALES.SS_STORE_SK)

[27] TableScan parents=[26]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.STORE_SALES
  expr: SS_SOLD_DATE_SK
  expr: SS_ITEM_SK
  expr: SS_CUSTOMER_SK
  expr: SS_STORE_SK
  expr: SS_TICKET_NUMBER
  expr: SS_NET_PROFIT
  io: partitions=70412/72718, bytes=1.1TB

[28] Filter parents=[21]
  expr: (CATALOG_SALES.CS_BILL_CUSTOMER_SK IS NOT NULL) AND (CATALOG_SALES.CS_SOLD_DATE_SK IS NOT NULL)

[29] JoinFilter parents=[28]
  expr: joinKey: (ITEM.I_ITEM_SK = STORE_SALES.SS_ITEM_SK)

[30] TableScan parents=[29]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.CATALOG_SALES
  expr: CS_SOLD_DATE_SK
  expr: CS_BILL_CUSTOMER_SK
  expr: CS_ITEM_SK
  expr: CS_NET_PROFIT
  io: partitions=54721/54922, bytes=857.0GB

[31] Filter parents=[8]
  expr: (STORE_RETURNS.SR_CUSTOMER_SK IS NOT NULL) AND (STORE_RETURNS.SR_RETURNED_DATE_SK IS NOT NULL)

[32] JoinFilter parents=[31]
  expr: joinKey: (D2.D_DATE_SK = STORE_RETURNS.SR_RETURNED_DATE_SK)

[33] TableScan parents=[32]
  objects: SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.STORE_RETURNS
  expr: SR_RETURNED_DATE_SK
  expr: SR_ITEM_SK
  expr: SR_CUSTOMER_SK
  expr: SR_TICKET_NUMBER
  expr: SR_NET_LOSS
  io: partitions=7070/7070, bytes=116.2GB
```

## Current TREE Node Map
```
## Base Tree Spec
Use this as the authoritative node tree for rewrite proposals.

node: final_select
  parent_node_id: None
  sources: []
  outputs: ['i_item_id', 'i_item_desc', 's_store_id', 's_store_name', 'store_sales_profit', 'store_returns_loss', 'catalog_sales_profit']
  sql: OMITTED

root_node_id: final_select
```

## Transform Catalog (full list; not pre-filtered)

- runtime_dialect: `snowflake`
- selection_policy: prioritize native/universal transforms first.
- portability_policy: non-native transforms may be used as exploration probes when runtime syntax/semantics remain valid and engine knowledge does not contraindicate.

- `date_cte_isolate` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Dimension Isolation: extract small dimension lookups into CTEs so they materialize once and subsequent joins probe a tiny hash table instead of rescanning.
- `dimension_cte_isolate` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Early Selection: pre-filter dimension tables into CTEs returning only surrogate keys before joining with fact tables. Each dimension CTE is tiny, creating small hash tables that speed up the fact table probe.
- `early_filter` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Early Selection: filter small dimension tables first, then join to large fact tables. This reduces the fact table scan to only rows matching the filter, rather than scanning all rows and filtering after the join.
- `multi_date_range_cte` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Early Selection per Alias: when a query joins the same dimension table multiple times with different filters (d1, d2, d3), create separate CTEs for each filter and pre-join with fact tables to reduce rows entering the main join.
- `multi_dimension_prefetch` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Multi-Dimension Prefetch: when multiple dimension tables have selective filters, pre-filter ALL of them into CTEs before the fact table join. Combined selectivity compounds — each dimension CTE reduces the fact scan further.
- `prefetch_fact_join` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Staged Join Pipeline: build a CTE chain that progressively reduces data — first CTE filters the dimension, second CTE pre-joins filtered dimension keys with the fact table, subsequent CTEs join remaining dimensions against the already-reduced fact set.
- `sf_sk_pushdown_multi_fact` (Family A, gap `PREDICATE_TRANSITIVITY_FAILURE`, support `native_or_universal`, engines `snowflake`): Add date_sk BETWEEN to each fact table when joined to date_dim via comma join
- `sf_sk_pushdown_union_all` (Family A, gap `PREDICATE_TRANSITIVITY_FAILURE`, support `native_or_universal`, engines `snowflake`): Push date_sk BETWEEN into UNION ALL branches for micro-partition pruning
- `shared_dimension_multi_channel` (Family A, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Shared Dimension Extraction: when multiple channel CTEs (store/catalog/web) apply identical dimension filters, extract those shared filters into one CTE and reference it from each channel. Avoids redundant dimension scans.
- `composite_decorrelate_union` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `portability_candidate`, engines `duckdb`): Composite Decorrelation: when multiple correlated EXISTS share common filters, extract shared dimensions into a single CTE and decorrelate the EXISTS checks into pre-materialized key sets joined via UNION.
- `decorrelate` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `portability_candidate`, engines `duckdb`): Decorrelation: convert correlated subqueries to standalone CTEs with GROUP BY, then JOIN. Correlated subqueries re-execute per outer row; a pre-computed CTE executes once.
- `early_filter_decorrelate` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `portability_candidate`, engines `postgresql`): Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.
- `inline_decorrelate_materialized` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `portability_candidate`, engines `postgresql`): Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs — (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data — then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.
- `sf_inline_decorrelate` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `native_or_universal`, engines `snowflake`): Decompose correlated scalar subquery with aggregation into 3 CTEs: shared scan, per-key threshold, filtered main query
- `sf_shared_scan_decorrelate` (Family B, gap `CORRELATED_SUBQUERY_PARALYSIS`, support `native_or_universal`, engines `snowflake`): Shared-scan variant: inner and outer scan same fact table with same filters, decompose into shared CTE + threshold CTE
- `aggregate_pushdown` (Family C, gap `AGGREGATE_BELOW_JOIN_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Push aggregation below joins: when a GROUP BY + aggregate operates on a single fact table joined with dimensions, pre-aggregate the fact table on the join key first, THEN join with dimensions. Reduces rows entering the join from millions to thousands.
- `channel_bitmap_aggregation` (Family C, gap `REDUNDANT_SCAN_ELIMINATION`, support `portability_candidate`, engines `duckdb`): Consolidate repeated scans of the same fact table (one per time/channel bucket) into a single scan with CASE WHEN labels and conditional aggregation
- `deferred_window_aggregation` (Family C, gap `None`, support `portability_candidate`, engines `duckdb`): Deferred Aggregation: delay expensive operations (window functions) until after joins reduce the dataset. Computing window functions inside individual CTEs then joining is more expensive than joining first and computing windows once on the combined result.
- `early_filter` (Family C, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): Scan Consolidation: when multiple subqueries scan the same table with similar patterns, consolidate them into CTEs that compute all needed aggregates in fewer passes. Reduces N scans to fewer scans.
- `single_pass_aggregation` (Family C, gap `REDUNDANT_SCAN_ELIMINATION`, support `portability_candidate`, engines `duckdb`): Single-Pass Aggregation: consolidate multiple scalar subqueries on the same table into one CTE using CASE expressions inside aggregate functions. Reduces N separate table scans to 1 pass.
- `intersect_to_exists` (Family D, gap `None`, support `portability_candidate`, engines `duckdb`): Semi-Join Short-Circuit: replace INTERSECT with EXISTS to avoid full materialization and sorting. INTERSECT must compute complete result sets before intersecting; EXISTS stops at the first match per row, enabling semi-join optimizations.
- `multi_intersect_exists_cte` (Family D, gap `None`, support `portability_candidate`, engines `duckdb`): Convert cascading INTERSECT operations into correlated EXISTS subqueries with pre-materialized date and channel CTEs
- `or_to_union` (Family D, gap `CROSS_COLUMN_OR_DECOMPOSITION`, support `portability_candidate`, engines `duckdb`): OR-to-UNION Decomposition: split OR conditions on different columns into separate UNION ALL branches, each with a focused predicate. The optimizer can use different access paths per branch instead of a single scan with a complex filter.
- `rollup_to_union_windowing` (Family D, gap `UNION_CTE_SELF_JOIN_DECOMPOSITION`, support `portability_candidate`, engines `duckdb`): Replace GROUP BY ROLLUP with explicit UNION ALL of pre-aggregated CTEs at each hierarchy level, combined with window functions for ranking
- `union_cte_split` (Family D, gap `UNION_CTE_SELF_JOIN_DECOMPOSITION`, support `portability_candidate`, engines `duckdb`): CTE Specialization: when a generic CTE is scanned multiple times with different filters (e.g., by year), split it into specialized CTEs that embed the filter in their definition. Each specialized CTE processes only its relevant subset, eliminating redundant scans.
- `materialize_cte` (Family E, gap `None`, support `portability_candidate`, engines `duckdb`): Shared Materialization: extract repeated subquery patterns into CTEs to avoid recomputation. When the same logical check appears multiple times, compute it once and reference the result.
- `pg_self_join_decomposition` (Family E, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `postgresql`): Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.
- `date_cte_explicit_join` (Family F, gap `COMMA_JOIN_WEAKNESS`, support `portability_candidate`, engines `postgresql`): Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.
- `dimension_prefetch_star` (Family F, gap `COMMA_JOIN_WEAKNESS`, support `portability_candidate`, engines `postgresql`): Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.
- `inner_join_conversion` (Family F, gap `LEFT_JOIN_FILTER_ORDER_RIGIDITY`, support `portability_candidate`, engines `duckdb`): When a LEFT JOIN is immediately followed by a WHERE filter on the right table that eliminates NULL rows, convert to INNER JOIN + early filter CTE. The WHERE clause already makes the LEFT JOIN behave as an INNER JOIN, but the optimizer keeps the LEFT JOIN semantics (preserving all left rows), wasting work on rows that are filtered out.
- `materialized_dimension_fact_prefilter` (Family F, gap `NON_EQUI_JOIN_INPUT_BLINDNESS`, support `portability_candidate`, engines `postgresql`): Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.
- `self_join_decomposition` (Family F, gap `CROSS_CTE_PREDICATE_BLINDNESS`, support `portability_candidate`, engines `duckdb`): When a CTE is self-joined with different filter values (e.g., inv1.d_moy=1 AND inv2.d_moy=2), split into separate CTEs each embedding their filter. The optimizer cannot push the outer WHERE filter into the CTE's GROUP BY, causing full materialization and post-filtering.


## Estimation Errors (Q-Error)
### §2b-i. Cardinality Estimation Routing (Q-Error)

Pathology routing: P1
(Locus+Direction routing is 85% accurate at predicting where the winning transform operates)

Structural signals:
  - ESTIMATE_ONLY: Snowflake EXPLAIN is estimate-only here (no per-node actual rows) — use structural routing + query-map row flow
  - REPEATED_TABLE: same table scanned multiple times → single-pass opportunity (P1)

IMPORTANT: Cross-check structural signals against the PRUNING GUIDE in §III. If the EXPLAIN shows no nested loops, skip P2. If each table appears once, skip P1. The pruning guide overrides routing suggestions.


## Gold TREE Pattern Cards
Use these as pattern priors; adapt shape, not literal table names.
- family A: `sf_sk_pushdown_union_all` (2.13x), changed_nodes=`final_select`
- family B: `sf_inline_decorrelate` (23.17x), changed_nodes=`final_select`
- family C: `aggregate_pushdown` (42.90x), changed_nodes=`final_select`
- family D: `intersect_to_exists` (1.83x), changed_nodes=`final_select`
- family E: `multi_dimension_prefetch` (2.71x), changed_nodes=`final_select`
- family F: `inner_join_conversion` (3.44x), changed_nodes=`final_select`

## Engine-Specific Knowledge
## Dialect Intelligence (SNOWFLAKE)

# Snowflake Dialect Knowledge

## Metadata
- dialect: `snowflake`
- version: `2026-02-17-format-v1`
- source_of_truth:
  - engine_profile: `constraints/engine_profile_snowflake.json`
  - transforms: `knowledge/transforms.json`
  - examples: `examples/snowflake/*.json`
- generated_from: `hybrid`
- last_updated: `2026-02-17`

## Engine Strengths (Do Not Fight)
| Strength ID | Summary | Implication | Evidence |
|---|---|---|---|
| `MICRO_PARTITION_PRUNING` | Clustered filter predicates prune partitions early. | Avoid wrapping filter columns in functions when pruning is critical. | `engine_profile_snowflake.json` |
| `COLUMN_PRUNING` | Only referenced columns are read through query graph. | Keep projections narrow; avoid unnecessary wide intermediate selects. | `engine_profile_snowflake.json` |
| `PREDICATE_PUSHDOWN` | Filters push into storage and single-ref CTE paths. | Do not duplicate already-effective filters blindly. | `engine_profile_snowflake.json` |
| `CORRELATED_DECORRELATION` | Simple EXISTS/IN correlation often decorrelates to joins. | Reserve manual decorrelation for scalar aggregate correlation cases. | `engine_profile_snowflake.json` |
| `SEMI_JOIN` | EXISTS patterns get early-stop semi-join behavior. | Protect EXISTS from materialization rewrites. | `engine_profile_snowflake.json` |
| `JOIN_FILTER` | Join-filter pushdown commonly appears on star-schema joins. | Avoid plan-shape rewrites that remove effective join filters without reason. | `engine_profile_snowflake.json`, `benchmarks/snowflake_tpcds/explains/*.json` |
| `COST_BASED_JOIN_ORDER` | Join ordering is generally cost-driven and robust. | Prefer cardinality reduction over forced join-order plans. | `engine_profile_snowflake.json` |
| `QUALIFY_OPTIMIZATION` | QUALIFY is native and efficient for window filtering. | Prefer QUALIFY-form filter placement where semantics permit. | `engine_profile_snowflake.json` |

## Global Guards
| Guard ID | Rule | Severity | Fail Action | Source |
|---|---|---|---|---|
| `G_SF_EXISTS_PROTECTED` | Never materialize `EXISTS/NOT EXISTS` into broad CTE branches. | `BLOCKER` | `SKIP_TRANSFORM` | `SEMI_JOIN` strength |
| `G_SF_FILTER_FUNCTION_WRAP` | Do not wrap partition/filter keys in functions when pruning matters. | `HIGH` | `SKIP_TRANSFORM` | `MICRO_PARTITION_PRUNING` strength |
| `G_SF_JOINFILTER_PRESERVE` | Avoid destructive shape rewrites when join-filter behavior is already strong. | `MEDIUM` | `REQUIRE_MANUAL_REVIEW` | `JOIN_FILTER` strength |
| `G_SF_UNION_BRANCH_LIMIT` | Keep UNION ALL branch count modest for branch-level scan costs. | `MEDIUM` | `DOWNRANK_TO_EXPLORATION` | legacy playbook |
| `G_SF_CTE_REUSE_RULE` | Single-ref CTEs tend to inline; multi-ref CTEs need explicit reason. | `MEDIUM` | `DOWNRANK_TO_EXPLORATION` | legacy playbook |
| `G_SF_NOTIN_NULL_SAFETY` | Use NULL-safe anti-join semantics (prefer NOT EXISTS to unsafe NOT IN patterns). | `HIGH` | `REQUIRE_MANUAL_REVIEW` | legacy playbook |
| `G_SF_LOW_BASELINE_SKIP_HEAVY` | If baseline is low (`<100ms`), skip structural rewrite churn. | `MEDIUM` | `DOWNRANK_TO_EXPLORATION` | legacy playbook |

## Decision Gates (Normative Contract)
| Gate ID | Scope | Type | Severity | Check | Pass Criteria | Fail Action | Evidence Required |
|---|---|---|---|---|---|---|---|
| `DG_TYPE_ENUM` | global | `SEMANTIC_RISK` | `BLOCKER` | Gate type validity | One of `SQL_PATTERN`, `PLAN_SIGNAL`, `RUNTIME_CONTEXT`, `SEMANTIC_RISK` | `REQUIRE_MANUAL_REVIEW` | gate row schema |
| `DG_SEVERITY_ENUM` | global | `SEMANTIC_RISK` | `BLOCKER` | Severity validity | One of `BLOCKER`, `HIGH`, `MEDIUM` | `REQUIRE_MANUAL_REVIEW` | gate row schema |
| `DG_FAIL_ACTION_ENUM` | global | `SEMANTIC_RISK` | `BLOCKER` | Fail action validity | One of `SKIP_PATHOLOGY`, `SKIP_TRANSFORM`, `DOWNRANK_TO_EXPLORATION`, `REQUIRE_MANUAL_REVIEW` | `REQUIRE_MANUAL_REVIEW` | gate row schema |
| `DG_BLOCKER_POLICY` | global | `RUNTIME_CONTEXT` | `BLOCKER` | Any blocker failed | Failed blocker always blocks that pattern/transform path | `SKIP_PATHOLOGY` | failed gate log |
| `DG_MIN_PATTERN_GATES` | pattern | `RUNTIME_CONTEXT` | `HIGH` | Gate coverage | Each pattern has at least 1 `SEMANTIC_RISK`, 1 `PLAN_SIGNAL`, 1 `RUNTIME_CONTEXT` gate | `REQUIRE_MANUAL_REVIEW` | pattern gate table |
| `DG_EVIDENCE_BINDING` | global | `RUNTIME_CONTEXT` | `HIGH` | Claim traceability | Quantitative claims map to example IDs or benchmark artifacts | `REQUIRE_MANUAL_REVIEW` | evidence table row |

## Gap-Driven Optimization Patterns

### Pattern ID: `CORRELATED_SUBQUERY_PARALYSIS` (`HIGH`)
- Goal: `DECORRELATE`
- Detect: correlated scalar aggregate subquery re-scans fact table per outer row.
- Preferred transforms: `sf_inline_decorrelate`, `sf_shared_scan_decorrelate`.

#### Decision Gates for `CORRELATED_SUBQUERY_PARALYSIS`
| Gate ID | Type | Severity | Check | Pass Criteria | Fail Action | Evidence |
|---|---|---|---|---|---|---|
| `G_SF_CORR_SCALAR_REQUIRED` | `SQL_PATTERN` | `BLOCKER` | Correlated scalar aggregate exists | AVG/SUM/COUNT scalar correlation present | `SKIP_PATHOLOGY` | SQL + parse |
| `G_SF_CORR_SIMPLE_EXISTS_SKIP` | `PLAN_SIGNAL` | `HIGH` | Already simple decorrelation class | Skip manual rewrite when simple EXISTS/IN already optimized | `SKIP_TRANSFORM` | EXPLAIN shape |
| `G_SF_CORR_FACT_CONTEXT` | `RUNTIME_CONTEXT` | `MEDIUM` | Fact-table involvement | Inner query actually touches fact-table path | `DOWNRANK_TO_EXPLORATION` | SQL relation map |
| `G_SF_CORR_SEMANTIC_KEYS` | `SEMANTIC_RISK` | `HIGH` | Correlation key and aggregate semantics preserved | Correlation predicates and aggregate semantics unchanged | `REQUIRE_MANUAL_REVIEW` | rewrite diff |

#### Evidence Table
| Example ID | Query | Warehouse | Validation | Orig ms | Opt ms | Speedup | Outcome |
|---|---|---|---|---:|---:|---:|---|
| `sf_inline_decorrelate` | `n/a` | `MEDIUM` | `3x3 (discard warmup, average last 2)` | `69414.7` | `2995.5` | `23.17x` | `WIN` |
| `sf_shared_scan_decorrelate` | `n/a` | `MEDIUM` | `3x3 (discard warmup, average last 2)` | `8024.6` | `1026.1` | `7.82x` | `WIN` |

#### Failure Modes
| Pattern | Impact | Triggered Gate | Mitigation |
|---|---|---|---|
| none observed in curated examples | `n/a` | `n/a` | keep blocker gates enforced |

### Pattern ID: `PREDICATE_TRANSITIVITY_FAILURE` (`n/a in engine_profile`)
- Goal: `SK_PUSHDOWN`
- Detect: date_dim filter exists but sold_date_sk range is not pushed into fact scans, often across UNION ALL or multi-fact comma-join shapes.
- Preferred transforms: `sf_sk_pushdown_union_all`, `sf_sk_pushdown_multi_fact`.

#### Decision Gates for `PREDICATE_TRANSITIVITY_FAILURE`
| Gate ID | Type | Severity | Check | Pass Criteria | Fail Action | Evidence |
|---|---|---|---|---|---|---|
| `G_SF_SK_DATE_FILTER_REQUIRED` | `SQL_PATTERN` | `BLOCKER` | Date filter on date_dim exists | Date filter plus sold_date_sk join path present | `SKIP_PATHOLOGY` | SQL parse |
| `G_SF_SK_SCAN_PRESSURE` | `PLAN_SIGNAL` | `HIGH` | Fact scan pressure | Fact scan appears broad enough to justify pushdown | `DOWNRANK_TO_EXPLORATION` | EXPLAIN table scan stats |
| `G_SF_SK_COMPUTE_BOUND_SKIP` | `RUNTIME_CONTEXT` | `HIGH` | Compute-bound workload | Skip when dominant cost is compute-heavy aggregate/rollup path | `SKIP_TRANSFORM` | operator profile |
| `G_SF_SK_RANGE_SEMANTICS` | `SEMANTIC_RISK` | `HIGH` | Date key range correctness | Date_sk range derived from same predicate domain as original query | `REQUIRE_MANUAL_REVIEW` | range derivation audit |

#### Evidence Table
| Example ID | Query | Warehouse | Validation | Orig ms | Opt ms | Speedup | Outcome |
|---|---|---|---|---:|---:|---:|---|
| `sf_sk_pushdown_union_all` | `Q2` | `X-Small` | `5x trimmed mean (discard min/max, average middle 3)` | `229847.3` | `107982.0` | `2.13x` | `WIN` |
| `sf_sk_pushdown_3fact` | `Q56` | `X-Small` | `5x trimmed mean (discard min/max, average middle 3)` | `10233.6` | `8729.9` | `1.17x` | `WIN` |

#### Failure Modes
| Pattern | Impact | Triggered Gate | Mitigation |
|---|---|---|---|
| Wide-range pushdown gave neutral result | `0.97x` (legacy note) | `G_SF_SK_SCAN_PRESSURE` | require strong scan-pressure evidence |
| Compute-bound rollup path timed out | timeout (legacy note) | `G_SF_SK_COMPUTE_BOUND_SKIP` | skip pushdown-only strategy on compute-bound plans |

## Pruning Guide
| Plan shows | Skip |
|---|---|
| No correlated scalar aggregate pattern | `CORRELATED_SUBQUERY_PARALYSIS` |
| Correlation is simple EXISTS/IN already optimized | `CORRELATED_SUBQUERY_PARALYSIS` |
| No date_dim filter or no sold_date_sk join linkage | `PREDICATE_TRANSITIVITY_FAILURE` |
| Low scan pressure on fact tables | `PREDICATE_TRANSITIVITY_FAILURE` |
| Dominant compute-bound aggregate/rollup path | `PREDICATE_TRANSITIVITY_FAILURE` |
| Baseline < 100ms | most structural rewrite paths |

## Regression Registry
| Severity | Transform | Speedup | Query | Root Cause |
|---|---|---:|---|---|
| `INFO` | `sf_sk_pushdown_union_all` | `0.97x` | `Q17` | wide date range reduced pruning benefit (legacy playbook note) |
| `INFO` | `sf_sk_pushdown_union_all` | `timeout` | `Q67` | compute-bound rollup path, not scan-bound (legacy playbook note) |

## Notes
- `PREDICATE_TRANSITIVITY_FAILURE` is represented in transforms and examples, but is not yet listed in `engine_profile_snowflake.json` gaps.
- Consider promoting this pattern into the Snowflake engine profile to keep profile and playbook fully aligned.

## Additional Intelligence
### AST Feature Detection

- **dimension_cte_isolate**: 100% match (DATE_DIM, GROUP_BY, MULTI_TABLE_5+) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: CROSS_JOIN_3_DIMS, UNFILTERED_CTE] [SUPPORT: portability_candidate; engines=duckdb]
- **sf_sk_pushdown_multi_fact**: 100% match (DATE_DIM, MULTI_TABLE_5+) (gap: PREDICATE_TRANSITIVITY_FAILURE)  [SUPPORT: native_or_universal]
- **multi_date_range_cte**: 83% match (BETWEEN, DATE_DIM, GROUP_BY, MULTI_TABLE_5+) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [SUPPORT: portability_candidate; engines=duckdb]
  Missing: AGG_AVG
- **prefetch_fact_join**: 75% match (DATE_DIM, GROUP_BY, STAR_JOIN) (gap: CROSS_CTE_PREDICATE_BLINDNESS) [CAUTION: MAX_2_CHAINS] [SUPPORT: portability_candidate; engines=duckdb]
  Missing: AGG_SUM
- **sf_sk_pushdown_union_all**: 67% match (DATE_DIM, MULTI_CHANNEL) (gap: PREDICATE_TRANSITIVITY_FAILURE)  [SUPPORT: native_or_universal]
  Missing: UNION
