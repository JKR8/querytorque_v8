You are a senior query optimization architect. Your job is to deeply analyze a SQL query and produce a structured briefing for 4 specialist workers who will each write a different optimized version.

You are the ONLY call that sees all the data: EXPLAIN plans, logical-tree costs, full constraint list, global knowledge, and the complete example catalog. The workers will only see what YOU put in their briefings. Your output quality directly determines their success.

## Query: query_78
## Dialect: snowflake

```sql
 1 | with ws as
 2 |   (select d_year AS ws_sold_year, ws_item_sk,
 3 |     ws_bill_customer_sk ws_customer_sk,
 4 |     sum(ws_quantity) ws_qty,
 5 |     sum(ws_wholesale_cost) ws_wc,
 6 |     sum(ws_sales_price) ws_sp
 7 |    from web_sales
 8 |    left join web_returns on wr_order_number=ws_order_number and ws_item_sk=wr_item_sk
 9 |    join date_dim on ws_sold_date_sk = d_date_sk
10 |    where wr_order_number is null
11 |    group by d_year, ws_item_sk, ws_bill_customer_sk
12 |    ),
13 | cs as
14 |   (select d_year AS cs_sold_year, cs_item_sk,
15 |     cs_bill_customer_sk cs_customer_sk,
16 |     sum(cs_quantity) cs_qty,
17 |     sum(cs_wholesale_cost) cs_wc,
18 |     sum(cs_sales_price) cs_sp
19 |    from catalog_sales
20 |    left join catalog_returns on cr_order_number=cs_order_number and cs_item_sk=cr_item_sk
21 |    join date_dim on cs_sold_date_sk = d_date_sk
22 |    where cr_order_number is null
23 |    group by d_year, cs_item_sk, cs_bill_customer_sk
24 |    ),
25 | ss as
26 |   (select d_year AS ss_sold_year, ss_item_sk,
27 |     ss_customer_sk,
28 |     sum(ss_quantity) ss_qty,
29 |     sum(ss_wholesale_cost) ss_wc,
30 |     sum(ss_sales_price) ss_sp
31 |    from store_sales
32 |    left join store_returns on sr_ticket_number=ss_ticket_number and ss_item_sk=sr_item_sk
33 |    join date_dim on ss_sold_date_sk = d_date_sk
34 |    where sr_ticket_number is null
35 |    group by d_year, ss_item_sk, ss_customer_sk
36 |    )
37 |  select
38 | ss_item_sk,
39 | round(ss_qty/(coalesce(ws_qty,0)+coalesce(cs_qty,0)),2) ratio,
40 | ss_qty store_qty, ss_wc store_wholesale_cost, ss_sp store_sales_price,
41 | coalesce(ws_qty,0)+coalesce(cs_qty,0) other_chan_qty,
42 | coalesce(ws_wc,0)+coalesce(cs_wc,0) other_chan_wholesale_cost,
43 | coalesce(ws_sp,0)+coalesce(cs_sp,0) other_chan_sales_price
44 | from ss
45 | left join ws on (ws_sold_year=ss_sold_year and ws_item_sk=ss_item_sk and ws_customer_sk=ss_customer_sk)
46 | left join cs on (cs_sold_year=ss_sold_year and cs_item_sk=ss_item_sk and cs_customer_sk=ss_customer_sk)
47 | where (coalesce(ws_qty,0)>0 or coalesce(cs_qty, 0)>0) and ss_sold_year=2000
48 | order by 
49 |   ss_item_sk,
50 |   ss_qty desc, ss_wc desc, ss_sp desc,
51 |   other_chan_qty,
52 |   other_chan_wholesale_cost,
53 |   other_chan_sales_price,
54 |   ratio
55 |  LIMIT 100;
```

## EXPLAIN ANALYZE Plan

```
GlobalStats | 169340 | 166828 | 2840220995584
1 | 0 | Result | STORE_SALES.SS_ITEM_SK, ROUND(SCALED_ROUND_INT_DIVIDE(SUM(SUM(SUM(STORE_SALES.SS_QUANTITY))), (IFNULL(SYS_VW.WS_QTY_2, 0)) + (IFNULL(SYS_VW.CS_QTY_2, 0))), 2), SUM(SUM(SUM(STORE_SALES.SS_QUANTITY))), SUM(SUM(SUM(STORE_SALES.SS_WHOLESALE_COST))), SUM(SUM(SUM(STORE_SALES.SS_SALES_PRICE))), (IFNULL(SYS_VW.WS_QTY_2, 0)) + (IFNULL(SYS_VW.CS_QTY_2, 0)), (IFNULL(SYS_VW.WS_WC_5, 0)) + (IFNULL(SYS_VW.CS_WC_5, 0)), (IFNULL(SYS_VW.WS_SP_4, 0)) + (IFNULL(SYS_VW.CS_SP_4, 0))
1 | 1 | [0] | SortWithLimit | sortKey: [SS.SS_ITEM_SK ASC NULLS LAST, SS.SS_QTY DESC NULLS FIRST, SS.SS_WC DESC NULLS FIRST, SS.SS_SP DESC NULLS FIRST, (IFNULL(SYS_VW.WS_QTY_2, 0)) + (IFNULL(SYS_VW.CS_QTY_2, 0)) ASC NULLS LAST, (IFNULL(SYS_VW.WS_WC_5, 0)) + (IFNULL(SYS_VW.CS_WC_5, 0)) ASC NULLS LAST, (IFNULL(SYS_VW.WS_SP_4, 0)) + (IFNULL(SYS_VW.CS_SP_4, 0)) ASC NULLS LAST, ROUND(SCALED_ROUND_INT_DIVIDE(SS.SS_QTY, (IFNULL(SYS_VW.WS_QTY_2, 0)) + (IFNULL(SYS_VW.CS_QTY_2, 0))), 2) ASC NULLS LAST], rowCount: 100
1 | 2 | [1] | Filter | ((IFNULL(SYS_VW.WS_QTY_2, 0)) > 0) OR ((IFNULL(SYS_VW.CS_QTY_2, 0)) > 0)
1 | 3 | [2] | LeftOuterJoin | joinKey: (SYS_VW.CS_CUSTOMER_SK_0 = STORE_SALES.SS_CUSTOMER_SK) AND (SYS_VW.CS_ITEM_SK_1 = STORE_SALES.SS_ITEM_SK)
1 | 4 | [3] | Aggregate | aggExprs: [SUM(SUM(SUM(CATALOG_SALES.CS_QUANTITY))), SUM(SUM(SUM(CATALOG_SALES.CS_WHOLESALE_COST))), SUM(SUM(SUM(CATALOG_SALES.CS_SALES_PRICE)))], groupKeys: [DATE_DIM.D_YEAR, CATALOG_SALES.CS_ITEM_SK, CATALOG_SALES.CS_BILL_CUSTOMER_SK]
1 | 5 | [4] | Aggregate | aggExprs: [SUM(SUM(CATALOG_SALES.CS_QUANTITY)), SUM(SUM(CATALOG_SALES.CS_WHOLESALE_COST)), SUM(SUM(CATALOG_SALES.CS_SALES_PRICE))], groupKeys: [DATE_DIM.D_YEAR, CATALOG_SALES.CS_ITEM_SK, CATALOG_SALES.CS_BILL_CUSTOMER_SK]
1 | 6 | [5] | InnerJoin | joinKey: (DATE_DIM.D_DATE_SK = CATALOG_SALES.CS_SOLD_DATE_SK)
1 | 7 | [6] | Filter | ((DATE_DIM.D_YEAR) = 2000) AND (DATE_DIM.D_YEAR IS NOT NULL)
1 | 8 | [7] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM | D_DATE_SK, D_YEAR | 1 | 1 | 2138624
1 | 9 | [6] | Aggregate | aggExprs: [SUM(CATALOG_SALES.CS_QUANTITY), SUM(CATALOG_SALES.CS_WHOLESALE_COST), SUM(CATALOG_SALES.CS_SALES_PRICE)], groupKeys: [CATALOG_SALES.CS_ITEM_SK, CATALOG_SALES.CS_BILL_CUSTOMER_SK, CATALOG_SALES.CS_SOLD_DATE_SK]
1 | 10 | [9] | AntiJoin | joinKey: (CATALOG_RETURNS.CR_ITEM_SK = CATALOG_SALES.CS_ITEM_SK) AND (CATALOG_RETURNS.CR_ORDER_NUMBER = CATALOG_SALES.CS_ORDER_NUMBER)
1 | 11 | [10] | Aggregate | groupKeys: [CATALOG_RETURNS.CR_ITEM_SK, CATALOG_RETURNS.CR_ORDER_NUMBER]
1 | 12 | [11] | Filter | null IS NULL
1 | 13 | [12] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.CATALOG_RETURNS | CR_ITEM_SK, CR_ORDER_NUMBER | 4759 | 4759 | 82532796416
1 | 14 | [10] | Filter | (CATALOG_SALES.CS_BILL_CUSTOMER_SK IS NOT NULL) AND (CATALOG_SALES.CS_ITEM_SK IS NOT NULL) AND (null IS NULL) AND (CATALOG_SALES.CS_SOLD_DATE_SK IS NOT NULL)
1 | 15 | [14] | JoinFilter | joinKey: (DATE_DIM.D_DATE_SK = CATALOG_SALES.CS_SOLD_DATE_SK)
1 | 16 | [15] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.CATALOG_SALES | CS_SOLD_DATE_SK, CS_BILL_CUSTOMER_SK, CS_ITEM_SK, CS_ORDER_NUMBER, CS_QUANTITY, CS_WHOLESALE_COST, CS_SALES_PRICE | 54922 | 54721 | 920184101376
1 | 17 | [3] | LeftOuterJoin | joinKey: (SYS_VW.WS_CUSTOMER_SK_0 = STORE_SALES.SS_CUSTOMER_SK) AND (SYS_VW.WS_ITEM_SK_1 = STORE_SALES.SS_ITEM_SK)
1 | 18 | [17] | Aggregate | aggExprs: [SUM(SUM(SUM(WEB_SALES.WS_QUANTITY))), SUM(SUM(SUM(WEB_SALES.WS_WHOLESALE_COST))), SUM(SUM(SUM(WEB_SALES.WS_SALES_PRICE)))], groupKeys: [DATE_DIM.D_YEAR, WEB_SALES.WS_ITEM_SK, WEB_SALES.WS_BILL_CUSTOMER_SK]
1 | 19 | [18] | Aggregate | aggExprs: [SUM(SUM(WEB_SALES.WS_QUANTITY)), SUM(SUM(WEB_SALES.WS_WHOLESALE_COST)), SUM(SUM(WEB_SALES.WS_SALES_PRICE))], groupKeys: [DATE_DIM.D_YEAR, WEB_SALES.WS_ITEM_SK, WEB_SALES.WS_BILL_CUSTOMER_SK]
1 | 20 | [19] | InnerJoin | joinKey: (DATE_DIM.D_DATE_SK = WEB_SALES.WS_SOLD_DATE_SK)
1 | 21 | [20] | Filter | ((DATE_DIM.D_YEAR) = 2000) AND (DATE_DIM.D_YEAR IS NOT NULL)
1 | 22 | [21] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM | D_DATE_SK, D_YEAR | 1 | 1 | 2138624
1 | 23 | [20] | Aggregate | aggExprs: [SUM(WEB_SALES.WS_QUANTITY), SUM(WEB_SALES.WS_WHOLESALE_COST), SUM(WEB_SALES.WS_SALES_PRICE)], groupKeys: [WEB_SALES.WS_ITEM_SK, WEB_SALES.WS_BILL_CUSTOMER_SK, WEB_SALES.WS_SOLD_DATE_SK]
1 | 24 | [23] | AntiJoin | joinKey: (WEB_RETURNS.WR_ITEM_SK = WEB_SALES.WS_ITEM_SK) AND (WEB_RETURNS.WR_ORDER_NUMBER = WEB_SALES.WS_ORDER_NUMBER)
1 | 25 | [24] | Aggregate | groupKeys: [WEB_RETURNS.WR_ITEM_SK, WEB_RETURNS.WR_ORDER_NUMBER]
1 | 26 | [25] | Filter | null IS NULL
1 | 27 | [26] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.WEB_RETURNS | WR_ITEM_SK, WR_ORDER_NUMBER | 2289 | 2289 | 39149218304
1 | 28 | [24] | Filter | (WEB_SALES.WS_BILL_CUSTOMER_SK IS NOT NULL) AND (WEB_SALES.WS_ITEM_SK IS NOT NULL) AND (null IS NULL) AND (WEB_SALES.WS_SOLD_DATE_SK IS NOT NULL)
1 | 29 | [28] | JoinFilter | joinKey: (DATE_DIM.D_DATE_SK = WEB_SALES.WS_SOLD_DATE_SK)
1 | 30 | [29] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.WEB_SALES | WS_SOLD_DATE_SK, WS_ITEM_SK, WS_BILL_CUSTOMER_SK, WS_ORDER_NUMBER, WS_QUANTITY, WS_WHOLESALE_COST, WS_SALES_PRICE | 27579 | 27574 | 460956759040
1 | 31 | [17] | Aggregate | aggExprs: [SUM(SUM(SUM(STORE_SALES.SS_QUANTITY))), SUM(SUM(SUM(STORE_SALES.SS_WHOLESALE_COST))), SUM(SUM(SUM(STORE_SALES.SS_SALES_PRICE)))], groupKeys: [DATE_DIM.D_YEAR, STORE_SALES.SS_ITEM_SK, STORE_SALES.SS_CUSTOMER_SK]
1 | 32 | [31] | Aggregate | aggExprs: [SUM(SUM(STORE_SALES.SS_QUANTITY)), SUM(SUM(STORE_SALES.SS_WHOLESALE_COST)), SUM(SUM(STORE_SALES.SS_SALES_PRICE))], groupKeys: [DATE_DIM.D_YEAR, STORE_SALES.SS_ITEM_SK, STORE_SALES.SS_CUSTOMER_SK]
1 | 33 | [32] | InnerJoin | joinKey: (DATE_DIM.D_DATE_SK = STORE_SALES.SS_SOLD_DATE_SK)
1 | 34 | [33] | Filter | DATE_DIM.D_YEAR = 2000
1 | 35 | [34] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM | D_DATE_SK, D_YEAR | 1 | 1 | 2138624
1 | 36 | [33] | Aggregate | aggExprs: [SUM(STORE_SALES.SS_QUANTITY), SUM(STORE_SALES.SS_WHOLESALE_COST), SUM(STORE_SALES.SS_SALES_PRICE)], groupKeys: [STORE_SALES.SS_ITEM_SK, STORE_SALES.SS_CUSTOMER_SK, STORE_SALES.SS_SOLD_DATE_SK]
1 | 37 | [36] | AntiJoin | joinKey: (STORE_RETURNS.SR_ITEM_SK = STORE_SALES.SS_ITEM_SK) AND (STORE_RETURNS.SR_TICKET_NUMBER = STORE_SALES.SS_TICKET_NUMBER)
1 | 38 | [37] | Aggregate | groupKeys: [STORE_RETURNS.SR_ITEM_SK, STORE_RETURNS.SR_TICKET_NUMBER]
1 | 39 | [38] | Filter | null IS NULL
1 | 40 | [39] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.STORE_RETURNS | SR_ITEM_SK, SR_TICKET_NUMBER | 7070 | 7070 | 124763446272
1 | 41 | [37] | Filter | (null IS NULL) AND (STORE_SALES.SS_SOLD_DATE_SK IS NOT NULL)
1 | 42 | [41] | JoinFilter | joinKey: (SYS_VW.WS_CUSTOMER_SK_0 = STORE_SALES.SS_CUSTOMER_SK) AND (SYS_VW.WS_ITEM_SK_1 = STORE_SALES.SS_ITEM_SK) OR (SYS_VW.CS_CUSTOMER_SK_0 = STORE_SALES.SS_CUSTOMER_SK) AND (SYS_VW.CS_ITEM_SK_1 = STORE_SALES.SS_ITEM_SK)
1 | 43 | [42] | TableScan | SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.STORE_SALES | SS_SOLD_DATE_SK, SS_ITEM_SK, SS_CUSTOMER_SK, SS_TICKET_NUMBER, SS_QUANTITY, SS_WHOLESALE_COST, SS_SALES_PRICE | 72718 | 70412 | 1212628258304
```

**NOTE:** The EXPLAIN plan shows the PHYSICAL execution structure, which may differ significantly from the logical tree below. The optimizer may have already split CTEs, reordered joins, or pushed predicates. When the EXPLAIN and the logical tree disagree, the EXPLAIN is ground truth for what the optimizer is already doing.

Use EXPLAIN ANALYZE timings as ground truth. logical-tree cost percentages are derived metrics that may not reflect actual execution time.

## Query Structure (Logic Tree)

```
QUERY: (single statement)
├── [CTE] cs  [=]  Cost: 25%  Rows: ~1K
│   ├── SCAN (catalog_sales, catalog_returns (join), date_dim (join))
│   ├── FILTER (cr_order_number IS NULL)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (cs_sold_year, cs_item_sk, cs_customer_sk, cs_qty, cs_wc, cs_sp)
├── [CTE] ss  [=]  Cost: 25%  Rows: ~1K
│   ├── SCAN (store_sales, store_returns (join), date_dim (join))
│   ├── FILTER (sr_ticket_number IS NULL)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (ss_sold_year, ss_item_sk, ss_customer_sk, ss_qty, ss_wc, ss_sp)
├── [CTE] ws  [=]  Cost: 25%  Rows: ~1K
│   ├── SCAN (web_sales, web_returns (join), date_dim (join))
│   ├── FILTER (wr_order_number IS NULL)
│   ├── AGG (GROUP BY)
│   └── OUTPUT (ws_sold_year, ws_item_sk, ws_customer_sk, ws_qty, ws_wc, ws_sp)
└── [MAIN] main_query  [=]  Cost: 25%  Rows: ~1K
    ├── SCAN (ss, ws (join), cs (join))
    ├── FILTER ((COALESCE(ws_qty, 0) > 0 OR COALESCE(cs_qty, 0) > 0))
    ├── FILTER (ss_sold_year = 2000)
    ├── AGG (GROUP BY)
    ├── SORT (ss_item_sk ASC, ss_qty DESC, ss_wc DESC, ss_sp DESC, other_chan_qty ASC, other_chan_wholesale_cost ASC, other_chan_sales_price ASC, ratio ASC)
    └── OUTPUT (ss_item_sk, ratio, store_qty, store_wholesale_cost, store_sales_price, other_chan_qty, other_chan_wholesale_cost, other_chan_sales_price)
```

## Node Details

### 1. ws
**Role**: CTE (Definition Order: 0)
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [ws_sold_year, ws_item_sk, ws_customer_sk, ws_qty, ws_wc, ws_sp]
**Dependencies**: web_sales, web_returns (join), date_dim (join)
**Filters**: wr_order_number IS NULL
**Operators**: HASH_GROUP_BY, SEQ_SCAN[web_sales], SEQ_SCAN[web_returns], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  d_year AS ws_sold_year,
  ws_item_sk,
  ws_bill_customer_sk AS ws_customer_sk,
  SUM(ws_quantity) AS ws_qty,
  SUM(ws_wholesale_cost) AS ws_wc,
  SUM(ws_sales_price) AS ws_sp
FROM web_sales
LEFT JOIN web_returns
  ON wr_order_number = ws_order_number AND ws_item_sk = wr_item_sk
JOIN date_dim
  ON ws_sold_date_sk = d_date_sk
WHERE
  wr_order_number IS NULL
GROUP BY
  d_year,
  ws_item_sk,
  ws_bill_customer_sk
```

### 2. cs
**Role**: CTE (Definition Order: 0)
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [cs_sold_year, cs_item_sk, cs_customer_sk, cs_qty, cs_wc, cs_sp]
**Dependencies**: catalog_sales, catalog_returns (join), date_dim (join)
**Filters**: cr_order_number IS NULL
**Operators**: HASH_GROUP_BY, SEQ_SCAN[catalog_sales], SEQ_SCAN[catalog_returns], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  d_year AS cs_sold_year,
  cs_item_sk,
  cs_bill_customer_sk AS cs_customer_sk,
  SUM(cs_quantity) AS cs_qty,
  SUM(cs_wholesale_cost) AS cs_wc,
  SUM(cs_sales_price) AS cs_sp
FROM catalog_sales
LEFT JOIN catalog_returns
  ON cr_order_number = cs_order_number AND cs_item_sk = cr_item_sk
JOIN date_dim
  ON cs_sold_date_sk = d_date_sk
WHERE
  cr_order_number IS NULL
GROUP BY
  d_year,
  cs_item_sk,
  cs_bill_customer_sk
```

### 3. ss
**Role**: CTE (Definition Order: 0)
**Stats**: 25% Cost | ~1k rows
**Flags**: GROUP_BY
**Outputs**: [ss_sold_year, ss_item_sk, ss_customer_sk, ss_qty, ss_wc, ss_sp]
**Dependencies**: store_sales, store_returns (join), date_dim (join)
**Filters**: sr_ticket_number IS NULL
**Operators**: HASH_GROUP_BY, SEQ_SCAN[store_sales], SEQ_SCAN[store_returns], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT
  d_year AS ss_sold_year,
  ss_item_sk,
  ss_customer_sk,
  SUM(ss_quantity) AS ss_qty,
  SUM(ss_wholesale_cost) AS ss_wc,
  SUM(ss_sales_price) AS ss_sp
FROM store_sales
LEFT JOIN store_returns
  ON sr_ticket_number = ss_ticket_number AND ss_item_sk = sr_item_sk
JOIN date_dim
  ON ss_sold_date_sk = d_date_sk
WHERE
  sr_ticket_number IS NULL
GROUP BY
  d_year,
  ss_item_sk,
  ss_customer_sk
```

### 4. main_query
**Role**: Root / Output (Definition Order: 1)
**Stats**: 25% Cost | ~1k rows processed → 100 rows output
**Flags**: GROUP_BY, ORDER_BY, LIMIT(100)
**Outputs**: [ss_item_sk, ratio, store_qty, store_wholesale_cost, store_sales_price, other_chan_qty, other_chan_wholesale_cost, other_chan_sales_price] — ordered by ss_item_sk ASC, ss_qty DESC, ss_wc DESC, ss_sp DESC, other_chan_qty ASC, other_chan_wholesale_cost ASC, other_chan_sales_price ASC, ratio ASC
**Dependencies**: ss, ws (join), cs (join)
**Filters**: (COALESCE(ws_qty, 0) > 0 OR COALESCE(cs_qty, 0) > 0) | ss_sold_year = 2000
**Operators**: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[ss], SEQ_SCAN[ws], SEQ_SCAN[cs]
**Key Logic (SQL)**:
```sql
SELECT
  ss_item_sk,
  ROUND(ss_qty / (
    COALESCE(ws_qty, 0) + COALESCE(cs_qty, 0)
  ), 2) AS ratio,
  ss_qty AS store_qty,
  ss_wc AS store_wholesale_cost,
  ss_sp AS store_sales_price,
  COALESCE(ws_qty, 0) + COALESCE(cs_qty, 0) AS other_chan_qty,
  COALESCE(ws_wc, 0) + COALESCE(cs_wc, 0) AS other_chan_wholesale_cost,
  COALESCE(ws_sp, 0) + COALESCE(cs_sp, 0) AS other_chan_sales_price
FROM ss
LEFT JOIN ws
  ON (
    ws_sold_year = ss_sold_year
    AND ws_item_sk = ss_item_sk
    AND ws_customer_sk = ss_customer_sk
  )
LEFT JOIN cs
  ON (
...
```

### Edges
- ss → main_query
- ws → main_query
- cs → main_query


## Aggregation Semantics Check

You MUST verify aggregation equivalence for any proposed restructuring:

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Returns NULL for 0-1 values. Changing group membership changes the result.
- `STDDEV_SAMP(x) FILTER (WHERE year=1999)` over a combined (1999,2000) group is NOT equivalent to `STDDEV_SAMP(x)` over only 1999 rows — FILTER still uses the combined group's membership for the stddev denominator.
- **AVG and STDDEV are NOT duplicate-safe**: if a join introduces row duplication, the aggregate result changes.
- When splitting a UNION ALL CTE with GROUP BY + aggregate, each split branch must preserve the exact GROUP BY columns and filter to the exact same row set as the original.
- **SAFE ALTERNATIVE**: If GROUP BY includes the discriminator column (e.g., d_year), each group is already partitioned. STDDEV_SAMP computed per-group is correct. You can then pivot using `MAX(CASE WHEN year = 1999 THEN year_total END) AS year_total_1999` because the GROUP BY guarantees exactly one row per (customer, year) — the MAX is just a row selector, not a real aggregation.

## Top 3 Tag-Matched Examples

### snowflake_decorrelate (Nonex)
**Description:** Convert correlated subquery to JOIN + GROUP BY for parallel MPP execution on Snowflake.

### snowflake_date_cte_isolate (Nonex)
**Description:** Pre-filter date_dim in a CTE to reduce fact table join input. Helps Snowflake's micro-partition pruning on date-clustered fact tables.

### snowflake_single_pass_aggregation (hypothesizedx)
**Description:** Replace N subqueries scanning the same fact table with a single scan using CASE WHEN conditional aggregation. Reduces remote I/O on Snowflake.
**Principle:** Scan Consolidation: scan large fact table once with conditional aggregation instead of N times with N subqueries. Critical for Snowflake where each scan hits remote storage.

## Optimization Principles (from benchmark history)

**reduce_scan_volume** (0.0x avg)
**consolidate_repeated_scans** (0.0x avg)
**decorrelate_subqueries** (0.0x avg)
**minimize_cte_count** (0.0x avg)

## Exploit Algorithm: Evidence-Based Gap Intelligence

The following describes known optimizer gaps with detection rules, procedural exploit steps, and evidence. Use DETECT rules to match structural features of the query, then follow EXPLOIT_STEPS.

# Snowflake Rewrite Playbook
# INITIAL — no empirical wins/regressions yet | TPC-DS SF10TCL

## HOW TO USE THIS DOCUMENT

Work in phase order. Each phase changes the plan shape — re-evaluate later phases after each.

  Phase 1: Reduce scan volume (P1, P2) — always first. Micro-partition pruning is Snowflake's #1 lever.
  Phase 2: Eliminate redundant work (P3, P4)
  Phase 3: Fix structural inefficiencies (P5, P6, P7)

Before choosing any strategy, scan the query profile for:
- Partitions scanned vs total: high ratio = pruning opportunity (add clustering key filters).
- Spilling to disk: bytes spilled > 0 = memory pressure, reduce intermediate sizes.
- Repeated table access: same table N times = consolidation candidate.
- Remote I/O vs cache: low cache hit = partition pruning or clustering issue.
- Join explosion: output rows >> input rows = join fanout, pre-aggregate.
- Subquery type: correlated subquery = decorrelation candidate (Snowflake handles well but not always optimally).

## ENGINE STRENGTHS — do NOT rewrite these patterns

1. **Micro-partition pruning**: Snowflake prunes partitions based on min/max metadata on clustering keys. If a filter matches the clustering key, pruning is automatic — do NOT restructure.
2. **Automatic clustering**: Snowflake re-clusters data in the background. Manual sort-based CTEs are unnecessary.
3. **Result set caching**: Identical queries return cached results in <100ms. Don't optimize for cache — focus on cold-run performance.
4. **MPP parallel execution**: Queries execute across warehouse nodes. Simple scans and aggregations are already parallelized.
5. **Semi-join optimization**: EXISTS/NOT EXISTS uses semi-join. Never materialize these patterns.
6. **Predicate pushdown**: Simple WHERE predicates pushed into scan. Don't wrap in CTEs for pushdown.
7. **Adaptive join selection**: Hash join vs merge join vs nested loop chosen automatically based on statistics.

## CORRECTNESS RULES

- Identical rows, columns, ordering as original.
- Copy ALL literals exactly (strings, numbers, dates).
- Preserve NULL semantics — NOT IN with NULLs behaves differently than NOT EXISTS.
- Every CTE must SELECT all columns referenced downstream.
- Never drop, rename, or reorder output columns.
- Preserve LIMIT semantics — no result set expansion.

## GLOBAL GUARDS (check always, before any rewrite)

1. EXISTS/NOT EXISTS → never materialize into CTEs (kills semi-join optimization)
2. Clustering key filters → never restructure (breaks micro-partition pruning)
3. Simple OR conditions → don't split to UNION ALL unless proven beneficial
4. Baseline < 500ms → skip CTE rewrites (network/compilation overhead on Snowflake > DuckDB)
5. CTEs are materialized by default — each CTE creates a temp result. Use sparingly.
6. UNION ALL creates separate execution branches — each scans independently. Limit to ≤3 branches.
7. Avoid creating more intermediate results than necessary — Snowflake spills large intermediates to remote storage.
8. Don't over-decompose — Snowflake's MPP engine handles large joins well if inputs are pruned.
9. Preserve window function partitioning — Snowflake optimizes PARTITION BY with sort-based execution.
10. QUALIFY clause is native to Snowflake — prefer over subquery-based row filtering.

## PATHOLOGY P1: CROSS_CTE_PREDICATE_BLINDNESS
**Signal**: Filter applied late (after join), large intermediate before filter.
**Transform**: date_cte_isolate, dimension_cte_isolate, early_filter
**Gate**: Original must scan > 1M rows before applying the selective filter.
**Strategy**: Pre-filter dimension tables in CTEs, join to fact table. Reduces partition scanning.
**Risk**: LOW — Snowflake CTEs are materialized, but small dimension CTEs are cheap.
**Example pattern**:
```sql
-- BEFORE: filter buried in WHERE after large join
SELECT ... FROM fact JOIN dim ON ... WHERE dim.col = 'value'
-- AFTER: pre-filter dimension, then join
WITH dim_filtered AS (SELECT ... FROM dim WHERE col = 'value')
SELECT ... FROM fact JOIN dim_filtered ON ...
```

## PATHOLOGY P2: REDUNDANT_SCAN_ELIMINATION
**Signal**: Same large table scanned multiple times in subqueries.
**Transform**: single_pass_aggregation, consolidate_scans
**Gate**: Table appears 3+ times in query, each scan > 100K rows.
**Strategy**: Scan once with CASE WHEN aggregation or a single CTE, then reference multiple times.
**Risk**: LOW — reduces I/O which is Snowflake's bottleneck.
**Example pattern**:
```sql
-- BEFORE: 8 subqueries each scanning store_sales
SELECT (SELECT count(*) FROM store_sales WHERE ...) as s1,
       (SELECT count(*) FROM store_sales WHERE ...) as s2
-- AFTER: single scan with conditional aggregation
SELECT count(CASE WHEN ... THEN 1 END) as s1,
       count(CASE WHEN ... THEN 1 END) as s2
FROM store_sales WHERE ...
```

## PATHOLOGY P3: CORRELATED_SUBQUERY_PARALYSIS
**Signal**: Correlated subquery in WHERE or SELECT list. EXPLAIN shows nested loop.
**Transform**: decorrelate, lateral_to_join
**Gate**: Outer query returns > 10K rows (correlation cost scales linearly).
**Strategy**: Convert correlated subquery to JOIN + GROUP BY.
**Risk**: MEDIUM — Snowflake handles some correlated subqueries well. Check if EXPLAIN already shows a join.

## PATHOLOGY P4: COMMA_JOIN_TO_EXPLICIT
**Signal**: FROM a, b, c WHERE a.x = b.x AND b.y = c.y (implicit cross joins).
**Transform**: explicit_join
**Gate**: 3+ tables in comma-join style.
**Strategy**: Convert to explicit JOIN...ON. Snowflake optimizer may handle both, but explicit joins provide clearer join ordering hints.
**Risk**: LOW — semantically equivalent, may help optimizer.

## PATHOLOGY P5: REPEATED_DIMENSION_PREFETCH
**Signal**: Same dimension table joined to multiple fact subqueries.
**Transform**: dimension_cte_isolate, multi_dimension_prefetch
**Gate**: Dimension joined 2+ times with same filter.
**Strategy**: Pre-filter dimension once in CTE, reuse across subqueries.
**Risk**: LOW — reduces redundant dimension scans.

## PATHOLOGY P6: SET_OPERATION_OPTIMIZATION
**Signal**: INTERSECT, EXCEPT used where EXISTS/NOT EXISTS would suffice.
**Transform**: intersect_to_exists, except_to_not_exists
**Gate**: Set operation on large result sets (> 100K rows per side).
**Strategy**: Convert to EXISTS/NOT EXISTS for semi-join optimization.
**Risk**: LOW — preserves semantics, enables semi-join.

## PATHOLOGY P7: WINDOW_FUNCTION_PUSHDOWN
**Signal**: Window function computed over large dataset, then filtered.
**Transform**: qualify_pushdown, pre_filter_window
**Gate**: Window function input > 1M rows, post-filter keeps < 10%.
**Strategy**: Use QUALIFY clause (Snowflake-native) or pre-filter input.
**Risk**: MEDIUM — QUALIFY changes execution order.

## VERIFICATION CHECKLIST

Before submitting any rewrite:
1. Row count: must match original exactly.
2. Column count and names: must match original exactly.
3. NULL handling: NOT IN/NOT EXISTS semantics preserved.
4. ORDER BY: must match original if present.
5. LIMIT: must match original if present.
6. No new tables introduced.
7. No columns removed from output.

## PRUNING GUIDE

Skip optimization if:
- Query baseline < 500ms (Snowflake overhead makes marginal gains invisible).
- Query is a simple point lookup (micro-partition pruning already optimal).
- Query uses only indexed/clustered columns in WHERE (already pruned).
- Only 1 table in FROM (nothing to restructure).

## REGRESSION REGISTRY

*No regressions recorded yet — this section will be populated after initial benchmark.*

| Query | Rewrite | Regression | Root Cause |
|-------|---------|------------|------------|
| — | — | — | — |


## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 — prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY
  - Failure: Q76 — filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## Your Task

First, use a `<reasoning>` block for your internal analysis. This will be stripped before parsing. Work through these steps IN ORDER:

1. **CLASSIFY**: What structural archetype is this query?
   (channel-comparison self-join / correlated-aggregate filter / star-join with late dim filter / repeated fact scan / multi-channel UNION ALL / EXISTS-set operations / other)

2. **EXPLAIN PLAN ANALYSIS**: From the EXPLAIN ANALYZE output, identify:
   - Compute wall-clock ms per EXPLAIN node. Sum repeated operations (e.g., 2x store_sales joins = total cost). The EXPLAIN is ground truth, not the logical-tree cost percentages.
   - Which nodes consume >10% of runtime and WHY
   - Where row counts drop sharply (existing selectivity)
   - Where row counts DON'T drop (missed optimization opportunity)
   - Whether the optimizer already splits CTEs, pushes predicates, or performs transforms you might otherwise assign
   - Count scans per base table. If a fact table is scanned N times, a restructuring that reduces it to 1 scan saves (N-1)/N of that table's I/O cost. Prioritize transforms that reduce scan count on the largest tables.
   - Whether the CTE is materialized once and probed multiple times, or re-executed per reference

3. **GAP MATCHING**: Compare the EXPLAIN analysis to the Engine Profile gaps above. For each gap:
   - Does this query exhibit the gap? (e.g., is a predicate NOT pushed into a CTE? Is the same fact table scanned multiple times?)
   - Check the 'opportunity' — does this query's structure match?
   - Check 'what_didnt_work' and 'field_notes' — any disqualifiers for this query?
   - Also verify: is the optimizer ALREADY handling this well? (Check the Optimizer Strengths above — if the engine already does it, your transform adds overhead, not value.)

4. **AGGREGATION TRAP CHECK**: For every aggregate function in the query, verify: does my proposed restructuring change which rows participate in each group? STDDEV_SAMP, VARIANCE, PERCENTILE_CONT, CORR are grouping-sensitive. SUM, COUNT, MIN, MAX are grouping-insensitive (modulo duplicates). If the query uses FILTER clauses or conditional aggregation, verify equivalence explicitly.

5. **TRANSFORM SELECTION**: From the matched engine gaps, select transforms that exploit the specific gaps present in THIS query. Rank by expected value (rows affected × historical speedup from evidence). Select 4 that are structurally diverse — each attacking a different gap or bottleneck.
   REJECT tag-matched examples whose primary technique requires a structural feature this query lacks (e.g., reject intersect_to_exists if query has no INTERSECT; reject decorrelate if query has no correlated subquery). Tag matching is approximate — always verify structural applicability.

6. **LOGICAL TREE DESIGN**: For each worker's strategy, define the target logical tree topology. Verify that every node contract has exhaustive output columns by checking downstream references.
   CTE materialization matters for your design: a CTE referenced by 2+ consumers will likely be materialized (good — computed once, probed many). A CTE referenced once may be inlined (no materialization benefit from 'sharing'). Design shared CTEs only when multiple downstream nodes consume them. See CTE_INLINING in Engine Profile strengths.

Then produce the structured briefing in EXACTLY this format:

```
=== SHARED BRIEFING ===

SEMANTIC_CONTRACT: (80-150 tokens, cover ONLY:)
(a) One sentence of business intent (start from pre-computed intent if available).
(b) JOIN type semantics that constrain rewrites (INNER = intersection = all sides must match).
(c) Any aggregation function traps specific to THIS query.
(d) Any filter dependencies that a rewrite could break.
Do NOT repeat information already in ACTIVE_CONSTRAINTS or REGRESSION_WARNINGS.

BOTTLENECK_DIAGNOSIS:
[Which operation dominates cost and WHY (not just '50% cost').
Scan-bound vs join-bound vs aggregation-bound.
Cardinality flow (how many rows at each stage).
What the optimizer already handles well (don't re-optimize).
Whether logical-tree cost percentages are misleading.]

ACTIVE_CONSTRAINTS:
- [CORRECTNESS_CONSTRAINT_ID]: [Why it applies to this query, 1 line]
- [ENGINE_GAP_ID]: [Evidence from EXPLAIN that this gap is active]
(List all 4 correctness constraints + the 1-3 engine gaps that
are active for THIS query based on your EXPLAIN analysis.)

REGRESSION_WARNINGS:
1. [Pattern name] ([observed regression]):
   CAUSE: [What happened mechanistically]
   RULE: [Actionable avoidance rule for THIS query]
(If no regression warnings are relevant, write 'None applicable.')

=== WORKER 1 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 2 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 3 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 4 BRIEFING === (EXPLORATION WORKER)

STRATEGY: [strategy_name]
TARGET_LOGICAL_TREE:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_ADAPTATION:
[For each example: what aspect to apply to THIS strategy,
and what to IGNORE (e.g., 'apply the date CTE pattern; ignore the
decorrelation — Q74 has no correlated subquery').]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]
CONSTRAINT_OVERRIDE: [CONSTRAINT_ID or 'None']
OVERRIDE_REASONING: [Why this query's structure differs from the observed failure, or 'N/A']
EXPLORATION_TYPE: [constraint_relaxation | compound_strategy | novel_combination]

```

## Section Validation Checklist (MUST pass before final output)

Use this checklist to verify content quality, not just section presence:

### SHARED BRIEFING
- `SEMANTIC_CONTRACT`: 40-200 tokens and includes business intent, JOIN semantics, aggregation trap, and filter dependency.
- `BOTTLENECK_DIAGNOSIS`: states dominant mechanism, bound type (`scan-bound`/`join-bound`/`aggregation-bound`), cardinality flow, and what optimizer already handles well.
- `ACTIVE_CONSTRAINTS`: includes all 4 correctness IDs plus 1-3 active engine gaps with EXPLAIN evidence.
- `REGRESSION_WARNINGS`: either `None applicable.` or numbered entries with both `CAUSE:` and `RULE:`.

### WORKER N BRIEFING (N=1..4)
- `STRATEGY`: non-empty and unique across workers.
- `TARGET_LOGICAL_TREE`: explicit node chain (e.g., `a -> b -> c`).
- `NODE_CONTRACTS`: every logical tree node has a contract with `FROM`, `OUTPUT` (explicit columns), and `CONSUMERS`.
- `EXAMPLES`: 1-3 IDs per worker. Sharing an example across workers is allowed if each worker's EXAMPLE_ADAPTATION explains a different aspect to apply.
- `EXAMPLE_ADAPTATION`: for each example, states what to adapt and what to ignore for this worker's strategy.
- `HAZARD_FLAGS`: query-specific risks, not generic cautions.

### WORKER 4 EXPLORATION FIELDS
- Includes `CONSTRAINT_OVERRIDE`, `OVERRIDE_REASONING`, and `EXPLORATION_TYPE`.

## Transform Catalog

Select 4 transforms that are applicable to THIS query, maximizing structural diversity (each must attack a different part of the execution plan).

### Predicate Movement
- **global_predicate_pushdown**: Trace selective predicates from late in the CTE chain back to the earliest scan via join equivalences. Biggest win when a dimension filter is applied after a large intermediate materialization.
  Maps to examples: pushdown, early_filter, date_cte_isolate
- **transitive_predicate_propagation**: Infer predicates through join equivalence chains (A.key = B.key AND B.key = 5 -> A.key = 5). Especially across CTE boundaries where optimizers stop propagating.
  Maps to examples: early_filter, dimension_cte_isolate
- **null_rejecting_join_simplification**: When downstream WHERE rejects NULLs from the outer side of a LEFT JOIN, convert to INNER. Enables reordering and predicate pushdown. CHECK: does the query actually have LEFT/OUTER joins before assigning this.
  Maps to examples: (no direct gold example — novel transform)

### Join Restructuring
- **self_join_elimination**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, split into N pre-partitioned CTEs. Eliminates discriminator filtering and repeated hash probes on rows that don't match.
  Maps to examples: union_cte_split, shared_dimension_multi_channel
- **decorrelation**: Convert correlated EXISTS/IN/scalar subqueries to CTE + JOIN. CHECK: does the query actually have correlated subqueries before assigning this.
  Maps to examples: decorrelate, composite_decorrelate_union
- **aggregate_pushdown**: When GROUP BY follows a multi-table join but aggregation only uses columns from one side, push the GROUP BY below the join. CHECK: verify the join doesn't change row multiplicity for the aggregate (one-to-many breaks AVG/STDDEV).
  Maps to examples: (no direct gold example — novel transform)
- **late_attribute_binding**: When a dimension table is joined only to resolve display columns (names, descriptions) that aren't used in filters, aggregations, or join conditions, defer that join until after all filtering and aggregation is complete. Join on the surrogate key once against the final reduced result set. This eliminates N-1 dimension scans when the CTE references the dimension N times. CHECK: verify the deferred columns aren't used in WHERE, GROUP BY, or JOIN ON — only in the final SELECT.
  Maps to examples: dimension_cte_isolate (partial pattern), early_filter

### Scan Optimization
- **star_join_prefetch**: Pre-filter ALL dimension tables into CTEs, then probe fact table with the combined key intersection.
  Maps to examples: dimension_cte_isolate, multi_dimension_prefetch, prefetch_fact_join, date_cte_isolate
- **single_pass_aggregation**: Merge N subqueries on the same fact table into 1 scan with CASE/FILTER inside aggregates. CHECK: STDDEV_SAMP/VARIANCE are grouping-sensitive — FILTER over a combined group != separate per-group computation.
  Maps to examples: single_pass_aggregation, channel_bitmap_aggregation
- **scan_consolidation_pivot**: When a CTE is self-joined N times with each reference filtering to a different discriminator (e.g., year, channel), consolidate into fewer scans that GROUP BY the discriminator, then pivot rows to columns using MAX(CASE WHEN discriminator = X THEN agg_value END). This halves the fact scans and dimension joins. SAFE when GROUP BY includes the discriminator — each group is naturally partitioned, so aggregates like STDDEV_SAMP are computed correctly per-partition. The pivot MAX is just a row selector (one row per group), not a real aggregation.
  Maps to examples: single_pass_aggregation, union_cte_split

### Structural Transforms
- **union_consolidation**: Share dimension lookups across UNION ALL branches that scan different fact tables with the same dim joins.
  Maps to examples: shared_dimension_multi_channel
- **window_optimization**: Push filters before window functions when they don't affect the frame. Convert ROW_NUMBER + filter to LATERAL + LIMIT. Merge same-PARTITION windows into one sort pass.
  Maps to examples: deferred_window_aggregation
- **exists_restructuring**: Convert INTERSECT to EXISTS for semi-join short-circuit, or restructure complex EXISTS with shared CTEs. CHECK: does the query actually have INTERSECT or complex EXISTS.
  Maps to examples: intersect_to_exists, multi_intersect_exists_cte

## Strategy Selection Rules

1. **CHECK APPLICABILITY**: Each transform has a structural prerequisite (correlated subquery, UNION ALL CTE, LEFT JOIN, etc.). Verify the query actually has the prerequisite before assigning a transform. DO NOT assign decorrelation if there are no correlated subqueries.
2. **CHECK OPTIMIZER OVERLAP**: Read the EXPLAIN plan. If the optimizer already performs a transform (e.g., already splits a UNION CTE, already pushes a predicate), that transform will have marginal benefit. Note this in your reasoning and prefer transforms the optimizer is NOT already doing.
3. **MAXIMIZE DIVERSITY**: Each worker must attack a different part of the execution plan. Do not assign 'pushdown variant A' and 'pushdown variant B'. Assign transforms from different categories above.
4. **ASSESS RISK PER-QUERY**: Risk is a function of (transform x query complexity), not an inherent property of the transform. Decorrelation is low-risk on a simple EXISTS and high-risk on nested correlation inside a CTE. Assess per-assignment.
5. **COMPOSITION IS ALLOWED AND ENCOURAGED**: A strategy can combine 2-3 transforms from different categories (e.g., star_join_prefetch + scan_consolidation_pivot, or date_cte_isolate + early_filter + decorrelate). The TARGET_LOGICAL_TREE should reflect the combined structure. Compound strategies are often the source of the biggest wins.
6. **MINIMAL-CHANGE BASELINE**: If the EXPLAIN shows the optimizer already handles the primary bottleneck (e.g., already splits CTEs, already pushes predicates), consider assigning one worker as a minimal-change baseline: explicit JOINs only, no structural changes. This provides a regression-safe fallback.

Each worker gets 1-3 examples. If fewer than 2 examples genuinely match the worker's strategy, assign 1 and state 'No additional examples apply.' Do NOT pad with irrelevant examples — an irrelevant example is worse than no example because the worker will try to apply its pattern. No duplicate examples across workers. Use example IDs from the catalog above.

For TARGET_LOGICAL_TREE: Define the CTE structure you want produced. For NODE_CONTRACTS: Be exhaustive with OUTPUT columns — missing columns cause semantic breaks.

## Exploration Budget (Worker 4)

Workers 1-3 follow the engine profile's proven patterns. **Worker 4 is the EXPLORATION worker** with a different mandate:

Worker 4 MAY (in priority order — prefer higher-value exploration):
  (c) **PREFERRED**: Attempt a novel technique not listed in the engine profile, if the EXPLAIN plan reveals an optimizer blind spot not yet documented. This is the highest-value exploration — new discoveries expand the engine profile for all future queries.
  (b) Combine 2-3 transforms from different engine gaps into a compound strategy that hasn't been tested before. Medium value — tests interaction effects between known patterns.
  (a) Retry a technique from 'what_didnt_work', IF the structural context of THIS query differs materially from the observed failure — explain the structural difference in HAZARD_FLAGS. Lowest priority — only when the query structure clearly diverges from the failed case.

Worker 4 may NEVER violate correctness constraints (LITERAL_PRESERVATION, SEMANTIC_EQUIVALENCE, COMPLETE_OUTPUT, CTE_COLUMN_COMPLETENESS).

The exploration worker's output is tagged EXPLORATORY and tracked separately. Past failures documented in the engine profile are context-specific — they happened on specific queries with specific structures. Worker 4's job is to test whether those failures generalize or not. If Worker 4 discovers a new win, it becomes field intelligence for the engine profile.

## Output Consumption Spec

Each worker receives:
1. SHARED BRIEFING (SEMANTIC_CONTRACT + BOTTLENECK_DIAGNOSIS + ACTIVE_CONSTRAINTS + REGRESSION_WARNINGS)
2. Their specific WORKER N BRIEFING (STRATEGY + TARGET_LOGICAL_TREE + NODE_CONTRACTS + EXAMPLES + EXAMPLE_ADAPTATION + HAZARD_FLAGS)
3. Full before/after SQL for their assigned examples (retrieved by example ID)
4. The original query SQL (full, as reference)
5. Column completeness contract + output format spec

Workers do NOT see other workers' briefings.
Presentation order: briefing first (understanding), then examples (patterns), then original SQL (source), then output format (mechanics).