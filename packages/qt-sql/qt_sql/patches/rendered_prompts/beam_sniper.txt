## Role

You are a SQL optimization specialist for PostgreSQL. You will receive the results of transform probes (BDA) fired against a query. Your task: analyze the probe results, identify what worked and why, then design **exactly 2 patch plans** that build on the best insights — combining or refining winning strategies.

## Prompt Map

### Phase A — Cached Context
A1. Dialect Profile
A2. Optimization Families (with decision gates)
A3. Regression Registry
A4. Combination Rules
A5. Task Contract

### Phase B — Query-Specific Input (after cache boundary)
B1. Query SQL
B2. Original Execution Plan
B3. IR Structure
B4. BDA Results (winning SQL + EXPLAIN + patch shape)

## Dialect Profile (POSTGRES)

**Combined Intelligence Baseline**: Combined intelligence baseline from 53 validated DSB queries at SF5-SF10, plus regression registry outcomes. PostgreSQL has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here.

### Optimizer Strengths (don't fight these)
- `BITMAP_OR_SCAN`: avoid splitting OR conditions into UNION ALL by default. Only consider OR→UNION when EXPLAIN shows OR blocks index usage and UNION branches become index scans.
- `SEMI_JOIN_EXISTS`: NEVER convert EXISTS to IN/NOT IN or materialized CTEs. 0.50x, 0.75x observed. Note: NOT EXISTS anti-join decorrelation can still be valid when replacing large correlated anti patterns.
- `INNER_JOIN_REORDERING`: Don't restructure INNER JOIN orders. Focus on LEFT JOIN blocking or comma-join confusion.
- `INDEX_ONLY_SCAN`: Small dimension lookups (<10K rows) may not need CTEs.

### Known Gaps (exploit these)
- `COMMA_JOIN_WEAKNESS` [HIGH] detect: FROM t1, t2, t3 WHERE t1.key = t2.key (comma joins, no explicit JOIN). Poor row estimates in EXPLAIN. | action: Convert comma-joins to explicit JOIN...ON syntax. Best when combined with date_cte_isolate.
- `CORRELATED_SUBQUERY_PARALYSIS` [HIGH] detect: Nested loop in EXPLAIN, inner re-executes aggregate per outer row. SQL: WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key). Hash… | action: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
- `NON_EQUI_JOIN_INPUT_BLINDNESS` [HIGH] detect: Expensive non-equi join (BETWEEN, <, >) with large inputs on both sides. Neither side filtered. | action: Reduce fact table input size via filtered CTE before the non-equi join.
- `CTE_MATERIALIZATION_FENCE` [MEDIUM] detect: Large CTE + small post-filter. Multi-referenced CTE that blocks predicate pushdown. | action: Materialize STRATEGICALLY: only when CTE is expensive and reused. Avoid fencing single-use cases.
- `CROSS_CTE_PREDICATE_BLINDNESS` [MEDIUM] detect: Sequential scan on dimension table without index condition. Late filter after large scan/join. | action: Pre-filter into CTE definition. But be more cautious than on DuckDB.

## Optimization Families

6/6 families have validated gold examples on this dialect. Treat these as priors, not hard rules.

Prioritize by: EXPLAIN bottleneck, transform precondition fit, and dialect gap match.


### Family A: Early Filtering (Predicate Pushback)
**Description**: Push small filters into CTEs early, reduce row count before expensive operations
**Speedup Range**: 1.3–4.0x (~35% of all wins)
**Use When**:
  1. Late WHERE filters on dimension tables
  2. Cascading CTEs with filters applied downstream
  3. Expensive joins after filters could be pushed earlier
**Decision Gates (STOP when):**
  1. Filter ratio is weak and baseline runtime is already low
  2. Target CTE already contains the relevant selective predicate
  3. Three or more fact tables in a deep CTE chain (join-order lock risk)

**Gold Example**: `pg_date_consolidation` (3.10x)
**Canonical transforms**: `date_cte_isolate`
**Targeted gaps**: `CROSS_CTE_PREDICATE_BLINDNESS`
**Pattern**: Date Dimension Consolidation: when 2+ date_dim instances have overlapping predicates (same year, overlapping months), extract one CTE with the union of all needed date keys. Each downstream join adds its specific MOY condition. Reduces N date_dim scans to 1.
**Patch shape**: insert_cte → replace_from → delete_expr_subtree



### Family B: Decorrelation (Sets Over Loops)
**Description**: Convert correlated subqueries to standalone CTEs with GROUP BY, eliminate per-row re-execution
**Speedup Range**: 2.4–2.9x (~15% of all wins)
**Use When**:
  1. Correlated subqueries in WHERE clause
  2. Scalar aggregates computed per outer row
  3. DELIM_SCAN in execution plan (indicates correlation)
**Decision Gates (STOP when):**
  1. EXPLAIN already shows a hash semi join on the same correlation key
  2. Simple EXISTS path already optimized by semi-join
  3. Outer side is already tiny after early filtering

**Gold Example**: `early_filter_decorrelate` (27.80x (V2 DSB SF10, was 1.13x in V1))
**Canonical transforms**: `decorrelate`, `early_filter`
**Targeted gaps**: `CORRELATED_SUBQUERY_PARALYSIS`, `CROSS_CTE_PREDICATE_BLINDNESS`
**Pattern**: Principle: Early Selection + Decorrelation — push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subq…
**Patch shape**: insert_cte → insert_cte+replace_from → replace_from → replace_where_predicate



### Family C: Aggregation Pushdown (Minimize Rows Touched)
**Description**: Aggregate before expensive joins when GROUP BY keys ⊇ join keys, reduce intermediate sizes
**Speedup Range**: 1.3–15.3x (~5% of all wins (high variance))
**Use When**:
  1. GROUP BY happens after large joins
  2. GROUP BY keys are subset of join keys
  3. Intermediate result size >> final result size
**Decision Gates (STOP when):**
  1. GROUP BY keys are not compatible with join keys (semantic risk)
  2. Aggregation includes grouping-sensitive metrics (e.g., STDDEV/VARIANCE)
  3. Rewrite introduces join duplication before AVG/STDDEV-style aggregates

**Gold Example**: `pg_self_join_pivot` (1.79x)
**Canonical transforms**: `single_pass_aggregation`
**Targeted gaps**: `REDUNDANT_SCAN_ELIMINATION`
**Pattern**: Self-Join Elimination via Pivot: when a query computes the same aggregation across N time periods by self-joining N copies of the same CTE, materialize the base scan once and pivot time periods using CASE WHEN or FILTER (WHERE quarter = X) aggregation.
**Patch shape**: insert_cte → insert_cte → insert_cte → insert_cte → replace_from → replace_where_predicate



### Family D: Set Operation Optimization (Sets Over Loops)
**Description**: Replace INTERSECT/UNION-based patterns with EXISTS/NOT EXISTS, avoid full materialization
**Speedup Range**: 1.7–2.7x (~8% of all wins)
**Use When**:
  1. INTERSECT patterns between large sets
  2. UNION ALL with duplicate elimination
  3. Set operations materializing full intermediate results
**Decision Gates (STOP when):**
  1. Both set-operation sides are already small
  2. Result needs columns from both sides (semi-join rewrite invalid)

**Gold Example**: `pg_intersect_to_exists` (1.78x)
**Canonical transforms**: `intersect_to_exists`
**Pattern**: INTERSECT to EXISTS: INTERSECT materializes both sides fully, sorts, and compares. EXISTS uses semi-join with index + early termination per row. When both sides produce 10K+ rows, EXISTS is cheaper because it stops at first match.
**Patch shape**: insert_cte → replace_from → replace_where_predicate



### Family E: Materialization / Prefetch (Don't Repeat Work)
**Description**: Extract repeated scans or pre-compute intermediate results for reuse across multiple consumers
**Speedup Range**: 1.3–6.2x (~18% of all wins)
**Use When**:
  1. Repeated scans of same table with different filters
  2. Dimension filters applied independently multiple times
  3. CTE referenced multiple times with implicit re-evaluation
**Decision Gates (STOP when):**
  1. CTE is single-use and not expensive
  2. New CTE would be unfiltered (materialize-everything pattern)
  3. Original source scan would remain alongside replacement (orphan risk)

**Gold Example**: `pg_self_join_decomposition` (3.93x)
**Canonical transforms**: `materialize_cte`
**Pattern**: Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely eff…
**Patch shape**: insert_cte → insert_cte → insert_cte → insert_cte → insert_cte → replace_from



### Family F: Join Transform (Right Shape First)
**Description**: Restructure join topology: convert comma joins to explicit INNER JOIN, optimize join order, eliminate self-joins via single-pass aggregation
**Speedup Range**: 1.8–8.6x (~19% of all wins)
**Use When**:
  1. Comma-separated joins (implicit cross joins) in FROM clause
  2. Self-joins scanning same table multiple times
  3. Dimension-fact join order suboptimal for predicate pushdown
**Decision Gates (STOP when):**
  1. Tiny join graph where optimizer is already accurate
  2. EXPLAIN shows good cardinality estimates and stable join shape

**Gold Example**: `pg_explicit_join_materialized` (8.56x)
**Canonical transforms**: `date_cte_explicit_join`
**Targeted gaps**: `COMMA_JOIN_WEAKNESS`
**Pattern**: Explicit Join + Materialized CTE: comma joins with 5+ tables produce poor cardinality estimates. Converting to explicit INNER JOIN + materializing selective dimensions into small CTEs gives the planner accurate row counts at each join step.
**Patch shape**: insert_cte → insert_cte+replace_from → insert_cte+replace_from → insert_cte+replace_from → replace_from → delete_expr_subtree



## Regression Registry

Hard failures to gate against:
- Materialized simple EXISTS path -> severe regressions (semi-join lost)
- Same-column OR split to UNION ALL -> regressions on bitmap-or capable plans
- Orphaned original CTE/table after replacement -> double materialization
- Unfiltered new CTE -> materialize-everything anti-pattern
- Over-deep fact-table CTE chains -> join-order lock / parallelism loss

## Combination Rules

- Non-overlapping targets compose cleanly.
- Overlapping WHERE rewrites must be merged, not applied sequentially.
- Overlapping FROM rewrites must unify joins without duplicate sources.
- If two new CTEs overlap strongly, keep the more selective one.
- Use best-speedup winner as foundation; layer one change at a time.

## Your Task

Use BDA evidence to design exactly 2 refined plans:

1. Rank winners by measured speedup; pick the best as foundation.
2. Compare winner EXPLAIN vs original to identify remaining bottleneck.
3. Use failure signals to avoid bad combinations.
4. Build:
- Plan 1: refine the best winner.
- Plan 2: combine two compatible winners (or salvage a failed-but-sound strategy).

BDA data requirements for winner analysis:
- full rewritten SQL
- explain summary with operators/rows/timings
- worker patch shape (ops)

Output exactly **2 patch plans** as a JSON array.

Required per plan:
- `plan_id`, `family`, `transform`, `hypothesis`, `target_ir`, `dialect`, `steps`
- optional: `based_on` as a string (comma-separated IDs for multiple sources; never an array)
- `steps[]` item: `step_id`, `op`, `target`, optional `payload`
- `target.by_node_id` MUST be `"S0"` (use `by_anchor_hash` only when needed)

Allowed `op` values:
- `insert_cte`
- `replace_from`
- `replace_where_predicate`
- `replace_body`
- `replace_expr_subtree`
- `delete_expr_subtree`

Semantic guards (MUST preserve):
- all WHERE/HAVING/ON logic
- all literals exactly
- columns/aliases/ORDER BY/LIMIT
- row count and semantics
- no orphaned CTEs or duplicated source scans after replacement

Rules:
- output exactly 2 plans
- each plan must use a different strategy (`family` + `transform`)
- payload SQL fragments must be complete/executable (no ellipsis)
- cite EXPLAIN evidence in `hypothesis`

Tier-0 Output Contract:
- first character must be `[` (no leading whitespace/newlines)
- top-level value must be an array of objects
- no markdown fences, prose, or commentary
- never emit key `sql`; use `sql_fragment` for SQL fragments

Output ONLY JSON array.

---

## Cache Boundary
Everything below is query-specific input.

## Query to Analyze

**Dialect**: POSTGRES

```sql
select 
  cd_gender,
  cd_marital_status,
  cd_education_status,
  count(*) cnt1,
  cd_purchase_estimate,
  count(*) cnt2,
  cd_credit_rating,
  count(*) cnt3
 from
  customer c,customer_address ca,customer_demographics
 where
  c.c_current_addr_sk = ca.ca_address_sk and
  ca_state in ('CO','NC','TX') and
  cd_demo_sk = c.c_current_cdemo_sk
  and cd_marital_status in ('S', 'M', 'U')
  and cd_education_status in ('Primary', 'College') and
  exists (select *
          from store_sales,date_dim
          where c.c_customer_sk = ss_customer_sk and
                ss_sold_date_sk = d_date_sk and
                d_year = 2002 and
                d_moy between 10 and 10+2
                and ss_list_price between 80 and 169
          ) and
   (not exists (select *
            from web_sales,date_dim
            where c.c_customer_sk = ws_bill_customer_sk and
                  ws_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and ws_list_price between 80 and 169
            ) and
    not exists (select *
            from catalog_sales,date_dim
            where c.c_customer_sk = cs_ship_customer_sk and
                  cs_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and cs_list_price between 80 and 169)
            )
 group by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 order by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 limit 100;
```

### Execution Plan

```
Total execution time: 33245.8ms
Planning time: 1.6ms

-> Limit  (rows=80 loops=1 time=33245.8ms)
  -> Aggregate  (rows=80 loops=1 time=33245.8ms)
    -> Nested Loop Anti  (rows=964 loops=1 time=33241.7ms)
       Join Filter: (c.c_customer_sk = catalog_sales.cs_ship_customer_sk)
      -> Nested Loop Anti  (rows=1,105 loops=1 time=7671.7ms)
         Join Filter: (c.c_customer_sk = web_sales.ws_bill_customer_sk)
        -> Gather Merge  (rows=1,128 loops=1 time=604.1ms)
           Workers: 2/2 launched
          -> Sort  (rows=376 loops=3 time=592.0ms)
             Sort Method: quicksort  Space: 56kB (Memory)
            -> Nested Loop Inner  (rows=376 loops=3 time=591.3ms)
              -> Hash Join Semi  (rows=1,691 loops=3 time=573.6ms)
                 Hash Cond: (c.c_customer_sk = store_sales.ss_customer_sk)
                -> Hash Join Inner  (rows=22K loops=3 time=48.2ms)
                   Hash Cond: (c.c_current_addr_sk = ca.ca_address_sk)
                  -> Seq Scan on customer c  (rows=167K loops=3 time=15.9ms)
                  -> Hash  (rows=11K loops=3 time=14.1ms)
                    -> Seq Scan on customer_address ca  (rows=11K loops=3 time=12.7ms)
                       Filter: (ca_state = ANY ('{CO,NC,TX}'::bpchar[]))
                       Rows Removed by Filter: 72K
                -> Hash  (rows=151K loops=3 time=507.3ms)
                   Batches: 8  Memory: 3264kB
                  -> Nested Loop Inner  (rows=151K loops=3 time=217.2ms)
                    -> Index Only Scan on date_dim  (rows=31 loops=3 time=0.6ms)
                       Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
                    -> Index Only Scan on store_sales  (rows=4,912 loops=92 time=6.8ms)
                       Filter: ((ss_list_price >= '80'::numeric) AND (ss_list_price <= '169'::numeric))
                       Index Cond: (ss_sold_date_sk = date_dim.d_date_sk)
                       Rows Removed by Filter: 7,334
              -> Index Scan on customer_demographics  (rows=0 loops=5074 time=0.0ms)
                 Filter: ((cd_education_status = ANY ('{Primary,College}'::bpchar[])) AND (cd_marital_status = ANY ('{S,M,...
                 Index Cond: (cd_demo_sk = c.c_current_cdemo_sk)
                 Rows Removed by Filter: 1
        -> Materialize  (rows=84K loops=1128 time=3.4ms)
          -> Gather  (rows=84K loops=1 time=77.3ms)
             Workers: 2/2 launched
            -> Nested Loop Inner  (rows=28K loops=3 time=35.5ms)
              -> Index Only Scan on date_dim date_dim_1  (rows=31 loops=3 time=0.5ms)
                 Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
              -> Index Scan on web_sales  (rows=917 loops=92 time=1.1ms)
                 Filter: ((ws_list_price >= '80'::numeric) AND (ws_list_price <= '169'::numeric))
                 Index Cond: (ws_sold_date_sk = date_dim_1.d_date_sk)
                 Rows Removed by Filter: 1,827
      -> Materialize  (rows=311K loops=1105 time=12.5ms)
        -> Nested Loop Inner  (rows=332K loops=1 time=246.2ms)
          -> Index Scan on date_dim date_dim_2  (rows=92 loops=1 time=0.1ms)
             Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
          -> Index Scan on catalog_sales  (rows=3,614 loops=92 time=2.5ms)
             Filter: ((cs_list_price >= '80'::numeric) AND (cs_list_price <= '169'::numeric))
             Index Cond: (cs_sold_date_sk = date_dim_2.d_date_sk)
             Rows Removed by Filter: 7,612
```

### IR Structure (for patch targeting)

```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: customer c, customer_address ca, customer_demographics
    WHERE [dae945277e160f9b]: c.c_current_addr_sk = ca.ca_address_sk AND ca_state IN ('CO', 'NC', 'TX') AND cd_demo_sk = c.c_cu...
    GROUP BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating
    ORDER BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**Note**: Use `by_node_id` (e.g., "S0") and `by_anchor_hash` (16-char hex) from map above to target patch operations.

**Probe summary**: 4 probes fired, 3 passed validation, 2 showed speedup.

### Strike BDA (Battle Damage Assessment)

| Probe | Transform | Family | Status | Speedup | Error/Notes |
|-------|-----------|--------|--------|---------|-------------|
| p01 | decorrelate_not_exists_to_cte | B | WIN | 1.45x |  |
| p02 | date_cte_isolate | A | PASS | 1.02x |  |
| p03 | comma_join_to_explicit | F | FAIL | - | Syntax error near line 12 |
| p04 | or_to_union | D | WIN | 1.30x |  |

### EXPLAIN Plans (winning strikes)

#### p01: decorrelate_not_exists_to_cte (1.45x)
```
HashJoin (actual rows=150, loops=1)
  -> SeqScan store_sales (actual rows=28000)
```

#### p04: or_to_union (1.30x)
```
Append (actual rows=200)
  -> Index Scan (actual rows=100)
  -> Index Scan (actual rows=100)
```


### SQL of Winning Strikes

#### p01: decorrelate_not_exists_to_cte (1.45x)
```sql
WITH cte AS (SELECT DISTINCT c_customer_sk FROM customer) SELECT c_customer_sk FROM cte
```

#### p04: or_to_union (1.30x)
```sql
SELECT c_customer_sk FROM customer WHERE c_customer_sk < 100 UNION ALL SELECT c_customer_sk FROM customer WHERE c_customer_sk >= 100
```
