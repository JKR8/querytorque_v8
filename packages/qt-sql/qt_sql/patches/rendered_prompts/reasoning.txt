## Role

You are a SQL optimization specialist for PostgreSQL. Your task is to analyze this query's execution plan, identify the primary bottleneck, and propose **exactly 2 independent patch plans** that target different optimization strategies.

Each patch plan must:
- Be atomic (steps applied sequentially: s1 → s2 → s3 → ...)
- Transform the original query using patch operations
- Preserve semantic equivalence (same rows, columns, ordering)
- Follow the patterns shown in reference examples below

## Query: query069_multi_i1

**Dialect**: POSTGRES

```sql
select 
  cd_gender,
  cd_marital_status,
  cd_education_status,
  count(*) cnt1,
  cd_purchase_estimate,
  count(*) cnt2,
  cd_credit_rating,
  count(*) cnt3
 from
  customer c,customer_address ca,customer_demographics
 where
  c.c_current_addr_sk = ca.ca_address_sk and
  ca_state in ('CO','NC','TX') and
  cd_demo_sk = c.c_current_cdemo_sk
  and cd_marital_status in ('S', 'M', 'U')
  and cd_education_status in ('Primary', 'College') and
  exists (select *
          from store_sales,date_dim
          where c.c_customer_sk = ss_customer_sk and
                ss_sold_date_sk = d_date_sk and
                d_year = 2002 and
                d_moy between 10 and 10+2
                and ss_list_price between 80 and 169
          ) and
   (not exists (select *
            from web_sales,date_dim
            where c.c_customer_sk = ws_bill_customer_sk and
                  ws_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and ws_list_price between 80 and 169
            ) and
    not exists (select *
            from catalog_sales,date_dim
            where c.c_customer_sk = cs_ship_customer_sk and
                  cs_sold_date_sk = d_date_sk and
                  d_year = 2002 and
                  d_moy between 10 and 10+2
                  and cs_list_price between 80 and 169)
            )
 group by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 order by cd_gender,
          cd_marital_status,
          cd_education_status,
          cd_purchase_estimate,
          cd_credit_rating
 limit 100;
```


## Current Execution Plan

```
Total execution time: 33245.8ms
Planning time: 1.6ms

-> Limit  (rows=80 loops=1 time=33245.8ms)
  -> Aggregate  (rows=80 loops=1 time=33245.8ms)
    -> Nested Loop Anti  (rows=964 loops=1 time=33241.7ms)
       Join Filter: (c.c_customer_sk = catalog_sales.cs_ship_customer_sk)
      -> Nested Loop Anti  (rows=1,105 loops=1 time=7671.7ms)
         Join Filter: (c.c_customer_sk = web_sales.ws_bill_customer_sk)
        -> Gather Merge  (rows=1,128 loops=1 time=604.1ms)
           Workers: 2/2 launched
          -> Sort  (rows=376 loops=3 time=592.0ms)
             Sort Method: quicksort  Space: 56kB (Memory)
            -> Nested Loop Inner  (rows=376 loops=3 time=591.3ms)
              -> Hash Join Semi  (rows=1,691 loops=3 time=573.6ms)
                 Hash Cond: (c.c_customer_sk = store_sales.ss_customer_sk)
                -> Hash Join Inner  (rows=22K loops=3 time=48.2ms)
                   Hash Cond: (c.c_current_addr_sk = ca.ca_address_sk)
                  -> Seq Scan on customer c  (rows=167K loops=3 time=15.9ms)
                  -> Hash  (rows=11K loops=3 time=14.1ms)
                    -> Seq Scan on customer_address ca  (rows=11K loops=3 time=12.7ms)
                       Filter: (ca_state = ANY ('{CO,NC,TX}'::bpchar[]))
                       Rows Removed by Filter: 72K
                -> Hash  (rows=151K loops=3 time=507.3ms)
                   Batches: 8  Memory: 3264kB
                  -> Nested Loop Inner  (rows=151K loops=3 time=217.2ms)
                    -> Index Only Scan on date_dim  (rows=31 loops=3 time=0.6ms)
                       Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
                    -> Index Only Scan on store_sales  (rows=4,912 loops=92 time=6.8ms)
                       Filter: ((ss_list_price >= '80'::numeric) AND (ss_list_price <= '169'::numeric))
                       Index Cond: (ss_sold_date_sk = date_dim.d_date_sk)
                       Rows Removed by Filter: 7,334
              -> Index Scan on customer_demographics  (rows=0 loops=5074 time=0.0ms)
                 Filter: ((cd_education_status = ANY ('{Primary,College}'::bpchar[])) AND (cd_marital_status = ANY ('{S,M,...
                 Index Cond: (cd_demo_sk = c.c_current_cdemo_sk)
                 Rows Removed by Filter: 1
        -> Materialize  (rows=84K loops=1128 time=3.4ms)
          -> Gather  (rows=84K loops=1 time=77.3ms)
             Workers: 2/2 launched
            -> Nested Loop Inner  (rows=28K loops=3 time=35.5ms)
              -> Index Only Scan on date_dim date_dim_1  (rows=31 loops=3 time=0.5ms)
                 Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
              -> Index Scan on web_sales  (rows=917 loops=92 time=1.1ms)
                 Filter: ((ws_list_price >= '80'::numeric) AND (ws_list_price <= '169'::numeric))
                 Index Cond: (ws_sold_date_sk = date_dim_1.d_date_sk)
                 Rows Removed by Filter: 1,827
      -> Materialize  (rows=311K loops=1105 time=12.5ms)
        -> Nested Loop Inner  (rows=332K loops=1 time=246.2ms)
          -> Index Scan on date_dim date_dim_2  (rows=92 loops=1 time=0.1ms)
             Index Cond: ((d_year = 2002) AND (d_moy >= 10) AND (d_moy <= 12))
          -> Index Scan on catalog_sales  (rows=3,614 loops=92 time=2.5ms)
             Filter: ((cs_list_price >= '80'::numeric) AND (cs_list_price <= '169'::numeric))
             Index Cond: (cs_sold_date_sk = date_dim_2.d_date_sk)
             Rows Removed by Filter: 7,612
```


## IR Structure (for patch targeting)

```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: customer c, customer_address ca, customer_demographics
    WHERE [dae945277e160f9b]: c.c_current_addr_sk = ca.ca_address_sk AND ca_state IN ('CO', 'NC', 'TX') AND cd_demo_sk = c.c_cu...
    GROUP BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating
    ORDER BY: cd_gender, cd_marital_status, cd_education_status, cd_purchase_estimate, cd_credit_rating

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**Note**: Use `by_node_id` (e.g., "S0") and `by_anchor_hash` (16-char hex) from map above to target patch operations.


## Optimization Families

This is a briefing of what we know, not a set of hard rules. Use your judgement about what's worth trying based on the EXPLAIN plan above.

**3 families have proven gold examples** on this engine. All 6 families are listed — those without gold examples are still valid strategies if the EXPLAIN plan warrants them.

Choose the most relevant families for this query based on:
- Query structure (CTEs, subqueries, joins, aggregations, set operations)
- Execution plan signals (WHERE placement, repeated scans, correlated subqueries)
- Problem signature (cardinality estimation errors, loops vs sets, filter ordering)



### Family A: Early Filtering (Predicate Pushback)
**Description**: Push small filters into CTEs early, reduce row count before expensive operations
**Speedup Range**: 1.3–4.0x (~35% of all wins)
**Use When**:
  1. Late WHERE filters on dimension tables
  2. Cascading CTEs with filters applied downstream
  3. Expensive joins after filters could be pushed earlier

**Gold Example**: None yet observed on PostgreSQL. Strategy is valid if EXPLAIN evidence supports it.



### Family B: Decorrelation (Sets Over Loops)
**Description**: Convert correlated subqueries to standalone CTEs with GROUP BY, eliminate per-row re-execution
**Speedup Range**: 2.4–2.9x (~15% of all wins)
**Use When**:
  1. Correlated subqueries in WHERE clause
  2. Scalar aggregates computed per outer row
  3. DELIM_SCAN in execution plan (indicates correlation)

**Gold Example**: `early_filter_decorrelate` (27.80x (V2 DSB SF10, was 1.13x in V1))

**BEFORE (slow):**
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         sr_reason_sk AS ctr_reason_sk,
         SUM(SR_REFUNDED_CASH) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk
    AND d_year = 2001
    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer, customer_demographics
WHERE ctr1.ctr_total_return > (
    SELECT AVG(ctr_total_return) * 1.2
    FROM customer_total_return ctr2
    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state IN ('MI', 'NC', 'WI')
  AND ctr1.ctr_customer_sk = c_customer_sk
  AND c_current_cdemo_sk = cd_demo_sk
  AND cd_marital_status IN ('W', 'W')
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

**AFTER (fast):**
```sql
WITH customer_total_return AS (
    SELECT sr_customer_sk AS ctr_customer_sk,
           sr_store_sk AS ctr_store_sk,
           sr_reason_sk AS ctr_reason_sk,
           SUM(SR_REFUNDED_CASH) AS ctr_total_return
    FROM store_returns
    JOIN date_dim ON sr_returned_date_sk = d_date_sk
    JOIN store ON sr_store_sk = s_store_sk
    WHERE d_year = 2001
      AND s_state IN ('MI', 'NC', 'WI')
      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
),
store_thresholds AS (
    SELECT ctr_store_sk,
           AVG(ctr_total_return) * 1.2 AS avg_limit
    FROM customer_total_return
    GROUP BY ctr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1
JOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk
JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk
JOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk
JOIN store s ON ctr1.ctr_store_sk = s.s_store_sk
WHERE ctr1.ctr_total_return > st.avg_limit
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s.s_state IN ('MI', 'NC', 'WI')
  AND cd_marital_status = 'W'
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

**IR BEFORE:**
```
S0 [SELECT]
  CTE: customer_total_return  (via CTE_Q_S0_customer_total_return)
    FROM: store_returns, date_dim
    WHERE [4fa12f115227afec]: sr_returned_date_sk = d_date_sk AND d_year = 2001 AND sr_return_amt / sr_return_quantity BETWEEN ...
    GROUP BY: sr_customer_sk, sr_store_sk, sr_reason_sk
  MAIN QUERY (via Q_S0)
    FROM: customer_total_return ctr1, store, customer, customer_demographics
    WHERE [6eef64f061a2aa84]: ctr1.ctr_total_return > (SELECT AVG(ctr_total_return) * 1.2 FROM customer_total_return AS ctr2 WH...
    ORDER BY: c_customer_id

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**IR TARGET:**
```
S0 [SELECT]
  CTE: customer_total_return  (via CTE_Q_S0_customer_total_return)
    FROM: store_returns, date_dim, store
    WHERE [1fc3c21c19b840aa]: d_year = 2001 AND s_state IN ('MI', 'NC', 'WI') AND sr_return_amt / sr_return_quantity BETWEEN 23...
    GROUP BY: sr_customer_sk, sr_store_sk, sr_reason_sk
  CTE: store_thresholds  (via CTE_Q_S0_store_thresholds)
    FROM: customer_total_return
    GROUP BY: ctr_store_sk
  MAIN QUERY (via Q_S0)
    FROM: customer_total_return ctr1, store_thresholds st, customer, customer_demographics, store s
    WHERE [fa3d3cbec1ba162d]: ctr1.ctr_total_return > st.avg_limit AND ctr1.ctr_reason_sk BETWEEN 28 AND 31 AND s.s_state IN ('...
    ORDER BY: c_customer_id

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**PATCH PLAN:**
```json
{
  "plan_id": "gold_postgres_early_filter_decorrelate",
  "dialect": "postgres",
  "description": "Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.",
  "preconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "postconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "steps": [
    {
      "step_id": "s1",
      "op": "insert_cte",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "cte_name": "store_thresholds",
        "cte_query_sql": "SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS avg_limit FROM customer_total_return GROUP BY ctr_store_sk"
      },
      "description": "Insert CTE 'store_thresholds' for pre-aggregated computation"
    },
    {
      "step_id": "s2",
      "op": "replace_block_with_cte_pair",
      "target": {
        "by_node_id": "S0",
        "by_label": "customer_total_return"
      },
      "payload": {
        "sql_fragment": "customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, sr_reason_sk AS ctr_reason_sk, SUM(SR_REFUNDED_CASH) AS ctr_total_return FROM store_returns JOIN date_dim ON sr_returned_date_sk = d_date_sk JOIN store ON sr_store_sk = s_store_sk WHERE d_year = 2001 AND s_state IN ('MI', 'NC', 'WI') AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295 GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk)"
      },
      "description": "Replace CTE 'customer_total_return' body with optimized version"
    },
    {
      "step_id": "s3",
      "op": "replace_from",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "from_sql": "customer_total_return AS ctr1 JOIN store_thresholds AS st ON ctr1.ctr_store_sk = st.ctr_store_sk JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk JOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk JOIN store AS s ON ctr1.ctr_store_sk = s.s_store_sk"
      },
      "description": "Replace comma-join FROM with explicit JOINs"
    },
    {
      "step_id": "s4",
      "op": "replace_where_predicate",
      "target": {
        "by_node_id": "S0",
        "by_anchor_hash": "6eef64f061a2aa84"
      },
      "payload": {
        "expr_sql": "ctr1.ctr_total_return > st.avg_limit AND ctr1.ctr_reason_sk BETWEEN 28 AND 31 AND s.s_state IN ('MI', 'NC', 'WI') AND cd_marital_status = 'W' AND cd_education_status IN ('4 yr Degree', 'College') AND cd_gender = 'M' AND c_birth_month = 5 AND c_birth_year BETWEEN 1950 AND 1956"
      },
      "description": "Replace WHERE predicate with optimized version"
    }
  ]
}
```



### Family C: Aggregation Pushdown (Minimize Rows Touched)
**Description**: Aggregate before expensive joins when GROUP BY keys ⊇ join keys, reduce intermediate sizes
**Speedup Range**: 1.3–15.3x (~5% of all wins (high variance))
**Use When**:
  1. GROUP BY happens after large joins
  2. GROUP BY keys are subset of join keys
  3. Intermediate result size >> final result size

**Gold Example**: None yet observed on PostgreSQL. Strategy is valid if EXPLAIN evidence supports it.



### Family D: Set Operation Optimization (Sets Over Loops)
**Description**: Replace INTERSECT/UNION-based patterns with EXISTS/NOT EXISTS, avoid full materialization
**Speedup Range**: 1.7–2.7x (~8% of all wins)
**Use When**:
  1. INTERSECT patterns between large sets
  2. UNION ALL with duplicate elimination
  3. Set operations materializing full intermediate results

**Gold Example**: None yet observed on PostgreSQL. Strategy is valid if EXPLAIN evidence supports it.



### Family E: Materialization / Prefetch (Don't Repeat Work)
**Description**: Extract repeated scans or pre-compute intermediate results for reuse across multiple consumers
**Speedup Range**: 1.3–6.2x (~18% of all wins)
**Use When**:
  1. Repeated scans of same table with different filters
  2. Dimension filters applied independently multiple times
  3. CTE referenced multiple times with implicit re-evaluation

**Gold Example**: `pg_date_consolidation` (3.10x)

**BEFORE (slow):**
```sql
select 
 i_item_id, i_item_desc, s_store_id, s_store_name,
 sum(ss_net_profit) as store_sales_profit,
 sum(sr_net_loss) as store_returns_loss,
 sum(cs_net_profit) as catalog_sales_profit
from
 store_sales, store_returns, catalog_sales,
 date_dim d1, date_dim d2, date_dim d3,
 store, item
where
 d1.d_moy = 5 and d1.d_year = 1999
 and d1.d_date_sk = ss_sold_date_sk
 and i_item_sk = ss_item_sk
 and s_store_sk = ss_store_sk
 and ss_customer_sk = sr_customer_sk
 and ss_item_sk = sr_item_sk
 and ss_ticket_number = sr_ticket_number
 and sr_returned_date_sk = d2.d_date_sk
 and d2.d_moy between 5 and 7
 and d2.d_year = 1999
 and sr_customer_sk = cs_bill_customer_sk
 and sr_item_sk = cs_item_sk
 and cs_sold_date_sk = d3.d_date_sk
 and d3.d_moy between 5 and 7
 and d3.d_year = 1999
group by i_item_id, i_item_desc, s_store_id, s_store_name
order by i_item_id, i_item_desc, s_store_id, s_store_name
limit 100;
```

**AFTER (fast):**
```sql
WITH all_dates AS (
  SELECT d_date_sk, d_moy
  FROM date_dim
  WHERE d_year = 1999 AND ((d_moy = 5) OR (d_moy BETWEEN 5 AND 7))
)
SELECT i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name,
       SUM(ss.ss_net_profit) AS store_sales_profit,
       SUM(sr.sr_net_loss) AS store_returns_loss,
       SUM(cs.cs_net_profit) AS catalog_sales_profit
FROM store_sales ss
INNER JOIN all_dates d1 ON ss.ss_sold_date_sk = d1.d_date_sk AND d1.d_moy = 5
INNER JOIN store s ON s.s_store_sk = ss.ss_store_sk
INNER JOIN item i ON i.i_item_sk = ss.ss_item_sk
INNER JOIN store_returns sr
  ON ss.ss_customer_sk = sr.sr_customer_sk
  AND ss.ss_item_sk = sr.sr_item_sk
  AND ss.ss_ticket_number = sr.sr_ticket_number
INNER JOIN all_dates d2 ON sr.sr_returned_date_sk = d2.d_date_sk AND d2.d_moy BETWEEN 5 AND 7
INNER JOIN catalog_sales cs
  ON sr.sr_customer_sk = cs.cs_bill_customer_sk
  AND sr.sr_item_sk = cs.cs_item_sk
INNER JOIN all_dates d3 ON cs.cs_sold_date_sk = d3.d_date_sk AND d3.d_moy BETWEEN 5 AND 7
GROUP BY i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name
ORDER BY i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name
LIMIT 100
```

**IR BEFORE:**
```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: store_sales, store_returns, catalog_sales, date_dim d1, date_dim d2, date_dim d3, store, item
    WHERE [05457d75ca8c9d26]: d1.d_moy = 5 AND d1.d_year = 1999 AND d1.d_date_sk = ss_sold_date_sk AND i_item_sk = ss_item_sk A...
    GROUP BY: i_item_id, i_item_desc, s_store_id, s_store_name
    ORDER BY: i_item_id, i_item_desc, s_store_id, s_store_name

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**IR TARGET:**
```
S0 [SELECT]
  CTE: all_dates  (via CTE_Q_S0_all_dates)
    FROM: date_dim
    WHERE [5aa6dc3dedbc3bb0]: d_year = 1999 AND ((d_moy = 5) OR (d_moy BETWEEN 5 AND 7))
  MAIN QUERY (via Q_S0)
    FROM: store_sales ss, all_dates d1, store s, item i, store_returns sr, all_dates d2, catalog_sales cs, all_dates d3
    GROUP BY: i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name
    ORDER BY: i.i_item_id, i.i_item_desc, s.s_store_id, s.s_store_name

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**PATCH PLAN:**
```json
{
  "plan_id": "gold_postgres_pg_date_consolidation",
  "dialect": "postgres",
  "description": "When a query references date_dim 3+ times (d1 for sold, d2 for returned, d3 for shipped) with overlapping year/month filters, consolidate all date filters into a single all_dates CTE. Each fact table join references this shared CTE with specific MOY conditions, eliminating redundant date_dim scans.",
  "preconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "postconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "steps": [
    {
      "step_id": "s1",
      "op": "insert_cte",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "cte_name": "all_dates",
        "cte_query_sql": "SELECT d_date_sk, d_moy FROM date_dim WHERE d_year = 1999 AND ((d_moy = 5) OR (d_moy BETWEEN 5 AND 7))"
      },
      "description": "Insert CTE 'all_dates' for date dimension filtering"
    },
    {
      "step_id": "s2",
      "op": "replace_from",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "from_sql": "store_sales AS ss INNER JOIN all_dates AS d1 ON ss.ss_sold_date_sk = d1.d_date_sk AND d1.d_moy = 5 INNER JOIN store AS s ON s.s_store_sk = ss.ss_store_sk INNER JOIN item AS i ON i.i_item_sk = ss.ss_item_sk INNER JOIN store_returns AS sr ON ss.ss_customer_sk = sr.sr_customer_sk AND ss.ss_item_sk = sr.sr_item_sk AND ss.ss_ticket_number = sr.sr_ticket_number INNER JOIN all_dates AS d2 ON sr.sr_returned_date_sk = d2.d_date_sk AND d2.d_moy BETWEEN 5 AND 7 INNER JOIN catalog_sales AS cs ON sr.sr_customer_sk = cs.cs_bill_customer_sk AND sr.sr_item_sk = cs.cs_item_sk INNER JOIN all_dates AS d3 ON cs.cs_sold_date_sk = d3.d_date_sk AND d3.d_moy BETWEEN 5 AND 7"
      },
      "description": "Replace comma-join FROM with explicit JOINs"
    },
    {
      "step_id": "s3",
      "op": "delete_expr_subtree",
      "target": {
        "by_node_id": "S0",
        "by_anchor_hash": "05457d75ca8c9d26"
      },
      "description": "Remove WHERE clause (conditions moved to CTEs)"
    }
  ]
}
```



### Family F: Join Transform (Right Shape First)
**Description**: Restructure join topology: convert comma joins to explicit INNER JOIN, optimize join order, eliminate self-joins via single-pass aggregation
**Speedup Range**: 1.8–8.6x (~19% of all wins)
**Use When**:
  1. Comma-separated joins (implicit cross joins) in FROM clause
  2. Self-joins scanning same table multiple times
  3. Dimension-fact join order suboptimal for predicate pushdown

**Gold Example**: `pg_date_cte_explicit_join` (2.28x)

**BEFORE (slow):**
```sql
select 
   substring(w_warehouse_name,1,20)
  ,sm_type
  ,cc_name
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30 ) then 1 else 0 end)  as "30 days"
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 30) and
                 (cs_ship_date_sk - cs_sold_date_sk <= 60) then 1 else 0 end )  as "31-60 days"
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 60) and
                 (cs_ship_date_sk - cs_sold_date_sk <= 90) then 1 else 0 end)  as "61-90 days"
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 90) and
                 (cs_ship_date_sk - cs_sold_date_sk <= 120) then 1 else 0 end)  as "91-120 days"
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk  > 120) then 1 else 0 end)  as ">120 days"
from
   catalog_sales
  ,warehouse
  ,ship_mode
  ,call_center
  ,date_dim
where
d_month_seq between 1193 and 1193 + 23
and cs_ship_date_sk   = d_date_sk
and cs_warehouse_sk   = w_warehouse_sk
and cs_ship_mode_sk   = sm_ship_mode_sk
and cs_call_center_sk = cc_call_center_sk
and cs_list_price between 271 and 300
and sm_type = 'REGULAR'
and cc_class = 'small'
and w_gmt_offset = -5
group by
   substring(w_warehouse_name,1,20)
  ,sm_type
  ,cc_name
order by substring(w_warehouse_name,1,20)
        ,sm_type
        ,cc_name
limit 100;
```

**AFTER (fast):**
```sql
WITH filtered_dates AS (SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1193 + 23)
SELECT SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name, SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS "30 days", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 30) AND (cs_ship_date_sk - cs_sold_date_sk <= 60) THEN 1 ELSE 0 END) AS "31-60 days", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 60) AND (cs_ship_date_sk - cs_sold_date_sk <= 90) THEN 1 ELSE 0 END) AS "61-90 days", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 90) AND (cs_ship_date_sk - cs_sold_date_sk <= 120) THEN 1 ELSE 0 END) AS "91-120 days", SUM(CASE WHEN (cs_ship_date_sk - cs_sold_date_sk > 120) THEN 1 ELSE 0 END) AS ">120 days" FROM catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk WHERE cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5 GROUP BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name ORDER BY SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name LIMIT 100
```

**IR BEFORE:**
```
S0 [SELECT]
  MAIN QUERY (via Q_S0)
    FROM: catalog_sales, warehouse, ship_mode, call_center, date_dim
    WHERE [c20744375cf92d6b]: d_month_seq BETWEEN 1193 AND 1193 + 23 AND cs_ship_date_sk = d_date_sk AND cs_warehouse_sk = w_wa...
    GROUP BY: SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name
    ORDER BY: SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**IR TARGET:**
```
S0 [SELECT]
  CTE: filtered_dates  (via CTE_Q_S0_filtered_dates)
    FROM: date_dim
    WHERE [880a037892a9dd1f]: d_month_seq BETWEEN 1193 AND 1193 + 23
  MAIN QUERY (via Q_S0)
    FROM: catalog_sales, filtered_dates, warehouse, ship_mode, call_center
    WHERE [956f7c02cb3b2c44]: cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset...
    GROUP BY: SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name
    ORDER BY: SUBSTRING(w_warehouse_name FROM 1 FOR 20), sm_type, cc_name

Patch operations: insert_cte, replace_expr_subtree, replace_where_predicate, replace_from, delete_expr_subtree
Target: by_node_id (statement, e.g. "S0") + by_anchor_hash (expression)
```

**PATCH PLAN:**
```json
{
  "plan_id": "gold_postgres_pg_date_cte_explicit_join",
  "dialect": "postgres",
  "description": "Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.",
  "preconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "postconditions": [
    {
      "kind": "parse_ok"
    }
  ],
  "steps": [
    {
      "step_id": "s1",
      "op": "insert_cte",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "cte_name": "filtered_dates",
        "cte_query_sql": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1193 AND 1193 + 23"
      },
      "description": "Insert CTE 'filtered_dates' for date dimension filtering"
    },
    {
      "step_id": "s2",
      "op": "replace_from",
      "target": {
        "by_node_id": "S0"
      },
      "payload": {
        "from_sql": "catalog_sales JOIN filtered_dates ON cs_ship_date_sk = d_date_sk JOIN warehouse ON cs_warehouse_sk = w_warehouse_sk JOIN ship_mode ON cs_ship_mode_sk = sm_ship_mode_sk JOIN call_center ON cs_call_center_sk = cc_call_center_sk"
      },
      "description": "Replace comma-join FROM with explicit JOINs"
    },
    {
      "step_id": "s3",
      "op": "replace_where_predicate",
      "target": {
        "by_node_id": "S0",
        "by_anchor_hash": "c20744375cf92d6b"
      },
      "payload": {
        "expr_sql": "cs_list_price BETWEEN 271 AND 300 AND sm_type = 'REGULAR' AND cc_class = 'small' AND w_gmt_offset = -5"
      },
      "description": "Replace WHERE predicate with optimized version"
    }
  ]
}
```



## Engine Playbook (POSTGRES)

# PostgreSQL Rewrite Playbook
# DSB SF10 field intelligence

## ENGINE STRENGTHS — do NOT rewrite these patterns

1. **BITMAP_OR_SCAN**: Multi-branch ORs on indexed columns handled via bitmap combination in one scan. Splitting ORs to UNION ALL is lethal (0.21x observed).
2. **EXISTS semi-join**: Uses early termination. Converting a single EXISTS to a materializing CTE caused 0.50x, 0.75x — semi-join destroyed. **Exception**: When 3+ correlated NOT EXISTS channels scan different fact tables (e.g., Q069 17.48x), pre-materializing each channel into DISTINCT CTEs with LEFT JOIN IS NULL anti-pattern eliminates repeated Materialize node re-scans.
3. **INNER JOIN reordering**: Freely reorders INNER JOINs by selectivity estimates. Do NOT manually restructure INNER JOIN order.
4. **Index-only scan**: Reads only index when covering all requested columns. Small dimension lookups may not need CTEs.
5. **Parallel query execution**: Large scans and aggregations parallelized across workers. CTEs block parallelism (materialization is single-threaded).
6. **JIT compilation**: JIT-compiles complex expressions for long-running queries (>100ms).

## GLOBAL GUARDS

1. OR conditions on indexed columns → never split to UNION ALL (0.21x observed)
2. Single EXISTS → never materialize into CTE (0.50x, 0.75x — semi-join destroyed). Exception: 3+ NOT EXISTS channels → pre-materialize each into DISTINCT CTE + LEFT JOIN IS NULL (17.48x observed)
3. INNER JOIN order → never restructure (optimizer handles reordering)
4. Small dimensions (< 10K rows) → index-only scan may be faster than CTE
5. Baseline < 100ms → skip CTE-based rewrites (overhead exceeds savings)
6. CTEs block parallel execution — only use when benefit outweighs parallelism loss
7. Use AS MATERIALIZED when CTE must not be inlined (decorrelation, shared scans)
8. Preserve efficient existing CTEs — don't decompose working patterns
9. Verify NULL semantics in NOT IN conversions
10. ROLLUP/window in same query → CTE may prevent pushdown optimizations
11. Never inline a large UNION CTE — re-execution multiplied per reference (0.16x — 6 fact scans re-executed)
12. Max 2 cascading fact-table CTE chains — deeper chains block parallelism
13. EXPLAIN cost gaps ≠ runtime gains for config tuning — 6 false positives caught (up to 84% EXPLAIN gap → 0% runtime). Always 3-race validate config changes.

---

## DOCUMENTED CASES

Cases ordered by safety (zero-regression cases first, then by decreasing risk).

**P6: Multiple Date_dim Aliases with Overlapping Filters** (SMALLEST SET FIRST) — ZERO REGRESSIONS

| Aspect | Detail |
|---|---|
| Detect | 2+ date_dim aliases in FROM with similar year/month_seq/moy predicates. |
| Gates | 2+ date_dim instances with overlapping date predicates. Selectivity < 1% of date_dim (always true for year+month). Combine with explicit JOIN conversion when comma joins present. |
| Treatments | date_consolidation (1 win, 3.10x), date_cte_isolate (3 wins + 7 improved). Apply first — date CTE is smallest, most reliable transform. |
| Failures | None observed. |

**P3: Same Fact+Dimension Scan Repeated Across Subquery Boundaries** (DON'T REPEAT WORK) — ZERO REGRESSIONS

| Aspect | Detail |
|---|---|
| Detect | Identical scan subtrees appearing 2+ times in EXPLAIN with similar costs. Same fact table joined to same dimensions in multiple subqueries, or self-join with different GROUP BY granularity. |
| Gates | 2+ subqueries scanning same fact table with identical filters. COUNT/SUM/AVG/MIN/MAX only (not STDDEV/PERCENTILE). Self-join → consolidate into single CTE. 3 channel scans → single_pass_aggregation. |
| Treatments | single_pass_aggregation (1 win, 1.98x), self_join_pivot (1 win, 1.79x) |
| Failures | None observed. |

**P4: Non-Equi Join Without Prefiltering** (MINIMIZE ROWS TOUCHED) — ZERO REGRESSIONS

| Aspect | Detail |
|---|---|
| Detect | Expensive non-equi join (BETWEEN, <, >) in EXPLAIN with large inputs. Neither side filtered. |
| Gates | Non-equi join predicate exists. Both join inputs > 10K rows. At least one side has selective dimension filter available. |
| Treatments | pg_materialized_dimension_fact_prefilter (1 win, 12.07x). Apply after P1 and P6. |
| Failures | None observed. |

**P1: Comma Join Confusing Cardinality Estimation** (ARM THE OPTIMIZER)

| Aspect | Detail |
|---|---|
| Detect | FROM t1, t2, t3 WHERE t1.key = t2.key (comma joins, no explicit JOIN). Hash/nested-loop join with poor row estimates in EXPLAIN. |
| Gates | Multiple tables in comma-separated FROM with equi-join predicates. Dimension filters available. 1-2 fact tables only (3+ → join order lock). Max 3-4 dimension CTEs. Stop if all JOINs already explicit → skip to P6/P7. |
| Treatments | pg_date_cte_explicit_join (4 wins, 2.1x avg), pg_dimension_prefetch_star (3 wins, 2.8x avg), explicit_join_materialized (2 wins, 5.9x avg) |
| Failures | 0.88x (explicit join overhead on simple query) |

**P5: Set Operation Materializing Full Result Sets** (SETS OVER LOOPS)

| Aspect | Detail |
|---|---|
| Detect | INTERSECT/EXCEPT between large result sets. Correlated EXISTS on 3+ channels (store, web, catalog). |
| Gates | INTERSECT with 10K+ rows → convert to EXISTS. Correlated NOT EXISTS on 3+ channels → materialize channel sets. Simple EXISTS (single channel) → KEEP EXISTS. NOT EXISTS already hash anti-join in EXPLAIN → STOP. |
| Treatments | intersect_to_exists (1 win, 1.78x), set_operation_materialization (1 win, 17.48x) |
| Failures | 0.75x (over-materialized date CTE in EXISTS path) |

**P2: Correlated Subquery Executing Per Outer Row** (SETS OVER LOOPS) — HIGHEST IMPACT

| Aspect | Detail |
|---|---|
| Detect | Nested loop in EXPLAIN, inner side re-executes aggregate per outer row. SQL: WHERE col > (SELECT AGG(...) FROM ... WHERE outer.key = inner.key). If EXPLAIN shows hash join on correlation key → already decorrelated → STOP. |
| Gates | Correlated scalar subquery with aggregate (AVG, SUM, COUNT). NOT EXISTS: NEVER decorrelate (destroys semi-join, 0.50x). Inner = outer table → extract common scan to shared CTE. ALWAYS use AS MATERIALIZED. 1-2 fact tables safe, 3+ → STOP. |
| Treatments | inline_decorrelate_materialized (3 wins, avg 500x), decorrelate (8 wins, avg 3.2x), shared_scan + decorrelate (2 wins, avg 7000x) |
| Failures | 0.51x (multi-fact join lock), 0.75x (EXISTS materialized) |

**P7: Multi-Dimension Prefetch for Star-Schema Aggregation** (SMALLEST SET FIRST) — CAUTION

| Aspect | Detail |
|---|---|
| Detect | Large fact table scan followed by late dimension filter in EXPLAIN. Star schema with 3+ dimension filters in WHERE. |
| Gates | 3+ selective dimension filters, each < 10% of dimension table. Single fact table, NOT self-join or multi-fact. Stop if self-join → use P3 (0.25x). Stop if multi-fact → join order lock (0.51x). |
| Treatments | pg_dimension_prefetch_star (2 wins, 2.5x avg), multi_dimension_prefetch (1 win, 2.50x) |
| Failures | 0.25x (self-join), 0.51x (multi-fact) |

---

## PRUNING GUIDE

| Plan shows | Skip |
|---|---|
| No comma joins (all explicit JOINs) | P1 (comma join fix) |
| No nested loops on large tables | P2 (decorrelation) |
| Each table appears once | P3 (repeated scans) |
| No non-equi joins (BETWEEN, <, >) | P4 (non-equi prefilter) |
| No INTERSECT/EXCEPT and no correlated multi-channel EXISTS | P5 (set operation) |
| Single date_dim reference | P6 (date consolidation) |
| No GROUP BY or only 1 dimension filter | P7 (multi-dim prefetch) |
| Baseline < 100ms | ALL CTE-based transforms |
| Bitmap OR scan present | OR→UNION rewrites |
| Parallel workers active + query fast | CTE-heavy transforms |

## REGRESSION REGISTRY

| Severity | Transform | Result | Root cause |
|----------|-----------|--------|------------|
| CATASTROPHIC | cte_inlining | 0.16x | Inlined large UNION CTE → 6 fact scans re-executed 2x each |
| SEVERE | multi_dim_prefetch | 0.15x | CTEs blocked date-predicate pushdown on 90-day interval join |
| SEVERE | dimension_prefetch | 0.25x | Applied star-schema pattern to 6-way self-join → parallelism destroyed |
| MAJOR | cte_materialization | 0.30x | Multi-scan CTE overhead similar to above cte_inlining pattern |
| MAJOR | early_fact_filtering | 0.51x | Disabled nestloop too aggressively + DISTINCT forced hash spill |
| MAJOR | date_cte_prefetch | 0.75x | Over-materialized date CTE in EXISTS path → destroyed semi-join |
| MODERATE | explicit_join | 0.88x | Explicit join conversion overhead exceeded benefit on simple query |
| CATASTROPHIC | forced_parallelism (C3) | 7.34x regr | Worker startup + coordination overhead on 244ms query. NEVER force par on < 500ms |
| CATASTROPHIC | enable_nestloop_off (C5) | -1454% | NL was correct plan. Disabling forced catastrophic merge/hash on unsuitable query |
| MAJOR | geqo_off | -254% | Exhaustive planner found "better" cost plan on 19 joins but cardinality errors made it catastrophic |
| MAJOR | par4_without_wm | -15.3% | Parallelism without sufficient work_mem causes hash spill under parallel execution |


Output exactly **2 patch plans** as a JSON array.

### Patch Operations

| Op | Description | Payload |
|----|-------------|---------|
| insert_cte | Add a new CTE to the WITH clause | cte_name, cte_query_sql |
| replace_from | Replace the FROM clause | from_sql |
| replace_where_predicate | Replace the WHERE clause | expr_sql |
| replace_body | Replace entire query body (SELECT, FROM, WHERE, GROUP BY) | sql_fragment |
| replace_expr_subtree | Replace a specific expression | expr_sql (+ by_anchor_hash) |
| delete_expr_subtree | Remove a specific expression | (target only, no payload) |

### Output Format

```json
[
  {
    "plan_id": "r1",
    "family": "B",
    "transform": "decorrelate",
    "hypothesis": "Correlated subquery re-scans store_sales per row...",
    "target_ir": "S0 [SELECT]\n  CTE: thresholds (GROUP BY item_sk)...",
    "dialect": "postgres",
    "steps": [
      {"step_id": "s1", "op": "insert_cte", "target": {"by_node_id": "S0"}, "payload": {"cte_name": "...", "cte_query_sql": "..."}},
      {"step_id": "s2", "op": "replace_from", "target": {"by_node_id": "S0"}, "payload": {"from_sql": "..."}}
    ]
  },
  {
    "plan_id": "r2",
    "family": "A",
    "transform": "early_filter",
    "hypothesis": "Late date filter after full scan...",
    "target_ir": "...",
    "dialect": "postgres",
    "steps": [...]
  }
]
```

### Semantic Guards (MUST preserve)
- All WHERE/HAVING/ON conditions preserved exactly
- All literal values unchanged (35*0.01 stays as 35*0.01, NOT 0.35)
- Column names, aliases, ORDER BY, and LIMIT exactly
- Do NOT add new filter conditions
- Same row count as original query

### Rules
- Output exactly 2 plans — each targeting a DIFFERENT optimization strategy
- Each plan must have unique plan_id, family, transform
- Each step's SQL in payloads must be complete, executable (no ellipsis)
- Target all steps at by_node_id: "S0" (the main statement)
- Include hypothesis explaining WHY this optimization should help (cite EXPLAIN evidence)

Output ONLY the JSON array (no markdown fences, no explanation before/after):