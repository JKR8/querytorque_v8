# DuckDB Query Optimization Prompt

You are an expert DuckDB query optimizer specializing in analytical workloads. Your task is to rewrite the provided query to achieve better execution performance while preserving exact semantic equivalence.

**Critical context:** DuckDB is a columnar, vectorized, in-process analytical database. Its optimizer is significantly more advanced than traditional row-store optimizers in several areas â€” particularly automatic subquery decorrelation and filter pushdown. However, it has distinct weaknesses around join ordering with skewed data, cardinality estimation without HyperLogLog statistics, grouped TopN recognition, and cyclic/non-equi join patterns. Your rewrites must target DuckDB's actual blind spots, not generic SQL anti-patterns that DuckDB already handles automatically.

---

## Input Context

### DuckDB Version & Configuration
```
v1.4.4
```

**Relevant Settings:**
```
memory_limit = 25.0 GiB
preserve_insertion_order = true
temp_directory = /mnt/d/TPC-DS/tpcds_sf10.duckdb.tmp
threads = 12
```

### Data Source Format
```
Native DuckDB tables (has HyperLogLog stats)
```

---

### Database Schema

```sql
-- store_returns: 2,877,532 rows
-- Columns: sr_returned_date_sk INTEGER, sr_return_time_sk INTEGER, sr_item_sk INTEGER, sr_customer_sk INTEGER, sr_cdemo_sk INTEGER, sr_hdemo_sk INTEGER, sr_addr_sk INTEGER, sr_store_sk INTEGER

-- date_dim: 73,049 rows
-- Columns: d_date_sk INTEGER, d_date_id VARCHAR, d_date DATE, d_month_seq INTEGER, d_week_seq INTEGER, d_quarter_seq INTEGER, d_year INTEGER, d_dow INTEGER

-- store: 102 rows
-- Columns: s_store_sk INTEGER, s_store_id VARCHAR, s_rec_start_date DATE, s_rec_end_date DATE, s_closed_date_sk INTEGER, s_store_name VARCHAR, s_number_employees INTEGER, s_floor_space INTEGER

-- customer: 500,000 rows
-- Columns: c_customer_sk INTEGER, c_customer_id VARCHAR, c_current_cdemo_sk INTEGER, c_current_hdemo_sk INTEGER, c_current_addr_sk INTEGER, c_first_shipto_date_sk INTEGER, c_first_sales_date_sk INTEGER, c_salutation VARCHAR
```

---

### Table Statistics

```
Table: store_returns
  Row count: 2,877,532
  Storage: native DuckDB

Table: date_dim
  Row count: 73,049
  Storage: native DuckDB

Table: store
  Row count: 102
  Storage: native DuckDB

Table: customer
  Row count: 500,000
  Storage: native DuckDB
```

---

### Original Query

```sql
-- start query 1 in stream 0 using template query1.tpl
with customer_total_return as
(select sr_customer_sk as ctr_customer_sk
,sr_store_sk as ctr_store_sk
,sum(SR_FEE) as ctr_total_return
from store_returns
,date_dim
where sr_returned_date_sk = d_date_sk
and d_year =2000
group by sr_customer_sk
,sr_store_sk)
 select c_customer_id
from customer_total_return ctr1
,store
,customer
where ctr1.ctr_total_return > (select avg(ctr_total_return)*1.2
from customer_total_return ctr2
where ctr1.ctr_store_sk = ctr2.ctr_store_sk)
and s_store_sk = ctr1.ctr_store_sk
and s_state = 'SD'
and ctr1.ctr_customer_sk = c_customer_sk
order by c_customer_id
 LIMIT 100;

-- end query 1 in stream 0 using template query1.tpl

```

---

### Execution Plan Analysis

**Timing:** 1195ms total | 100 rows returned | 37,103,535 rows scanned

**Operators (by cost):**
```
 69% |   819.3ms |    557,705 rows | SEQ_SCAN on store_returns
 14% |   168.9ms |    539,331 rows | HASH_GROUP_BY
  4% |    49.7ms |    499,996 rows | SEQ_SCAN on customer
  2% |    28.9ms |    157,785 rows | HASH_JOIN
  2% |    20.7ms |     61,974 rows | FILTER
  1% |    17.9ms |        100 rows | TOP_N
```

**Cardinality Misestimates (optimizer blind spots):**
- HASH_GROUP_BY: expected 23,035 got 539,331 (23x off)
- HASH_JOIN: expected 1 got 157,785 (157785x off)
- FILTER: expected 1 got 61,974 (61974x off)

**âš ï¸ Bottleneck:** SEQ_SCAN on **store_returns** consumes 69% of query time

---

## CONSTRAINTS (Learned from Benchmark Failures)

The following constraints are MANDATORY based on observed failures:

### ðŸš¨ LITERAL_PRESERVATION (CRITICAL)
CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

### âš ï¸ OR_TO_UNION_LIMIT (HIGH)
CAUTION with ORâ†’UNION: Only split OR conditions into UNION ALL when there are â‰¤3 simple branches AND they have different access patterns. If you have nested ORs (e.g., 3 conditions Ã— 3 values = 9 combinations), DO NOT expand them - keep the original OR structure. DuckDB handles OR predicates efficiently. Over-splitting causes multiple scans of fact tables and severe regressions (0.23x-0.41x observed). When in doubt, preserve the original OR structure.

---

## What DuckDB Already Handles Automatically

**Do NOT rewrite for these â€” DuckDB's optimizer handles them:**

1. **Correlated subquery decorrelation**: DuckDB uses a state-of-the-art algorithm (Unnesting Arbitrary Queries, Neumann & Kemper) to automatically decorrelate ALL subqueries into efficient joins. Unlike PostgreSQL, correlated subqueries do NOT imply performance degradation in DuckDB. Do not manually convert correlated subqueries to joins unless the EXPLAIN ANALYZE shows the decorrelation produced a suboptimal plan.

2. **NOT IN / NOT EXISTS equivalence**: DuckDB automatically detects anti-join patterns from both NOT IN and NOT EXISTS. No manual rewrite needed.

3. **Filter pushdown/pull-up**: DuckDB aggressively pushes filters through joins, projections, and aggregations â€” and pulls them up across equivalence sets to duplicate filters on both sides of joins.

4. **Join filter pushdown**: DuckDB automatically creates min/max range filters from the build side of hash joins and pushes them into probe-side scans (up to 10Ã— improvement automatically).

5. **TopN optimization**: For simple `ORDER BY ... LIMIT N`, DuckDB avoids full sorting and uses a heap-based top-N algorithm automatically.

6. **Expression simplification**: Constant folding, arithmetic simplification, CASE simplification, date part simplification, comparison normalization, and IN clause rewriting are all automatic.

7. **Common subexpression elimination**: DuckDB extracts and caches duplicate expressions across projections and filters.

8. **CTE inlining**: DuckDB automatically decides whether to materialize or inline CTEs (controlled by the MATERIALIZED_CTE optimizer rule).

---

## Optimization Analysis Framework

### Step 1: Identify Performance Bottlenecks

Examine the EXPLAIN ANALYZE output for these DuckDB-specific red flags:

1. **Cardinality misestimates (EC vs actual)**: Ratio > 10Ã— indicates optimizer made wrong decisions downstream. Especially common with:
   - Correlated/skewed distributions
   - Parquet sources without HyperLogLog stats
   - Multi-column filter correlations (DuckDB assumes independence)

2. **Nested loop joins**: Always a red flag in DuckDB's analytical context. Usually indicates the optimizer thinks one side is tiny (EC=1) when it's not. Look for `NESTED_LOOP_JOIN` operators.

3. **Large intermediate results in wrong join order**: If a `HASH_JOIN` produces far more rows than estimated, subsequent operators process too much data. Look for actual >> EC on join outputs.

4. **Missing filter pushdown**: A `SEQ_SCAN` or `PARQUET_SCAN` without filter info, followed by a separate `FILTER` operator, means the filter wasn't pushed into the scan.

5. **Full sort for grouped TopN**: DuckDB doesn't recognize `ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...) <= N` as a grouped top-N aggregation. It performs a full sort per group.

6. **Disk spilling**: If `temp_directory` is active and the query spills, look for operators that could be restructured to reduce memory pressure.

7. **Excessive column reads**: A scan reading many columns when only a few are needed downstream.

### Step 2: Match Bottlenecks to Rewrite Patterns

Apply these DuckDB-specific rewrites in priority order:

---

#### Pattern A: Join Order Override (10-1000Ã— improvement potential)

**The #1 issue for analytical queries on DuckDB.** DuckDB uses the DPccp algorithm for join ordering, but it relies on cardinality estimates. Correlated data causes systematic estimation errors, leading to catastrophic join orders.

**Detect**: Large actual-vs-estimated gaps on join outputs; fact table joined before selective dimension filters are applied.

```sql
-- Disable join optimizer and force explicit order
SET disabled_optimizers = 'join_order,build_side_probe_side';

-- Write joins in optimal order: most selective dimensions first,
-- fact tables last, ordered by increasing expected intermediate size
SELECT ...
FROM most_selective_dimension d1
JOIN fact_table f ON d1.sk = f.d1_sk
JOIN less_selective_dimension d2 ON f.d2_sk = d2.sk
WHERE d1.filter_col = 'value';

RESET disabled_optimizers;
```

**Star-schema guidance**: Optimal order is typically:
1. Most selective dimension table (fewest rows after WHERE filters)
2. Fact table joined to that dimension
3. Remaining dimensions joined in order of selectivity

**Alternative â€” pre-aggregate before joining:**
```sql
SELECT d.name, f_agg.total
FROM (
    SELECT dim_sk, SUM(amount) as total
    FROM fact_table
    WHERE date_sk BETWEEN 2451911 AND 2451941
    GROUP BY dim_sk
) f_agg
JOIN dimension d ON f_agg.dim_sk = d.sk
WHERE d.category = 'Electronics';
```

---

#### Pattern B: Sargable Predicate Rewriting (2-100Ã— improvement)

**Detect**: Filters that prevent zonemap/pushdown optimization â€” functions on columns, implicit casts, expressions.

```sql
-- BEFORE: Function on column blocks zonemap usage
SELECT * FROM sales WHERE YEAR(sale_date) = 2023;

-- AFTER: Range predicate enables zonemap skip
SELECT * FROM sales
WHERE sale_date >= DATE '2023-01-01'
  AND sale_date < DATE '2024-01-01';
```

```sql
-- BEFORE: CAST blocks pushdown
SELECT * FROM t WHERE col::VARCHAR = '100';

-- AFTER: Cast the literal, not the column
SELECT * FROM t WHERE col = 100;
```

---

#### Pattern C: Grouped TopN â†’ QUALIFY or arg_max (5-50Ã— improvement)

**Detect**: `ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...)` followed by filter `<= N`.

```sql
-- BEFORE: Full sort per group
SELECT * FROM (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) as rn
    FROM sales
) sub WHERE rn <= 3;

-- AFTER: Use QUALIFY
SELECT *
FROM sales
QUALIFY ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) <= 3;

-- AFTER (for top-1 only): Use arg_max
SELECT region, arg_max(customer_id, amount) as top_customer, max(amount) as top_amount
FROM sales
GROUP BY region;
```

---

#### Pattern D: Avoid Unnecessary DISTINCT / Redundant Grouping

**Detect**: `DISTINCT` on columns known to be unique from join structure.

```sql
-- BEFORE: DISTINCT forces hash aggregate
SELECT DISTINCT d.category, d.name
FROM dimension d
JOIN fact_table f ON d.sk = f.dim_sk
WHERE f.amount > 100;

-- AFTER: Use semi-join via EXISTS
SELECT d.category, d.name
FROM dimension d
WHERE EXISTS (SELECT 1 FROM fact_table f WHERE f.dim_sk = d.sk AND f.amount > 100);
```

---

#### Pattern E: UNION ALL â†’ Single Scan with Conditional Aggregation

**Detect**: Multiple scans of the same large table with different filters.

```sql
-- BEFORE: Multiple scans
SELECT 'cat_a' as cat, SUM(amount) FROM sales WHERE category = 'A'
UNION ALL
SELECT 'cat_b' as cat, SUM(amount) FROM sales WHERE category = 'B';

-- AFTER: Single scan with FILTER
SELECT
    SUM(amount) FILTER (WHERE category = 'A') as cat_a_total,
    SUM(amount) FILTER (WHERE category = 'B') as cat_b_total
FROM sales WHERE category IN ('A', 'B');
```

---

#### Pattern F: Exploit Data Ordering for Zonemap Effectiveness

**Detect**: Selective filters on columns that don't align with the table's sort order.

```sql
-- If table is sorted by date but frequently filtered by region:
CREATE TABLE sales_by_region AS
    SELECT * FROM sales ORDER BY region, sale_date;
```

---

#### Pattern G: Replace Non-Equi Joins with Range Joins or ASOF Joins

**Detect**: `NESTED_LOOP_JOIN` on non-equality conditions.

```sql
-- For temporal lookups, use ASOF JOIN (DuckDB-native):
SELECT s.*, p.price
FROM sales s
ASOF JOIN prices p
  ON s.product_id = p.product_id
  AND s.sale_date >= p.effective_date;
```

---

#### Pattern H: Leverage DuckDB-Specific Syntax

```sql
-- GROUP BY ALL: avoids redundant column listing
SELECT region, category, SUM(amount), COUNT(*)
FROM sales
GROUP BY ALL;

-- QUALIFY: filter window results without subquery
SELECT customer_id, amount, sale_date
FROM sales
QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_date DESC) = 1;

-- FILTER clause on aggregates
SELECT region,
    COUNT(*) FILTER (WHERE amount > 100) as high_value_count,
    AVG(amount) FILTER (WHERE category = 'A') as cat_a_avg
FROM sales
GROUP BY region;
```

---

#### Pattern I: Memory Pressure Management

```sql
-- Increase memory for hash-heavy queries
SET memory_limit = '16GB';

-- Disable preservation of insertion order
SET preserve_insertion_order = false;

-- Pre-aggregate before joining to reduce hash table sizes
SELECT d.name, f_agg.total
FROM (SELECT sk, SUM(amount) as total FROM fact GROUP BY sk) f_agg
JOIN dim d ON f_agg.sk = d.sk;
```

---

### Step 3: Verify Semantic Equivalence

For each rewrite, verify:
1. **NULL handling**: Manual rewrites must preserve NULL behavior
2. **Duplicate handling**: Converting DISTINCT to EXISTS changes semantics if not careful
3. **Sort stability**: DuckDB does not guarantee stable sorts
4. **Empty group handling**: `QUALIFY` returns no rows when no matches exist per group
5. **Type coercion**: Rewriting predicates may change type resolution

---

## Output Format

Provide your response in this structure:

### Analysis

**Identified Bottlenecks:**
1. [Bottleneck description with operator/EC-vs-actual from EXPLAIN ANALYZE]

**Root Causes:**
- [Why DuckDB's optimizer made this choice]

**DuckDB auto-optimizations already in effect:**
- [Note which optimizations should NOT be undone]

### Recommended Rewrites

**Rewrite 1: [Pattern Name]**

*Rationale:* [Why this rewrite helps]

*Original fragment:*
```sql
[relevant portion]
```

*Rewritten fragment:*
```sql
[optimized version]
```

*Expected improvement:* [Estimated speedup]

### Complete Optimized Query

```sql
-- Your complete optimized query here
```

### Verification Checklist

- [ ] NULL semantics preserved
- [ ] Duplicate rows handled correctly
- [ ] Empty group/no-match cases return same results
- [ ] All columns in SELECT/ORDER BY still accessible
- [ ] All literal values EXACTLY match original query
- [ ] OR conditions NOT over-expanded (â‰¤3 UNION branches max)

