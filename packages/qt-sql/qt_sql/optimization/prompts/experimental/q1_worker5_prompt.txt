You are a SQL optimizer. Rewrite the ENTIRE query for maximum performance.

## Adversarial Explore Mode
Be creative and aggressive. Try radical structural rewrites that the database
engine is unlikely to do automatically. Don't be constrained by incremental changes.

## Original Query
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         SUM(SR_FEE) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
  GROUP BY sr_customer_sk, sr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (
    SELECT avg(ctr_total_return)*1.2
    FROM customer_total_return ctr2
    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'SD'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id
LIMIT 100
```

## Full Execution Plan (EXPLAIN ANALYZE)
```
Operators by cost:
- SEQ_SCAN: 69.4% cost, 1,999,335 rows
- HASH_JOIN: 7.7% cost, 7,986 rows
- SEQ_SCAN: 4.7% cost, 56,138 rows
- HASH_GROUP_BY: 4.2% cost, 55,341 rows
- LEFT_DELIM_JOIN: 3.9% cost, 0 rows

Scans:
- store_returns: 865,743 rows (no filter)
- date_dim: 73,049 â†’ 366 rows (filtered)
- customer: 24,000,000 rows (no filter)

Joins:
- HASH_JOIN: store_returns x date_dim -> 56,138 rows
- LEFT_DELIM_JOIN: ctr1 x ctr2 -> 0 rows (correlated)
- HASH_JOIN: customer x ctr1 -> 7,986 rows
```

## Instructions
1. Analyze the execution plan bottlenecks
2. Rewrite the entire query for maximum performance
3. Try transforms like:
   - Decorrelating subqueries
   - Converting OR to UNION ALL
   - Pushing down filters aggressively
   - Materializing CTEs strategically
   - Reordering joins
   - Eliminating redundant operations

## Output Format
Return ONLY the complete optimized SQL query. No JSON. No explanation. Just SQL.

Example output:
WITH cte1 AS (
  SELECT ...
)
SELECT ...
FROM cte1
...
