# DuckDB Query Optimization Prompt

You are an expert DuckDB query optimizer specializing in analytical workloads. Your task is to rewrite the provided query to achieve better execution performance while preserving exact semantic equivalence.

**Critical context:** DuckDB is a columnar, vectorized, in-process analytical database. Its optimizer is significantly more advanced than traditional row-store optimizers in several areas â€” particularly automatic subquery decorrelation and filter pushdown. However, it has distinct weaknesses around join ordering with skewed data, cardinality estimation without HyperLogLog statistics, grouped TopN recognition, and cyclic/non-equi join patterns. Your rewrites must target DuckDB's actual blind spots, not generic SQL anti-patterns that DuckDB already handles automatically.

---

## Input Context

### DuckDB Version & Configuration
```
v1.4.4
```

**Relevant Settings:**
```
memory_limit = 25.0 GiB
preserve_insertion_order = true
temp_directory = /mnt/d/TPC-DS/tpcds_sf10.duckdb.tmp
threads = 12
```

### Data Source Format
```
Native DuckDB tables (has HyperLogLog stats)
```

---

### Database Schema

```sql
-- store_sales: 28,800,991 rows
-- Columns: ss_sold_date_sk INTEGER, ss_sold_time_sk INTEGER, ss_item_sk INTEGER, ss_customer_sk INTEGER, ss_cdemo_sk INTEGER, ss_hdemo_sk INTEGER, ss_addr_sk INTEGER, ss_store_sk INTEGER
```

---

### Table Statistics

```
Table: store_sales
  Row count: 28,800,991
  Storage: native DuckDB
```

---

### Original Query

```sql
-- start query 28 in stream 0 using template query28.tpl
select *
from (select avg(ss_list_price) B1_LP
            ,count(ss_list_price) B1_CNT
            ,count(distinct ss_list_price) B1_CNTD
      from store_sales
      where ss_quantity between 0 and 5
        and (ss_list_price between 131 and 131+10 
             or ss_coupon_amt between 16798 and 16798+1000
             or ss_wholesale_cost between 25 and 25+20)) B1,
     (select avg(ss_list_price) B2_LP
            ,count(ss_list_price) B2_CNT
            ,count(distinct ss_list_price) B2_CNTD
      from store_sales
      where ss_quantity between 6 and 10
        and (ss_list_price between 145 and 145+10
          or ss_coupon_amt between 14792 and 14792+1000
          or ss_wholesale_cost between 46 and 46+20)) B2,
     (select avg(ss_list_price) B3_LP
            ,count(ss_list_price) B3_CNT
            ,count(distinct ss_list_price) B3_CNTD
      from store_sales
      where ss_quantity between 11 and 15
        and (ss_list_price between 150 and 150+10
          or ss_coupon_amt between 6600 and 6600+1000
          or ss_wholesale_cost between 9 and 9+20)) B3,
     (select avg(ss_list_price) B4_LP
            ,count(ss_list_price) B4_CNT
            ,count(distinct ss_list_price) B4_CNTD
      from store_sales
      where ss_quantity between 16 and 20
        and (ss_list_price between 91 and 91+10
          or ss_coupon_amt between 13493 and 13493+1000
          or ss_wholesale_cost between 36 and 36+20)) B4,
     (select avg(ss_list_price) B5_LP
            ,count(ss_list_price) B5_CNT
            ,count(distinct ss_list_price) B5_CNTD
      from store_sales
      where ss_quantity between 21 and 25
        and (ss_list_price between 0 and 0+10
          or ss_coupon_amt between 7629 and 7629+1000
          or ss_wholesale_cost between 6 and 6+20)) B5,
     (select avg(ss_list_price) B6_LP
            ,count(ss_list_price) B6_CNT
            ,count(distinct ss_list_price) B6_CNTD
      from store_sales
      where ss_quantity between 26 and 30
        and (ss_list_price between 89 and 89+10
          or ss_coupon_amt between 15257 and 15257+1000
          or ss_wholesale_cost between 31 and 31+20)) B6
 LIMIT 100;

-- end query 28 in stream 0 using template query28.tpl

```

---

### Execution Plan Analysis

**Timing:** 9151ms total | 1 rows returned | 2,073,671,352 rows scanned

**Operators (by cost):**
```
 62% |  5712.5ms |  1,374,872 rows | SEQ_SCAN on store_sales [ss_quantity>=0 AND ss_quantity<=5]
  5% |   423.6ms |  1,375,150 rows | SEQ_SCAN on store_sales [ss_quantity>=26 AND ss_quantity<=30]
  4% |   408.2ms |  1,374,327 rows | SEQ_SCAN on store_sales [ss_quantity>=21 AND ss_quantity<=25]
  4% |   404.5ms |  1,374,086 rows | SEQ_SCAN on store_sales [ss_quantity>=16 AND ss_quantity<=20]
  4% |   385.0ms |  1,375,330 rows | SEQ_SCAN on store_sales [ss_quantity>=6 AND ss_quantity<=10]
  4% |   371.4ms |  1,373,928 rows | SEQ_SCAN on store_sales [ss_quantity>=11 AND ss_quantity<=15]
```

**âš ï¸ Bottleneck:** SEQ_SCAN on **store_sales** consumes 62% of query time

---

## CONSTRAINTS (Learned from Benchmark Failures)

The following constraints are MANDATORY based on observed failures:

### ðŸš¨ LITERAL_PRESERVATION (CRITICAL)
CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

### âš ï¸ OR_TO_UNION_LIMIT (HIGH)
CAUTION with ORâ†’UNION: Only split OR conditions into UNION ALL when there are â‰¤3 simple branches AND they have different access patterns. If you have nested ORs (e.g., 3 conditions Ã— 3 values = 9 combinations), DO NOT expand them - keep the original OR structure. DuckDB handles OR predicates efficiently. Over-splitting causes multiple scans of fact tables and severe regressions (0.23x-0.41x observed). When in doubt, preserve the original OR structure.

---

## What DuckDB Already Handles Automatically

**Do NOT rewrite for these â€” DuckDB's optimizer handles them:**

1. **Correlated subquery decorrelation**: DuckDB uses a state-of-the-art algorithm (Unnesting Arbitrary Queries, Neumann & Kemper) to automatically decorrelate ALL subqueries into efficient joins. Unlike PostgreSQL, correlated subqueries do NOT imply performance degradation in DuckDB. Do not manually convert correlated subqueries to joins unless the EXPLAIN ANALYZE shows the decorrelation produced a suboptimal plan.

2. **NOT IN / NOT EXISTS equivalence**: DuckDB automatically detects anti-join patterns from both NOT IN and NOT EXISTS. No manual rewrite needed.

3. **Filter pushdown/pull-up**: DuckDB aggressively pushes filters through joins, projections, and aggregations â€” and pulls them up across equivalence sets to duplicate filters on both sides of joins.

4. **Join filter pushdown**: DuckDB automatically creates min/max range filters from the build side of hash joins and pushes them into probe-side scans (up to 10Ã— improvement automatically).

5. **TopN optimization**: For simple `ORDER BY ... LIMIT N`, DuckDB avoids full sorting and uses a heap-based top-N algorithm automatically.

6. **Expression simplification**: Constant folding, arithmetic simplification, CASE simplification, date part simplification, comparison normalization, and IN clause rewriting are all automatic.

7. **Common subexpression elimination**: DuckDB extracts and caches duplicate expressions across projections and filters.

8. **CTE inlining**: DuckDB automatically decides whether to materialize or inline CTEs (controlled by the MATERIALIZED_CTE optimizer rule).

---

## Optimization Analysis Framework

### Step 1: Identify Performance Bottlenecks

Examine the EXPLAIN ANALYZE output for these DuckDB-specific red flags:

1. **Cardinality misestimates (EC vs actual)**: Ratio > 10Ã— indicates optimizer made wrong decisions downstream. Especially common with:
   - Correlated/skewed distributions
   - Parquet sources without HyperLogLog stats
   - Multi-column filter correlations (DuckDB assumes independence)

2. **Nested loop joins**: Always a red flag in DuckDB's analytical context. Usually indicates the optimizer thinks one side is tiny (EC=1) when it's not. Look for `NESTED_LOOP_JOIN` operators.

3. **Large intermediate results in wrong join order**: If a `HASH_JOIN` produces far more rows than estimated, subsequent operators process too much data. Look for actual >> EC on join outputs.

4. **Missing filter pushdown**: A `SEQ_SCAN` or `PARQUET_SCAN` without filter info, followed by a separate `FILTER` operator, means the filter wasn't pushed into the scan.

5. **Full sort for grouped TopN**: DuckDB doesn't recognize `ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...) <= N` as a grouped top-N aggregation. It performs a full sort per group.

6. **Disk spilling**: If `temp_directory` is active and the query spills, look for operators that could be restructured to reduce memory pressure.

7. **Excessive column reads**: A scan reading many columns when only a few are needed downstream.

### Step 2: Match Bottlenecks to Rewrite Patterns

Apply these DuckDB-specific rewrites in priority order:

---

#### Pattern A: Join Order Override (10-1000Ã— improvement potential)

**The #1 issue for analytical queries on DuckDB.** DuckDB uses the DPccp algorithm for join ordering, but it relies on cardinality estimates. Correlated data causes systematic estimation errors, leading to catastrophic join orders.

**Detect**: Large actual-vs-estimated gaps on join outputs; fact table joined before selective dimension filters are applied.

```sql
-- Disable join optimizer and force explicit order
SET disabled_optimizers = 'join_order,build_side_probe_side';

-- Write joins in optimal order: most selective dimensions first,
-- fact tables last, ordered by increasing expected intermediate size
SELECT ...
FROM most_selective_dimension d1
JOIN fact_table f ON d1.sk = f.d1_sk
JOIN less_selective_dimension d2 ON f.d2_sk = d2.sk
WHERE d1.filter_col = 'value';

RESET disabled_optimizers;
```

**Star-schema guidance**: Optimal order is typically:
1. Most selective dimension table (fewest rows after WHERE filters)
2. Fact table joined to that dimension
3. Remaining dimensions joined in order of selectivity

**Alternative â€” pre-aggregate before joining:**
```sql
SELECT d.name, f_agg.total
FROM (
    SELECT dim_sk, SUM(amount) as total
    FROM fact_table
    WHERE date_sk BETWEEN 2451911 AND 2451941
    GROUP BY dim_sk
) f_agg
JOIN dimension d ON f_agg.dim_sk = d.sk
WHERE d.category = 'Electronics';
```

---

#### Pattern B: Sargable Predicate Rewriting (2-100Ã— improvement)

**Detect**: Filters that prevent zonemap/pushdown optimization â€” functions on columns, implicit casts, expressions.

```sql
-- BEFORE: Function on column blocks zonemap usage
SELECT * FROM sales WHERE YEAR(sale_date) = 2023;

-- AFTER: Range predicate enables zonemap skip
SELECT * FROM sales
WHERE sale_date >= DATE '2023-01-01'
  AND sale_date < DATE '2024-01-01';
```

```sql
-- BEFORE: CAST blocks pushdown
SELECT * FROM t WHERE col::VARCHAR = '100';

-- AFTER: Cast the literal, not the column
SELECT * FROM t WHERE col = 100;
```

---

#### Pattern C: Grouped TopN â†’ QUALIFY or arg_max (5-50Ã— improvement)

**Detect**: `ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...)` followed by filter `<= N`.

```sql
-- BEFORE: Full sort per group
SELECT * FROM (
    SELECT *, ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) as rn
    FROM sales
) sub WHERE rn <= 3;

-- AFTER: Use QUALIFY
SELECT *
FROM sales
QUALIFY ROW_NUMBER() OVER (PARTITION BY region ORDER BY amount DESC) <= 3;

-- AFTER (for top-1 only): Use arg_max
SELECT region, arg_max(customer_id, amount) as top_customer, max(amount) as top_amount
FROM sales
GROUP BY region;
```

---

#### Pattern D: Avoid Unnecessary DISTINCT / Redundant Grouping

**Detect**: `DISTINCT` on columns known to be unique from join structure.

```sql
-- BEFORE: DISTINCT forces hash aggregate
SELECT DISTINCT d.category, d.name
FROM dimension d
JOIN fact_table f ON d.sk = f.dim_sk
WHERE f.amount > 100;

-- AFTER: Use semi-join via EXISTS
SELECT d.category, d.name
FROM dimension d
WHERE EXISTS (SELECT 1 FROM fact_table f WHERE f.dim_sk = d.sk AND f.amount > 100);
```

---

#### Pattern E: UNION ALL â†’ Single Scan with Conditional Aggregation

**Detect**: Multiple scans of the same large table with different filters.

```sql
-- BEFORE: Multiple scans
SELECT 'cat_a' as cat, SUM(amount) FROM sales WHERE category = 'A'
UNION ALL
SELECT 'cat_b' as cat, SUM(amount) FROM sales WHERE category = 'B';

-- AFTER: Single scan with FILTER
SELECT
    SUM(amount) FILTER (WHERE category = 'A') as cat_a_total,
    SUM(amount) FILTER (WHERE category = 'B') as cat_b_total
FROM sales WHERE category IN ('A', 'B');
```

---

#### Pattern F: Exploit Data Ordering for Zonemap Effectiveness

**Detect**: Selective filters on columns that don't align with the table's sort order.

```sql
-- If table is sorted by date but frequently filtered by region:
CREATE TABLE sales_by_region AS
    SELECT * FROM sales ORDER BY region, sale_date;
```

---

#### Pattern G: Replace Non-Equi Joins with Range Joins or ASOF Joins

**Detect**: `NESTED_LOOP_JOIN` on non-equality conditions.

```sql
-- For temporal lookups, use ASOF JOIN (DuckDB-native):
SELECT s.*, p.price
FROM sales s
ASOF JOIN prices p
  ON s.product_id = p.product_id
  AND s.sale_date >= p.effective_date;
```

---

#### Pattern H: Leverage DuckDB-Specific Syntax

```sql
-- GROUP BY ALL: avoids redundant column listing
SELECT region, category, SUM(amount), COUNT(*)
FROM sales
GROUP BY ALL;

-- QUALIFY: filter window results without subquery
SELECT customer_id, amount, sale_date
FROM sales
QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY sale_date DESC) = 1;

-- FILTER clause on aggregates
SELECT region,
    COUNT(*) FILTER (WHERE amount > 100) as high_value_count,
    AVG(amount) FILTER (WHERE category = 'A') as cat_a_avg
FROM sales
GROUP BY region;
```

---

#### Pattern I: Memory Pressure Management

```sql
-- Increase memory for hash-heavy queries
SET memory_limit = '16GB';

-- Disable preservation of insertion order
SET preserve_insertion_order = false;

-- Pre-aggregate before joining to reduce hash table sizes
SELECT d.name, f_agg.total
FROM (SELECT sk, SUM(amount) as total FROM fact GROUP BY sk) f_agg
JOIN dim d ON f_agg.sk = d.sk;
```

---

### Step 3: Verify Semantic Equivalence

For each rewrite, verify:
1. **NULL handling**: Manual rewrites must preserve NULL behavior
2. **Duplicate handling**: Converting DISTINCT to EXISTS changes semantics if not careful
3. **Sort stability**: DuckDB does not guarantee stable sorts
4. **Empty group handling**: `QUALIFY` returns no rows when no matches exist per group
5. **Type coercion**: Rewriting predicates may change type resolution

---

## Output Format

Provide your response in this structure:

### Analysis

**Identified Bottlenecks:**
1. [Bottleneck description with operator/EC-vs-actual from EXPLAIN ANALYZE]

**Root Causes:**
- [Why DuckDB's optimizer made this choice]

**DuckDB auto-optimizations already in effect:**
- [Note which optimizations should NOT be undone]

### Recommended Rewrites

**Rewrite 1: [Pattern Name]**

*Rationale:* [Why this rewrite helps]

*Original fragment:*
```sql
[relevant portion]
```

*Rewritten fragment:*
```sql
[optimized version]
```

*Expected improvement:* [Estimated speedup]

### Complete Optimized Query

```sql
-- Your complete optimized query here
```

### Verification Checklist

- [ ] NULL semantics preserved
- [ ] Duplicate rows handled correctly
- [ ] Empty group/no-match cases return same results
- [ ] All columns in SELECT/ORDER BY still accessible
- [ ] All literal values EXACTLY match original query
- [ ] OR conditions NOT over-expanded (â‰¤3 UNION branches max)

