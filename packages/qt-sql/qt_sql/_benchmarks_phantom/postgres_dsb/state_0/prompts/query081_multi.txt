You are a SQL query rewrite engine.

Your goal: rewrite the complete SQL query to maximize execution speed
while preserving exact semantic equivalence (same rows, same columns,
same ordering).

You will receive the full query, its DAG structure showing how CTEs and
subqueries connect, cost analysis per node, and suggested rewrite patterns.
You may restructure the query freely: create new CTEs, merge existing ones,
push filters across node boundaries, or decompose subqueries.

## Query: query081_multi

```sql
WITH customer_total_return AS (
  SELECT
    cr_returning_customer_sk AS ctr_customer_sk,
    ca_state AS ctr_state,
    SUM(cr_return_amt_inc_tax) AS ctr_total_return
  FROM catalog_returns, date_dim, customer_address
  WHERE
    cr_returned_date_sk = d_date_sk
    AND d_year = 1998
    AND cr_returning_addr_sk = ca_address_sk
  GROUP BY
    cr_returning_customer_sk,
    ca_state
)
SELECT
  c_customer_id,
  c_salutation,
  c_first_name,
  c_last_name,
  ca_street_number,
  ca_street_name,
  ca_street_type,
  ca_suite_number,
  ca_city,
  ca_county,
  ca_state,
  ca_zip,
  ca_country,
  ca_gmt_offset,
  ca_location_type,
  ctr_total_return
FROM customer_total_return AS ctr1, customer_address, customer
WHERE
  ctr1.ctr_total_return > (
    SELECT
      AVG(ctr_total_return) * 1.2
    FROM customer_total_return AS ctr2
    WHERE
      ctr1.ctr_state = ctr2.ctr_state
  )
  AND ca_address_sk = c_current_addr_sk
  AND ca_state = 'VA'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY
  c_customer_id,
  c_salutation,
  c_first_name,
  c_last_name,
  ca_street_number,
  ca_street_name,
  ca_street_type,
  ca_suite_number,
  ca_city,
  ca_county,
  ca_state,
  ca_zip,
  ca_country,
  ca_gmt_offset,
  ca_location_type,
  ctr_total_return
LIMIT 100
```

## DAG Topology

```sql
-- DAG TOPOLOGY
-- Depth 0:
--   customer_total_return (cte, 50% cost) [GROUP_BY]
--     outputs: [ctr_customer_sk, ctr_state, ctr_total_return]
-- Depth 1:
--   main_query (main, 50% cost) [GROUP_BY, CORRELATED] ← reads [customer_total_return, customer_total_return]
--     outputs: [c_customer_id, c_salutation, c_first_name, c_last_name, ca_street_number, ca_street_name, ca_street_type, ca_suite_number, ...]
-- Edges:
--   customer_total_return → main_query
--   customer_total_return → main_query
```

## Performance Profile

**customer_total_return**: 50% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, SEQ_SCAN[catalog_returns], SEQ_SCAN[date_dim], SEQ_SCAN[customer_address]
**main_query**: 50% of total cost, ~1,000 rows
  operators: HASH_GROUP_BY, HASH_JOIN, SEQ_SCAN[customer_total_return], SEQ_SCAN[customer_address], SEQ_SCAN[customer]

## Suggested Rewrite Strategy

Phase 2 analysis identified these optimization opportunities:

- **customer_total_return** → apply **pushdown**
  Heuristic: 50.0% cost, flags=['GROUP_BY']
- **main_query** → apply **decorrelate**
  Heuristic: 50.0% cost, flags=['GROUP_BY', 'CORRELATED']

## Reference Examples

### 1. early_filter_decorrelate (1.13x)

**BEFORE (slow):**
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         sr_reason_sk AS ctr_reason_sk,
         SUM(SR_REFUNDED_CASH) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk
    AND d_year = 2001
    AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
  GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer, customer_demographics
WHERE ctr1.ctr_total_return > (
    SELECT AVG(ctr_total_return) * 1.2
    FROM customer_total_return ctr2
    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state IN ('MI', 'NC', 'WI')
  AND ctr1.ctr_customer_sk = c_customer_sk
  AND c_current_cdemo_sk = cd_demo_sk
  AND cd_marital_status IN ('W', 'W')
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

**Key insight:** Push dimension filters INTO the CTE definition before materialization. Decorrelate the correlated subquery by pre-computing the AVG threshold in a separate CTE and JOINing on it. This reduces rows processed early and eliminates row-by-row subquery execution.

**AFTER (fast):**
```sql
WITH customer_total_return AS (
    SELECT sr_customer_sk AS ctr_customer_sk,
           sr_store_sk AS ctr_store_sk,
           sr_reason_sk AS ctr_reason_sk,
           SUM(SR_REFUNDED_CASH) AS ctr_total_return
    FROM store_returns
    JOIN date_dim ON sr_returned_date_sk = d_date_sk
    JOIN store ON sr_store_sk = s_store_sk
    WHERE d_year = 2001
      AND s_state IN ('MI', 'NC', 'WI')
      AND sr_return_amt / sr_return_quantity BETWEEN 236 AND 295
    GROUP BY sr_customer_sk, sr_store_sk, sr_reason_sk
),
store_thresholds AS (
    SELECT ctr_store_sk,
           AVG(ctr_total_return) * 1.2 AS avg_limit
    FROM customer_total_return
    GROUP BY ctr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1
JOIN store_thresholds st ON ctr1.ctr_store_sk = st.ctr_store_sk
JOIN customer ON ctr1.ctr_customer_sk = c_customer_sk
JOIN customer_demographics ON c_current_cdemo_sk = cd_demo_sk
JOIN store s ON ctr1.ctr_store_sk = s.s_store_sk
WHERE ctr1.ctr_total_return > st.avg_limit
  AND ctr1.ctr_reason_sk BETWEEN 28 AND 31
  AND s.s_state IN ('MI', 'NC', 'WI')
  AND cd_marital_status = 'W'
  AND cd_education_status IN ('4 yr Degree', 'College')
  AND cd_gender = 'M'
  AND c_birth_month = 5
  AND c_birth_year BETWEEN 1950 AND 1956
ORDER BY c_customer_id
LIMIT 100
```

### 2. pg_self_join_decomposition (3.93x)

**BEFORE (slow):**
```sql
select s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand
from store, item,
  (select ss_store_sk, avg(revenue) as ave
   from (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
         from store_sales, date_dim
         where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224
           and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48
         group by ss_store_sk, ss_item_sk) sa
   group by ss_store_sk) sb,
  (select ss_store_sk, ss_item_sk, sum(ss_sales_price) as revenue
   from store_sales, date_dim
   where ss_sold_date_sk = d_date_sk and d_month_seq between 1213 and 1224
     and ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48
   group by ss_store_sk, ss_item_sk) sc
where sb.ss_store_sk = sc.ss_store_sk
  and sc.revenue <= 0.1 * sb.ave
  and s_store_sk = sc.ss_store_sk
  and i_item_sk = sc.ss_item_sk
  and i_manager_id BETWEEN 32 AND 36
  and s_state in ('TN','TX','VA')
order by s_store_name, i_item_desc
limit 100
```

**Key insight:** The original scans store_sales+date_dim TWICE with identical predicates (sa/sc subqueries). On PostgreSQL, CTE materialization computes the fact table aggregation ONCE and reuses it for both per-item revenue and per-store averages. Combined with dimension pre-filtering (store, item), this eliminates the dominant I/O cost.

**AFTER (fast):**
[date_filter]:
```sql
SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1213 AND 1224
```
[store_sales_revenue]:
```sql
SELECT ss_store_sk, ss_item_sk, SUM(ss_sales_price) AS revenue FROM store_sales JOIN date_filter ON ss_sold_date_sk = d_date_sk WHERE ss_sales_price / ss_list_price BETWEEN 0.38 AND 0.48 GROUP BY ss_store_sk, ss_item_sk
```
[store_avg_revenue]:
```sql
SELECT ss_store_sk, AVG(revenue) AS ave FROM store_sales_revenue GROUP BY ss_store_sk
```
[filtered_store]:
```sql
SELECT s_store_sk, s_store_name FROM store WHERE s_state IN ('TN', 'TX', 'VA')
```
[filtered_item]:
```sql
SELECT i_item_sk, i_item_desc, i_current_price, i_wholesale_cost, i_brand FROM item WHERE i_manager_id BETWEEN 32 AND 36
```
[main_query]:
```sql
SELECT s_store_name, i_item_desc, sc.revenue, i_current_price, i_wholesale_cost, i_brand FROM store_avg_revenue AS sb JOIN store_sales_revenue AS sc ON sb.ss_store_sk = sc.ss_store_sk JOIN filtered_store AS s ON sc.ss_store_sk = s.s_store_sk JOIN filtered_item AS i ON sc.ss_item_sk = i.i_item_sk WHERE sc.revenue <= 0.1 * sb.ave ORDER BY s_store_name, i_item_desc LIMIT 100
```

### 3. CTE_OPTIMIZATION_FENCE_MATERIALIZED (unknown)

**BEFORE (slow):**
```sql
-- May be inlined and computed multiple times in PG12+
      WITH expensive_calc AS (
          SELECT id, complex_function(data) AS result 
          FROM big_table
      )
      SELECT * FROM expensive_calc e1
      JOIN expensive_calc e2 ON e1.result = e2.result;
```

**Key insight:** - CTE contains expensive computation that should run once
      - Need to isolate subquery planning to prevent bad plan choices
      - Force specific join ordering

## Constraints

### CRITICAL — Correctness Guards (top of sandwich)

**SEMANTIC_EQUIVALENCE**
The rewritten query MUST return exactly the same rows, columns, and
ordering as the original. This is the prime directive.

**LITERAL_PRESERVATION**
Keep all literal values (dates, strings, numbers) exactly as they appear in
the original SQL. Do not round, truncate, or reformat them.

### HIGH — Performance and Style Rules (middle of sandwich)

**NO_UNFILTERED_DIM_CTE**
When creating a new CTE that scans a dimension table, include at least one
filter predicate. Never materialize an entire dimension without a WHERE clause.

**OR_TO_UNION_LIMIT**
When converting OR predicates to UNION ALL, limit to 4 branches maximum.
Beyond 4, the UNION overhead exceeds the OR scan cost for most planners.

**EXPLICIT_JOINS**
Convert comma-separated implicit joins to explicit JOIN ... ON syntax.
This gives the optimizer better join-order freedom.

### CRITICAL — Correctness Guards (bottom of sandwich)

**KEEP_EXISTS_AS_EXISTS**
Preserve EXISTS/NOT EXISTS subqueries as-is. Do not convert them to
IN/NOT IN or to JOINs — this risks NULL-handling semantic changes.

**COMPLETE_OUTPUT**
The rewritten query must output ALL columns from the original SELECT.
Never drop, rename, or reorder output columns.

## Output

Return the complete rewritten SQL query. The query must be syntactically
valid and ready to execute.

```sql
-- Your rewritten query here
```

After the SQL, briefly explain what you changed:

```
Changes: <1-2 sentence summary of the rewrite>
Expected speedup: <estimate>
```

Now output your rewritten SQL: