FAILURE_ANALYSIS:
All previous attempts fell short because they focused solely on dimension isolation through CTEs, which DuckDB's optimizer already handles automatically via predicate pushdown. The real bottleneck is the massive fact table scans (store_sales, catalog_sales, web_sales) and the subsequent hash joins with dimension tables. The attempts didn't address the fundamental cost of scanning and joining large fact tables. Additionally, they missed opportunities to leverage semi-joins for early filtering and to restructure aggregations to reduce intermediate data volume.

UNEXPLORED_OPPORTUNITIES:
1. **Semi-join pushdown**: Use IN subqueries with pre-materialized dimension keys to filter fact tables before joins, reducing the rows early.
2. **Early aggregation on fact tables**: Aggregate fact tables by dimension keys before joining with dimensions, minimizing the data flowing into joins.
3. **Conditional aggregation across channels**: Consolidate the three fact table scans into a single CTE using UNION ALL and perform a single aggregation pass after joining with dimensions.
4. **Limit pushdown optimization**: Since the final query has LIMIT 100, explore whether partial aggregation or top-k can be pushed down to reduce work.

REFINED_STRATEGY:
Create filtered dimension CTEs, then rewrite each fact table scan using IN subqueries to leverage semi-joins, reducing the fact table rows early. After filtering, join only with the item dimension to get i_item_id and aggregate. This minimizes the intermediate data by applying dimension filters via semi-joins before the expensive joins.

EXAMPLES: pushdown, early_filter, single_pass_aggregation
HINT: Use CTEs for filtered dimension keys, then apply IN subqueries in fact table WHERE clauses to filter rows before joins. Finally, join only with the item dimension to preserve the correct grouping.