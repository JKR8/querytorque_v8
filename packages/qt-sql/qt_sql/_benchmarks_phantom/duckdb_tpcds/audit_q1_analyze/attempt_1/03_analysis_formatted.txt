## Expert Analysis

### Query Structure
**customer_total_return CTE**: Computes total return fees for each customer at each store during the year 2000. It joins the large `store_returns` fact table (~2.8B rows) with `date_dim` (~73k rows) and groups by customer+store. Outputs ~557k aggregated rows.

**main_query**: Finds customers in SD stores whose total return is 20% above the store's average, joining the CTE twice (once directly, once in correlated subquery) with `store` (~1k rows) and `customer` (~2M rows). Processes 539k rows but outputs only 100 after ordering. The correlated subquery recalculates per-store averages repeatedly.

### Performance Bottleneck
The dominant cost (85%) is the initial CTE scanning `store_returns` + `date_dim`. However, the **performance mechanism** is: *Scanning the entire fact table without store-state filtering, then re-processing all 557k rows repeatedly in a correlated subquery.*

Specifically:
1. **Unfiltered fact scan**: The CTE joins `store_returns` with `date_dim` but doesn't filter by `s_state = 'SD'`. It processes all stores globally, though only SD stores are needed.
2. **Correlated subquery N² pattern**: For each qualifying SD store row, the subquery `AVG(ctr_total_return) * 1.2` rescans the entire CTE (557k rows) filtered only by matching `ctr_store_sk`. This creates O(N*M) work where N = SD stores and M = total stores.
3. **Late dimension filtering**: The `store` table filter (`s_state = 'SD'`) happens after aggregation, missing pushdown opportunities.

### Proposed Optimization Strategy
### Change 1: Decorate and pre-aggregate per-store averages
**What**: Extract the correlated subquery into a separate CTE that computes store averages once, then join instead of correlate.
**Why**: Eliminates repeated scanning of the 557k-row CTE. The subquery currently runs for each SD store row; moving to a single scan + join reduces work from O(N*M) to O(N+M).
**Risk**: Must ensure NULL handling for stores without returns is consistent (should be fine as original uses INNER logic).
**Impact**: Significant (removes main quadratic component).

### Change 2: Push store-state filter into initial CTE via early dimension join
**What**: Join `store` table early in the CTE to filter by `s_state = 'SD'` before aggregating `store_returns`.
**Why**: Reduces the fact table scan from processing all stores to only SD stores. With ~1k stores total and SD being a small subset, this could reduce `store_returns` rows processed by 95%+.
**Risk**: Must preserve correct aggregation granularity (customer+store). The filter might exclude stores with returns but no matching store record (unlikely in star schema).
**Impact**: Significant (reduces largest table scan).

### Change 3: Merge the two CTE scans into single-pass aggregation
**What**: Compute both customer-level aggregates and store-level averages in one CTE using window functions or two-level aggregation.
**Why**: Avoids materializing and scanning the 557k-row CTE twice (once for main rows, once for averages).
**Risk**: Window function might require careful partitioning; must match original subquery scope (per-store averages).
**Impact**: Moderate (reduces intermediate data volume).

### Recommended Approach
**Implement all three changes in sequence**:

1. **Pre-filter SD stores**: Create a CTE `sd_stores` with just `s_store_sk` where `s_state = 'SD'`.
2. **Single-pass aggregation**: Build a CTE that joins `store_returns` → `date_dim` → `sd_stores`, filters to `d_year = 2000`, then aggregates with `GROUP BY sr_customer_sk, sr_store_sk` to get customer totals, and simultaneously computes store averages via `AVG(SUM(SR_FEE)) OVER (PARTITION BY sr_store_sk)` or a second grouped aggregate.
3. **Direct join**: Replace the correlated subquery with a simple filter comparing `ctr_total_return > store_avg * 1.2` using the precomputed store average from step 2.
4. **Final join** with `customer` for IDs, order, and limit.

This transforms the query from:
- Scan all returns → aggregate all stores → filter SD stores → N² correlated subquery
To:
- Scan only SD store returns → single aggregation with per-store averages → simple filter → final join

## 6. EXAMPLE SELECTION

FAISS picks are relevant but incomplete. The optimal pattern combines:
- **decorrelate** for the subquery
- **early_filter** for store-state pushdown  
- **single_pass_aggregation** for computing customer totals and store averages together

**EXAMPLES: decorrelate, early_filter, single_pass_aggregation**

Apply the recommended strategy above. The analysis has already identified the bottleneck and the specific structural change needed. Focus on implementing it correctly while preserving semantic equivalence.