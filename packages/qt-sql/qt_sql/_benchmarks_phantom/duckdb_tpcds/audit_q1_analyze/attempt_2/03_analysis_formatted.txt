## Expert Analysis

### Query Structure
**customer_total_return CTE**: Computes total return fees for each customer-store combination in the year 2000. It joins store_returns (~likely millions of rows) with date_dim (~thousands) and groups by customer+store, outputting ~557k aggregated rows.

**main_query**: Finds customers in SD stores whose total return exceeds 120% of their store's average return. It scans the CTE twice (once for main rows, once per store in correlated subquery), joins with store and customer tables, processes ~539k rows, and outputs the top 100 customer IDs ordered.

### Performance Bottleneck
**Dominant cost**: The CTE `customer_total_return` at 85% cost, but the **real mechanism** is the **correlated subquery execution pattern**. For each of the ~539k CTE rows (after store filtering), DuckDB re-evaluates the subquery `AVG(ctr_total_return) * 1.2` by scanning the entire CTE again filtered only by matching `ctr_store_sk`. This creates O(n²) behavior: each store's average is computed repeatedly as the outer loop iterates through customers of that store.

**Secondary issue**: The CTE computes returns for **all stores**, but the main query only needs stores in 'SD'. This unnecessary computation inflates the CTE size before filtering.

### Proposed Optimization Strategy
**Change 1: Decorrelate the subquery into a separate pre-aggregated CTE**
- **What**: Create `store_avg_return` CTE with `ctr_store_sk, AVG(ctr_total_return) as store_avg`, then JOIN instead of using correlated subquery.
- **Why**: Eliminates repeated scanning/computation of store averages. Each store's average is computed once.
- **Risk**: Must ensure we only include stores that pass the s_state='SD' filter to maintain correctness.
- **Impact**: Significant (3-5x expected)

**Change 2: Push store filter into CTE computation**
- **What**: Join store table early in the CTE to filter by s_state='SD' before aggregation.
- **Why**: Reduces CTE cardinality from ~557k to only SD stores, shrinking hash tables and subsequent joins.
- **Risk**: The subquery average must be computed over the same filtered set (SD stores only), which matches original semantics since outer query only considers SD stores.
- **Impact**: Moderate (1.5-2x)

**Change 3: Materialize CTE explicitly**
- **What**: Add `MATERIALIZED` hint or restructure to prevent recomputation of CTE for subquery.
- **Why**: DuckDB may inline CTEs, causing recomputation. Materialization ensures single computation.
- **Risk**: Materialization increases memory usage but reduces CPU.
- **Impact**: Minor (1.2-1.5x)

### Lessons from Previous Failures
**Previous attempt "unknown → WIN (1.85x)"**: Likely attempted materialization or basic decorrelation but didn't address the store filtering pushdown. The 1.85x improvement suggests partial decorrelation but still computing averages over all stores unnecessarily.

**Constraint learned**: Any optimization must preserve that store averages are computed **per store** and compared against **individual customer returns** for that same store. Filtering stores must happen consistently across both comparisons.

### Recommended Approach
Implement a **three-CTE rewrite**:
1. `sd_stores`: Pre-filter store table for s_state='SD' 
2. `customer_store_returns`: Join store_returns → date_dim → sd_stores, aggregate by customer+store
3. `store_avg_returns`: Aggregate customer_store_returns by store to get averages
4. Main query: JOIN customer_store_returns → store_avg_returns → customer, filter where return > avg * 1.2

This eliminates correlation, reduces CTE size via early filtering, and ensures single computation of aggregates.

## 6. EXAMPLE SELECTION

The FAISS picks are partially relevant:
- **decorrelate**: Directly addresses the correlated subquery → **HIGH RELEVANCE**
- **materialize_cte**: Could help but addresses symptom, not root cause → **MEDIUM RELEVANCE**
- **union_cte_split**: Not relevant for this query pattern

More relevant examples:
- **early_filter**: For pushing store filter down
- **dimension_cte_isolate**: For isolating store dimension filtering
- **single_pass_aggregation**: For computing store averages in one pass

**EXAMPLES: decorrelate, early_filter, dimension_cte_isolate**

Apply the recommended strategy above. The analysis has already identified the bottleneck and the specific structural change needed. Focus on implementing it correctly while preserving semantic equivalence.