## Expert Analysis

### Query Structure
**ws CTE**: Computes annual aggregates (quantity, costs, revenue) for web sales that were not returned, grouped by year, item, and customer. Scans web_sales (~millions), filters via LEFT JOIN to web_returns, and joins date_dim (~365K rows). Outputs ~1K rows (all years).

**cs CTE**: Same logic as ws but for catalog sales. Scans catalog_sales (~millions), catalog_returns, and date_dim. Outputs ~1K rows (all years).

**ss CTE**: Same logic as ws/cs but for store sales. Scans store_sales (~millions), store_returns, and date_dim. Outputs ~1K rows (all years).

**main_query**: Joins all three CTEs on (year, item, customer), filters to year 2000 and requires at least some web/catalog sales, computes ratios, and returns top 100 by complex ordering. Processes ~1K rows from each CTE, but the joins happen *after* each CTE has aggregated all years.

### Performance Bottleneck
The dominant cost mechanism is **three independent full scans of massive fact tables (web_sales, catalog_sales, store_sales) with GROUP BY aggregations across ALL YEARS**, when only year 2000 data is needed in the final output.

**Root cause**: The date filter (`ss_sold_year = 2000`) is applied in the main query *after* the CTEs have already aggregated all historical data. Each CTE:
1. Scans the entire fact table (millions of rows)
2. Joins with date_dim to get d_year (without filtering)
3. Groups by d_year (all years)
4. Outputs aggregated data for all years

The main query then filters to year 2000, discarding 99%+ of the aggregated work. This is a classic **late filtering** problem where expensive aggregations run before predicates are applied.

### Proposed Optimization Strategy
**Change 1: Push year filter into CTEs**
- **What**: Add `WHERE d_year = 2000` to each CTE's date_dim join condition
- **Why**: Reduces each fact table scan from millions to thousands of rows, and cuts GROUP BY cardinality by >99%
- **Risk**: None – main query already filters to 2000, and CTEs are only joined on matching years
- **Impact**: **Significant** (reduces each CTE's work by ~99%)

**Change 2: Create a pre-filtered date_dim CTE**
- **What**: Extract `SELECT d_date_sk FROM date_dim WHERE d_year = 2000` into a CTE, then join all fact tables to it
- **Why**: Ensures optimizer applies the filter early; DuckDB can use this small CTE for efficient hash joins
- **Risk**: None – maintains same semantics with explicit early filtering
- **Impact**: **Moderate** (ensures filter pushdown, helps join planning)

**Change 3: Combine the three CTEs' date_dim joins**
- **What**: Create a single date CTE: `dates AS (SELECT d_date_sk, d_year FROM date_dim WHERE d_year = 2000)` and have all CTEs reference it
- **Why**: Avoids three separate scans of date_dim; the small filtered result is reused
- **Risk**: None – date_dim is a dimension table, safe to pre-filter
- **Impact**: **Minor** (date_dim is small, but reduces repeated work)

### Recommended Approach
Implement **Change 1 + Change 2**: Create a pre-filtered date CTE for year 2000, then modify each sales CTE to:
1. Join to the filtered date CTE instead of the full date_dim
2. Keep the LEFT JOIN to returns tables unchanged
3. Maintain the `wr/cr/sr_order_number IS NULL` filter

This ensures:
- Each fact table scans only 2000 data (~1/7th of typical TPC-DS scale)
- The GROUP BY processes ~1/7th the data
- Hash joins use the small date CTE as build side
- All semantic constraints preserved (non-returned sales only)

Implementation:
```sql
WITH filtered_dates AS (
  SELECT d_date_sk, d_year 
  FROM date_dim 
  WHERE d_year = 2000
),
ws AS (
  SELECT
    d.d_year AS ws_sold_year,
    ws_item_sk,
    ws_bill_customer_sk AS ws_customer_sk,
    SUM(ws_quantity) AS ws_qty,
    SUM(ws_wholesale_cost) AS ws_wc,
    SUM(ws_sales_price) AS ws_sp
  FROM web_sales
  LEFT JOIN web_returns
    ON wr_order_number = ws_order_number AND ws_item_sk = wr_item_sk
  JOIN filtered_dates d
    ON ws_sold_date_sk = d.d_date_sk
  WHERE wr_order_number IS NULL
  GROUP BY d.d_year, ws_item_sk, ws_bill_customer_sk
),
... (similar for cs, ss)
-- main query unchanged (but remove ss_sold_year = 2000 filter as it's now redundant)
```

## 6. EXAMPLE SELECTION

The FAISS selection is partially relevant:
- **shared_dimension_multi_channel**: Good match – we're applying the same date filter across multiple sales channels
- **deferred_window_aggregation**: Not relevant – no window functions here
- **intersect_to_exists**: Not relevant – no INTERSECT operations

Additional relevant examples:
- **early_filter**: Directly addresses pushing dimension filters before fact table joins
- **pushdown**: Specifically about pushing outer filters into CTEs
- **dimension_cte_isolate**: Pre-filtering dimension tables into CTEs

**Final selection**:
```
EXAMPLES: shared_dimension_multi_channel, early_filter, pushdown
```

Apply the recommended strategy above. The analysis has already identified the bottleneck and the specific structural change needed. Focus on implementing it correctly while preserving semantic equivalence.