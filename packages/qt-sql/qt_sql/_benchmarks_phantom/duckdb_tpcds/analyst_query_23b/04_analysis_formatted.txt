## Expert Analysis

### Query Structure
**frequent_ss_items**: Finds store items sold frequently (>4 times) on specific dates within 2000-2003. Scans store_sales (fact), date_dim, and item (dimension). Outputs ~1k rows of (item_sk, solddate, itemdesc, count).

**max_store_sales**: Computes the maximum total sales amount per customer within 2000-2003 from store_sales. Scans store_sales, customer, and date_dim. Outputs a single row with one scalar value.

**best_ss_customer**: Identifies customers whose total store sales (across all time) exceed 95% of the maximum found in max_store_sales. Scans store_sales and customer. Outputs ~1k rows of qualifying customer_sk.

**main_query**: Aggregates sales from catalog_sales and web_sales for May 2000, filtered to frequent items and best customers. Scans two large fact tables (catalog_sales, web_sales) plus customer and date_dim. Outputs 100 rows of customer names with sales totals.

### Performance Bottleneck
The dominant cost center is the **triple scanning of store_sales** with different join patterns and aggregation levels:

1. **frequent_ss_items**: Full scan of store_sales with joins to date_dim and item
2. **max_store_sales**: Full scan of store_sales with joins to date_dim and customer  
3. **best_ss_customer**: Full scan of store_sales with join to customer

**Mechanism**: Each CTE performs a separate full table scan on the massive store_sales table (billions of rows) with overlapping date filters (years 2000-2003). The redundant scans are expensive because:
- Same date range filtering applied three times independently
- Large hash joins rebuilt three times
- No predicate pushdown optimization between CTEs

Additionally, the **best_ss_customer** CTE has a semantic inconsistency: it compares customers' lifetime sales (no date filter) against a maximum computed from 2000-2003 only. This causes an unnecessary full scan where a filtered scan could suffice if business logic permits.

### Proposed Optimization Strategy
### Change 1: Single-pass store_sales aggregation with multiple grouping sets
**What**: Merge frequent_ss_items and max_store_sales into one CTE that scans store_sales once with date filtering, computing both item-date aggregates and customer aggregates.

**Why**: Eliminates two redundant scans of store_sales (~28M rows each). Single pass computes both aggregates simultaneously.

**Risk**: Must ensure grouping logic remains correct - frequent_ss_items groups by item+date, max_store_sales groups by customer.

**Impact**: Significant (reduces store_sales scans from 3 to 1)

### Change 2: Decouple best_ss_customer from date filtering inconsistency
**What**: Modify best_ss_customer to filter store_sales by the same 2000-2003 date range as max_store_sales, making the comparison semantically consistent.

**Why**: Enables reuse of the pre-aggregated customer totals from Change 1, eliminating the third store_sales scan entirely.

**Risk**: Changes business logic - now compares customer sales from 2000-2003 against maximum from same period. Must verify if lifetime comparison was intentional.

**Impact**: Moderate to significant (eliminates another full scan)

### Change 3: Pre-materialize date_dim CTE for shared filtering
**What**: Extract date_dim filtering for 2000-2003 into a reusable CTE, then join fact tables to this pre-filtered dimension.

**Why**: Reduces repeated date_dim scans and enables better predicate pushdown to fact tables.

**Risk**: Minimal - preserves exact same filtering logic.

**Impact**: Moderate (reduces dimension table processing)

Apply the recommended strategy above. The analysis has already identified the bottleneck and the specific structural change needed. Focus on implementing it correctly while preserving semantic equivalence.