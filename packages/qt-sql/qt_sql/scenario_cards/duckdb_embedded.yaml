# Scenario Card: DuckDB Embedded
# In-process analytical engine. Memory is the main constraint.

name: duckdb_embedded
description: |
  DuckDB runs in-process with configurable memory limit.
  No remote spill â€” all data must fit in memory or use
  out-of-core execution (significantly slower).

resource_envelope:
  memory: "Configurable via SET memory_limit (default: 80% of system RAM)"
  compute: "All available CPU cores (auto-parallelism)"
  storage_io: "Local SSD, columnar parquet/native storage"

failure_definitions:
  - metric: query_duration
    threshold: ">120s"
    severity: fatal
  - metric: out_of_memory
    threshold: "any"
    severity: fatal
  - metric: out_of_core_execution
    threshold: "any"
    severity: warning

strategy_priorities:
  - "Reduce intermediate sizes to fit in memory_limit"
  - "Exploit columnar scan efficiency (predicate pushdown)"
  - "Minimize redundant scans (same table scanned multiple times)"
  - "Push filters into CTEs (every CTE must have WHERE)"
  - "Pre-aggregate before joins to reduce hash table sizes"

strategy_avoid:
  - "Cross-joining 3+ dimension CTEs (0.0076x regression)"
  - "OR splitting on same column (engine handles via BitmapOr equivalent)"
  - "Materializing EXISTS subqueries (0.14x regression)"
