# V5 Optimizer Storage Strategy

**Date:** 2026-02-05
**Goal:** Complete audit trail of all inputs and outputs

---

## Storage Structure

### Complete Directory Layout

```
results/{query_id}_{timestamp}/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ original.sql                    # Original query as submitted
â”‚   â”œâ”€â”€ query_dag.json                  # Parsed DAG with contracts
â”‚   â”œâ”€â”€ explain_plan_sample.txt         # EXPLAIN from sample DB
â”‚   â”œâ”€â”€ explain_plan_full.txt           # EXPLAIN from full DB (if available)
â”‚   â”œâ”€â”€ metadata.json                   # Query metadata
â”‚   â””â”€â”€ ml_recommendations.json         # ML-selected examples
â”‚
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ worker_1/
â”‚   â”‚   â”œâ”€â”€ config.json                 # Worker configuration
â”‚   â”‚   â”œâ”€â”€ examples.json               # Gold examples assigned
â”‚   â”‚   â”œâ”€â”€ prompt.txt                  # Complete prompt sent to LLM
â”‚   â”‚   â”œâ”€â”€ llm_request.json            # Request payload
â”‚   â”‚   â”œâ”€â”€ llm_response.json           # Raw response from LLM
â”‚   â”‚   â”œâ”€â”€ reasoning.txt               # Reasoning content (if available)
â”‚   â”‚   â”œâ”€â”€ rewrite_sets.json           # Parsed rewrite_sets
â”‚   â”‚   â”œâ”€â”€ optimized.sql               # Assembled SQL
â”‚   â”‚   â”œâ”€â”€ validation_sample.json      # Sample DB validation result
â”‚   â”‚   â”œâ”€â”€ benchmark_full.json         # Full DB benchmark (if run)
â”‚   â”‚   â””â”€â”€ errors.log                  # Any errors encountered
â”‚   â”œâ”€â”€ worker_2/
â”‚   â”‚   â””â”€â”€ ... (same structure)
â”‚   â”œâ”€â”€ worker_3/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ worker_4/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ worker_5/
â”‚       â”œâ”€â”€ config.json                 # No examples (explore mode)
â”‚       â”œâ”€â”€ prompt.txt                  # Full SQL prompt
â”‚       â””â”€â”€ ... (rest same)
â”‚
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ sample_db/
â”‚   â”‚   â”œâ”€â”€ original_result.json        # Original query result
â”‚   â”‚   â”œâ”€â”€ original_timing.json        # Timing metrics
â”‚   â”‚   â”œâ”€â”€ worker_1_result.json        # Each candidate result
â”‚   â”‚   â”œâ”€â”€ worker_1_timing.json
â”‚   â”‚   â”œâ”€â”€ worker_2_result.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ equivalence_checks.json         # Row count, checksum comparisons
â”‚
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ full_db/
â”‚   â”‚   â”œâ”€â”€ original_runs.json          # 5-run trimmed mean data
â”‚   â”‚   â”œâ”€â”€ worker_1_runs.json          # 5-run trimmed mean data
â”‚   â”‚   â”œâ”€â”€ worker_2_runs.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ speedup_calculations.json       # All speedup computations
â”‚
â”œâ”€â”€ winner/
â”‚   â”œâ”€â”€ optimized.sql                   # Final winning SQL
â”‚   â”œâ”€â”€ comparison.html                 # Side-by-side diff view
â”‚   â”œâ”€â”€ comparison.txt                  # Text diff
â”‚   â”œâ”€â”€ speedup_report.json             # Detailed metrics
â”‚   â”œâ”€â”€ worker_id.txt                   # Which worker won (e.g., "1")
â”‚   â””â”€â”€ verification.json               # Final validation proof
â”‚
â”œâ”€â”€ summary.json                        # High-level summary
â”œâ”€â”€ timeline.json                       # Execution timeline with durations
â”œâ”€â”€ run_config.json                     # Complete CLI configuration
â”œâ”€â”€ system_info.json                    # System, model, versions
â””â”€â”€ run.log                             # Complete execution log
```

---

## File Formats

### input/original.sql
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         SUM(SR_FEE) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
  GROUP BY sr_customer_sk, sr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (
    SELECT avg(ctr_total_return)*1.2
    FROM customer_total_return ctr2
    WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
  )
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'SD'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id
LIMIT 100
```

### input/metadata.json
```json
{
  "query_id": "q1",
  "source_file": "queries/q1.sql",
  "size_bytes": 542,
  "line_count": 18,
  "detected_patterns": [
    "correlated_subquery",
    "aggregate_in_subquery",
    "multiple_joins"
  ],
  "complexity_score": 7.5,
  "timestamp": "2026-02-05T10:30:00Z"
}
```

### input/ml_recommendations.json
```json
{
  "query_id": "q1",
  "recommendations": [
    {
      "rank": 1,
      "example_id": "decorrelate",
      "confidence": 0.76,
      "expected_speedup": "2.92x",
      "rationale": "Correlated subquery detected"
    },
    {
      "rank": 2,
      "example_id": "early_filter",
      "confidence": 0.35,
      "expected_speedup": "2.15x",
      "rationale": "Multiple dimension tables"
    }
  ],
  "total_examples_available": 13,
  "selected_for_workers": 12
}
```

### workers/worker_1/config.json
```json
{
  "worker_id": 1,
  "format": "dag_json",
  "examples": [
    "date_cte_isolate",
    "decorrelate",
    "early_filter"
  ],
  "example_source": "ml_recommendations",
  "prompt_size": 9963,
  "model": "deepseek-reasoner",
  "provider": "deepseek"
}
```

### workers/worker_1/prompt.txt
```
## Example: Date CTE Isolation (DATE_CTE_ISOLATE)
Verified speedup: 1.5-2.5x

### Input:
...

You are an autonomous Query Rewrite Engine. Your goal is to maximize execution
speed while strictly preserving semantic invariants.
...
```

### workers/worker_1/llm_request.json
```json
{
  "model": "deepseek-reasoner",
  "messages": [
    {
      "role": "user",
      "content": "..."
    }
  ],
  "max_tokens": 16384,
  "temperature": 0,
  "timestamp": "2026-02-05T10:30:15Z"
}
```

### workers/worker_1/llm_response.json
```json
{
  "id": "chatcmpl-xyz",
  "model": "deepseek-reasoner",
  "usage": {
    "prompt_tokens": 2450,
    "completion_tokens": 892,
    "total_tokens": 3342
  },
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "{\"rewrite_sets\": [...]}",
        "reasoning_content": "Analyzing the query structure..."
      }
    }
  ],
  "duration_ms": 3245,
  "timestamp": "2026-02-05T10:30:18Z"
}
```

### workers/worker_1/reasoning.txt
```
Analyzing the query structure... The correlated subquery computes average
ctr_total_return per store. If I push the s_state='SD' filter into
filtered_store_returns before aggregation, I change the scope of the average
calculation to only SD stores. This violates the semantic invariant that the
average must be computed over all stores. Therefore, I must keep the filter
in the main query WHERE clause after decorrelation.
```

### workers/worker_1/rewrite_sets.json
```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "decorrelate",
      "nodes": {
        "filtered_store_returns": "...",
        "customer_total_return": "...",
        "store_avg_return": "...",
        "main_query": "..."
      },
      "invariants_kept": [
        "same result rows",
        "same ordering",
        "same column output"
      ],
      "expected_speedup": "2.90x",
      "risk": "low"
    }
  ],
  "explanation": "The correlated subquery computing average..."
}
```

### workers/worker_1/validation_sample.json
```json
{
  "status": "PASS",
  "original": {
    "row_count": 100,
    "execution_time_ms": 234.5,
    "checksum": "a7f3d9e2"
  },
  "optimized": {
    "row_count": 100,
    "execution_time_ms": 75.8,
    "checksum": "a7f3d9e2"
  },
  "speedup": 3.09,
  "equivalence": "exact_match",
  "timestamp": "2026-02-05T10:30:20Z"
}
```

### workers/worker_1/benchmark_full.json
```json
{
  "status": "PASS",
  "runs": 5,
  "method": "trimmed_mean",
  "original": {
    "run_1_ms": 12450,
    "run_2_ms": 12389,
    "run_3_ms": 12512,
    "run_4_ms": 12445,
    "run_5_ms": 12398,
    "trimmed_mean_ms": 12431.3,
    "discarded": ["min: 12389", "max: 12512"]
  },
  "optimized": {
    "run_1_ms": 4265,
    "run_2_ms": 4251,
    "run_3_ms": 4289,
    "run_4_ms": 4268,
    "run_5_ms": 4243,
    "trimmed_mean_ms": 4261.3,
    "discarded": ["min: 4243", "max: 4289"]
  },
  "speedup": 2.92,
  "target_met": true,
  "timestamp": "2026-02-05T10:35:45Z"
}
```

### summary.json
```json
{
  "query_id": "q1",
  "timestamp": "2026-02-05T10:30:00Z",
  "duration_total_ms": 325450,
  "configuration": {
    "provider": "deepseek",
    "model": "deepseek-reasoner",
    "workers": 5,
    "target_speedup": 2.0
  },
  "results": {
    "workers_total": 5,
    "workers_valid": 4,
    "workers_benchmarked": 2,
    "winner": {
      "worker_id": 1,
      "speedup": 2.92,
      "status": "target_met"
    }
  },
  "performance": {
    "original_time_ms": 12431.3,
    "optimized_time_ms": 4261.3,
    "speedup": 2.92,
    "improvement_pct": 65.7
  }
}
```

### timeline.json
```json
{
  "start": "2026-02-05T10:30:00.000Z",
  "end": "2026-02-05T10:35:25.450Z",
  "duration_ms": 325450,
  "phases": [
    {
      "phase": "load_query",
      "start_ms": 0,
      "duration_ms": 125,
      "status": "success"
    },
    {
      "phase": "generate_prompts",
      "start_ms": 125,
      "duration_ms": 450,
      "status": "success"
    },
    {
      "phase": "llm_optimization",
      "start_ms": 575,
      "duration_ms": 18230,
      "workers": [
        {
          "worker_id": 1,
          "start_ms": 575,
          "duration_ms": 3245,
          "status": "success"
        },
        {
          "worker_id": 2,
          "start_ms": 580,
          "duration_ms": 2834,
          "status": "success"
        }
      ]
    },
    {
      "phase": "sample_validation",
      "start_ms": 18805,
      "duration_ms": 1245,
      "status": "success"
    },
    {
      "phase": "full_benchmark",
      "start_ms": 20050,
      "duration_ms": 305400,
      "workers_benchmarked": [1, 4],
      "status": "success"
    }
  ]
}
```

### run_config.json
```json
{
  "cli_version": "0.5.0",
  "command": "qt-sql optimize q1.sql --sample-db tpcds_sf1.duckdb --full-db tpcds_sf100.duckdb",
  "parameters": {
    "query_file": "q1.sql",
    "version": "v5",
    "provider": "deepseek",
    "model": "deepseek-reasoner",
    "sample_db": "tpcds_sf1.duckdb",
    "full_db": "tpcds_sf100.duckdb",
    "query_id": "q1",
    "target_speedup": 2.0,
    "workers": 5,
    "output": "stdout",
    "save_results": "results/q1_20260205_103000/"
  },
  "environment": {
    "QT_LLM_PROVIDER": "deepseek",
    "QT_SAMPLE_DB": "tpcds_sf1.duckdb"
  }
}
```

### system_info.json
```json
{
  "platform": "linux",
  "python_version": "3.11.5",
  "qt_sql_version": "0.5.0",
  "qt_shared_version": "0.5.0",
  "dependencies": {
    "sqlglot": "20.10.0",
    "duckdb": "0.9.2",
    "openai": "1.10.0"
  },
  "hardware": {
    "cpu_cores": 8,
    "memory_gb": 32
  }
}
```

---

## Storage Lifecycle

### Phase 1: Input Capture (Immediate)
```
input/
â”œâ”€â”€ original.sql          âœ“ Saved immediately
â”œâ”€â”€ metadata.json         âœ“ After parsing
â”œâ”€â”€ query_dag.json        âœ“ After DAG building
â”œâ”€â”€ explain_plan_*.txt    âœ“ After EXPLAIN
â””â”€â”€ ml_recommendations.json âœ“ After ML lookup
```

### Phase 2: Worker Execution (Per Worker)
```
workers/worker_X/
â”œâ”€â”€ config.json          âœ“ Before LLM call
â”œâ”€â”€ examples.json        âœ“ Before LLM call
â”œâ”€â”€ prompt.txt           âœ“ Before LLM call
â”œâ”€â”€ llm_request.json     âœ“ Before LLM call
â”œâ”€â”€ llm_response.json    âœ“ After LLM response
â”œâ”€â”€ reasoning.txt        âœ“ After response (if available)
â”œâ”€â”€ rewrite_sets.json    âœ“ After JSON parse
â”œâ”€â”€ optimized.sql        âœ“ After assembly
â”œâ”€â”€ validation_sample.json âœ“ After sample validation
â””â”€â”€ benchmark_full.json  âœ“ After full benchmark (if run)
```

### Phase 3: Validation & Benchmark
```
validation/
â””â”€â”€ sample_db/
    â”œâ”€â”€ original_result.json   âœ“ After original execution
    â””â”€â”€ worker_X_result.json   âœ“ After each candidate

benchmark/
â””â”€â”€ full_db/
    â”œâ”€â”€ original_runs.json     âœ“ After 5 runs
    â””â”€â”€ worker_X_runs.json     âœ“ After each candidate runs
```

### Phase 4: Winner Selection
```
winner/
â”œâ”€â”€ optimized.sql        âœ“ Copy of winning SQL
â”œâ”€â”€ comparison.*         âœ“ Generated after selection
â”œâ”€â”€ speedup_report.json  âœ“ Generated after selection
â””â”€â”€ verification.json    âœ“ Final validation proof
```

### Phase 5: Summary
```
summary.json             âœ“ After completion
timeline.json            âœ“ After completion
run_config.json          âœ“ At start (updated at end)
system_info.json         âœ“ At start
run.log                  âœ“ Continuous append
```

---

## CLI Integration

### Automatic Storage

By default, all runs automatically save to timestamped directories:

```bash
qt-sql optimize q1.sql --sample-db tpcds.duckdb

# Auto-creates:
# results/q1_20260205_103000/
```

### Custom Storage Location

```bash
qt-sql optimize q1.sql \
  --save-results /custom/path/q1_run_1/
```

### Disable Storage (stdout only)

```bash
qt-sql optimize q1.sql --no-save
```

### Storage Level Control

```bash
# Minimal (only summary + winner)
qt-sql optimize q1.sql --save-level minimal

# Standard (summary + winner + worker results)
qt-sql optimize q1.sql --save-level standard

# Full (everything, including prompts/responses)
qt-sql optimize q1.sql --save-level full  # Default
```

---

## Benefits

### 1. Complete Audit Trail
- Every input saved
- Every output saved
- Every intermediate step saved

### 2. Reproducibility
- Exact prompts can be replayed
- LLM responses are preserved
- Timing data is complete

### 3. Debugging
- Easy to identify where failures occurred
- Can inspect exact LLM reasoning
- Can verify semantic correctness

### 4. Analysis
- Compare different runs
- Analyze which examples work best
- Track model performance over time

### 5. Compliance
- Full audit trail for production use
- Proof of optimization correctness
- Track all decisions made

---

## Storage Size Estimates

### Per Query Run

```
input/           ~50 KB
workers/         ~500 KB (5 workers Ã— 100 KB each)
validation/      ~200 KB
benchmark/       ~100 KB
winner/          ~50 KB
summary files    ~50 KB
Total:           ~950 KB per run
```

### 99 TPC-DS Queries

```
99 queries Ã— 950 KB = ~94 MB per complete benchmark run
```

### Compression

```bash
# Compress old runs
tar -czf q1_20260205_103000.tar.gz results/q1_20260205_103000/

# Reduces to ~200 KB (5x compression)
```

---

## Retrieval API

### List Saved Runs

```bash
qt-sql results list
qt-sql results list --query q1
qt-sql results list --date 2026-02-05
```

### View Specific Run

```bash
qt-sql results show q1_20260205_103000
qt-sql results show --worker 1 q1_20260205_103000
```

### Compare Runs

```bash
qt-sql results compare q1_20260205_103000 q1_20260205_143000
```

### Export Run

```bash
qt-sql results export q1_20260205_103000 --format html
qt-sql results export q1_20260205_103000 --format pdf
```

---

## Summary

âœ… **Complete storage of all inputs and outputs**
âœ… **Timestamped directories for each run**
âœ… **Worker-specific subdirectories**
âœ… **Full audit trail from input â†’ LLM â†’ output**
âœ… **Easy retrieval and analysis**
âœ… **Reproducibility guaranteed**

Every run creates a complete, self-contained record of the optimization process! ðŸŽ¯
