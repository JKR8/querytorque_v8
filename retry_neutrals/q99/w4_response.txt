Looking at this query, I can see a prime opportunity for the **Date CTE Isolation** pattern since there's a date range filter on `date_dim` that can be applied early to significantly reduce the fact table (`catalog_sales`) rows before joining with other tables.

The current plan shows:
1. Full sequential scan of `catalog_sales` (172M rows)
2. Late filtering via join with `date_dim` (only 366 rows after filter)
3. This creates a massive intermediate result (2.8M rows) that then gets joined with small dimension tables

By isolating the date filter first, we can filter `catalog_sales` down to only rows with `cs_ship_date_sk` values that exist in the filtered date range, dramatically reducing the working set.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "date_cte_isolate",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1200 AND 1200 + 11",
        "filtered_catalog_sales": "SELECT cs.* FROM catalog_sales cs JOIN filtered_dates fd ON cs.cs_ship_date_sk = fd.d_date_sk",
        "main_query": "SELECT SUBSTRING(w.w_warehouse_name, 1, 20) AS w_substr, sm.sm_type, LOWER(cc.cc_name) AS cc_name_lower, SUM(CASE WHEN (cs.cs_ship_date_sk - cs.cs_sold_date_sk <= 30) THEN 1 ELSE 0 END) AS \"30 days\", SUM(CASE WHEN (cs.cs_ship_date_sk - cs.cs_sold_date_sk > 30) AND (cs.cs_ship_date_sk - cs.cs_sold_date_sk <= 60) THEN 1 ELSE 0 END) AS \"31-60 days\", SUM(CASE WHEN (cs.cs_ship_date_sk - cs.cs_sold_date_sk > 60) AND (cs.cs_ship_date_sk - cs.cs_sold_date_sk <= 90) THEN 1 ELSE 0 END) AS \"61-90 days\", SUM(CASE WHEN (cs.cs_ship_date_sk - cs.cs_sold_date_sk > 90) AND (cs.cs_ship_date_sk - cs.cs_sold_date_sk <= 120) THEN 1 ELSE 0 END) AS \"91-120 days\", SUM(CASE WHEN (cs.cs_ship_date_sk - cs.cs_sold_date_sk > 120) THEN 1 ELSE 0 END) AS \">120 days\" FROM filtered_catalog_sales cs, warehouse w, ship_mode sm, call_center cc WHERE cs.cs_warehouse_sk = w.w_warehouse_sk AND cs.cs_ship_mode_sk = sm.sm_ship_mode_sk AND cs.cs_call_center_sk = cc.cc_call_center_sk GROUP BY SUBSTRING(w.w_warehouse_name, 1, 20), sm.sm_type, cc.cc_name ORDER BY w_substr NULLS FIRST, sm.sm_type NULLS FIRST, cc_name_lower NULLS FIRST LIMIT 100"
      },
      "invariants_kept": [
        "same result rows",
        "same output columns",
        "same grouping and aggregation",
        "same ordering",
        "same limit"
      ],
      "expected_speedup": "3.5x",
      "risk": "low"
    }
  ],
  "explanation": "Isolated the date_dim filter into a CTE to reduce catalog_sales rows early. Instead of scanning all 172M catalog_sales rows and then filtering, we first get the 366 valid date keys and join only matching catalog_sales rows. This reduces the working set from 172M rows to approximately 2.8M rows before joining with dimension tables."
}
```