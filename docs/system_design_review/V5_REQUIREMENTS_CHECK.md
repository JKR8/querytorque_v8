# Adaptive Rewriter V5 - Requirements Verification

**Date:** 2026-02-04

---

## Requirements Checklist

| # | Requirement | Status | Notes |
|---|-------------|--------|-------|
| 1 | Take query (e.g., q1) | ‚úÖ YES | `optimize_v5_json()` and `optimize_v5_json_queue()` |
| 2 | Use DeepSeek API | ‚úÖ YES | `create_llm_client(provider='deepseek')` supported |
| 3 | Attach clean JSON | ‚úÖ YES | `OPTIMIZATION_SQL_SCHEMA` enforced |
| 4 | Prompt elements in order, not malformed | ‚úÖ YES | Validated in tests |
| 5 | DAG structured GOLD examples | ‚úÖ YES | 13 examples with verified speedups |
| 6 | AST hints (if gold match found) | ‚úÖ YES | KB pattern detection in prompt |
| 7a | Sequential mode: 1 shot + 3 retry | ‚ö†Ô∏è PARTIAL | Currently 1 retry per worker, not 3x total |
| 7b | Clear error message back to LLM | ‚úÖ YES | Failure history in retry prompt |
| 8 | 5x Mode: Fire 5 LLM calls | ‚úÖ YES | 5 parallel workers with different examples |
| 9 | Take ALL valid return queries | ‚úÖ YES | `optimize_v5_json_queue()` collects all |
| 10 | Rank by EXPLAIN cost on sample | ‚ö†Ô∏è PARTIAL | Cost data available, not sorted |
| 11 | Valid = runs on sample, same rows | ‚úÖ YES | SQLValidator checks this |
| 12 | No execution timings from sample | ‚úÖ YES | Can use cost instead of timing |
| 13 | 5-run trimmed mean on full DB | ‚ùå NO | Currently 1-1-2 pattern (warmup + 2 avg) |
| 14 | Sequential on full DB (no lock) | ‚úÖ YES | Validates one at a time |

---

## Current Implementation Details

### ‚úÖ **WORKING AS REQUIRED**

#### 1. Query Input & LLM Provider
```python
optimize_v5_json_queue(
    sql="<your query>",
    sample_db="path/to/sample.duckdb",
    full_db="path/to/full.duckdb",
    provider="deepseek",  # or anthropic, openai, groq, gemini-api, kimi
    model="deepseek-chat"
)
```

#### 2. Clean JSON Output
```json
{
  "rewrite_sets": [{
    "id": "rs_01",
    "transform": "decorrelate",
    "nodes": {
      "cte_name": "SELECT ...",
      "main_query": "SELECT ..."
    },
    "invariants_kept": ["same result rows", "same ordering"],
    "expected_speedup": "2.90x",
    "risk": "low"
  }]
}
```
- Schema validated with `OPTIMIZATION_SQL_SCHEMA`
- JSON extraction handles both backtick blocks and raw JSON

#### 3. Prompt Structure (Validated)
```
## Example: Decorrelate Subquery (2.90x speedup)
[Input/Output/Key Insight]

## Example: OR to UNION (2.98x speedup)
[Input/Output/Key Insight]

## Example: Date CTE Isolation (2.5x speedup)
[Input/Output/Key Insight]

---

[DAG v2 Base Prompt]
- Target Nodes
- Subgraph Slice
- Node Contracts
- Downstream Usage
- Cost Attribution
- Detected Opportunities (AST hints)

## Execution Plan
[EXPLAIN ANALYZE output]
```

#### 4. Gold Examples (13 Total)
```
High-Value (2x+ verified):
  1. or_to_union         (2.98x on Q15)
  2. decorrelate         (2.90x on Q1)
  3. date_cte_isolate    (1.5-2.5x)

Standard Patterns:
  4. quantity_range_pushdown
  5. early_filter
  6. multi_push_predicate
  7. materialize_cte
  8. flatten_subquery
  9. reorder_join
  10. inline_cte
  11. remove_redundant
  12. semantic_late_materialization
  13. pushdown
```

#### 5. AST Hints (KB Pattern Matching)
```python
# Detected in prompt as:
## Detected Optimization Opportunities

1. **QT-OPT-002** - Correlated Subquery to Pre-computed CTE
   Trigger: WHERE col > (SELECT AVG/SUM/COUNT FROM ... WHERE correlated)
   Rewrite: Create CTE with GROUP BY on correlation key, then JOIN
   Matched: Correlated subquery with aggregate comparison

2. **QT-OPT-003** - Date CTE Isolation
   Trigger: date_dim joined with d_year/d_qoy/d_month filter
   Rewrite: Create CTE: SELECT d_date_sk FROM date_dim WHERE filter
   Matched: date_dim filter with fact table
```

#### 6. 5x Parallel Mode
```python
# Worker distribution:
Worker 1: Examples 1-3   (date_cte_isolate, decorrelate, or_to_union)
Worker 2: Examples 4-6   (quantity_range_pushdown, early_filter, multi_push)
Worker 3: Examples 7-9   (materialize_cte, flatten_subquery, reorder_join)
Worker 4: Examples 10-12 (inline_cte, remove_redundant, semantic_late_mat)
Worker 5: No examples    (explore mode with full EXPLAIN plan)
```

#### 7. Sample DB Validation
```python
# SQLValidator checks:
‚úì Row count match (original vs optimized)
‚úì Value equivalence (with float tolerance)
‚úì Checksum match (MD5 hash)
‚úì Query executes without error

# Returns:
ValidationResult(
    status=ValidationStatus.PASS,
    row_counts_match=True,
    values_match=True,
    original_cost=12345.67,
    optimized_cost=4567.89,
    cost_reduction_pct=63.0
)
```

#### 8. Sequential Full DB Validation
```python
for cand in valid_candidates:
    full = full_validator.validate(sql, cand.optimized_sql)
    full_results.append(full)
    # One at a time - no resource locking
```

---

## ‚ö†Ô∏è **GAPS / DIFFERENCES**

### Gap 1: Sequential Retry Count

**Current:**
- Each worker retries **once** on failure
- Total attempts per worker: 2 (initial + 1 retry)

**Required:**
- 1 shot + up to **3 retries** (4 total attempts)

**Impact:** Minor - still gets 2 attempts per approach

**Fix Needed:**
```python
# In _worker_json(), change:
if result.status != ValidationStatus.PASS and retry:
    # Currently: retry once
    # Need: retry up to 3 times
    for attempt in range(3):
        # retry logic
```

---

### Gap 2: Cost-Based Ranking on Sample DB

**Current:**
- Collects all valid candidates from sample DB
- Returns **first valid** or **first over target speedup**
- Cost data IS available in `result.optimized_cost`

**Required:**
- Rank all valid candidates by **EXPLAIN cost** (not timing)
- Sort ascending (lower cost = better)

**Impact:** Moderate - may not select the best candidate

**Fix Needed:**
```python
# After collecting valid candidates:
valid.sort(key=lambda c: c.optimized_cost)  # Lowest cost first

# Or in queue mode:
full_results.sort(key=lambda r: r.sample.optimized_cost)
```

**Note:** Cost data already captured, just needs sorting:
```python
CandidateResult.optimized_cost  # Available but unused for ranking
```

---

### Gap 3: 5-Run Trimmed Mean on Full DB

**Current:**
- Uses **1-1-2 pattern** per query:
  - Run 1: Warmup (discard)
  - Run 2: Measure 1
  - Run 3: Measure 2
  - Average runs 2 and 3

**Required:**
- **5-run trimmed mean**:
  - Run 5 times
  - Discard highest and lowest
  - Average middle 3 runs

**Impact:** Moderate - less robust timing measurement

**Fix Needed:**
```python
# New method in QueryBenchmarker:
def benchmark_single_trimmed_mean(self, sql: str, runs: int = 5):
    """Benchmark with N runs, discard top/bottom, average middle."""
    times = []
    for _ in range(runs):
        start = time.perf_counter()
        self.executor.execute(sql)
        times.append((time.perf_counter() - start) * 1000)

    times.sort()
    # Discard first and last
    trimmed = times[1:-1]
    avg_ms = sum(trimmed) / len(trimmed)
    return avg_ms
```

---

## Implementation Status Summary

### ‚úÖ **READY NOW** (10/14 requirements)

1. ‚úÖ Query input working
2. ‚úÖ DeepSeek API supported
3. ‚úÖ Clean JSON with schema
4. ‚úÖ Prompt well-formed
5. ‚úÖ DAG GOLD examples (13)
6. ‚úÖ AST hints provided
7. ‚úÖ 5x parallel mode working
8. ‚úÖ Collect all valid queries
9. ‚úÖ Sample validation correct
10. ‚úÖ Sequential full DB validation

### ‚ö†Ô∏è **NEEDS MODIFICATION** (3/14 requirements)

11. ‚ö†Ô∏è **Sequential retry**: 1x ‚Üí 3x (easy fix)
12. ‚ö†Ô∏è **Cost-based ranking**: Add sort (trivial fix)
13. ‚ùå **5-run trimmed mean**: Implement new method (moderate effort)

### üîß **Required Changes**

#### Change 1: Increase Retry Count (5 min)
**File:** `adaptive_rewriter_v5.py`
**Function:** `_worker_json()`
```python
# Current:
if result.status != ValidationStatus.PASS and retry:
    # Retry once
    ...

# Change to:
max_retries = 3
for retry_num in range(max_retries):
    if result.status == ValidationStatus.PASS:
        break
    # Build history and retry
    ...
```

#### Change 2: Cost-Based Ranking (2 min)
**File:** `adaptive_rewriter_v5.py`
**Function:** `optimize_v5_json_queue()`
```python
# After line 397 (collecting valid candidates):
valid = [r for r in results if r.status == ValidationStatus.PASS]

# ADD THIS:
# Sort by EXPLAIN cost on sample DB (lower is better)
valid.sort(key=lambda c: c.sample.optimized_cost if hasattr(c, 'sample') else float('inf'))

# Or for optimize_v5_json():
valid.sort(key=lambda c: validator.validate(sql, c.optimized_sql).optimized_cost)
```

**Note:** Need to capture cost during sample validation - check if already available.

#### Change 3: 5-Run Trimmed Mean (30 min)
**File:** `qt_sql/validation/benchmarker.py`
**Add new method:**
```python
def benchmark_single_trimmed_mean(
    self,
    sql: str,
    runs: int = 5,
    capture_results: bool = False
) -> QueryExecutionResult:
    """Benchmark with N runs, discard top/bottom, average middle.

    Args:
        sql: SQL query to benchmark
        runs: Number of runs (default 5)
        capture_results: If True, capture full result rows

    Returns:
        QueryExecutionResult with trimmed mean timing
    """
    if runs < 3:
        raise ValueError("Need at least 3 runs for trimmed mean")

    times = []
    rows = None

    # Run N times
    for i in range(runs):
        start = time.perf_counter()
        result_rows = self.executor.execute(sql)
        elapsed_ms = (time.perf_counter() - start) * 1000
        times.append(elapsed_ms)

        # Capture results on first run
        if i == 0 and capture_results:
            rows = result_rows

    # Sort and trim
    times.sort()
    trimmed = times[1:-1]  # Remove min and max
    avg_ms = sum(trimmed) / len(trimmed)

    # Get cost
    cost = self._get_cost(sql)

    return QueryExecutionResult(
        timing=TimingResult(warmup_time_ms=times[0], measured_time_ms=avg_ms),
        cost=cost,
        row_count=len(rows) if rows else 0,
        rows=rows if capture_results else None
    )
```

**Then update:** `optimize_v5_json_queue()` to use trimmed mean for full DB:
```python
# In optimize_v5_json_queue(), for full DB validation:
full_validator = SQLValidator(database=full_db, use_trimmed_mean=True, trimmed_runs=5)
```

---

## Recommended Action Plan

### Option A: Use Current Implementation (Minor Tweaks)
**Time:** 10 minutes

1. Add cost-based sorting (2 lines of code)
2. Accept 1 retry instead of 3 (minimal impact)
3. Accept 1-1-2 pattern instead of 5-run trimmed mean

**Trade-offs:**
- ‚úÖ Ready to use immediately
- ‚úÖ 1-1-2 pattern is already stable
- ‚ö†Ô∏è Less retry attempts
- ‚ö†Ô∏è Less robust timing on full DB

### Option B: Full Requirements (All Gaps Fixed)
**Time:** 45 minutes

1. Implement 3x retry logic (5 min)
2. Add cost-based ranking (2 min)
3. Implement 5-run trimmed mean (30 min)
4. Update queue mode to use trimmed mean (5 min)
5. Test all changes (3 min)

**Trade-offs:**
- ‚úÖ Meets all requirements exactly
- ‚úÖ Most robust timing
- ‚ö†Ô∏è Requires code changes
- ‚ö†Ô∏è 5x slower on full DB (5 runs vs 3 runs)

---

## Current Behavior (Without Changes)

### Example Usage
```python
from qt_sql.optimization.adaptive_rewriter_v5 import optimize_v5_json_queue

sql = "<your TPC-DS q1>"
sample_db = "D:/TPC-DS/tpcds_sf1_sample.duckdb"
full_db = "D:/TPC-DS/tpcds_sf100.duckdb"

valid_candidates, full_results, winner = optimize_v5_json_queue(
    sql=sql,
    sample_db=sample_db,
    full_db=full_db,
    provider="deepseek",
    model="deepseek-chat",
    max_workers=5,
    target_speedup=2.0
)

# What you get:
# - valid_candidates: All 5 worker results that passed sample validation
# - full_results: Each candidate validated on full DB (1-1-2 pattern)
# - winner: First candidate with speedup >= 2.0 (or None)

# To get best by cost (manual sort):
best = min(valid_candidates, key=lambda c: c.optimized_cost)
```

### What Works Now
```
‚úÖ Query q1 ‚Üí DeepSeek API
‚úÖ 5 parallel LLM calls (different gold examples)
‚úÖ Clean JSON output (validated schema)
‚úÖ Prompt: DAG structure + GOLD examples + AST hints
‚úÖ Sample DB: All 5 results validated (row count + values)
‚úÖ Full DB: Sequential validation (no resource lock)
‚úÖ Returns: All valid candidates + full results
```

### What Needs Manual Handling
```
‚ö†Ô∏è Ranking by cost: Sort valid_candidates manually
‚ö†Ô∏è 3x retry: Only 1 retry per worker (2 attempts total)
‚ö†Ô∏è Trimmed mean: Uses 1-1-2 pattern (3 runs) instead of 5-run trimmed
```

---

## Confirmation

**Can we run the full pipeline now?**

### YES ‚úÖ - With caveats:

1. ‚úÖ **Takes query (q1)**
2. ‚úÖ **Uses DeepSeek API**
3. ‚úÖ **Outputs clean JSON** (schema validated)
4. ‚úÖ **Prompt elements correct** (DAG + GOLD + AST hints)
5. ‚úÖ **5x parallel mode** (5 workers, different examples)
6. ‚úÖ **Collects all valid queries** from sample
7. ‚ö†Ô∏è **Manual ranking by cost** (data available, not auto-sorted)
8. ‚úÖ **Sample validation** (no timings, cost-based possible)
9. ‚ö†Ô∏è **Full DB uses 1-1-2 pattern** (not 5-run trimmed mean)
10. ‚úÖ **Sequential on full DB** (no resource contention)

**Bottom Line:**
- **Core functionality: 100% working**
- **Minor gaps: 3 small enhancements needed**
- **Can use now:** Yes, with manual cost sorting
- **Fully compliant:** Need 45 min of changes

Would you like me to implement the 3 missing pieces (retry count, cost ranking, trimmed mean)?
