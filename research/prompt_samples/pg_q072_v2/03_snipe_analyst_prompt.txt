You are the diagnostic analyst for query query072_agg_s1. You've seen 4 parallel attempts at >=2.0x speedup on this query. Your job: diagnose what worked, what didn't, and WHY — then design a strategy the sniper couldn't have known without these empirical results.

## Target: >=2.0x speedup
Anything below 2.0x is a miss. The sniper you deploy must be given a strategy with genuine headroom to reach this bar.

## Previous Optimization Attempts
Target: **>=2.0x** | 4 workers tried | none reached target

### W1: star_join_prefetch → 1.15x ★ BEST [PASS, below target (1.15x)]
- **Examples**: pg_date_cte_explicit_join, pg_dimension_prefetch_star
- **Transforms**: pushdown, date_cte_isolate
- **Approach**: Pre-filter date dim into CTE, then join star schema
- **Optimized SQL:**
```sql
WITH filtered_dates AS (
  SELECT d_date_sk, d_week_seq, d_date
  FROM date_dim
  WHERE d_year = 1998
)
SELECT i_item_desc, w_warehouse_name, d_week_seq
FROM catalog_sales cs
JOIN filtered_dates fd ON cs.cs_sold_date_sk = fd.d_date_sk
-- ... rest of query
```

### W4: exploration_novel → 0.92x [REGRESSION (0.92x)]
- **Transforms**: novel
- **Optimized SQL:**
```sql
SELECT i_item_desc, w_warehouse_name, d_week_seq
FROM catalog_sales cs
JOIN date_dim d1 ON cs.cs_sold_date_sk = d1.d_date_sk
-- ... novel approach
```

### W2: decorrelate_subquery → 0.85x [ERROR]
- **Examples**: pg_decorrelate_exists
- **Transforms**: decorrelate
- **Error**: Row count mismatch: expected 142, got 198
- **Optimized SQL:**
```sql
SELECT i_item_desc, w_warehouse_name, d_week_seq
FROM catalog_sales cs
JOIN inventory inv ON inv.inv_item_sk = cs.cs_item_sk
-- ... decorrelated version
```

### W3: scan_consolidation → 0.0x [ERROR]
- **Examples**: pg_single_pass_agg
- **Error**: syntax error at or near 'FILTER' (line 12)
- **Optimized SQL:**
```sql
-- attempted scan consolidation
SELECT 1
```

## Original SQL (query072_agg_s1, postgresql v14.3)

```sql
 1 | select  i_item_desc
 2 |       ,w_warehouse_name
 3 |       ,d1.d_week_seq
 4 |       ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo
 5 |       ,sum(case when p_promo_sk is not null then 1 else 0 end) promo
 6 |       ,count(*) total_cnt
 7 | from catalog_sales
 8 | join inventory on (cs_item_sk = inv_item_sk)
 9 | join warehouse on (w_warehouse_sk=inv_warehouse_sk)
10 | join item on (i_item_sk = cs_item_sk)
11 | join customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)
12 | join household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)
13 | join date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)
14 | join date_dim d2 on (inv_date_sk = d2.d_date_sk)
15 | join date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)
16 | left outer join promotion on (cs_promo_sk=p_promo_sk)
17 | left outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)
18 | where d1.d_week_seq = d2.d_week_seq
19 |   and inv_quantity_on_hand < cs_quantity
20 |   and d3.d_date > d1.d_date + interval '3 day'
21 |   and hd_buy_potential = '>10000'
22 |   and d1.d_year = 1998
23 |   and cd_marital_status = 'U'
24 |   and cd_dep_count between 9 and 11
25 |   and i_category IN ('Children', 'Jewelry', 'Men')
26 |   and cs_wholesale_cost BETWEEN 35 AND 55
27 | group by i_item_desc,w_warehouse_name,d1.d_week_seq
28 | order by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq
29 | limit 100;
```

## EXPLAIN ANALYZE Plan

```
-> Limit  (rows=0 loops=1 time=0.0ms)
  -> Sort  (rows=0 loops=1 time=0.0ms)
    -> Aggregate  (rows=0 loops=1 time=0.0ms)
      -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
         Join Filter: (d3.d_date > (d1.d_date + '3 days'::interval))
        -> Gather Merge  (rows=0 loops=1 time=0.0ms)
           Workers: None/1 launched
          -> Sort  (rows=0 loops=1 time=0.0ms)
            -> Nested Loop Left  (rows=0 loops=1 time=0.0ms)
              -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
                -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
                   Join Filter: (inventory.inv_warehouse_sk = warehouse.w_warehouse_sk)
                  -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
                    -> Hash Join Inner  (rows=0 loops=1 time=0.0ms)
                       Hash Cond: (d2.d_week_seq = d1.d_week_seq)
                      -> Seq Scan on date_dim d2  (rows=0 loops=1 time=0.0ms)
                      -> Hash  (rows=0 loops=1 time=0.0ms)
                        -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
                          -> Hash Join Inner  (rows=0 loops=1 time=0.0ms)
                             Hash Cond: (catalog_sales.cs_bill_hdemo_sk = household_demographics.hd_demo_sk)
                            -> Nested Loop Inner  (rows=0 loops=1 time=0.0ms)
                              -> Index Scan on date_dim d1  (rows=0 loops=1 time=0.0ms)
                                 Filter: (d1.d_year = 1998)
                              -> Index Scan on catalog_sales  (rows=0 loops=1 time=0.0ms)
                                 Filter: ((catalog_sales.cs_wholesale_cost >= '35'::numeric) AND (catalog_sales.cs_wholesale_cost <= '55':...
                                 Index Cond: (catalog_sales.cs_sold_date_sk = d1.d_date_sk)
                            -> Hash  (rows=0 loops=1 time=0.0ms)
                              -> Seq Scan on household_demographics  (rows=0 loops=1 time=0.0ms)
                                 Filter: (household_demographics.hd_buy_potential = '>10000'::bpchar)
                          -> Index Scan on customer_demographics  (rows=0 loops=1 time=0.0ms)
                             Filter: ((customer_demographics.cd_dep_count >= 9) AND (customer_demographics.cd_dep_count <= 11) AND (cu...
                             Index Cond: (customer_demographics.cd_demo_sk = catalog_sales.cs_bill_cdemo_sk)
                    -> Index Scan on inventory  (rows=0 loops=1 time=0.0ms)
                       Filter: (inventory.inv_quantity_on_hand < catalog_sales.cs_quantity)
                       Index Cond: ((inventory.inv_date_sk = d2.d_date_sk) AND (inventory.inv_item_sk = catalog_sales.cs_item_sk))
                  -> Seq Scan on warehouse  (rows=0 loops=1 time=0.0ms)
                -> Index Scan on item  (rows=0 loops=1 time=0.0ms)
                   Filter: (item.i_category = ANY ('{Children,Jewelry,Men}'::bpchar[]))
                   Index Cond: (item.i_item_sk = catalog_sales.cs_item_sk)
              -> Index Only Scan on promotion  (rows=0 loops=1 time=0.0ms)
                 Index Cond: (promotion.p_promo_sk = catalog_sales.cs_promo_sk)
        -> Index Scan on date_dim d3  (rows=0 loops=1 time=0.0ms)
           Index Cond: (d3.d_date_sk = catalog_sales.cs_ship_date_sk)
```

## Query Structure (DAG)

### 1. main_query
**Role**: Root / Output (Definition Order: 0)
**Stats**: 0% Cost | ~0 rows
**Flags**: multi_join, non_equi_join, left_outer_join
**Outputs**: [?]
**Dependencies**: catalog_sales, inventory, warehouse, item, customer_demographics, household_demographics, date_dim, promotion, catalog_returns
**Key Logic (SQL)**:
```sql
select i_item_desc ,w_warehouse_name ,d1.d_week_seq ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo ,sum(case when p_promo_sk is not null then 1 else 0 end) promo ,count(*) total_cnt from catalog_sales join inventory on (cs_item_sk = inv_item_sk) join warehouse on (w_warehouse_sk=inv_warehouse_sk) join item on (i_item_sk = cs_item_sk) join customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk) join household_demographics on (cs_bill_hdemo_sk = hd_demo_sk) join date_dim d1 on (cs_ ...
```


## Aggregation Semantics Check

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Changing group membership changes the result.
- **AVG and STDDEV are NOT duplicate-safe**: join-introduced row duplication changes the aggregate.
- When splitting with GROUP BY + aggregate, each branch must preserve exact GROUP BY columns and filter to the same row set.

## Engine Profile

*This is field intelligence gathered from 53 DSB queries at SF5-SF10. PostgreSQL is a fundamentally different optimizer than DuckDB — it has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here. Use this to guide your analysis but apply your own judgment — every query is different. Add to this knowledge if you observe something new.*

### Optimizer Strengths (DO NOT fight these)
- **BITMAP_OR_SCAN**: Multi-branch OR conditions on indexed columns are handled via BitmapOr — a single fact table scan with bitmap combination. Extremely efficient.
- **SEMI_JOIN_EXISTS**: EXISTS/NOT EXISTS uses semi-join with early termination. Stops scanning after the first match per outer row.
- **INNER_JOIN_REORDERING**: PostgreSQL freely reorders INNER JOINs based on estimated selectivity. The cost model works well for explicit JOIN...ON syntax.
- **INDEX_ONLY_SCAN**: When an index covers all requested columns, PostgreSQL reads only the index without touching the heap.
- **PARALLEL_QUERY_EXECUTION**: PostgreSQL parallelizes large scans and aggregations across worker processes with partial aggregation finalization.
- **JIT_COMPILATION**: PostgreSQL JIT-compiles complex expressions and tuple deforming for long-running queries.

### Optimizer Gaps (opportunities)
- **COMMA_JOIN_WEAKNESS**: Implicit comma-separated FROM tables (FROM t1, t2, t3 WHERE t1.id = t2.id) are treated as cross products initially. The cost model is significantly weaker on comma-joins than on explicit JOIN...ON syntax.
  Opportunity: Convert comma-joins to explicit JOIN...ON syntax. This alone can unlock 2-3x improvements. Best when combined with date_cte_isolate.
- **CORRELATED_SUBQUERY_PARALYSIS**: Cannot automatically decorrelate complex correlated subqueries. Correlated scalar subqueries with aggregates are executed as nested-loop with repeated evaluation.
  Opportunity: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
- **NON_EQUI_JOIN_INPUT_BLINDNESS**: Cannot pre-filter fact tables before non-equi join operations (date arithmetic, range comparisons, quantity < quantity). Non-equi joins fall back to nested-loop, which is O(N*M).
  Opportunity: Reduce fact table input size via filtered CTE before the non-equi join.
- **CTE_MATERIALIZATION_FENCE**: PostgreSQL materializes CTEs by default (multi-referenced) or by choice (AS MATERIALIZED). This creates a hard optimization fence — no predicate pushdown from outer query into CTE. This makes CTE-based strategies a double-edged sword on PG.
  Opportunity: Use materialization STRATEGICALLY: materialize when the CTE is expensive and reused multiple times. Avoid CTEs that fence off predicate pushdown for single-use cases.
- **CROSS_CTE_PREDICATE_BLINDNESS**: Same gap as DuckDB but WORSE on PostgreSQL because CTE materialization fence makes it more impactful. Predicates in the outer WHERE cannot propagate into materialized CTEs.
  Opportunity: Same as DuckDB: pre-filter into CTE definition. But be more cautious — only when the CTE is clearly suboptimal.

## Tag-Matched Examples (6)

### pg_date_cte_explicit_join (2.28x)
**Description:** Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.
**Principle:** Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.

### pg_dimension_prefetch_star (3.32x)
**Description:** On multi-channel UNION queries with comma-separated implicit joins, pre-filter dimension tables (date, item, promotion) into CTEs and convert to explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates and join ordering from explicit JOINs with pre-materialized small dimension results.
**Principle:** Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.

### early_filter_decorrelate (1.13x)
**Principle:** Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.

### inline_decorrelate_materialized (timeout_rescuex)
**Principle:** Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs — (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data — then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.

### pg_materialized_dimension_fact_prefilter (2.68x)
**Description:** Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then join with explicit JOIN syntax. On queries with expensive non-equi joins (inventory quantity < sales quantity, week_seq correlation), reducing both dimension AND fact table sizes before the join dramatically cuts the search space. The MATERIALIZED keyword on PG12+ forces early execution of each CTE.
**Principle:** Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.

### pg_self_join_decomposition (3.93x)
**Description:** Eliminate duplicate fact table scans in self-join patterns by computing the aggregation ONCE in a CTE and deriving both per-item and per-store averages from the same materialized result. PostgreSQL materializes CTEs by default, making this extremely effective.
**Principle:** Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.

## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] LITERAL_PRESERVATION**: Correctness gate: Literal Preservation

**[CRITICAL] SEMANTIC_EQUIVALENCE**: Correctness gate: Semantic Equivalence

**[CRITICAL] COMPLETE_OUTPUT**: Correctness gate: Complete Output

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: Correctness gate: Cte Column Completeness

## Your Task

Work through these 3 steps in a `<reasoning>` block, then output the structured briefing below:

1. **DIAGNOSE**: Why did the best worker achieve 1.15x instead of the 2.0x target? Why did each other worker fail or regress? Be specific about structural mechanisms.
2. **IDENTIFY**: What optimization angles couldn't have been designed BEFORE seeing these empirical results? What did the results reveal about the query's actual execution behavior?
3. **SYNTHESIZE**: Design a strategy for the sniper that builds on the best foundation (if any) and exploits the newly-revealed angles. The sniper has full freedom — give it direction, not constraints.

### Output Format (follow EXACTLY)

```
=== SNIPE BRIEFING ===

FAILURE_SYNTHESIS:
<WHY the best worker won, WHY each other failed — structural mechanisms>

BEST_FOUNDATION:
<What to build on from the best result, or 'None — start fresh' if all regressed>

UNEXPLORED_ANGLES:
<What optimization approaches couldn't have been designed pre-empirically>

STRATEGY_GUIDANCE:
<Synthesized approach for the sniper — ADVISORY, not mandatory>

EXAMPLES: <ex1>, <ex2>, <ex3>

EXAMPLE_ADAPTATION:
<For each example: what to APPLY and what to IGNORE>

HAZARD_FLAGS:
<Risks based on observed failures — what NOT to do>

RETRY_WORTHINESS: high|low — <reason>
(Is there genuine headroom for a second sniper attempt if the first misses 2.0x?)

RETRY_DIGEST:
<5-10 line compact failure guide for sniper2 IF retry is needed.
What broke, why, what to change. The lesson, not the artifact.>
```