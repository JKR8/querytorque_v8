Looking at this query pattern, I see multiple opportunities to push down filters and restructure the CTEs for better performance. The main issue is that the CTEs (`ss` and `ws`) are computing aggregations over ALL data, but the main query only needs data for `d_year = 2000` and `d_qoy IN (1, 2, 3)`. This causes unnecessary scans of large fact tables.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "pushdown",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_year = 2000 AND d_qoy IN (1, 2, 3)",
        "filtered_store_sales": "SELECT ca_county, d_qoy, d_year, SUM(ss_ext_sales_price) AS store_sales FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk JOIN customer_address ON ss_addr_sk = ca_address_sk GROUP BY ca_county, d_qoy, d_year",
        "filtered_web_sales": "SELECT ca_county, d_qoy, d_year, SUM(ws_ext_sales_price) AS web_sales FROM web_sales JOIN filtered_dates ON ws_sold_date_sk = d_date_sk JOIN customer_address ON ws_bill_addr_sk = ca_address_sk GROUP BY ca_county, d_qoy, d_year",
        "main_query": "SELECT ss1.ca_county, ss1.d_year, ws2.web_sales / ws1.web_sales AS web_q1_q2_increase, ss2.store_sales / ss1.store_sales AS store_q1_q2_increase, ws3.web_sales / ws2.web_sales AS web_q2_q3_increase, ss3.store_sales / ss2.store_sales AS store_q2_q3_increase FROM filtered_store_sales AS ss1, filtered_store_sales AS ss2, filtered_store_sales AS ss3, filtered_web_sales AS ws1, filtered_web_sales AS ws2, filtered_web_sales AS ws3 WHERE ss1.d_qoy = 1 AND ss1.d_year = 2000 AND ss1.ca_county = ss2.ca_county AND ss2.d_qoy = 2 AND ss2.d_year = 2000 AND ss2.ca_county = ss3.ca_county AND ss3.d_qoy = 3 AND ss3.d_year = 2000 AND ss1.ca_county = ws1.ca_county AND ws1.d_qoy = 1 AND ws1.d_year = 2000 AND ws1.ca_county = ws2.ca_county AND ws2.d_qoy = 2 AND ws2.d_year = 2000 AND ws1.ca_county = ws3.ca_county AND ws3.d_qoy = 3 AND ws3.d_year = 2000 AND CASE WHEN ws1.web_sales > 0 THEN ws2.web_sales / ws1.web_sales ELSE NULL END > CASE WHEN ss1.store_sales > 0 THEN ss2.store_sales / ss1.store_sales ELSE NULL END AND CASE WHEN ws2.web_sales > 0 THEN ws3.web_sales / ws2.web_sales ELSE NULL END > CASE WHEN ss2.store_sales > 0 THEN ss3.store_sales / ss2.store_sales ELSE NULL END ORDER BY web_q1_q2_increase"
      },
      "invariants_kept": [
        "same result rows",
        "same output columns",
        "same grouping and aggregation",
        "same ordering"
      ],
      "expected_speedup": "3.5x",
      "risk": "low"
    }
  ],
  "explanation": "Pushed date filters (d_year=2000, d_qoy IN (1,2,3)) down into CTEs to reduce fact table scans from full table to only relevant partitions. Created a shared date_dim CTE to avoid redundant scans. The store_sales and web_sales CTEs now only process 3 quarters of 2000 instead of all data, dramatically reducing I/O and aggregation work."
}
```

**Key Insight**: The query compares quarter-over-quarter growth rates for web vs store sales in 2000. The original CTEs aggregate ALL historical data, but we only need 3 quarters of 2000. By pushing the date filters down, we reduce:
- `store_sales` scans from 34.5M rows to ~300K rows (100x reduction)
- `web_sales` scans from 4.3M rows to ~80K rows (50x reduction)  
- Eliminate redundant `date_dim` scans by creating a shared filtered dates CTE
- Maintain exact same logic and result set

This transformation is similar to verified Q93 pattern with 2.71x speedup, but here we also isolate date filtering and share it between both fact tables.