Optimize this SQL query.

## Execution Plan

**Operators by cost:**
- SEQ_SCAN (store_sales): 67.4% cost, 137,600 rows
- SEQ_SCAN (store_sales): 4.6% cost, 137,433 rows
- SEQ_SCAN (store_sales): 3.6% cost, 137,987 rows
- SEQ_SCAN (store_sales): 3.4% cost, 137,144 rows
- SEQ_SCAN (store_sales): 3.3% cost, 137,677 rows

**Table scans:**
- store_sales: 137,334 rows ← FILTERED by ss_quantity>=21 AND ss_quantity<=25
- store_sales: 137,433 rows ← FILTERED by ss_quantity>=26 AND ss_quantity<=30
- store_sales: 137,987 rows ← FILTERED by ss_quantity>=16 AND ss_quantity<=20
- store_sales: 137,600 rows ← FILTERED by ss_quantity>=11 AND ss_quantity<=15
- store_sales: 137,144 rows ← FILTERED by ss_quantity>=6 AND ss_quantity<=10
- store_sales: 137,677 rows ← FILTERED by ss_quantity>=0 AND ss_quantity<=5

---

## Block Map
```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ BLOCK                  │ CLAUSE   │ CONTENT SUMMARY                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│ main_query             │ .select  │ *                                             │
│                        │ .from    │ (subquery: store_sales)                       │
│                        │ .where   │ ss_quantity BETWEEN 0 AND 5 AND (ss_list_p... │
└─────────────────────────────────────────────────────────────────────────────────┘

```

---

## Optimization Patterns

These patterns have produced >2x speedups:

1. **Dimension filter hoisting**: If a filtered dimension is in main_query but the CTE aggregates fact data that COULD be filtered by it (via FK), move the dimension join+filter INTO the CTE to filter early.

2. **Correlated subquery to window function**: A correlated subquery computes an aggregate per group. Fix: Replace with a window function in the CTE (e.g., `AVG(...) OVER (PARTITION BY group_col)`).

3. **Join elimination**: A table is joined only to validate a foreign key exists, but no columns from it are used. Fix: Remove the join, add `WHERE fk_column IS NOT NULL`.

4. **UNION ALL decomposition**: Complex OR conditions cause full scans. Fix: Split into separate queries with simple filters, UNION ALL results.

5. **Scan consolidation**: Same table scanned multiple times with different filters. Fix: Single scan with CASE WHEN expressions to compute multiple aggregates conditionally.

**Verify**: Optimized query must return identical results.

---

## SQL
```sql
-- start query 28 in stream 0 using template query28.tpl
select *
from (select avg(ss_list_price) B1_LP
            ,count(ss_list_price) B1_CNT
            ,count(distinct ss_list_price) B1_CNTD
      from store_sales
      where ss_quantity between 0 and 5
        and (ss_list_price between 131 and 131+10 
             or ss_coupon_amt between 16798 and 16798+1000
             or ss_wholesale_cost between 25 and 25+20)) B1,
     (select avg(ss_list_price) B2_LP
            ,count(ss_list_price) B2_CNT
            ,count(distinct ss_list_price) B2_CNTD
      from store_sales
      where ss_quantity between 6 and 10
        and (ss_list_price between 145 and 145+10
          or ss_coupon_amt between 14792 and 14792+1000
          or ss_wholesale_cost between 46 and 46+20)) B2,
     (select avg(ss_list_price) B3_LP
            ,count(ss_list_price) B3_CNT
            ,count(distinct ss_list_price) B3_CNTD
      from store_sales
      where ss_quantity between 11 and 15
        and (ss_list_price between 150 and 150+10
          or ss_coupon_amt between 6600 and 6600+1000
          or ss_wholesale_cost between 9 and 9+20)) B3,
     (select avg(ss_list_price) B4_LP
            ,count(ss_list_price) B4_CNT
            ,count(distinct ss_list_price) B4_CNTD
      from store_sales
      where ss_quantity between 16 and 20
        and (ss_list_price between 91 and 91+10
          or ss_coupon_amt between 13493 and 13493+1000
          or ss_wholesale_cost between 36 and 36+20)) B4,
     (select avg(ss_list_price) B5_LP
            ,count(ss_list_price) B5_CNT
            ,count(distinct ss_list_price) B5_CNTD
      from store_sales
      where ss_quantity between 21 and 25
        and (ss_list_price between 0 and 0+10
          or ss_coupon_amt between 7629 and 7629+1000
          or ss_wholesale_cost between 6 and 6+20)) B5,
     (select avg(ss_list_price) B6_LP
            ,count(ss_list_price) B6_CNT
            ,count(distinct ss_list_price) B6_CNTD
      from store_sales
      where ss_quantity between 26 and 30
        and (ss_list_price between 89 and 89+10
          or ss_coupon_amt between 15257 and 15257+1000
          or ss_wholesale_cost between 31 and 31+20)) B6
 LIMIT 100;

-- end query 28 in stream 0 using template query28.tpl
```

---

## Output

Return JSON:
```json
{
  "operations": [...],
  "semantic_warnings": [],
  "explanation": "..."
}
```

### Operations

| Op | Fields | Description |
|----|--------|-------------|
| `add_cte` | `after`, `name`, `sql` | Insert new CTE |
| `delete_cte` | `name` | Remove CTE |
| `replace_cte` | `name`, `sql` | Replace entire CTE body |
| `replace_clause` | `target`, `sql` | Replace clause (`""` to remove) |
| `patch` | `target`, `patches[]` | Snippet search/replace |

### Example
```json
{
  "operations": [
    {"op": "replace_cte", "name": "my_cte", "sql": "SELECT sk, SUM(val) FROM t WHERE sk IS NOT NULL GROUP BY sk"}
  ],
  "semantic_warnings": ["Removed join - added IS NOT NULL to preserve filtering"],
  "explanation": "Removed unnecessary dimension join, using FK directly"
}
```

### Block ID Syntax
```
{cte}.select    {cte}.from    {cte}.where    {cte}.group_by    {cte}.having
main_query.union[N].select    main_query.union[N].from    ...
```

### Rules
1. **Return 1-5 operations maximum** - focus on highest-impact changes first
2. Operations apply sequentially
3. `patch.search` must be unique within target clause
4. `add_cte.sql` = query body only (no CTE name)
5. All CTE refs must resolve after ops
6. When removing a join, update column references (e.g., `c_customer_sk` → `ss_customer_sk AS c_customer_sk`)

The system will iterate if more optimization is possible. You don't need to fix everything at once.