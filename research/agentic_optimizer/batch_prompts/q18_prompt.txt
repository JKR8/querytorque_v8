Optimize this SQL query.

## Execution Plan

**Operators by cost:**
- SEQ_SCAN (customer): 42.5% cost, 1,999,628 rows
- SEQ_SCAN (customer_address): 21.9% cost, 999,910 rows
- SEQ_SCAN (customer_demographics): 9.8% cost, 137,200 rows
- SEQ_SCAN (catalog_sales): 5.2% cost, 284,939 rows
- HASH_JOIN: 3.5% cost, 1,999,628 rows

**Table scans:**
- item: 203,864 rows ← FILTERED by i_item_sk<=203999
- customer_demographics: 1,920,309 rows ← FILTERED by cd_demo_sk<=1920798
- customer_address: 999,910 rows ← FILTERED by optional: ca_state IN ('WA', 'GA', 'NC', 'ME', 'WY
- customer: 1,999,628 rows ← FILTERED by optional: c_birth_month IN (1, 2, 4, 7, 8, 10)
- customer_demographics: 137,200 rows ← FILTERED by ["cd_gender='F'", "cd_education_status='Advanced Degree'"]
- catalog_sales: 284,939 rows (NO FILTER)
- date_dim: 365 rows ← FILTERED by d_year=1998

---

## Block Map
```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ BLOCK                  │ CLAUSE   │ CONTENT SUMMARY                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│ main_query             │ .select  │ i_item_id, ca_country, ca_state, ca_county    │
│                        │ .from    │ catalog_sales                                 │
│                        │ .where   │ cs_sold_date_sk = d_date_sk AND cs_item_sk... │
│                        │ .group_by │                                               │
└─────────────────────────────────────────────────────────────────────────────────┘

```

---

## Optimization Patterns

These patterns have produced >2x speedups:

1. **Dimension filter hoisting**: If a filtered dimension is in main_query but the CTE aggregates fact data that COULD be filtered by it (via FK), move the dimension join+filter INTO the CTE to filter early.

2. **Correlated subquery to window function**: A correlated subquery computes an aggregate per group. Fix: Replace with a window function in the CTE (e.g., `AVG(...) OVER (PARTITION BY group_col)`).

3. **Join elimination**: A table is joined only to validate a foreign key exists, but no columns from it are used. Fix: Remove the join, add `WHERE fk_column IS NOT NULL`.

4. **UNION ALL decomposition**: Complex OR conditions cause full scans. Fix: Split into separate queries with simple filters, UNION ALL results.

5. **Scan consolidation**: Same table scanned multiple times with different filters. Fix: Single scan with CASE WHEN expressions to compute multiple aggregates conditionally.

**Verify**: Optimized query must return identical results.

---

## SQL
```sql
-- start query 18 in stream 0 using template query18.tpl
select i_item_id,
        ca_country,
        ca_state, 
        ca_county,
        avg( cast(cs_quantity as decimal(12,2))) agg1,
        avg( cast(cs_list_price as decimal(12,2))) agg2,
        avg( cast(cs_coupon_amt as decimal(12,2))) agg3,
        avg( cast(cs_sales_price as decimal(12,2))) agg4,
        avg( cast(cs_net_profit as decimal(12,2))) agg5,
        avg( cast(c_birth_year as decimal(12,2))) agg6,
        avg( cast(cd1.cd_dep_count as decimal(12,2))) agg7
 from catalog_sales, customer_demographics cd1, 
      customer_demographics cd2, customer, customer_address, date_dim, item
 where cs_sold_date_sk = d_date_sk and
       cs_item_sk = i_item_sk and
       cs_bill_cdemo_sk = cd1.cd_demo_sk and
       cs_bill_customer_sk = c_customer_sk and
       cd1.cd_gender = 'F' and 
       cd1.cd_education_status = 'Advanced Degree' and
       c_current_cdemo_sk = cd2.cd_demo_sk and
       c_current_addr_sk = ca_address_sk and
       c_birth_month in (10,7,8,4,1,2) and
       d_year = 1998 and
       ca_state in ('WA','GA','NC'
                   ,'ME','WY','OK','IN')
 group by rollup (i_item_id, ca_country, ca_state, ca_county)
 order by ca_country,
        ca_state, 
        ca_county,
	i_item_id
 LIMIT 100;

-- end query 18 in stream 0 using template query18.tpl
```

---

## Output

Return JSON:
```json
{
  "operations": [...],
  "semantic_warnings": [],
  "explanation": "..."
}
```

### Operations

| Op | Fields | Description |
|----|--------|-------------|
| `add_cte` | `after`, `name`, `sql` | Insert new CTE |
| `delete_cte` | `name` | Remove CTE |
| `replace_cte` | `name`, `sql` | Replace entire CTE body |
| `replace_clause` | `target`, `sql` | Replace clause (`""` to remove) |
| `patch` | `target`, `patches[]` | Snippet search/replace |

### Example
```json
{
  "operations": [
    {"op": "replace_cte", "name": "my_cte", "sql": "SELECT sk, SUM(val) FROM t WHERE sk IS NOT NULL GROUP BY sk"}
  ],
  "semantic_warnings": ["Removed join - added IS NOT NULL to preserve filtering"],
  "explanation": "Removed unnecessary dimension join, using FK directly"
}
```

### Block ID Syntax
```
{cte}.select    {cte}.from    {cte}.where    {cte}.group_by    {cte}.having
main_query.union[N].select    main_query.union[N].from    ...
```

### Rules
1. **Return 1-5 operations maximum** - focus on highest-impact changes first
2. Operations apply sequentially
3. `patch.search` must be unique within target clause
4. `add_cte.sql` = query body only (no CTE name)
5. All CTE refs must resolve after ops
6. When removing a join, update column references (e.g., `c_customer_sk` → `ss_customer_sk AS c_customer_sk`)

The system will iterate if more optimization is possible. You don't need to fix everything at once.