Optimize this SQL query.

## Execution Plan

**Operators by cost:**
- SEQ_SCAN (store_sales): 43.5% cost, 3 rows
- SEQ_SCAN (customer): 21.3% cost, 1,990,698 rows
- SEQ_SCAN (customer_address): 17.2% cost, 995,262 rows
- SEQ_SCAN (web_sales): 10.1% cost, 6,957 rows
- SEQ_SCAN (item): 1.6% cost, 5,172 rows

**Table scans:**
- date_dim: 31 rows ← FILTERED by ['d_year=1998', 'd_moy=5']
- date_dim: 529 rows ← FILTERED by d_date_sk>=2450816 AND d_date_sk<=2452642
- store_sales: 3 rows ← FILTERED by ss_customer_sk>=3
- store: 343 rows (NO FILTER)
- customer_address: 995,262 rows (NO FILTER)
- customer: 1,990,698 rows ← FILTERED by c_customer_sk>=3
- catalog_sales: 13,979 rows ← FILTERED by (__internal_decompress_integral_bigint(__internal_
- web_sales: 6,957 rows ← FILTERED by (__internal_decompress_integral_bigint(__internal_
- date_dim: 31 rows ← FILTERED by ['d_moy=5', 'd_year=1998']
- item: 5,172 rows ← FILTERED by ["i_category='Women'", "i_class='maternity'"]
- date_dim: 31 rows ← FILTERED by ['d_year=1998', 'd_moy=5']

---

## Block Map
```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ BLOCK                  │ CLAUSE   │ CONTENT SUMMARY                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│ my_customers           │ .select  │ c_customer_sk, c_current_addr_sk              │
│                        │ .from    │ (subquery: catalog_sales, web_sales)          │
│                        │ .where   │ sold_date_sk = d_date_sk AND item_sk = i_i... │
├─────────────────────────────────────────────────────────────────────────────────┤
│ my_revenue             │ .select  │ c_customer_sk, revenue                        │
│                        │ .from    │                                               │
│                        │ .where   │ c_current_addr_sk = ca_address_sk AND ca_c... │
│                        │ .group_by │ c_customer_sk                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│ segments               │ .select  │ segment                                       │
│                        │ .from    │                                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│ main_query             │ .select  │ segment, num_customers, segment_base          │
│                        │ .from    │                                               │
│                        │ .where   │ sold_date_sk = d_date_sk AND item_sk = i_i... │
│                        │ .group_by │ segment                                       │
└─────────────────────────────────────────────────────────────────────────────────┘

Refs:
  my_revenue.from → my_customers
  segments.from → my_revenue
  main_query.from → segments

Repeated Scans:
  date_dim: 3× (my_customers.from, my_revenue.from, main_query.from)
  item: 2× (my_customers.from, main_query.from)
  customer: 2× (my_customers.from, main_query.from)
  store_sales: 2× (my_revenue.from, main_query.from)
  customer_address: 2× (my_revenue.from, main_query.from)
  store: 2× (my_revenue.from, main_query.from)

```

---

## Optimization Patterns

These patterns have produced >2x speedups:

1. **Dimension filter hoisting**: If a filtered dimension is in main_query but the CTE aggregates fact data that COULD be filtered by it (via FK), move the dimension join+filter INTO the CTE to filter early.

2. **Correlated subquery to window function**: A correlated subquery computes an aggregate per group. Fix: Replace with a window function in the CTE (e.g., `AVG(...) OVER (PARTITION BY group_col)`).

3. **Join elimination**: A table is joined only to validate a foreign key exists, but no columns from it are used. Fix: Remove the join, add `WHERE fk_column IS NOT NULL`.

4. **UNION ALL decomposition**: Complex OR conditions cause full scans. Fix: Split into separate queries with simple filters, UNION ALL results.

5. **Scan consolidation**: Same table scanned multiple times with different filters. Fix: Single scan with CASE WHEN expressions to compute multiple aggregates conditionally.

**Verify**: Optimized query must return identical results.

---

## SQL
```sql
-- start query 54 in stream 0 using template query54.tpl
with my_customers as (
 select distinct c_customer_sk
        , c_current_addr_sk
 from   
        ( select cs_sold_date_sk sold_date_sk,
                 cs_bill_customer_sk customer_sk,
                 cs_item_sk item_sk
          from   catalog_sales
          union all
          select ws_sold_date_sk sold_date_sk,
                 ws_bill_customer_sk customer_sk,
                 ws_item_sk item_sk
          from   web_sales
         ) cs_or_ws_sales,
         item,
         date_dim,
         customer
 where   sold_date_sk = d_date_sk
         and item_sk = i_item_sk
         and i_category = 'Women'
         and i_class = 'maternity'
         and c_customer_sk = cs_or_ws_sales.customer_sk
         and d_moy = 5
         and d_year = 1998
 )
 , my_revenue as (
 select c_customer_sk,
        sum(ss_ext_sales_price) as revenue
 from   my_customers,
        store_sales,
        customer_address,
        store,
        date_dim
 where  c_current_addr_sk = ca_address_sk
        and ca_county = s_county
        and ca_state = s_state
        and ss_sold_date_sk = d_date_sk
        and c_customer_sk = ss_customer_sk
        and d_month_seq between (select distinct d_month_seq+1
                                 from   date_dim where d_year = 1998 and d_moy = 5)
                           and  (select distinct d_month_seq+3
                                 from   date_dim where d_year = 1998 and d_moy = 5)
 group by c_customer_sk
 )
 , segments as
 (select cast((revenue/50) as int) as segment
  from   my_revenue
 )
  select segment, count(*) as num_customers, segment*50 as segment_base
 from segments
 group by segment
 order by segment, num_customers
 LIMIT 100;

-- end query 54 in stream 0 using template query54.tpl
```

---

## Output

Return JSON:
```json
{
  "operations": [...],
  "semantic_warnings": [],
  "explanation": "..."
}
```

### Operations

| Op | Fields | Description |
|----|--------|-------------|
| `add_cte` | `after`, `name`, `sql` | Insert new CTE |
| `delete_cte` | `name` | Remove CTE |
| `replace_cte` | `name`, `sql` | Replace entire CTE body |
| `replace_clause` | `target`, `sql` | Replace clause (`""` to remove) |
| `patch` | `target`, `patches[]` | Snippet search/replace |

### Example
```json
{
  "operations": [
    {"op": "replace_cte", "name": "my_cte", "sql": "SELECT sk, SUM(val) FROM t WHERE sk IS NOT NULL GROUP BY sk"}
  ],
  "semantic_warnings": ["Removed join - added IS NOT NULL to preserve filtering"],
  "explanation": "Removed unnecessary dimension join, using FK directly"
}
```

### Block ID Syntax
```
{cte}.select    {cte}.from    {cte}.where    {cte}.group_by    {cte}.having
main_query.union[N].select    main_query.union[N].from    ...
```

### Rules
1. **Return 1-5 operations maximum** - focus on highest-impact changes first
2. Operations apply sequentially
3. `patch.search` must be unique within target clause
4. `add_cte.sql` = query body only (no CTE name)
5. All CTE refs must resolve after ops
6. When removing a join, update column references (e.g., `c_customer_sk` → `ss_customer_sk AS c_customer_sk`)

The system will iterate if more optimization is possible. You don't need to fix everything at once.