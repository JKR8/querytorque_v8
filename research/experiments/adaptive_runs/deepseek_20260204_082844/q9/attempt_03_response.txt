Looking at the query and execution plan, I can see the main issue: **10 separate scans of the `store_sales` table** with different quantity range filters. Each scan processes ~575,621 rows. The query has 5 CASE expressions, each with 2 subqueries (COUNT and AVG), leading to 10 total scans.

The key insight is that **all subqueries scan the same table with different quantity ranges**. We can compute all metrics in a single pass using conditional aggregation.

**Strategy:** Use `semantic_rewrite` to transform the query to compute all metrics in one pass over `store_sales`.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "semantic_rewrite",
      "nodes": {
        "main_query": "/* start query 9 in stream 0 using template query9.tpl */ WITH store_sales_stats AS (\n  SELECT \n    COUNT(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN 1 END) as cnt_1_20,\n    AVG(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN ss_ext_sales_price END) as avg_ext_1_20,\n    AVG(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN ss_net_profit END) as avg_net_1_20,\n    COUNT(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN 1 END) as cnt_21_40,\n    AVG(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN ss_ext_sales_price END) as avg_ext_21_40,\n    AVG(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN ss_net_profit END) as avg_net_21_40,\n    COUNT(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN 1 END) as cnt_41_60,\n    AVG(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN ss_ext_sales_price END) as avg_ext_41_60,\n    AVG(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN ss_net_profit END) as avg_net_41_60,\n    COUNT(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN 1 END) as cnt_61_80,\n    AVG(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN ss_ext_sales_price END) as avg_ext_61_80,\n    AVG(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN ss_net_profit END) as avg_net_61_80,\n    COUNT(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN 1 END) as cnt_81_100,\n    AVG(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN ss_ext_sales_price END) as avg_ext_81_100,\n    AVG(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN ss_net_profit END) as avg_net_81_100\n  FROM store_sales\n)\nSELECT \n  CASE WHEN cnt_1_20 > 2972190 THEN avg_ext_1_20 ELSE avg_net_1_20 END AS bucket1,\n  CASE WHEN cnt_21_40 > 4505785 THEN avg_ext_21_40 ELSE avg_net_21_40 END AS bucket2,\n  CASE WHEN cnt_41_60 > 1575726 THEN avg_ext_41_60 ELSE avg_net_41_60 END AS bucket3,\n  CASE WHEN cnt_61_80 > 3188917 THEN avg_ext_61_80 ELSE avg_net_61_80 END AS bucket4,\n  CASE WHEN cnt_81_100 > 3525216 THEN avg_ext_81_100 ELSE avg_net_81_100 END AS bucket5\nFROM reason, store_sales_stats\nWHERE r_reason_sk = 1"
      },
      "invariants_kept": [
        "same result rows",
        "same aggregation results",
        "same output columns",
        "required predicate r_reason_sk = 1 preserved"
      ],
      "expected_speedup": "3.0x",
      "risk": "low"
    }
  ],
  "explanation": "Transformed 10 separate scans of store_sales table into a single scan with conditional aggregation. All metrics (COUNTs and AVGs for each quantity range) are computed in one pass over the table using CASE expressions within aggregate functions. This reduces I/O from 10 sequential scans to just 1, while maintaining identical results."
}
```

**Why this should beat 1.84x:**
1. **Current approach:** 10 sequential scans × ~575,621 rows each = ~5.76M rows processed
2. **Optimized approach:** 1 sequential scan × ~575,621 rows = ~0.58M rows processed
3. **Expected reduction:** ~10x less I/O, though some overhead for computing multiple aggregates
4. **Realistic speedup:** 3.0x (conservative estimate accounting for aggregation overhead)

The rewrite preserves all semantics:
- Same CASE logic with thresholds
- Same AVG calculations for each quantity range
- Same output columns (bucket1-bucket5)
- Required predicate `r_reason_sk = 1` preserved via cross join with reason table