# DSPy DAG Mode Prompt (SQLDagOptimizer)

DAG mode outputs JSON with specific node rewrites instead of full SQL.
Essential for production where queries can be 100s of lines.

## Signature

```python
class SQLDagOptimizer(dspy.Signature):
    """Optimize SQL by rewriting specific DAG nodes."""

    # INPUTS
    query_dag: str           # DAG structure showing nodes and dependencies
    node_sql: str            # SQL for each node in the DAG
    execution_plan: str      # Execution plan with operator costs
    optimization_hints: str  # Detected optimization opportunities

    # OUTPUTS
    rewrites: str       # JSON: {"node_id": "new SELECT...", ...}
    explanation: str    # What was optimized and why
```

---

## Few-Shot Example: Q1 Correlated Subquery (2.5x speedup)

**Full Original SQL:**
```sql
WITH customer_total_return AS (
  SELECT sr_customer_sk AS ctr_customer_sk,
         sr_store_sk AS ctr_store_sk,
         SUM(SR_FEE) AS ctr_total_return
  FROM store_returns, date_dim
  WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
  GROUP BY sr_customer_sk, sr_store_sk
)
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (
        SELECT avg(ctr_total_return) * 1.2
        FROM customer_total_return ctr2
        WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
      )
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'SD'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id
LIMIT 100
```

**query_dag:**
```
Nodes:
  [customer_total_return] type=cte tables=[store_returns, date_dim] GROUP_BY
  [main_query] type=main tables=[store, customer] refs=[customer_total_return] CORRELATED

Edges:
  customer_total_return → main_query
```

**node_sql:**
```sql
[customer_total_return]:
SELECT sr_customer_sk AS ctr_customer_sk,
       sr_store_sk AS ctr_store_sk,
       SUM(SR_FEE) AS ctr_total_return
FROM store_returns, date_dim
WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000
GROUP BY sr_customer_sk, sr_store_sk

[main_query]:
SELECT c_customer_id
FROM customer_total_return ctr1, store, customer
WHERE ctr1.ctr_total_return > (
        SELECT avg(ctr_total_return) * 1.2
        FROM customer_total_return ctr2
        WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk
      )
  AND s_store_sk = ctr1.ctr_store_sk
  AND s_state = 'SD'
  AND ctr1.ctr_customer_sk = c_customer_sk
ORDER BY c_customer_id
LIMIT 100
```

**execution_plan:**
```
SEQ_SCAN[customer]: 2M rows, 66% cost [NO FILTER]
HASH_JOIN: 9% cost
HASH_GROUP_BY[sr_customer_sk, sr_store_sk]: 5% cost
SEQ_SCAN[store_returns]: 288K rows
SEQ_SCAN[date_dim]: 73K rows [FILTERED by d_year]
```

**optimization_hints:**
```
SQL-SUB-001: Correlated Subquery to Window Function
  Trigger: Correlated subquery with AVG() in WHERE comparison
  Fix: Compute aggregate as window function in CTE
  Expected: 2.5x speedup by eliminating nested loop
```

**Expected rewrites output:**
```json
{
  "customer_total_return": "SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return, AVG(SUM(SR_FEE)) OVER (PARTITION BY sr_store_sk) * 1.2 AS ctr_avg_threshold FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk",
  "main_query": "SELECT c_customer_id FROM customer_total_return ctr1, store, customer WHERE ctr1.ctr_total_return > ctr1.ctr_avg_threshold AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100"
}
```

**Expected explanation:**
```
Decorrelated the subquery by computing AVG() OVER (PARTITION BY sr_store_sk) in the CTE.
The window function calculates the average return per store ONCE during aggregation,
storing it as ctr_avg_threshold. This eliminates the O(n²) correlated lookup.
Changed main_query WHERE to use ctr1.ctr_avg_threshold directly.
```

---

## Key Points

1. **Output is JSON** - only the nodes being changed, not full SQL
2. **Each node is a complete SELECT** - can be reassembled into full query
3. **CORRELATED flag** in DAG tells you where the problem is
4. **Window function pattern**: `AVG(SUM(x)) OVER (PARTITION BY key)` computes during GROUP BY
