# SQL Optimizer

You are optimizing a DuckDB query. Analyze the plan, apply the detected pattern, output patches.

## Algorithm

1. **READ** the Detected Opportunities - these are high-confidence patterns found by static analysis
2. **VERIFY** against the execution plan - confirm the bottleneck exists
3. **APPLY** the suggested fix using the patch format below
4. **OUTPUT** JSON patches only (not full SQL)

## Optimization Patterns

| Pattern | Fix | Speedup |
|---------|-----|---------|
| Correlated Subquery | Window function in CTE: `AVG(x) OVER (PARTITION BY key)` | 2.5x |
| Late Filter | Push filter INTO CTE before GROUP BY | 2.1x |
| OR on Different Cols | Split into UNION ALL branches | 2-3x |
| IN to EXISTS | `EXISTS (SELECT 1 FROM ... WHERE ...)` | 1.5x |

---

## Execution Plan

**Operators by cost:**
  SEQ_SCAN: 66.5% | 1,999,335 rows
  HASH_JOIN: 9.0% | 7,986 rows
  SEQ_SCAN: 6.3% | 56,138 rows
  HASH_GROUP_BY: 4.8% | 55,341 rows
  LEFT_DELIM_JOIN: 3.3% | 0 rows

**Table scans:**
  store_returns: 865,743 rows [NO FILTER] 
  date_dim: 73,049 rows [FILTERED] (0.5% selectivity)
  customer: 24,000,000 rows [NO FILTER] 
  CTE_SCAN: 55,341 rows [NO FILTER] 
  store: 402 rows [FILTERED] (13.2% selectivity)
  COLUMN_DATA_SCAN: 7,986 rows [NO FILTER] 
  CTE_SCAN: 55,341 rows [NO FILTER] 
  DELIM_SCAN: 0 rows [NO FILTER] 

**Joins:**
  HASH_JOIN: store_returns × ? → 56,138 rows
  LEFT_DELIM_JOIN: ? × ? → 0 rows
  HASH_JOIN: customer × ? → 7,986 rows
  HASH_JOIN: ? × store → 8,013 rows
  HASH_JOIN: ? × ? → 7,986 rows
  HASH_JOIN: ? × ? → 8,013 rows

**Cardinality misestimates:**
  PROJECTION: est 2,310 vs actual 55,341 (24.0x off)
  HASH_GROUP_BY: est 2,310 vs actual 55,341 (24.0x off)
  PROJECTION: est 2,311 vs actual 56,138 (24.3x off)
  PROJECTION: est 2,311 vs actual 56,138 (24.3x off)
  HASH_JOIN: est 2,311 vs actual 56,138 (24.3x off)
  SEQ_SCAN: est 288,581 vs actual 56,138 (5.1x off)
  PROJECTION: est 0 vs actual 3,167 (3167.0x off)
  FILTER: est 0 vs actual 3,167 (3167.0x off)
  HASH_JOIN: est 0 vs actual 7,986 (7986.0x off)
  HASH_JOIN: est 0 vs actual 8,013 (8013.0x off)
  CTE_SCAN: est 2,310 vs actual 55,341 (24.0x off)
  HASH_JOIN: est 0 vs actual 7,986 (7986.0x off)
  COLUMN_DATA_SCAN: est 0 vs actual 7,986 (7986.0x off)
  PROJECTION: est 0 vs actual 8,013 (8013.0x off)
  HASH_JOIN: est 0 vs actual 8,013 (8013.0x off)
  CTE_SCAN: est 2,310 vs actual 55,341 (24.0x off)


## Data Flow

  [customer_total_return]: store_returns, date_dim → GROUP BY
  [main_query]: store, customer, store_returns, date_dim

## Detected Opportunities

>>> SQL-SUB-001: Correlated Subquery to Window Function
    Detected: Correlated subquery with AVG() in WHERE comparison
    Fix: Compute aggregate as window function in CTE, avoid O(n²) re-scan
    Expected: 2.5x speedup by eliminating nested loop

---

## SQL Query (Annotated)

```sql
-- [BLOCK: customer_total_return] tables=store_returns,date_dim | GROUP BY
SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk

-- [BLOCK: main_query] CORRELATED SUBQUERY ⚠️
/* start query 1 in stream 0 using template query1.tpl */ WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year = 2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return AS ctr1, store, customer WHERE ctr1.ctr_total_return > (SELECT AVG(ctr_total_return) * 1.2 FROM customer_total_return AS ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'SD' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100
```

---

## Output Format

```json
{
  "patches": [
    {
      "op": "replace_cte",
      "name": "cte_name",
      "sql": "full rewritten CTE body"
    },
    {
      "op": "replace_clause",
      "target": "main_query.where",
      "search": "exact text to find",
      "replace": "replacement text"
    }
  ],
  "explanation": "what was optimized and why"
}
```

**Patch ops:** `replace_cte`, `replace_clause`, `add_cte`, `delete_join`

**Targets:** `{cte_name}.where`, `main_query.where`, `main_query.from`

---

## Example: Correlated Subquery → Window

**Pattern:** `WHERE x > (SELECT avg(x) FROM t WHERE t.key = outer.key)`

**Fix:**
```json
{
  "patches": [
    {
      "op": "replace_cte",
      "name": "customer_total_return",
      "sql": "SELECT ..., AVG(SUM(SR_FEE)) OVER (PARTITION BY sr_store_sk) * 1.2 AS threshold FROM ... GROUP BY ..."
    },
    {
      "op": "replace_clause",
      "target": "main_query.where",
      "search": "> (select avg(ctr_total_return)*1.2 from customer_total_return ctr2 where ctr1.ctr_store_sk = ctr2.ctr_store_sk)",
      "replace": "> ctr1.threshold"
    }
  ],
  "explanation": "Replaced correlated subquery with window function computed once in CTE"
}
```

Now output your patches:
