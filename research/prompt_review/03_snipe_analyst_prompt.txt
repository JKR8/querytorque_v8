You are the diagnostic analyst for query query072_agg_s1. You've seen 4 parallel attempts at >=2.0x speedup on this query. Your job: diagnose what worked, what didn't, and WHY — then design a strategy the sniper couldn't have known without these empirical results.

## Target: >=2.0x speedup
Anything below 2.0x is a miss. The sniper you deploy must be given a strategy with genuine headroom to reach this bar.

## Previous Optimization Attempts
Target: **>=2.0x** | 4 workers tried | none reached target

### W1: star_join_prefetch → 1.15x ★ BEST [PASS, below target (1.15x)]
- **Examples**: pg_date_cte_explicit_join, pg_dimension_prefetch_star
- **Transforms**: pushdown, date_cte_isolate
- **Approach**: Pre-filter date dim into CTE, then join star schema
- **Optimized SQL:**
```sql
WITH filtered_dates AS (
  SELECT d_date_sk, d_week_seq, d_date
  FROM date_dim
  WHERE d_year = 1998
)
SELECT i_item_desc, w_warehouse_name, d1.d_week_seq,
       sum(case when p_promo_sk is null then 1 else 0 end) no_promo,
       sum(case when p_promo_sk is not null then 1 else 0 end) promo,
       count(*) total_cnt
FROM catalog_sales cs
JOIN filtered_dates fd ON cs.cs_sold_date_sk = fd.d_date_sk
JOIN date_dim d2 ON d2.d_week_seq = fd.d_week_seq
JOIN inventory inv ON inv.inv_date_sk = d2.d_date_sk
  AND inv.inv_item_sk = cs.cs_item_sk
  AND inv.inv_quantity_on_hand < cs.cs_quantity
JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk
JOIN item i ON i.i_item_sk = cs.cs_item_sk
  AND i.i_category IN ('Children', 'Jewelry', 'Men')
JOIN customer_demographics cd ON cd.cd_demo_sk = cs.cs_bill_cdemo_sk
  AND cd.cd_marital_status = 'U'
  AND cd.cd_dep_count BETWEEN 9 AND 11
JOIN household_demographics hd ON hd.hd_demo_sk = cs.cs_bill_hdemo_sk
  AND hd.hd_buy_potential = '>10000'
JOIN date_dim d3 ON d3.d_date_sk = cs.cs_ship_date_sk
  AND d3.d_date > fd.d_date + interval '3 day'
LEFT JOIN promotion p ON p.p_promo_sk = cs.cs_promo_sk
LEFT JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk
  AND cr.cr_order_number = cs.cs_order_number
WHERE cs.cs_wholesale_cost BETWEEN 35 AND 55
GROUP BY i_item_desc, w_warehouse_name, d1.d_week_seq
ORDER BY total_cnt DESC, i_item_desc, w_warehouse_name, d1.d_week_seq
LIMIT 100
```

### W4: exploration_novel → 0.92x [REGRESSION (0.92x)]
- **Transforms**: novel
- **Optimized SQL:**
```sql
WITH inv_weekly AS (
  SELECT inv_item_sk, inv_warehouse_sk, d2.d_week_seq,
         min(inv_quantity_on_hand) AS min_inv_qty
  FROM inventory inv
  JOIN date_dim d2 ON d2.d_date_sk = inv.inv_date_sk
  GROUP BY inv_item_sk, inv_warehouse_sk, d2.d_week_seq
)
SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq,
       sum(case when p.p_promo_sk is null then 1 else 0 end) no_promo,
       sum(case when p.p_promo_sk is not null then 1 else 0 end) promo,
       count(*) total_cnt
FROM catalog_sales cs
JOIN date_dim d1 ON cs.cs_sold_date_sk = d1.d_date_sk AND d1.d_year = 1998
JOIN inv_weekly iw ON iw.inv_item_sk = cs.cs_item_sk
  AND iw.d_week_seq = d1.d_week_seq
  AND iw.min_inv_qty < cs.cs_quantity
JOIN warehouse w ON w.w_warehouse_sk = iw.inv_warehouse_sk
JOIN item i ON i.i_item_sk = cs.cs_item_sk
  AND i.i_category IN ('Children', 'Jewelry', 'Men')
JOIN customer_demographics cd ON cd.cd_demo_sk = cs.cs_bill_cdemo_sk
  AND cd.cd_marital_status = 'U' AND cd.cd_dep_count BETWEEN 9 AND 11
JOIN household_demographics hd ON hd.hd_demo_sk = cs.cs_bill_hdemo_sk
  AND hd.hd_buy_potential = '>10000'
JOIN date_dim d3 ON d3.d_date_sk = cs.cs_ship_date_sk
  AND d3.d_date > d1.d_date + interval '3 day'
LEFT JOIN promotion p ON p.p_promo_sk = cs.cs_promo_sk
LEFT JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk
  AND cr.cr_order_number = cs.cs_order_number
WHERE cs.cs_wholesale_cost BETWEEN 35 AND 55
GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq
ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq
LIMIT 100
```

### W2: decorrelate_subquery → 0.85x [ERROR]
- **Examples**: pg_decorrelate_exists
- **Transforms**: decorrelate
- **Error**: Row count mismatch: expected 142, got 198 — missing d3.d_date > d1.d_date + interval '3 day' filter
- **Optimized SQL:**
```sql
SELECT i.i_item_desc, w.w_warehouse_name, d1.d_week_seq,
       sum(case when p.p_promo_sk is null then 1 else 0 end) no_promo,
       sum(case when p.p_promo_sk is not null then 1 else 0 end) promo,
       count(*) total_cnt
FROM catalog_sales cs
JOIN date_dim d1 ON cs.cs_sold_date_sk = d1.d_date_sk AND d1.d_year = 1998
JOIN date_dim d2 ON d2.d_week_seq = d1.d_week_seq
JOIN inventory inv ON inv.inv_date_sk = d2.d_date_sk
  AND inv.inv_item_sk = cs.cs_item_sk
JOIN warehouse w ON w.w_warehouse_sk = inv.inv_warehouse_sk
JOIN item i ON i.i_item_sk = cs.cs_item_sk
  AND i.i_category IN ('Children', 'Jewelry', 'Men')
JOIN customer_demographics cd ON cd.cd_demo_sk = cs.cs_bill_cdemo_sk
  AND cd.cd_marital_status = 'U' AND cd.cd_dep_count BETWEEN 9 AND 11
JOIN household_demographics hd ON hd.hd_demo_sk = cs.cs_bill_hdemo_sk
  AND hd.hd_buy_potential = '>10000'
JOIN date_dim d3 ON d3.d_date_sk = cs.cs_ship_date_sk
LEFT JOIN promotion p ON p.p_promo_sk = cs.cs_promo_sk
LEFT JOIN catalog_returns cr ON cr.cr_item_sk = cs.cs_item_sk
  AND cr.cr_order_number = cs.cs_order_number
WHERE cs.cs_wholesale_cost BETWEEN 35 AND 55
  AND inv.inv_quantity_on_hand < cs.cs_quantity
GROUP BY i.i_item_desc, w.w_warehouse_name, d1.d_week_seq
ORDER BY total_cnt DESC, i.i_item_desc, w.w_warehouse_name, d1.d_week_seq
LIMIT 100
```

### W3: scan_consolidation → 0.0x [ERROR]
- **Examples**: pg_single_pass_agg
- **Error**: syntax error at or near 'FILTER' (line 12)
- **Optimized SQL:**
```sql
-- attempted scan consolidation
SELECT 1
```

## Original SQL (query072_agg_s1, postgresql v14.3)

```sql
 1 | select  i_item_desc
 2 |       ,w_warehouse_name
 3 |       ,d1.d_week_seq
 4 |       ,sum(case when p_promo_sk is null then 1 else 0 end) no_promo
 5 |       ,sum(case when p_promo_sk is not null then 1 else 0 end) promo
 6 |       ,count(*) total_cnt
 7 | from catalog_sales
 8 | join inventory on (cs_item_sk = inv_item_sk)
 9 | join warehouse on (w_warehouse_sk=inv_warehouse_sk)
10 | join item on (i_item_sk = cs_item_sk)
11 | join customer_demographics on (cs_bill_cdemo_sk = cd_demo_sk)
12 | join household_demographics on (cs_bill_hdemo_sk = hd_demo_sk)
13 | join date_dim d1 on (cs_sold_date_sk = d1.d_date_sk)
14 | join date_dim d2 on (inv_date_sk = d2.d_date_sk)
15 | join date_dim d3 on (cs_ship_date_sk = d3.d_date_sk)
16 | left outer join promotion on (cs_promo_sk=p_promo_sk)
17 | left outer join catalog_returns on (cr_item_sk = cs_item_sk and cr_order_number = cs_order_number)
18 | where d1.d_week_seq = d2.d_week_seq
19 |   and inv_quantity_on_hand < cs_quantity
20 |   and d3.d_date > d1.d_date + interval '3 day'
21 |   and hd_buy_potential = '>10000'
22 |   and d1.d_year = 1998
23 |   and cd_marital_status = 'U'
24 |   and cd_dep_count between 9 and 11
25 |   and i_category IN ('Children', 'Jewelry', 'Men')
26 |   and cs_wholesale_cost BETWEEN 35 AND 55
27 | group by i_item_desc,w_warehouse_name,d1.d_week_seq
28 | order by total_cnt desc, i_item_desc, w_warehouse_name, d_week_seq
29 | limit 100;
```

## EXPLAIN Plan (planner estimates)

```
NOTE: EXPLAIN only (no ANALYZE) — rows and costs are planner ESTIMATES, not measurements.
Total estimated cost: 141782

-> Limit  (est_rows=1 cost=141782)
  -> Sort  (est_rows=1 cost=141782)
    -> Aggregate  (est_rows=1 cost=141782)
      -> Nested Loop Inner  (est_rows=1 cost=141782)
         Join Filter: (d3.d_date > (d1.d_date + '3 days'::interval))
        -> Gather Merge  (est_rows=1 cost=141777)
           Workers: None/1 launched
          -> Sort  (est_rows=1 cost=140777)
            -> Nested Loop Left  (est_rows=1 cost=140777)
              -> Nested Loop Inner  (est_rows=1 cost=140773)
                -> Nested Loop Inner  (est_rows=1 cost=140768)
                   Join Filter: (inventory.inv_warehouse_sk = warehouse.w_warehouse_sk)
                  -> Nested Loop Inner  (est_rows=1 cost=140767)
                    -> Hash Join Inner  (est_rows=4 cost=140708)
                       Hash Cond: (d2.d_week_seq = d1.d_week_seq)
                      -> Seq Scan on date_dim d2  (est_rows=43K cost=1835)
                      -> Hash  (est_rows=1 cost=138712)
                        -> Nested Loop Inner  (est_rows=1 cost=138712)
                          -> Hash Join Inner  (est_rows=841 cost=134725)
                             Hash Cond: (catalog_sales.cs_bill_hdemo_sk = household_demographics.hd_demo_sk)
                            -> Nested Loop Inner  (est_rows=5,072 cost=134554)
                              -> Index Scan on date_dim d1  (est_rows=151 cost=2962)
                                 Filter: (d1.d_year = 1998)
                              -> Index Scan on catalog_sales  (est_rows=1,156 cost=860)
                                 Filter: ((catalog_sales.cs_wholesale_cost >= '35'::numeric) AND (catalog_sales.cs_wholesale_cost <= '55':...
                                 Index Cond: (catalog_sales.cs_sold_date_sk = d1.d_date_sk)
                            -> Hash  (est_rows=1,200 cost=143)
                              -> Seq Scan on household_demographics  (est_rows=1,200 cost=143)
                                 Filter: (household_demographics.hd_buy_potential = '>10000'::bpchar)
                          -> Index Scan on customer_demographics  (est_rows=1 cost=5)
                             Filter: ((customer_demographics.cd_dep_count >= 9) AND (customer_demographics.cd_dep_count <= 11) AND (cu...
                             Index Cond: (customer_demographics.cd_demo_sk = catalog_sales.cs_bill_cdemo_sk)
                    -> Index Scan on inventory  (est_rows=2 cost=15)
                       Filter: (inventory.inv_quantity_on_hand < catalog_sales.cs_quantity)
                       Index Cond: ((inventory.inv_date_sk = d2.d_date_sk) AND (inventory.inv_item_sk = catalog_sales.cs_item_sk))
                  -> Seq Scan on warehouse  (est_rows=10 cost=1)
                -> Index Scan on item  (est_rows=1 cost=5)
                   Filter: (item.i_category = ANY ('{Children,Jewelry,Men}'::bpchar[]))
                   Index Cond: (item.i_item_sk = catalog_sales.cs_item_sk)
              -> Index Only Scan on promotion  (est_rows=1 cost=4)
                 Index Cond: (promotion.p_promo_sk = catalog_sales.cs_promo_sk)
        -> Index Scan on date_dim d3  (est_rows=1 cost=5)
           Index Cond: (d3.d_date_sk = catalog_sales.cs_ship_date_sk)
```

## Query Structure (DAG)

### 1. main_query
**Role**: Root / Output (Definition Order: 0)
**Stats**: 100% Cost | ~142 rows
**Flags**: GROUP_BY
**Outputs**: [i_item_desc, w_warehouse_name, d_week_seq, no_promo, promo, total_cnt]
**Dependencies**: catalog_sales, inventory, warehouse, item, customer_demographics, household_demographics, date_dim, date_dim, date_dim, promotion, catalog_returns
**Operators**: Nested Loop, Hash Join, Index Scan, Seq Scan, Sort, GroupAggregate
**Key Logic (SQL)**:
```sql
SELECT i_item_desc, w_warehouse_name, d1.d_week_seq, SUM(CASE WHEN p_promo_sk IS NULL THEN 1 ELSE 0 END) AS no_promo, SUM(CASE WHEN NOT p_promo_sk IS NULL THEN 1 ELSE 0 END) AS promo, COUNT(*) AS total_cnt FROM catalog_sales JOIN inventory ON (cs_item_sk = inv_item_sk) JOIN warehouse ON (w_warehouse_sk = inv_warehouse_sk) JOIN item ON (i_item_sk = cs_item_sk) JOIN customer_demographics ON (cs_bill_cdemo_sk = cd_demo_sk) JOIN household_demographics ON (cs_bill_hdemo_sk = hd_demo_sk) JOIN date_dim ...
```


## Aggregation Semantics Check

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Changing group membership changes the result.
- **AVG and STDDEV are NOT duplicate-safe**: join-introduced row duplication changes the aggregate.
- When splitting with GROUP BY + aggregate, each branch must preserve exact GROUP BY columns and filter to the same row set.

## Engine Profile

*This is field intelligence gathered from 53 DSB queries at SF5-SF10. PostgreSQL is a fundamentally different optimizer than DuckDB — it has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here. Use this to guide your analysis but apply your own judgment — every query is different. Add to this knowledge if you observe something new.*

### Optimizer Strengths (DO NOT fight these)
- **BITMAP_OR_SCAN**: Multi-branch OR conditions on indexed columns are handled via BitmapOr — a single fact table scan with bitmap combination. Extremely efficient.
- **SEMI_JOIN_EXISTS**: EXISTS/NOT EXISTS uses semi-join with early termination. Stops scanning after the first match per outer row.
- **INNER_JOIN_REORDERING**: PostgreSQL freely reorders INNER JOINs based on estimated selectivity. The cost model works well for explicit JOIN...ON syntax.
- **INDEX_ONLY_SCAN**: When an index covers all requested columns, PostgreSQL reads only the index without touching the heap.
- **PARALLEL_QUERY_EXECUTION**: PostgreSQL parallelizes large scans and aggregations across worker processes with partial aggregation finalization.
- **JIT_COMPILATION**: PostgreSQL JIT-compiles complex expressions and tuple deforming for long-running queries.

### Optimizer Gaps (opportunities)
- **COMMA_JOIN_WEAKNESS**: Implicit comma-separated FROM tables (FROM t1, t2, t3 WHERE t1.id = t2.id) are treated as cross products initially. The cost model is significantly weaker on comma-joins than on explicit JOIN...ON syntax.
  Opportunity: Convert comma-joins to explicit JOIN...ON syntax. This alone can unlock 2-3x improvements. Best when combined with date_cte_isolate.
- **CORRELATED_SUBQUERY_PARALYSIS**: Cannot automatically decorrelate complex correlated subqueries. Correlated scalar subqueries with aggregates are executed as nested-loop with repeated evaluation.
  Opportunity: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
- **NON_EQUI_JOIN_INPUT_BLINDNESS**: Cannot pre-filter fact tables before non-equi join operations (date arithmetic, range comparisons, quantity < quantity). Non-equi joins fall back to nested-loop, which is O(N*M).
  Opportunity: Reduce fact table input size via filtered CTE before the non-equi join.
- **CTE_MATERIALIZATION_FENCE**: PostgreSQL materializes CTEs by default (multi-referenced) or by choice (AS MATERIALIZED). This creates a hard optimization fence — no predicate pushdown from outer query into CTE. This makes CTE-based strategies a double-edged sword on PG.
  Opportunity: Use materialization STRATEGICALLY: materialize when the CTE is expensive and reused multiple times. Avoid CTEs that fence off predicate pushdown for single-use cases.
- **CROSS_CTE_PREDICATE_BLINDNESS**: Same gap as DuckDB but WORSE on PostgreSQL because CTE materialization fence makes it more impactful. Predicates in the outer WHERE cannot propagate into materialized CTEs.
  Opportunity: Same as DuckDB: pre-filter into CTE definition. But be more cautious — only when the CTE is clearly suboptimal.

## Tag-Matched Examples (6)

### pg_date_cte_explicit_join (2.28x)
**Description:** Isolate a selective date_dim filter into a CTE AND convert all comma-separated joins to explicit JOIN syntax. The combination is key on PostgreSQL - the CTE alone can hurt, but CTE + explicit JOINs together enable better hash join planning with a tiny probe table.
**Principle:** Dimension Isolation + Explicit Joins: materialize selective dimension filters into CTEs to create tiny hash tables, AND convert comma-separated joins to explicit JOIN syntax. On PostgreSQL, the combination enables better hash join planning with a tiny probe table.

### pg_dimension_prefetch_star (3.32x)
**Description:** On multi-channel UNION queries with comma-separated implicit joins, pre-filter dimension tables (date, item, promotion) into CTEs and convert to explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates and join ordering from explicit JOINs with pre-materialized small dimension results.
**Principle:** Multi-Dimension Prefetch (PG): pre-filter all selective dimensions into CTEs to create tiny hash tables, combined with explicit JOIN syntax. PostgreSQL's optimizer gets better cardinality estimates from pre-materialized small dimension results.

### early_filter_decorrelate (1.13x)
**Principle:** Early Selection + Decorrelation: push dimension filters into CTE definitions before materialization, and decorrelate correlated subqueries by pre-computing thresholds in separate CTEs. Filters reduce rows early; decorrelation replaces per-row subquery execution with a single pre-computed JOIN.

### inline_decorrelate_materialized (timeout_rescuex)
**Principle:** Inline Decorrelation with MATERIALIZED CTEs: When a WHERE clause contains a correlated scalar subquery (e.g., col > (SELECT 1.3 * avg(col) FROM ... WHERE correlated_key = outer.key)), PostgreSQL re-executes the subquery per outer row. Fix: decompose into 3 MATERIALIZED CTEs — (1) pre-filter dimension table, (2) pre-filter fact table by date range, (3) compute per-key aggregate threshold from filtered data — then JOIN the threshold CTE in the final query. MATERIALIZED keyword prevents PG from inlining the CTEs back into correlated form.

### pg_materialized_dimension_fact_prefilter (2.68x)
**Description:** Pre-filter ALL dimension tables AND the fact table into MATERIALIZED CTEs, then join with explicit JOIN syntax. On queries with expensive non-equi joins (inventory quantity < sales quantity, week_seq correlation), reducing both dimension AND fact table sizes before the join dramatically cuts the search space. The MATERIALIZED keyword on PG12+ forces early execution of each CTE.
**Principle:** Staged Reduction for Non-Equi Joins: when queries have expensive non-equi joins, reduce BOTH dimension and fact table sizes via MATERIALIZED CTEs before the join. Combined selectivity dramatically cuts the search space for inequality predicates.

### pg_self_join_decomposition (3.93x)
**Description:** Eliminate duplicate fact table scans in self-join patterns by computing the aggregation ONCE in a CTE and deriving both per-item and per-store averages from the same materialized result. PostgreSQL materializes CTEs by default, making this extremely effective.
**Principle:** Shared Materialization (PG): when the same fact+dimension scan appears multiple times in self-join patterns, materialize it once as a CTE and derive all needed aggregates from the same result. PostgreSQL materializes CTEs by default, making this extremely effective.

## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 — prefetched_inventory CTE omits i_item_id but main query references it in SELECT and GROUP BY
  - Failure: Q76 — filtered_store_dates CTE omits d_year and d_qoy but aggregation CTE uses them in GROUP BY

## Your Task

Work through these 3 steps in a `<reasoning>` block, then output the structured briefing below:

1. **DIAGNOSE**: Why did the best worker achieve 1.15x instead of the 2.0x target? Why did each other worker fail or regress? Be specific about structural mechanisms.
2. **IDENTIFY**: What optimization angles couldn't have been designed BEFORE seeing these empirical results? What did the results reveal about the query's actual execution behavior?
3. **SYNTHESIZE**: Design a strategy for the sniper that builds on the best foundation (if any) and exploits the newly-revealed angles. The sniper has full freedom — give it direction, not constraints.

### Output Format (follow EXACTLY)

```
=== SNIPE BRIEFING ===

FAILURE_SYNTHESIS:
<WHY the best worker won, WHY each other failed — structural mechanisms>

BEST_FOUNDATION:
<What to build on from the best result, or 'None — start fresh' if all regressed>

UNEXPLORED_ANGLES:
<What optimization approaches couldn't have been designed pre-empirically>

STRATEGY_GUIDANCE:
<Synthesized approach for the sniper — ADVISORY, not mandatory>

EXAMPLES: <ex1>, <ex2>, <ex3>

EXAMPLE_ADAPTATION:
<For each example: what to APPLY and what to IGNORE>

HAZARD_FLAGS:
<Risks based on observed failures — what NOT to do>

RETRY_WORTHINESS: high|low — <reason>
(Is there genuine headroom for a second sniper attempt if the first misses 2.0x?)

RETRY_DIGEST:
<5-10 line compact failure guide for sniper2 IF retry is needed.
What broke, why, what to change. The lesson, not the artifact.>
```