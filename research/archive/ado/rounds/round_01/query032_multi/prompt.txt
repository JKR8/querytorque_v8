## Example: Dsb Self Join Decomposition (DSB_SELF_JOIN_DECOMPOSITION)
Verified speedup: unknown

### Input:
[main_query]:
-- DSB Query 100: items sold together
      SELECT i1.i_item_id, i2.i_item_id, COUNT(*)
      FROM store_sales ss1
      JOIN store_sales ss2 ON ss1.ss_ticket_number = ss2.ss_ticket_number 
                           AND ss1.ss_item_sk < ss2.ss_item_sk
      JOIN item i1 ON ss1.ss_item_sk = i1.i_item_sk
      JOIN item i2 ON ss2.ss_item_sk = i2.i_item_sk
      GROUP BY i1.i_item_id, i2.i_item_id;


**Key insight:** DSB Query 100 pattern: items frequently sold together.

---

## Example: Dsb Predicate Correlation Stats (DSB_PREDICATE_CORRELATION_STATS)
Verified speedup: unknown

### Input:
[main_query]:
-- PostgreSQL assumes independence, underestimates
      SELECT * FROM customer c
      WHERE c.c_birth_country = 'United States'
        AND c.c_current_addr_sk IN (
            SELECT ca_address_sk FROM customer_address 
            WHERE ca_state = 'CA'
        );


**Key insight:** DSB explicitly introduces cross-table correlations. Create
      statistics after identifying correlated filter patterns.

---

## Example: Aggregate Push Below Join (AGGREGATE_PUSH_BELOW_JOIN)
Verified speedup: unknown

### Input:
[main_query]:
SELECT c.region, SUM(o.amount)
      FROM customers c
      JOIN orders o ON c.id = o.customer_id
      GROUP BY c.region;


**Key insight:** Join followed by aggregation where one side contributes only to
      aggregate, not to grouping.

---

## CONSTRAINTS (Learned from Benchmark Failures)

The following constraints are MANDATORY based on observed failures:

### LITERAL_PRESERVATION [CRITICAL]
CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

### OR_TO_UNION_LIMIT [HIGH]
CAUTION with OR→UNION: Only split OR conditions into UNION ALL when there are ≤3 simple branches AND they have different access patterns. If you have nested ORs (e.g., 3 conditions × 3 values = 9 combinations), DO NOT expand them - keep the original OR structure. DuckDB handles OR predicates efficiently. Over-splitting causes multiple scans of fact tables and severe regressions (0.23x-0.41x observed). When in doubt, preserve the original OR structure.


---

You are an autonomous Query Rewrite Engine. Your goal is to maximize execution speed while strictly preserving semantic invariants.

Output atomic rewrite sets in JSON.

RULES:
- Primary Goal: Maximize execution speed while strictly preserving semantic invariants.
- Allowed Transforms: Use the provided list. If a standard SQL optimization applies that is not listed, label it "semantic_rewrite".
- Atomic Sets: Group dependent changes (e.g., creating a CTE and joining it) into a single rewrite_set.
- Contracts: Output columns, grain, and total result rows must remain invariant.
- Naming: Use descriptive CTE names (e.g., `filtered_returns` vs `cte1`).
- Column Aliasing: Permitted only for aggregations or disambiguation.

ALLOWED TRANSFORMS: pushdown, decorrelate, or_to_union, early_filter, date_cte_isolate, materialize_cte, flatten_subquery, reorder_join, multi_push_predicate, inline_cte, remove_redundant, semantic_rewrite

OUTPUT FORMAT:
```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "transform_name",
      "nodes": {
        "node_id": "new SQL..."
      },
      "invariants_kept": ["list of preserved semantics"],
      "expected_speedup": "2x",
      "risk": "low"
    }
  ],
  "explanation": "what was changed and why"
}
```

## Target Nodes
  [main_query] 

## Subgraph Slice
[main_query] type=main
```sql
SELECT SUM(cs_ext_discount_amt) AS "excess discount amount" FROM catalog_sales, item, date_dim WHERE (i_manufact_id IN (1, 78, 97, 516, 521) OR i_manager_id BETWEEN 25 AND 54) AND i_item_sk = cs_item_sk AND d_date BETWEEN '1999-03-07' AND CAST('1999-03-07' AS DATE) + INTERVAL '90' DAY AND d_date_sk = cs_sold_date_sk AND cs_ext_discount_amt > (SELECT 1.3 * AVG(cs_ext_discount_amt) FROM catalog_sales, date_dim WHERE cs_item_sk = i_item_sk AND d_date BETWEEN '1999-03-07' AND CAST('1999-03-07' AS DATE) + INTERVAL '90' DAY AND d_date_sk = cs_sold_date_sk AND cs_list_price BETWEEN 16 AND 45 AND cs_sales_price / cs_list_price BETWEEN 63 * 0.01 AND 83 * 0.01) ORDER BY SUM(cs_ext_discount_amt) LIMIT 100
```


## Node Contracts
[main_query]:
  output_columns: ['excess discount amount']
  required_predicates: ["cs_ext_discount_amt > (SELECT 1.3 * AVG(cs_ext_discount_amt) FROM catalog_sales, date_dim WHERE cs_item_sk = i_item_sk AND d_date BETWEEN '1999-03-...", 'd_date_sk = cs_sold_date_sk', 'i_item_sk = cs_item_sk']

## Downstream Usage
No usage data.

## Cost Attribution
No cost data.

## Detected Opportunities
## Knowledge Base Patterns (verified on TPC-DS)
## Detected Optimization Opportunities

1. **QT-OPT-002** - Correlated Subquery to Pre-computed CTE
  Trigger: WHERE col > (SELECT AVG/SUM/COUNT FROM ... WHERE correlated)
  Rewrite: Create CTE with GROUP BY on correlation key, then JOIN instead of correlated lookup
   Matched: Correlated subquery with aggregate comparison

2. **QT-OPT-003** - Date CTE Isolation
  Trigger: date_dim joined with d_year/d_qoy/d_month filter, fact table present
  Rewrite: Create CTE: SELECT d_date_sk FROM date_dim WHERE filter, join fact to CTE early
   Matched: date_dim filter with fact table


## Node-Specific Opportunities
OR_TO_UNION: [main_query] has OR condition
  Fix: Split into UNION ALL branches
  Expected: 2x speedup (verified Q15: 2.98x)

Now output your rewrite_sets: