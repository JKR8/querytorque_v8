FAILURE_ANALYSIS:
All attempts fell short because they failed to address the fundamental performance bottleneck: repeated full-table scans and inefficient OR predicate evaluation. Worker 1's CTE materialization (1.05x) added overhead without reducing scan count. Worker 3's single-pass aggregation (0.57x) overloaded CPU with complex CASE evaluations per row. Worker 4's OR-to-UNION (0.78x) multiplied scan count from 6 to 18 subqueries. Worker 5's refined single-pass (0.66x) still suffered from expensive DISTINCT calculations on conditional expressions. The root issue is that DuckDB's optimizer cannot efficiently push disjoint quantity-range filters into a single scan while optimizing the three OR conditions per range.

UNEXPLORED_OPPORTUNITIES:
1. **Predicate pre-computation**: Pre-compute which rows satisfy ANY condition across all buckets to filter table once.
2. **Columnar filter pushdown**: Leverage DuckDB's columnar execution by separating filters into separate scans per column, then combining.
3. **Bloom filter pre-filter**: Use approximate filtering via bloom filters on ss_quantity before evaluating expensive OR conditions.
4. **Partial aggregation with early discard**: Aggregate incrementally while scanning, discarding rows that don't match any bucket early.
5. **Vectorized range partitioning**: Partition data by ss_quantity ranges first, then apply column-specific filters within each partition.

REFINED_STRATEGY:
Combine single-pass scanning with predicate pre-computation and column-specific filtering:
1. Pre-filter store_sales to rows where ss_quantity BETWEEN 0 AND 30 (covering all ranges).
2. For each quantity range, compute three bitmasks indicating matches on each column (list_price, coupon_amt, wholesale_cost).
3. Use these bitmasks to efficiently assign rows to buckets without reevaluating complex OR conditions.
4. Perform conditional aggregation in a single pass with optimized DISTINCT calculation using pre-sorted values.

EXAMPLES: single_pass_aggregation, early_filter, pushdown
HINT: Use a single CTE that scans store_sales once, computes bitwise flags for each bucket condition, then aggregates with conditional SUM/COUNT while maintaining distinct value sets via array_agg for later deduplication.