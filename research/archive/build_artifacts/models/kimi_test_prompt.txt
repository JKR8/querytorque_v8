Optimize this SQL query.

## Algorithm

1. ANALYZE: Find where rows/cost are largest in the plan.
2. OPTIMIZE: For each bottleneck, ask "what could reduce it earlier?"
   - Can a filter be pushed inside a CTE instead of applied after?
   - Can a small table join happen inside an aggregation to filter before GROUP BY?
   - Is there a correlated subquery? Convert to CTE + JOIN.
3. VERIFY: Result must be semantically equivalent.

Principle: Reduce rows as early as possible.

## Plan

Operators by cost:
- SEQ_SCAN (store_sales): 94.2% cost, 2,859,381 rows
- SEQ_SCAN (item): 2.5% cost, 51 rows
- HASH_JOIN: 1.6% cost, 841 rows
- HASH_GROUP_BY: 0.8% cost, 76 rows

Scans:
- date_dim: 150 rows ← FILTERED by d_moy=11
- store_sales: 2,859,381 rows (NO FILTER)
- item: 51 rows ← FILTERED by i_manufact_id=816

## SQL

```sql
-- start query 3 in stream 0 using template query3.tpl
select dt.d_year 
       ,item.i_brand_id brand_id 
       ,item.i_brand brand
       ,sum(ss_sales_price) sum_agg
 from  date_dim dt 
      ,store_sales
      ,item
 where dt.d_date_sk = store_sales.ss_sold_date_sk
   and store_sales.ss_item_sk = item.i_item_sk
   and item.i_manufact_id = 816
   and dt.d_moy=11
 group by dt.d_year
      ,item.i_brand
      ,item.i_brand_id
 order by dt.d_year
         ,sum_agg desc
         ,brand_id
 LIMIT 100;

-- end query 3 in stream 0 using template query3.tpl
```

## Output

Return the optimized SQL query directly. Explain your changes briefly.
