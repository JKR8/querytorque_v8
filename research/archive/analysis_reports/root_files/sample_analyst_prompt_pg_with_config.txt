You are a senior query optimization architect. Your job is to deeply analyze a SQL query and produce a structured briefing for 4 specialist workers who will each write a different optimized version.

You are the ONLY call that sees all the data: EXPLAIN plans, DAG costs, full constraint list, global knowledge, and the complete example catalog. The workers will only see what YOU put in their briefings. Your output quality directly determines their success.

## Query: query059_multi
## Dialect: postgresql

```sql
 1 | with wss as
 2 |  (select d_week_seq,
 3 |         ss_store_sk,
 4 |         sum(case when (d_day_name='Sunday') then ss_sales_price else null end) sun_sales,
 5 |         sum(case when (d_day_name='Monday') then ss_sales_price else null end) mon_sales,
 6 |         sum(case when (d_day_name='Tuesday') then ss_sales_price else  null end) tue_sales,
 7 |         sum(case when (d_day_name='Wednesday') then ss_sales_price else null end) wed_sales,
 8 |         sum(case when (d_day_name='Thursday') then ss_sales_price else null end) thu_sales,
 9 |         sum(case when (d_day_name='Friday') then ss_sales_price else null end) fri_sales,
10 |         sum(case when (d_day_name='Saturday') then ss_sales_price else null end) sat_sales
11 |  from store_sales,date_dim
12 |  where d_date_sk = ss_sold_date_sk
13 |  and ss_sales_price / ss_list_price BETWEEN 11 * 0.01 AND 31 * 0.01
14 |  group by d_week_seq,ss_store_sk
15 |  )
16 |   select  s_store_name1,s_store_id1,d_week_seq1
17 |        ,sun_sales1/sun_sales2,mon_sales1/mon_sales2
18 |        ,tue_sales1/tue_sales2,wed_sales1/wed_sales2,thu_sales1/thu_sales2
19 |        ,fri_sales1/fri_sales2,sat_sales1/sat_sales2
20 |  from
21 |  (select s_store_name s_store_name1,wss.d_week_seq d_week_seq1
22 |         ,s_store_id s_store_id1,sun_sales sun_sales1
23 |         ,mon_sales mon_sales1,tue_sales tue_sales1
24 |         ,wed_sales wed_sales1,thu_sales thu_sales1
25 |         ,fri_sales fri_sales1,sat_sales sat_sales1
26 |   from wss,store,date_dim d
27 |   where d.d_week_seq = wss.d_week_seq and
28 |         ss_store_sk = s_store_sk and
29 |         d_month_seq between 1208 and 1208 + 11
30 |         and s_state in ('GA','IA','LA'
31 |                     ,'MO','SD','TN','TX','VA')
32 |         ) y,
33 |  (select s_store_name s_store_name2,wss.d_week_seq d_week_seq2
34 |         ,s_store_id s_store_id2,sun_sales sun_sales2
35 |         ,mon_sales mon_sales2,tue_sales tue_sales2
36 |         ,wed_sales wed_sales2,thu_sales thu_sales2
37 |         ,fri_sales fri_sales2,sat_sales sat_sales2
38 |   from wss,store,date_dim d
39 |   where d.d_week_seq = wss.d_week_seq and
40 |         ss_store_sk = s_store_sk and
41 |         d_month_seq between 1208+ 12 and 1208 + 23
42 |         and s_state in ('GA','IA','LA'
43 |                     ,'MO','SD','TN','TX','VA')) x
44 |  where s_store_id1=s_store_id2
45 |    and d_week_seq1=d_week_seq2-52
46 |  order by s_store_name1,s_store_id1,d_week_seq1
47 | limit 100;
```

## EXPLAIN ANALYZE Plan

```
Total execution time: 40248.2ms
Planning time: 2.8ms

-> Limit  (rows=100 loops=1 time=40248.2ms)
   Buffers: hit=12K read=513K temp_r=29K temp_w=29K
  -> Aggregate  (rows=9,176 loops=1 time=39352.7ms)
     Buffers: hit=12K read=513K temp_r=29K temp_w=29K
    -> Gather Merge  (rows=3.4M loops=1 time=38747.0ms)
       Workers: 2/2 launched
       Buffers: hit=12K read=513K temp_r=29K temp_w=29K
      -> Sort  (rows=1.1M loops=3 time=37880.4ms)
         Sort Method: external merge  Space: 39168kB (Disk)
         Buffers: hit=12K read=513K temp_r=29K temp_w=29K
        -> Nested Loop Inner  (rows=1.1M loops=3 time=35049.4ms)
           Buffers: hit=12K read=513K
          -> Seq Scan on store_sales  (rows=1.2M loops=3 time=34448.7ms)
             Filter: (((store_sales.ss_sales_price / store_sales.ss_list_price) >= 0.11) AND ((store_sales.ss_sales_pr...
             Rows Removed by Filter: 8.4M
             Buffers: read=513K
          -> Memoize  (rows=1 loops=3481050 time=0.0ms)
            -> Index Scan on date_dim  (rows=1 loops=4083 time=0.0ms)
               Index Cond: (date_dim.d_date_sk = store_sales.ss_sold_date_sk)
  -> Sort  (rows=100 loops=1 time=39512.8ms)
     Sort Method: top-N heapsort  Space: 60kB (Memory)
     Buffers: hit=4,070 read=172K temp_r=9,783 temp_w=9,805
    -> Hash Join Inner  (rows=30K loops=1 time=39507.6ms)
       Hash Cond: ((wss.ss_store_sk = store.s_store_sk) AND (store_1.s_store_id = store.s_store_id))
       Buffers: hit=4,070 read=172K temp_r=9,783 temp_w=9,805
      -> Hash Join Inner  (rows=942K loops=1 time=39440.5ms)
         Hash Cond: (wss.d_week_seq = d.d_week_seq)
         Buffers: hit=4,070 read=172K temp_r=9,783 temp_w=9,805
        -> CTE Scan wss (CTE: wss)  (rows=9,176 loops=1 time=37483.0ms)
           Buffers: hit=4,047 read=172K temp_r=5,388 temp_w=9,805
        -> Hash  (rows=30K loops=1 time=1888.4ms)
           Buffers: hit=23 read=4 temp_r=4,395
          -> Hash Join Inner  (rows=30K loops=1 time=1883.8ms)
             Hash Cond: ((wss_1.d_week_seq - 52) = d.d_week_seq)
             Buffers: hit=23 read=4 temp_r=4,395
            -> Hash Join Inner  (rows=4,380 loops=1 time=1879.3ms)
               Hash Cond: (wss_1.d_week_seq = d_1.d_week_seq)
               Buffers: hit=15 read=1 temp_r=4,395
              -> Hash Join Inner  (rows=3,552 loops=1 time=1878.0ms)
                 Hash Cond: (wss_1.ss_store_sk = store_1.s_store_sk)
                 Buffers: hit=5 temp_r=4,395
                -> CTE Scan wss_1 (CTE: wss)  (rows=9,176 loops=1 time=1875.0ms)
                   Buffers: temp_r=4,395
                -> Hash  (rows=38 loops=1 time=0.1ms)
                  -> Seq Scan on store store_1  (rows=38 loops=1 time=0.0ms)
                     Filter: (store_1.s_state = ANY ('{GA,IA,LA,MO,SD,TN,TX,VA}'::bpchar[]))
                     Rows Removed by Filter: 64
              -> Hash  (rows=365 loops=1 time=0.4ms)
                -> Index Scan on date_dim d_1  (rows=365 loops=1 time=0.3ms)
                   Index Cond: ((d_1.d_month_seq >= 1220) AND (d_1.d_month_seq <= 1231))
            -> Hash  (rows=365 loops=1 time=2.7ms)
              -> Index Scan on date_dim d  (rows=365 loops=1 time=2.7ms)
                 Index Cond: ((d.d_month_seq >= 1208) AND (d.d_month_seq <= 1219))
      -> Hash  (rows=38 loops=1 time=1.3ms)
        -> Seq Scan on store  (rows=38 loops=1 time=1.3ms)
           Filter: (store.s_state = ANY ('{GA,IA,LA,MO,SD,TN,TX,VA}'::bpchar[]))
           Rows Removed by Filter: 64

JIT: 96 functions, total=0.0ms (gen=0.0 opt=0.0 emit=0.0)
```

**NOTE:** The EXPLAIN plan shows the PHYSICAL execution structure, which may differ significantly from the LOGICAL DAG below. The optimizer may have already split CTEs, reordered joins, or pushed predicates. When the EXPLAIN and DAG disagree, the EXPLAIN is ground truth for what the optimizer is already doing.

Use EXPLAIN timings as ground truth. DAG cost percentages are derived metrics that may not reflect actual execution time.

## Query Structure (DAG)

### 1. wss
**Role**: CTE (Definition Order: 0)
**Stats**: 67% Cost | ~3.5M rows
**Flags**: GROUP_BY
**Outputs**: [d_week_seq, ss_store_sk, sun_sales, mon_sales, tue_sales, wed_sales, thu_sales, fri_sales, sat_sales]
**Dependencies**: store_sales, date_dim
**Operators**: SEQ_SCAN[store_sales]
**Key Logic (SQL)**:
```sql
SELECT d_week_seq, ss_store_sk, SUM(CASE WHEN (d_day_name = 'Sunday') THEN ss_sales_price ELSE NULL END) AS sun_sales, SUM(CASE WHEN (d_day_name = 'Monday') THEN ss_sales_price ELSE NULL END) AS mon_sales, SUM(CASE WHEN (d_day_name = 'Tuesday') THEN ss_sales_price ELSE NULL END) AS tue_sales, SUM(CASE WHEN (d_day_name = 'Wednesday') THEN ss_sales_price ELSE NULL END) AS wed_sales, SUM(CASE WHEN (d_day_name = 'Thursday') THEN ss_sales_price ELSE NULL END) AS thu_sales, SUM(CASE WHEN (d_day_name = ...
```

### 2. main_query
**Role**: Root / Output (Definition Order: 1)
**Stats**: 33% Cost | ~9k rows
**Flags**: GROUP_BY
**Outputs**: [s_store_name1, s_store_id1, d_week_seq1, sun_sales1 / NULLIF(sun_sales2, mon_sales1 / NULLIF(mon_sales2, tue_sales1 / NULLIF(tue_sales2, wed_sales1 / NULLIF(wed_sales2, thu_sales1 / NULLIF(thu_sales2, fri_sales1 / NULLIF(fri_sales2, sat_sales1 / NULLIF(sat_sales2]
**Dependencies**: wss, store, date_dim, wss, store, date_dim
**Operators**: SEQ_SCAN[date_dim], SEQ_SCAN[CTE Scan], SEQ_SCAN[CTE Scan], SEQ_SCAN[store], SEQ_SCAN[date_dim]
**Key Logic (SQL)**:
```sql
SELECT s_store_name1, s_store_id1, d_week_seq1, sun_sales1 / sun_sales2, mon_sales1 / mon_sales2, tue_sales1 / tue_sales2, wed_sales1 / wed_sales2, thu_sales1 / thu_sales2, fri_sales1 / fri_sales2, sat_sales1 / sat_sales2 FROM (SELECT s_store_name AS s_store_name1, wss.d_week_seq AS d_week_seq1, s_store_id AS s_store_id1, sun_sales AS sun_sales1, mon_sales AS mon_sales1, tue_sales AS tue_sales1, wed_sales AS wed_sales1, thu_sales AS thu_sales1, fri_sales AS fri_sales1, sat_sales AS sat_sales1 FR ...
```

### Edges
- wss → main_query
- wss → main_query


## Semantic Intent

*Not pre-computed. Infer business intent from the SQL.*

## Aggregation Semantics Check

You MUST verify aggregation equivalence for any proposed restructuring:

- **STDDEV_SAMP(x)** requires >=2 non-NULL values per group. Returns NULL for 0-1 values. Changing group membership changes the result.
- `STDDEV_SAMP(x) FILTER (WHERE year=1999)` over a combined (1999,2000) group is NOT equivalent to `STDDEV_SAMP(x)` over only 1999 rows — FILTER still uses the combined group's membership for the stddev denominator.
- **AVG and STDDEV are NOT duplicate-safe**: if a join introduces row duplication, the aggregate result changes.
- When splitting a UNION ALL CTE with GROUP BY + aggregate, each split branch must preserve the exact GROUP BY columns and filter to the exact same row set as the original.
- **SAFE ALTERNATIVE**: If GROUP BY includes the discriminator column (e.g., d_year), each group is already partitioned. STDDEV_SAMP computed per-group is correct. You can then pivot using `MAX(CASE WHEN year = 1999 THEN year_total END) AS year_total_1999` because the GROUP BY guarantees exactly one row per (customer, year) — the MAX is just a row selector, not a real aggregation.

## Engine Profile: Field Intelligence Briefing

*This is field intelligence gathered from 53 DSB queries at SF5-SF10. PostgreSQL is a fundamentally different optimizer than DuckDB — it has bitmap index scans, JIT compilation, and aggressive CTE materialization. Techniques that work on DuckDB often regress here. Use this to guide your analysis but apply your own judgment — every query is different. Add to this knowledge if you observe something new.*

### Optimizer Strengths (DO NOT fight these)

- **BITMAP_OR_SCAN**: Multi-branch OR conditions on indexed columns are handled via BitmapOr — a single fact table scan with bitmap combination. Extremely efficient.
  *Field note:* NEVER split OR conditions into UNION ALL branches on PostgreSQL. BitmapOr is categorically faster. We saw 0.21x on Q085 and 0.26x on Q091 — each UNION branch forced a full 7-table join + fact scan that BitmapOr avoids. The only conceivable case for OR-to-UNION on PG is when branches reference completely different tables, and even then it's risky.
- **SEMI_JOIN_EXISTS**: EXISTS/NOT EXISTS uses semi-join with early termination. Stops scanning after the first match per outer row.
  *Field note:* NEVER convert EXISTS to IN/NOT IN or to materialized CTEs with SELECT DISTINCT. The semi-join stops after first match — materializing forces a full DISTINCT scan of million-row fact tables. We saw 0.50x on Q069 (3 DISTINCT CTEs vs 3 semi-joins) and 0.86x on Q010 (UNION ALL CTE without dedup vs OR'd EXISTS short-circuits). Also: NOT IN has NULL-handling semantics that can block hash anti-join optimization.
- **INNER_JOIN_REORDERING**: PostgreSQL freely reorders INNER JOINs based on estimated selectivity. The cost model works well for explicit JOIN...ON syntax.
  *Field note:* Don't restructure INNER JOIN orders — the optimizer handles this well. Focus on queries where JOIN type (LEFT) prevents reordering, or where comma-joins confuse the cost model (see COMMA_JOIN_WEAKNESS gap).
- **INDEX_ONLY_SCAN**: When an index covers all requested columns, PostgreSQL reads only the index without touching the heap.
  *Field note:* Dimension table lookups are already fast via index-only scans. Pre-filtering small dimensions (<10K rows) into CTEs adds materialization overhead with minimal benefit.
- **PARALLEL_QUERY_EXECUTION**: PostgreSQL parallelizes large scans and aggregations across worker processes with partial aggregation finalization.
  *Field note:* Large fact table scans are already parallelized. Restructuring into CTEs may reduce parallelism opportunities because CTE materialization is single-threaded.
- **JIT_COMPILATION**: PostgreSQL JIT-compiles complex expressions and tuple deforming for long-running queries.
  *Field note:* Complex WHERE expressions have low per-row overhead due to JIT. Simplifying expressions for performance is usually unnecessary.

### Optimizer Gaps (hunt for these)

**COMMA_JOIN_WEAKNESS** [HIGH]
  What: Implicit comma-separated FROM tables (FROM t1, t2, t3 WHERE t1.id = t2.id) are treated as cross products initially. The cost model is significantly weaker on comma-joins than on explicit JOIN...ON syntax.
  Why: The planner's join search space is less constrained with comma-joins. Explicit JOINs provide structural hints that help the optimizer find better plans faster, especially for 5+ table joins.
  Opportunity: Convert comma-joins to explicit JOIN...ON syntax. This alone can unlock 2-3x improvements. Best when combined with date_cte_isolate.
  What worked:
    + Q080: 3.32x — comma-joins to explicit JOINs + date CTE on multi-channel UNION query
    + Q099: 2.28x — same pattern on star schema
    + Q054: 1.14x — JOIN conversion alone
  Field notes:
    * Look for FROM t1, t2, t3 WHERE ... syntax. 5+ comma-separated tables is the sweet spot.
    * EXPLAIN will show unexpected join orders or high-cost nested loops when comma-joins confuse the cost model.
    * The win usually comes from explicit JOINs + CTE together, not CTE alone. date_cte_isolate without JOIN conversion is often neutral or harmful.
    * This is our most reliable PG optimization — convert the implicit syntax and the optimizer rewards you.
    * Validate at target scale — SF5 wins don't predict SF10 on PG (Q027 went from 9.62x to 0.97x).

**CORRELATED_SUBQUERY_PARALYSIS** [HIGH]
  What: Cannot automatically decorrelate complex correlated subqueries. Correlated scalar subqueries with aggregates are executed as nested-loop with repeated evaluation.
  Why: Same limitation as DuckDB — correlation requires recognizing GROUP BY + JOIN equivalence. PostgreSQL does basic decorrelation for simple IN/EXISTS but fails on complex aggregate correlations.
  Opportunity: Convert correlated WHERE to explicit CTE with GROUP BY + JOIN.
  What worked:
    + Q092: 4428x — timeout recovery. Unbounded correlated subquery converted to explicit JOIN.
    + Q032: 391x — same pattern, timeout to sub-second.
  Field notes:
    * Look for WHERE col > (SELECT AGG FROM ... WHERE outer.key = inner.key) patterns.
    * EXPLAIN will show SubPlan or nested-loop with repeated subquery execution if the optimizer failed to decorrelate.
    * These are often the queries that time out — if a DSB query runs >10s, check for correlated scalar subqueries first.
    * Simple IN/EXISTS correlation is already handled by PG's semi-join optimization — only complex aggregate correlations need manual decorrelation.
    * CRITICAL: when decorrelating, preserve ALL filters from the original subquery in the new CTE.
    * Validate at target scale — decorrelation wins are usually robust across scales, but verify on SF10.

**NON_EQUI_JOIN_INPUT_BLINDNESS** [HIGH]
  What: Cannot pre-filter fact tables before non-equi join operations (date arithmetic, range comparisons, quantity < quantity). Non-equi joins fall back to nested-loop, which is O(N*M).
  Why: Hash joins require equi-conditions. Non-equi joins fall back to nested-loop, which processes all input rows. The optimizer cannot recognize that reducing N or M via pre-filtering would dramatically reduce cost.
  Opportunity: Reduce fact table input size via filtered CTE before the non-equi join.
  What worked:
    + Q072: 2.68x — pre-filtered catalog_sales by wholesale_cost range before non-equi quantity comparison with inventory. Reduced nested-loop input by ~70%.
  What didn't work:
    - Q013: 0.79x — pre-filtered with UNION/OR superset (loose filter). CTE fence blocked dimension predicate pushdown.
  Field notes:
    * Look for non-equi join conditions: >, <, BETWEEN, date arithmetic, quantity comparisons.
    * EXPLAIN will show nested-loop join with high row estimates on both sides.
    * A simple range filter on the fact table (e.g., wholesale_cost BETWEEN 34 AND 54) works well. A union/OR superset filter does NOT — it materializes too many rows.
    * Only pre-filter when one side of the non-equi join is a large fact table. Small dimension tables (<10K rows) don't benefit.
    * The CTE fence cost is negligible vs the non-equi join savings when the filter is tight.
    * Validate at target scale — non-equi join cost grows super-linearly, so wins tend to hold or improve at larger scales.

**CTE_MATERIALIZATION_FENCE** [MEDIUM]
  What: PostgreSQL materializes CTEs by default (multi-referenced) or by choice (AS MATERIALIZED). This creates a hard optimization fence — no predicate pushdown from outer query into CTE. This makes CTE-based strategies a double-edged sword on PG.
  Why: CTE is computed and stored in memory/temp before the outer query executes. Any WHERE clause filters in the outer query cannot be pushed back into the CTE definition. Single-reference CTEs may be inlined in PG 12+, but multi-referenced CTEs are always materialized.
  Opportunity: Use materialization STRATEGICALLY: materialize when the CTE is expensive and reused multiple times. Avoid CTEs that fence off predicate pushdown for single-use cases.
  What worked:
    + Q065: 1.95x — strategic materialization prevented redundant fact table scan multiplication
  What didn't work:
    - Q031: 0.74x — CTE fence blocked predicate pushdown that worked in original
    - Q038: 0.77x — date_cte_isolate added fence that blocked INTERSECT optimization
    - Q064: 0.65x — duplicated 18-table CTE body to push filters inside. NEVER do this — computing an 18-table join twice is always worse than computing once and filtering.
  Field notes:
    * NEVER duplicate a CTE body to push a filter inside when the CTE contains 5+ table joins. Filter the materialized result with WHERE, don't recompute.
    * Do NOT use the AS MATERIALIZED keyword on CTEs. Write plain CTEs: 'name AS (SELECT ...)'. PG auto-materializes when beneficial. Forcing materialization on small dimension CTEs (<1000 rows) adds temp-table I/O overhead (0.69x observed on Q080).
    * CTE fence + EXISTS = disaster. If the query uses EXISTS/NOT EXISTS, a CTE that fences off the semi-join optimization is actively harmful.
    * CTE fence + INTERSECT/EXCEPT = harmful. Set operations handle their inputs efficiently inline. A CTE fence per branch adds overhead.
    * A CTE result referenced 2+ times is materialized once, probed many — this IS the valid use case for CTEs on PG.
    * When a date_cte_isolate CTE is applied to UNION ALL branches, apply to ALL branches or NONE. Partial application creates asymmetric plans.

**CROSS_CTE_PREDICATE_BLINDNESS** [MEDIUM]
  What: Same gap as DuckDB but WORSE on PostgreSQL because CTE materialization fence makes it more impactful. Predicates in the outer WHERE cannot propagate into materialized CTEs.
  Why: Even single-reference CTEs may be materialized (version-dependent). The optimizer does not trace data lineage through CTE boundaries.
  Opportunity: Same as DuckDB: pre-filter into CTE definition. But be more cautious — only when the CTE is clearly suboptimal.
  What worked:
    + Q080: 3.32x — date filter + comma-join conversion (the combo is key)
    + Q099: 2.28x — date CTE with explicit JOIN
  What didn't work:
    - Q027: 0.97x — won at SF5 (9.62x) but neutral at SF10. Cost model unreliability across scale.
    - Q031: 0.55x — over-decomposed an already-efficient query
  Field notes:
    * Convert comma-joins to explicit JOINs simultaneously — the CTE alone often isn't enough on PG.
    * EXPLAIN will show sequential scan on dimension table without index condition — that's the signal.
    * Don't use this on queries with INTERSECT, EXCEPT, or set operations — the CTE fence blocks set operation optimization.
    * If the query already returns quickly (<100ms), the CTE materialization overhead can negate any savings.
    * Validate at target scale — SF5 wins don't reliably predict SF10 on PostgreSQL.

**SCALE WARNING**: PostgreSQL optimizations validated at SF5 do NOT reliably predict SF10 behavior. 7 queries that won at SF5 regressed at SF10 (Q027 9.62x at SF5 but 0.97x at SF10). Always validate at target scale. Cost estimates are overconfident on sample data.

## Per-Query Configuration Tuning (SET LOCAL)

In addition to SQL rewrites, you can recommend SET LOCAL configuration changes that fix planner-level bottlenecks (sort spills, parallelism thresholds, cost model miscalibration). SET LOCAL is transaction-scoped — settings revert on COMMIT, affecting no other connections.

### Current PostgreSQL Settings
```
  default_statistics_target = 100
  effective_cache_size = 5242888kB
  from_collapse_limit = 8
  geqo_threshold = 12
  hash_mem_multiplier = 2
  jit = on
  join_collapse_limit = 8
  max_connections = 100
  max_parallel_workers = 8
  max_parallel_workers_per_gather = 2
  max_worker_processes = 8
  parallel_setup_cost = 1000
  parallel_tuple_cost = 0.1
  random_page_cost = 4
  seq_page_cost = 1
  shared_buffers = 163848kB
  work_mem = 4096kB
```

**System cap**: max_parallel_workers = 8 (do NOT set max_parallel_workers_per_gather higher than this)

### Tunable Parameters (whitelist)

- **effective_cache_size** (1024MB to 65536MB): Advisory: how much OS cache to expect (MB). Safe to set aggressively.
- **enable_hashjoin** (on | off): Enable hash join plan type.
- **enable_mergejoin** (on | off): Enable merge join plan type.
- **enable_nestloop** (on | off): Enable nested-loop join plan type.
- **enable_seqscan** (on | off): Enable sequential scan plan type.
- **from_collapse_limit** (1 to 20): Max FROM items before subqueries stop being flattened.
- **geqo_threshold** (2 to 20): Number of FROM items that triggers genetic query optimizer.
- **hash_mem_multiplier** (1.0 to 10.0): Multiplier applied to work_mem for hash-based operations.
- **jit** (on | off): Enable JIT compilation.
- **jit_above_cost** (0.0 to 1000000.0): Query cost above which JIT is activated.
- **join_collapse_limit** (1 to 20): Max FROM items before planner stops trying all join orders.
- **max_parallel_workers_per_gather** (0 to 8): Max parallel workers per Gather node.
- **parallel_setup_cost** (0.0 to 10000.0): Planner estimate of cost to launch parallel workers.
- **parallel_tuple_cost** (0.0 to 1.0): Planner estimate of cost to transfer a tuple to parallel worker.
- **random_page_cost** (1.0 to 10.0): Planner estimate of cost of a random page fetch (1.0 = SSD, 4.0 = HDD).
- **work_mem** (64MB to 2048MB): Memory for sorts/hashes per operation (MB). Allocated PER-OPERATION, not per-query. Count hash/sort ops in EXPLAIN before sizing.

### Config Tuning Rules

- **Sort/Hash spills**: 'Sort Method: external merge' or 'Batches: N>1' → increase work_mem. Count hash/sort ops first: work_mem is PER-OPERATION.
- **Workers not launching**: 'Workers Planned: N, Launched: 0' → reduce parallel_setup_cost (try 100) and parallel_tuple_cost (try 0.001).
- **random_page_cost — CAREFUL**: Lowering from 4.0 to 1.1 causes 0.5x-0.7x REGRESSIONS on queries with optimal access paths. ONLY change if you see Seq Scans where index scans would be better.
- **JIT overhead**: If JIT total time > 5% of execution time, set jit=off.
- **Empty is valid**: If no planner bottleneck exists, recommend no config. Speculative tuning causes regressions.

## Correctness Constraints (4 — NEVER violate)

**[CRITICAL] COMPLETE_OUTPUT**: The rewritten query must output ALL columns from the original SELECT. Never drop, rename, or reorder output columns. Every column alias must be preserved exactly as in the original.

**[CRITICAL] CTE_COLUMN_COMPLETENESS**: CRITICAL: When creating or modifying a CTE, its SELECT list MUST include ALL columns referenced by downstream queries. Check the Node Contracts section: every column in downstream_refs MUST appear in the CTE output. Also ensure: (1) JOIN columns used by consumers are included in SELECT, (2) every table referenced in WHERE is present in FROM/JOIN, (3) no ambiguous column names between the CTE and re-joined tables. Dropping a column that a downstream node needs will cause an execution error.
  - Failure: Q21 regressed to ?
  - Failure: Q76 regressed to ?

**[CRITICAL] LITERAL_PRESERVATION**: CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.
  - Failure: Q2 regressed to ?
  - Failure: Q7 regressed to ?

**[CRITICAL] SEMANTIC_EQUIVALENCE**: The rewritten query MUST return exactly the same rows, columns, and ordering as the original. This is the prime directive. Any rewrite that changes the result set — even by one row, one column, or a different sort order — is WRONG and will be REJECTED.

## Your Task

First, use a `<reasoning>` block for your internal analysis. This will be stripped before parsing. Work through these steps IN ORDER:

1. **CLASSIFY**: What structural archetype is this query?
   (channel-comparison self-join / correlated-aggregate filter / star-join with late dim filter / repeated fact scan / multi-channel UNION ALL / EXISTS-set operations / other)

2. **EXPLAIN PLAN ANALYSIS**: From the EXPLAIN ANALYZE output, identify:
   - Compute wall-clock ms per EXPLAIN node. Sum repeated operations (e.g., 2x store_sales joins = total cost). The EXPLAIN is ground truth, not the DAG cost percentages.
   - Which nodes consume >10% of runtime and WHY
   - Where row counts drop sharply (existing selectivity)
   - Where row counts DON'T drop (missed optimization opportunity)
   - Whether the optimizer already splits CTEs, pushes predicates, or performs transforms you might otherwise assign
   - Count scans per base table. If a fact table is scanned N times, a restructuring that reduces it to 1 scan saves (N-1)/N of that table's I/O cost. Prioritize transforms that reduce scan count on the largest tables.
   - Whether the CTE is materialized once and probed multiple times, or re-executed per reference

3. **GAP MATCHING**: Compare the EXPLAIN analysis to the Engine Profile gaps above. For each gap:
   - Does this query exhibit the gap? (e.g., is a predicate NOT pushed into a CTE? Is the same fact table scanned multiple times?)
   - Check the 'when_to_exploit' conditions — does this query meet them?
   - Check the 'when_NOT_to_exploit' conditions — any disqualifiers?
   - Also verify: is the optimizer ALREADY handling this well? (Check the 'optimizer_handles_well' list — if the engine already does it, your transform adds overhead, not value.)

4. **AGGREGATION TRAP CHECK**: For every aggregate function in the query, verify: does my proposed restructuring change which rows participate in each group? STDDEV_SAMP, VARIANCE, PERCENTILE_CONT, CORR are grouping-sensitive. SUM, COUNT, MIN, MAX are grouping-insensitive (modulo duplicates). If the query uses FILTER clauses or conditional aggregation, verify equivalence explicitly.

5. **TRANSFORM SELECTION**: From the matched engine gaps, select transforms that exploit the specific gaps present in THIS query. Rank by expected value (rows affected × historical speedup from evidence). Select 4 that are structurally diverse — each attacking a different gap or bottleneck.

6. **DAG DESIGN**: For each worker's strategy, define the target DAG topology. Verify that every node contract has exhaustive output columns by checking downstream references.

7. **CONFIG ANALYSIS** (PostgreSQL only): From the EXPLAIN plan, identify planner-level bottlenecks that SET LOCAL can fix:
   - Sort spills to disk? → work_mem (count ops first)
   - Workers planned but not launched? → parallel_setup_cost, parallel_tuple_cost
   - Seq scans where index scans would be better? → random_page_cost (CAREFUL)
   - JIT overhead > 5% of execution time? → jit=off
   - Every recommended param MUST cite a specific EXPLAIN node as evidence

Then produce the structured briefing in EXACTLY this format:

```
=== SHARED BRIEFING ===

SEMANTIC_CONTRACT: (80-150 tokens, cover ONLY:)
(a) One sentence of business intent (start from pre-computed intent if available).
(b) JOIN type semantics that constrain rewrites (INNER = intersection = all sides must match).
(c) Any aggregation function traps specific to THIS query.
(d) Any filter dependencies that a rewrite could break.
Do NOT repeat information already in ACTIVE_CONSTRAINTS or REGRESSION_WARNINGS.

BOTTLENECK_DIAGNOSIS:
[Which operation dominates cost and WHY (not just '50% cost').
Scan-bound vs join-bound vs aggregation-bound.
Cardinality flow (how many rows at each stage).
What the optimizer already handles well (don't re-optimize).
Whether DAG cost percentages are misleading.]

ACTIVE_CONSTRAINTS:
- [CORRECTNESS_CONSTRAINT_ID]: [Why it applies to this query, 1 line]
- [ENGINE_GAP_ID]: [Evidence from EXPLAIN that this gap is active]
(List all 4 correctness constraints + the 1-3 engine gaps that
are active for THIS query based on your EXPLAIN analysis.)

REGRESSION_WARNINGS:
1. [Pattern name] ([observed regression]):
   CAUSE: [What happened mechanistically]
   RULE: [Actionable avoidance rule for THIS query]
(If no regression warnings are relevant, write 'None applicable.')

SET_LOCAL_CONFIG:
{
  "params": {"param_name": "value", ...},
  "reasoning": "Evidence from EXPLAIN plan for each param."
}
(JSON object. Only include params from the whitelist above.
Every param MUST cite a specific EXPLAIN node as evidence.
If no config changes help, use: {"params": {}, "reasoning": "No planner bottlenecks."})

=== WORKER 1 BRIEFING ===

STRATEGY: [strategy_name]
TARGET_DAG:
  [node] -> [node] -> [node]
NODE_CONTRACTS:
(Write all fields as SQL fragments, not natural language.
Example: 'WHERE: d_year IN (1999, 2000)' not 'WHERE: filter to target years'.
The worker uses these as specifications to code against.)
  [node_name]:
    FROM: [tables/CTEs]
    JOIN: [join conditions]
    WHERE: [filters]
    GROUP BY: [columns] (if applicable)
    AGGREGATE: [functions] (if applicable)
    OUTPUT: [exhaustive column list]
    EXPECTED_ROWS: [approximate row count from EXPLAIN analysis]
    CONSUMERS: [downstream nodes]
EXAMPLES: [ex1], [ex2], [ex3]
EXAMPLE_REASONING:
[Why each example's pattern matches THIS query's bottleneck.
What adaptation is needed.]
HAZARD_FLAGS:
- [Specific risk for this approach on this query]

=== WORKER 2 BRIEFING ===
[Same structure as Worker 1, DIFFERENT strategy]

=== WORKER 3 BRIEFING ===
[Same structure as Worker 1, DIFFERENT strategy]

=== WORKER 4 BRIEFING === (EXPLORATION WORKER)
[Same structure as Worker 1, DIFFERENT strategy]
CONSTRAINT_OVERRIDE: [CONSTRAINT_ID or 'None']
OVERRIDE_REASONING: [Why this query's structure differs from the observed failure, or 'N/A']
EXPLORATION_TYPE: [constraint_relaxation | compound_strategy | novel_combination]
```

## Transform Catalog

Select 4 transforms that are applicable to THIS query, maximizing structural diversity (each must attack a different part of the execution plan).

### Predicate Movement
- **global_predicate_pushdown**: Trace selective predicates from late in the CTE chain back to the earliest scan via join equivalences. Biggest win when a dimension filter is applied after a large intermediate materialization.
  Maps to examples: pushdown, early_filter, date_cte_isolate
- **transitive_predicate_propagation**: Infer predicates through join equivalence chains (A.key = B.key AND B.key = 5 -> A.key = 5). Especially across CTE boundaries where optimizers stop propagating.
  Maps to examples: early_filter, dimension_cte_isolate
- **null_rejecting_join_simplification**: When downstream WHERE rejects NULLs from the outer side of a LEFT JOIN, convert to INNER. Enables reordering and predicate pushdown. CHECK: does the query actually have LEFT/OUTER joins before assigning this.
  Maps to examples: (no direct gold example — novel transform)

### Join Restructuring
- **self_join_elimination**: When a UNION ALL CTE is self-joined N times with each join filtering to a different discriminator, split into N pre-partitioned CTEs. Eliminates discriminator filtering and repeated hash probes on rows that don't match.
  Maps to examples: union_cte_split, shared_dimension_multi_channel
- **decorrelation**: Convert correlated EXISTS/IN/scalar subqueries to CTE + JOIN. CHECK: does the query actually have correlated subqueries before assigning this.
  Maps to examples: decorrelate, composite_decorrelate_union
- **aggregate_pushdown**: When GROUP BY follows a multi-table join but aggregation only uses columns from one side, push the GROUP BY below the join. CHECK: verify the join doesn't change row multiplicity for the aggregate (one-to-many breaks AVG/STDDEV).
  Maps to examples: (no direct gold example — novel transform)
- **late_attribute_binding**: When a dimension table is joined only to resolve display columns (names, descriptions) that aren't used in filters, aggregations, or join conditions, defer that join until after all filtering and aggregation is complete. Join on the surrogate key once against the final reduced result set. This eliminates N-1 dimension scans when the CTE references the dimension N times. CHECK: verify the deferred columns aren't used in WHERE, GROUP BY, or JOIN ON — only in the final SELECT.
  Maps to examples: dimension_cte_isolate (partial pattern), early_filter

### Scan Optimization
- **star_join_prefetch**: Pre-filter ALL dimension tables into CTEs, then probe fact table with the combined key intersection.
  Maps to examples: dimension_cte_isolate, multi_dimension_prefetch, prefetch_fact_join, date_cte_isolate
- **single_pass_aggregation**: Merge N subqueries on the same fact table into 1 scan with CASE/FILTER inside aggregates. CHECK: STDDEV_SAMP/VARIANCE are grouping-sensitive — FILTER over a combined group != separate per-group computation.
  Maps to examples: single_pass_aggregation, channel_bitmap_aggregation
- **scan_consolidation_pivot**: When a CTE is self-joined N times with each reference filtering to a different discriminator (e.g., year, channel), consolidate into fewer scans that GROUP BY the discriminator, then pivot rows to columns using MAX(CASE WHEN discriminator = X THEN agg_value END). This halves the fact scans and dimension joins. SAFE when GROUP BY includes the discriminator — each group is naturally partitioned, so aggregates like STDDEV_SAMP are computed correctly per-partition. The pivot MAX is just a row selector (one row per group), not a real aggregation.
  Maps to examples: single_pass_aggregation, union_cte_split

### Structural Transforms
- **union_consolidation**: Share dimension lookups across UNION ALL branches that scan different fact tables with the same dim joins.
  Maps to examples: shared_dimension_multi_channel
- **window_optimization**: Push filters before window functions when they don't affect the frame. Convert ROW_NUMBER + filter to LATERAL + LIMIT. Merge same-PARTITION windows into one sort pass.
  Maps to examples: deferred_window_aggregation
- **exists_restructuring**: Convert INTERSECT to EXISTS for semi-join short-circuit, or restructure complex EXISTS with shared CTEs. CHECK: does the query actually have INTERSECT or complex EXISTS.
  Maps to examples: intersect_to_exists, multi_intersect_exists_cte

## Strategy Selection Rules

1. **CHECK APPLICABILITY**: Each transform has a structural prerequisite (correlated subquery, UNION ALL CTE, LEFT JOIN, etc.). Verify the query actually has the prerequisite before assigning a transform. DO NOT assign decorrelation if there are no correlated subqueries.
2. **CHECK OPTIMIZER OVERLAP**: Read the EXPLAIN plan. If the optimizer already performs a transform (e.g., already splits a UNION CTE, already pushes a predicate), that transform will have marginal benefit. Note this in your reasoning and prefer transforms the optimizer is NOT already doing.
3. **MAXIMIZE DIVERSITY**: Each worker must attack a different part of the execution plan. Do not assign 'pushdown variant A' and 'pushdown variant B'. Assign transforms from different categories above.
4. **ASSESS RISK PER-QUERY**: Risk is a function of (transform x query complexity), not an inherent property of the transform. Decorrelation is low-risk on a simple EXISTS and high-risk on nested correlation inside a CTE. Assess per-assignment.
5. **COMPOSITION IS ALLOWED AND ENCOURAGED**: A worker's strategy can combine 2-3 transforms from different categories (e.g., star_join_prefetch + scan_consolidation_pivot, or date_cte_isolate + early_filter + decorrelate). The TARGET_DAG should reflect the combined structure. Do not assign two workers the same composition — each must include at least one unique transform. Compound strategies are often the source of the biggest wins.
6. **MINIMAL-CHANGE BASELINE**: If the EXPLAIN shows the optimizer already handles the primary bottleneck (e.g., already splits CTEs, already pushes predicates), consider assigning one worker as a minimal-change baseline: explicit JOINs only, no structural changes. This provides a regression-safe fallback.

Each worker gets 1-3 examples. If fewer than 2 examples genuinely match the worker's strategy, assign 1 and state 'No additional examples apply.' Do NOT pad with irrelevant examples — an irrelevant example is worse than no example because the worker will try to apply its pattern. No duplicate examples across workers. Use example IDs from the catalog above.

For TARGET_DAG: Define the CTE structure you want the worker to produce. The worker's job becomes pure SQL generation within your defined structure. For NODE_CONTRACTS: Be exhaustive with OUTPUT columns — missing columns cause semantic breaks.

## Exploration Budget (Worker 4)

Workers 1-3 follow the engine profile's proven patterns. **Worker 4 is the EXPLORATION worker** with a different mandate:

Worker 4 MAY:
  (a) Try a technique that the engine profile's 'what_didnt_work' warns about, IF the structural context of THIS query differs from the observed failure — explain why in HAZARD_FLAGS
  (b) Combine 2-3 transforms from different engine gaps into a compound strategy that hasn't been tested before
  (c) Attempt a novel technique not listed in the engine profile, if the EXPLAIN plan reveals an optimizer blind spot not yet documented

Worker 4 may NEVER violate correctness constraints (LITERAL_PRESERVATION, SEMANTIC_EQUIVALENCE, COMPLETE_OUTPUT, CTE_COLUMN_COMPLETENESS).

The exploration worker's output is tagged EXPLORATORY and tracked separately. Past failures documented in the engine profile are context-specific — they happened on specific queries with specific structures. Worker 4's job is to test whether those failures generalize or not. If Worker 4 discovers a new win, it becomes field intelligence for the engine profile.

## Output Consumption Spec

Each worker receives:
1. SHARED BRIEFING (SEMANTIC_CONTRACT + BOTTLENECK_DIAGNOSIS + ACTIVE_CONSTRAINTS + REGRESSION_WARNINGS)
2. Their specific WORKER N BRIEFING (STRATEGY + TARGET_DAG + NODE_CONTRACTS + EXAMPLES + EXAMPLE_REASONING + HAZARD_FLAGS)
3. Full before/after SQL for their assigned examples (retrieved by example ID)
4. The original query SQL (full, as reference)
5. Column completeness contract + output format spec

Workers do NOT see other workers' briefings.
Presentation order: briefing first (understanding), then examples (patterns), then original SQL (source), then output format (mechanics).