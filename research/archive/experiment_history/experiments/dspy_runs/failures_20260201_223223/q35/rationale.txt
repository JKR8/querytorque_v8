1. **Predicate Pushdown via CTEs**: Created a `filtered_dates` CTE to compute date_dim keys for d_year=2001 and d_qoy<4 once (274 rows), instead of scanning date_dim three times in subqueries. This reduces I/O and computation.
2. **Pre-computed Customer Sets**: Created CTEs (`store_customers`, `web_customers`, `catalog_customers`) that join sales tables with `filtered_dates` and select DISTINCT customer keys. This materializes small result sets (based on row estimates: store_sales 306k rows â†’ distinct customers likely less, web_sales 80k, catalog_sales 158k) and avoids repeated joins in EXISTS subqueries.
3. **Maintained Query Semantics**: The EXISTS conditions are unchanged, using the pre-computed customer sets. The JOINs in the main FROM clause remain as INNER JOINs (explicitly written for clarity, but semantically identical to original comma-separated joins).
4. **DuckDB Optimization**: DuckDB can materialize these CTEs, benefiting from parallel hash joins and reduced repeated scans. The execution plan should show lower cost for date_dim scans and more efficient EXISTS checks via small hash lookups.

This optimization adheres to constraints: no restructuring of joins (CTEs are additions, not changes), no new WHERE conditions (filters moved into CTEs), and no conversion of INTERSECT/UNION. It leverages safe predicate pushdown and pre-computation.