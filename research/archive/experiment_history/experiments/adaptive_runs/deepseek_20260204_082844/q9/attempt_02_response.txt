Looking at the query and execution plan, I can see the main issue: there are 10 separate scans of the `store_sales` table (5 for COUNT(*) and 5 for AVG()), each with different quantity range filters. This is extremely inefficient since each scan processes ~575,621 rows.

The key insight is that we can compute all the necessary aggregates in a single pass over the data using conditional aggregation. This is a **semantic_rewrite** transformation.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "semantic_rewrite",
      "nodes": {
        "main_query": "/* start query 9 in stream 0 using template query9.tpl */ SELECT CASE WHEN cnt_1_20 > 2972190 THEN avg_ext_sales_1_20 ELSE avg_net_profit_1_20 END AS bucket1, CASE WHEN cnt_21_40 > 4505785 THEN avg_ext_sales_21_40 ELSE avg_net_profit_21_40 END AS bucket2, CASE WHEN cnt_41_60 > 1575726 THEN avg_ext_sales_41_60 ELSE avg_net_profit_41_60 END AS bucket3, CASE WHEN cnt_61_80 > 3188917 THEN avg_ext_sales_61_80 ELSE avg_net_profit_61_80 END AS bucket4, CASE WHEN cnt_81_100 > 3525216 THEN avg_ext_sales_81_100 ELSE avg_net_profit_81_100 END AS bucket5 FROM reason CROSS JOIN (SELECT COUNT(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN 1 END) AS cnt_1_20, AVG(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN ss_ext_sales_price END) AS avg_ext_sales_1_20, AVG(CASE WHEN ss_quantity BETWEEN 1 AND 20 THEN ss_net_profit END) AS avg_net_profit_1_20, COUNT(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN 1 END) AS cnt_21_40, AVG(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN ss_ext_sales_price END) AS avg_ext_sales_21_40, AVG(CASE WHEN ss_quantity BETWEEN 21 AND 40 THEN ss_net_profit END) AS avg_net_profit_21_40, COUNT(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN 1 END) AS cnt_41_60, AVG(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN ss_ext_sales_price END) AS avg_ext_sales_41_60, AVG(CASE WHEN ss_quantity BETWEEN 41 AND 60 THEN ss_net_profit END) AS avg_net_profit_41_60, COUNT(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN 1 END) AS cnt_61_80, AVG(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN ss_ext_sales_price END) AS avg_ext_sales_61_80, AVG(CASE WHEN ss_quantity BETWEEN 61 AND 80 THEN ss_net_profit END) AS avg_net_profit_61_80, COUNT(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN 1 END) AS cnt_81_100, AVG(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN ss_ext_sales_price END) AS avg_ext_sales_81_100, AVG(CASE WHEN ss_quantity BETWEEN 81 AND 100 THEN ss_net_profit END) AS avg_net_profit_81_100 FROM store_sales) AS aggregates WHERE r_reason_sk = 1"
      },
      "invariants_kept": [
        "output columns unchanged",
        "same result rows",
        "same aggregation results",
        "required predicate r_reason_sk = 1 preserved"
      ],
      "expected_speedup": "8x-10x",
      "risk": "low"
    }
  ],
  "explanation": "Rewrote 10 separate store_sales scans (5 COUNT(*) and 5 AVG() operations) into a single scan with conditional aggregation. This reduces I/O from scanning ~5.7M rows (10 Ã— 575,621) to scanning ~575,621 rows once. All aggregates are computed in parallel using CASE expressions within a single pass over the data, maintaining exact same semantics."
}
```

**Key insight:** Instead of 10 separate scans of the store_sales table (each processing ~575,621 rows), we scan it once and compute all aggregates using conditional aggregation with CASE expressions. This reduces total I/O from ~5.7M rows to ~575,621 rows - a theoretical 10x reduction in data scanning.