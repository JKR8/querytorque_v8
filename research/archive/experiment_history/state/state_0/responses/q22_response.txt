Looking at the execution plan, I see the major bottleneck is the full table scan of `inventory` (1.6B rows) followed by a hash join. The `filtered_dates` CTE is already efficient with only 366 rows. The issue is that the join between `inventory` and `filtered_dates` happens late, forcing a scan of all inventory rows first.

I'll apply a **prefetch_fact_join** transform similar to the example, but with a twist: pre-join `inventory` with the date filter early to create a filtered fact table first.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "prefetch_fact_join",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1200 AND 1200 + 11",
        "filtered_inventory": "SELECT inv_item_sk, inv_quantity_on_hand FROM inventory JOIN filtered_dates ON inv_date_sk = d_date_sk",
        "main_query": "SELECT i_product_name, i_brand, i_class, i_category, AVG(inv_quantity_on_hand) AS qoh FROM filtered_inventory JOIN item ON inv_item_sk = i_item_sk GROUP BY ROLLUP (i_product_name, i_brand, i_class, i_category) ORDER BY qoh NULLS FIRST, i_product_name NULLS FIRST, i_brand NULLS FIRST, i_class NULLS FIRST, i_category NULLS FIRST LIMIT 100"
      },
      "invariants_kept": [
        "same result rows", 
        "same ordering",
        "same column output",
        "same grouping and aggregation"
      ],
      "expected_speedup": "1.5x",
      "risk": "low"
    }
  ],
  "explanation": "Created a new CTE 'filtered_inventory' that pre-joins inventory with filtered_dates before joining with item. This reduces the intermediate fact table from 1.6B rows to ~26M rows early in the pipeline, minimizing the hash join cost. The main query now only joins the filtered inventory (26M rows) with item (102K rows) rather than scanning all inventory first."
}
```