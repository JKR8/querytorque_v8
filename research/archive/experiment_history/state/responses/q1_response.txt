Looking at this query pattern and cost structure, I can apply an **early_filter** transform to push the store and date filters earlier, but there's an even better opportunity: **date_cte_isolate** since we have a date_dim filter with fact table (store_returns) present.

The key insight from the pattern is: extract date_dim filtering into a CTE early, then join with fact table to reduce the fact table scan size immediately.

```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "date_cte_isolate",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_year = 2000",
        "filtered_stores": "SELECT s_store_sk FROM store WHERE s_state = 'SD'",
        "filtered_returns": "SELECT sr.sr_customer_sk, sr.sr_store_sk, sr.SR_FEE FROM store_returns AS sr JOIN filtered_dates ON sr.sr_returned_date_sk = d_date_sk JOIN filtered_stores ON sr.sr_store_sk = s_store_sk",
        "customer_total_return": "SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM filtered_returns GROUP BY sr_customer_sk, sr_store_sk",
        "store_avg_return": "SELECT ctr_store_sk, AVG(ctr_total_return) * 1.2 AS avg_return_threshold FROM customer_total_return GROUP BY ctr_store_sk",
        "main_query": "SELECT c.c_customer_id FROM customer_total_return AS ctr1 JOIN store_avg_return AS sar ON ctr1.ctr_store_sk = sar.ctr_store_sk JOIN customer AS c ON ctr1.ctr_customer_sk = c.c_customer_sk WHERE ctr1.ctr_total_return > sar.avg_return_threshold ORDER BY c.c_customer_id LIMIT 100"
      },
      "invariants_kept": [
        "same result rows",
        "same ordering",
        "same column output",
        "same grouping and aggregation",
        "literal values preserved exactly"
      ],
      "expected_speedup": "3.0x",
      "risk": "low"
    }
  ],
  "explanation": "Applied DATE_CTE_ISOLATE pattern: extracted date_dim filter (d_year=2000) and store filter (s_state='SD') into separate CTEs before joining with store_returns. This reduces the store_returns scan from scanning all 34.5M rows to only rows matching the filtered dates and stores. The filtered_dates CTE (73K → 366 rows) and filtered_stores CTE (102 → 35 rows) create small dimension filters that dramatically reduce the fact table scan early in the pipeline."
}
```

**Why this works:**
1. The `filtered_dates` CTE isolates the date_dim filter (`d_year = 2000`) and returns only the relevant `d_date_sk` values (366 rows instead of 73K)
2. The `filtered_stores` CTE isolates the store filter (`s_state = 'SD'`) and returns only relevant `s_store_sk` values (35 rows instead of 102)
3. The `filtered_returns` CTE now joins store_returns with these small filtered dimension tables, reducing the scan from 34.5M rows to only rows matching both filters
4. This is exactly the pattern shown in the DATE_CTE_ISOLATE example where filtering dimensions first dramatically reduces fact table joins

This approach should outperform simple decorrelation because it reduces the expensive store_returns scan early, which accounts for 82.4% of the cost.