## Example: Dsb Self Join Decomposition (DSB_SELF_JOIN_DECOMPOSITION)
Verified speedup: unknown

### Input:
[main_query]:
-- DSB Query 100: items sold together
      SELECT i1.i_item_id, i2.i_item_id, COUNT(*)
      FROM store_sales ss1
      JOIN store_sales ss2 ON ss1.ss_ticket_number = ss2.ss_ticket_number 
                           AND ss1.ss_item_sk < ss2.ss_item_sk
      JOIN item i1 ON ss1.ss_item_sk = i1.i_item_sk
      JOIN item i2 ON ss2.ss_item_sk = i2.i_item_sk
      GROUP BY i1.i_item_id, i2.i_item_id;


**Key insight:** DSB Query 100 pattern: items frequently sold together.

---

## Example: Aggregate Push Below Join (AGGREGATE_PUSH_BELOW_JOIN)
Verified speedup: unknown

### Input:
[main_query]:
SELECT c.region, SUM(o.amount)
      FROM customers c
      JOIN orders o ON c.id = o.customer_id
      GROUP BY c.region;


**Key insight:** Join followed by aggregation where one side contributes only to
      aggregate, not to grouping.

---

## Example: Star Schema Dimension Filter First (STAR_SCHEMA_DIMENSION_FILTER_FIRST)
Verified speedup: unknown

### Input:
[main_query]:
-- May scan fact table first
      SELECT f.*, d1.name, d2.category
      FROM fact_sales f
      JOIN dim_date d1 ON f.date_id = d1.id
      JOIN dim_product d2 ON f.product_id = d2.id
      WHERE d1.year = 2025
        AND d2.category = 'Electronics';


**Key insight:** Star schema queries with selective dimension filters where optimizer
      scans fact table first instead of filtering dimensions.

---

## CONSTRAINTS (Learned from Benchmark Failures)

The following constraints are MANDATORY based on observed failures:

### LITERAL_PRESERVATION [CRITICAL]
CRITICAL: When rewriting SQL, you MUST copy ALL literal values (strings, numbers, dates) EXACTLY from the original query. Do NOT invent, substitute, or 'improve' any filter values. If the original says d_year = 2000, your rewrite MUST say d_year = 2000. If the original says ca_state = 'GA', your rewrite MUST say ca_state = 'GA'. Changing these values will produce WRONG RESULTS and the rewrite will be REJECTED.

### OR_TO_UNION_LIMIT [HIGH]
CAUTION with OR→UNION: Only split OR conditions into UNION ALL when there are ≤3 simple branches AND they have different access patterns. If you have nested ORs (e.g., 3 conditions × 3 values = 9 combinations), DO NOT expand them - keep the original OR structure. DuckDB handles OR predicates efficiently. Over-splitting causes multiple scans of fact tables and severe regressions (0.23x-0.41x observed). When in doubt, preserve the original OR structure.


---

You are an autonomous Query Rewrite Engine. Your goal is to maximize execution speed while strictly preserving semantic invariants.

Output atomic rewrite sets in JSON.

RULES:
- Primary Goal: Maximize execution speed while strictly preserving semantic invariants.
- Allowed Transforms: Use the provided list. If a standard SQL optimization applies that is not listed, label it "semantic_rewrite".
- Atomic Sets: Group dependent changes (e.g., creating a CTE and joining it) into a single rewrite_set.
- Contracts: Output columns, grain, and total result rows must remain invariant.
- Naming: Use descriptive CTE names (e.g., `filtered_returns` vs `cte1`).
- Column Aliasing: Permitted only for aggregations or disambiguation.

ALLOWED TRANSFORMS: pushdown, decorrelate, or_to_union, early_filter, date_cte_isolate, materialize_cte, flatten_subquery, reorder_join, multi_push_predicate, inline_cte, remove_redundant, semantic_rewrite

OUTPUT FORMAT:
```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "transform_name",
      "nodes": {
        "node_id": "new SQL..."
      },
      "invariants_kept": ["list of preserved semantics"],
      "expected_speedup": "2x",
      "risk": "low"
    }
  ],
  "explanation": "what was changed and why"
}
```

## Target Nodes
  [frequent_ss_items] GROUP_BY
  [max_store_sales] GROUP_BY

## Subgraph Slice
[frequent_ss_items] type=cte
```sql
SELECT SUBSTRING(i_item_desc, 1, 30) AS itemdesc, i_item_sk AS item_sk, d_date AS solddate, COUNT(*) AS cnt FROM store_sales, date_dim, item WHERE ss_sold_date_sk = d_date_sk AND ss_item_sk = i_item_sk AND d_year = 1998 AND i_manager_id BETWEEN 81 AND 100 AND i_category IN ('Home', 'Jewelry', 'Music') GROUP BY SUBSTRING(i_item_desc, 1, 30), i_item_sk, d_date HAVING COUNT(*) > 4
```

[main_query] type=main
```sql
SELECT SUM(sales) FROM (SELECT cs_quantity * cs_list_price AS sales FROM catalog_sales, date_dim WHERE d_year = 1998 AND d_moy = 3 AND cs_sold_date_sk = d_date_sk AND cs_item_sk IN (SELECT item_sk FROM frequent_ss_items) AND cs_bill_customer_sk IN (SELECT c_customer_sk FROM best_ss_customer) AND cs_wholesale_cost BETWEEN 26 AND 36 UNION ALL SELECT ws_quantity * ws_list_price AS sales FROM web_sales, date_dim WHERE d_year = 1998 AND d_moy = 3 AND ws_sold_date_sk = d_date_sk AND ws_item_sk IN (SELECT item_sk FROM frequent_ss_items) AND ws_bill_customer_sk IN (SELECT c_customer_sk FROM best_ss_customer) AND ws_wholesale_cost BETWEEN 26 AND 36) AS tmp2 LIMIT 100
```

[max_store_sales] type=cte
```sql
SELECT MAX(csales) AS tpcds_cmax FROM (SELECT c_customer_sk, SUM(ss_quantity * ss_sales_price) AS csales FROM store_sales, customer, date_dim WHERE ss_customer_sk = c_customer_sk AND ss_sold_date_sk = d_date_sk AND d_year = 1998 AND ss_wholesale_cost BETWEEN 26 AND 36 GROUP BY c_customer_sk) AS tmp1
```

[best_ss_customer] type=cte
```sql
SELECT c_customer_sk, SUM(ss_quantity * ss_sales_price) AS ssales FROM store_sales, customer WHERE ss_customer_sk = c_customer_sk AND c_birth_year BETWEEN 1927 AND 1933 GROUP BY c_customer_sk HAVING SUM(ss_quantity * ss_sales_price) > (95 / 100.0) * (SELECT * FROM max_store_sales)
```


## Node Contracts
[frequent_ss_items]:
  output_columns: ['itemdesc', 'item_sk', 'solddate', 'cnt']
  grain: ['SUBSTRING(i_item_desc, 1, 30)', 'i_item_sk', 'd_date']
  required_predicates: ['d_year = 1998', 'ss_sold_date_sk = d_date_sk', 'ss_item_sk = i_item_sk']
[main_query]:
  output_columns: ['SUM(sales)']
  required_predicates: ['cs_sold_date_sk = d_date_sk', 'd_year = 1998', 'd_moy = 3']
[max_store_sales]:
  output_columns: ['tpcds_cmax']
  grain: ['c_customer_sk']
  required_predicates: ['d_year = 1998', 'ss_customer_sk = c_customer_sk', 'ss_sold_date_sk = d_date_sk']
[best_ss_customer]:
  output_columns: ['c_customer_sk', 'ssales']
  grain: ['c_customer_sk']
  required_predicates: ['ss_customer_sk = c_customer_sk']

## Downstream Usage
[frequent_ss_items]: downstream_refs=['item_sk']
[best_ss_customer]: downstream_refs=['c_customer_sk']

## Cost Attribution
No cost data.

## Detected Opportunities
## Knowledge Base Patterns (verified on TPC-DS)
## Detected Optimization Opportunities

1. **QT-OPT-003** - Date CTE Isolation
  Trigger: date_dim joined with d_year/d_qoy/d_month filter, fact table present
  Rewrite: Create CTE: SELECT d_date_sk FROM date_dim WHERE filter, join fact to CTE early
   Matched: date_dim filter with fact table

2. **QT-OPT-007** - Materialize Repeated Subquery
  Trigger: Same subquery pattern appears multiple times in query
  Rewrite: Extract to CTE with MATERIALIZED hint, reference by name
   Matched: Repeated subquery pattern


## Node-Specific Opportunities
PUSHDOWN: [main_query] filters on [best_ss_customer] after GROUP BY
  Fix: Push filter into CTE before aggregation
  Expected: 2x speedup (verified Q93: 2.71x)

PUSHDOWN: [main_query] filters on [best_ss_customer] after GROUP BY
  Fix: Push filter into CTE before aggregation
  Expected: 2x speedup (verified Q93: 2.71x)

PUSHDOWN: [main_query] filters on [frequent_ss_items] after GROUP BY
  Fix: Push filter into CTE before aggregation
  Expected: 2x speedup (verified Q93: 2.71x)

PUSHDOWN: [main_query] filters on [frequent_ss_items] after GROUP BY
  Fix: Push filter into CTE before aggregation
  Expected: 2x speedup (verified Q93: 2.71x)

Now output your rewrite_sets: