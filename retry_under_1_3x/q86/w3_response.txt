```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "date_cte_isolate",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_month_seq BETWEEN 1200 AND 1211",
        "main_query": "SELECT SUM(ws_net_paid) AS total_sum, i_category, i_class, GROUPING(i_category) + GROUPING(i_class) AS lochierarchy, RANK() OVER (PARTITION BY GROUPING(i_category) + GROUPING(i_class), CASE WHEN GROUPING(i_class) = 0 THEN i_category END ORDER BY SUM(ws_net_paid) DESC) AS rank_within_parent FROM web_sales JOIN filtered_dates ON ws_sold_date_sk = d_date_sk JOIN item ON i_item_sk = ws_item_sk GROUP BY ROLLUP (i_category, i_class) ORDER BY lochierarchy DESC NULLS FIRST, CASE WHEN GROUPING(i_category) + GROUPING(i_class) = 0 THEN i_category END NULLS FIRST, rank_within_parent NULLS FIRST LIMIT 100"
      },
      "invariants_kept": [
        "same result rows", 
        "same grouping and ordering",
        "same column output"
      ],
      "expected_speedup": "3.5x",
      "risk": "low"
    }
  ],
  "explanation": "Isolated the date_dim filter into a CTE (filtered_dates) that selects only the 366 matching d_date_sk values. This allows the database to first filter date_dim, then efficiently join with the 86M-row web_sales table using the selective date filter. The original plan scanned all 86M web_sales rows first, causing expensive hash joins with unfiltered dimension tables."
}
```