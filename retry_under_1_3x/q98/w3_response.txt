```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "date_cte_isolate",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1999-02-22' AS DATE) AND CAST('1999-03-24' AS DATE)",
        "filtered_items": "SELECT i_item_sk, i_item_id, i_item_desc, i_category, i_class, i_current_price FROM item WHERE i_category IN ('Sports', 'Books', 'Home')",
        "main_query": "SELECT i_item_id, i_item_desc, i_category, i_class, i_current_price, SUM(ss_ext_sales_price) AS itemrevenue, SUM(ss_ext_sales_price) * 100.0000 / SUM(SUM(ss_ext_sales_price)) OVER (PARTITION BY i_class) AS revenueratio FROM store_sales JOIN filtered_dates ON ss_sold_date_sk = d_date_sk JOIN filtered_items ON ss_item_sk = i_item_sk GROUP BY i_item_id, i_item_desc, i_category, i_class, i_current_price ORDER BY i_category NULLS FIRST, i_class NULLS FIRST, i_item_id NULLS FIRST, i_item_desc NULLS FIRST, revenueratio NULLS FIRST"
      },
      "invariants_kept": [
        "same result rows",
        "same ordering",
        "same column output",
        "same aggregation"
      ],
      "expected_speedup": "3.5x",
      "risk": "low"
    }
  ],
  "explanation": "Isolated the date_dim filter into a CTE (31 rows) and item filter into a CTE (102K rows) before joining with store_sales. This reduces the store_sales scan from 345M rows to only rows matching the date filter (estimated 264K rows). The CTEs are small and can be efficiently hash-joined with store_sales, avoiding scanning the entire fact table before applying dimension filters."
}
```