```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "date_cte_isolate",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_year = 1999 AND d_moy = 11",
        "filtered_items": "SELECT i_item_sk, i_brand_id, i_brand FROM item WHERE i_manager_id = 28",
        "filtered_sales": "SELECT ss_item_sk, ss_ext_sales_price FROM store_sales JOIN filtered_dates ON store_sales.ss_sold_date_sk = filtered_dates.d_date_sk",
        "main_query": "SELECT i.brand_id, i.brand, SUM(s.ss_ext_sales_price) AS ext_price FROM filtered_sales s JOIN filtered_items i ON s.ss_item_sk = i.i_item_sk GROUP BY i.brand_id, i.brand ORDER BY ext_price DESC, i.brand_id LIMIT 100"
      },
      "invariants_kept": [
        "same result rows",
        "same ordering",
        "same column output",
        "same grouping and aggregation"
      ],
      "expected_speedup": "3.50x",
      "risk": "low"
    }
  ],
  "explanation": "Created two CTEs: (1) filtered_dates pre-filters date_dim to only 30 rows (Nov 1999) instead of full table scan, (2) filtered_items pre-filters item to 1,800 rows (manager_id=28) instead of full scan. Then join these pre-filtered CTEs with store_sales, reducing the 345M row store_sales scan to only rows matching the date and item filters. This eliminates two full table scans and reduces the store_sales join cardinality by ~400x."
}
```