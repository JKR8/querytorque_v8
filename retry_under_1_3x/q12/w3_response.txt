```json
{
  "rewrite_sets": [
    {
      "id": "rs_01",
      "transform": "multi_dimension_prefetch",
      "nodes": {
        "filtered_dates": "SELECT d_date_sk FROM date_dim WHERE d_date BETWEEN CAST('1999-02-22' AS DATE) AND CAST('1999-03-24' AS DATE)",
        "filtered_items": "SELECT i_item_sk, i_item_id, i_item_desc, i_category, i_class, i_current_price FROM item WHERE i_category IN ('Sports', 'Books', 'Home')",
        "main_query": "SELECT i_item_id, i_item_desc, i_category, i_class, i_current_price, SUM(ws_ext_sales_price) AS itemrevenue, SUM(ws_ext_sales_price) * 100.0000 / SUM(SUM(ws_ext_sales_price)) OVER (PARTITION BY i_class) AS revenueratio FROM web_sales JOIN filtered_dates ON ws_sold_date_sk = d_date_sk JOIN filtered_items ON ws_item_sk = i_item_sk GROUP BY i_item_id, i_item_desc, i_category, i_class, i_current_price ORDER BY i_category, i_class, i_item_id, i_item_desc, revenueratio LIMIT 100"
      },
      "invariants_kept": [
        "same result rows",
        "same ordering",
        "same column output",
        "same aggregation",
        "same window calculation"
      ],
      "expected_speedup": "2.5x",
      "risk": "low"
    }
  ],
  "explanation": "Pre-filtered both date_dim (31 rows) and item (102k rows) into separate CTEs with only necessary columns. This reduces the fact table join from scanning 86M rows with filters applied during join to joining with pre-materialized, filtered dimension sets. The optimizer can now use smaller hash tables for the joins, significantly reducing the initial data volume before aggregation."
}
```